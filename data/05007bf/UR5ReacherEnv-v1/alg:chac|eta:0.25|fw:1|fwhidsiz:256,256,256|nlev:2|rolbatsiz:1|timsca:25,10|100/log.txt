Starting process id: 90749
T: 100
alg: chac
algorithm: src.chac
atomic_noise: 0.2
base_logdir: data
batch_size: 1024
bind_core: 0
buffer_size: 500
chac_params: {}
early_stop_data_column: test/success_rate
early_stop_threshold: 100.0
env_name: UR5ReacherEnv-v1
eta: 0.25
fw: 1
fw_hidden_size: 256,256,256
fw_lr: 0.001
gamma: 0.99
graph: 1
info: 
make_env: <function prepare_params.<locals>.make_env at 0x7fd62a0ec290>
max_try_idx: 199
mu_hidden_size: 64
mu_lr: 0.001
n_episodes: 100
n_levels: 2
n_pre_episodes: 30
n_test_rollouts: 25
n_train_batches: 40
n_train_rollouts: 100
num_threads: 1
q_hidden_size: 64
q_lr: 0.001
random_action_perc: 0.3
regularization: True
render: 0
rollout_batch_size: 1
subgoal_noise: 0.2
subgoal_test_perc: 0.3
time_scales: 25,10
try_start_idx: 100
use_mpi: False
verbose: False

*** Warning ***
You are running src.chac with just a single MPI worker. This will work, but the HER experiments that we report in Plappert et al. (2018, https://arxiv.org/abs/1802.09464) were obtained with --num_cpu 19. This makes a significant difference and if you are looking to reproduce those results, be aware of this. Please also refer to https://github.com/openai/baselines/issues/314 for further details.
****************

dims: action = 3, subgoal = 6, end_goal = 3
subgoal_bounds: symmetric [6.28318531 6.28318531 6.28318531 4.         4.         4.        ], offset [0. 0. 0. 0. 0. 0.]
Running on CPU ...
Creating a CHAC agent

Hierarchy Level 0 with time scale 25
Actor(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
  (fc4): Linear(in_features=64, out_features=3, bias=True)
)
Critic(
  (fc1): Linear(in_features=15, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
  (fc4): Linear(in_features=64, out_features=1, bias=True)
  (mse_loss): MSELoss()
)
ForwardModel(
  (mlp): Sequential(
    (0): Linear(in_features=9, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=256, bias=True)
    (5): ReLU()
    (6): Linear(in_features=256, out_features=6, bias=True)
    (7): Identity()
  )
  (mse_loss): MSELoss()
)

Hierarchy Level 1 with time scale 10
Actor(
  (fc1): Linear(in_features=9, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
  (fc4): Linear(in_features=64, out_features=6, bias=True)
)
Critic(
  (fc1): Linear(in_features=15, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
  (fc4): Linear(in_features=64, out_features=1, bias=True)
  (mse_loss): MSELoss()
)
ForwardModel(
  (mlp): Sequential(
    (0): Linear(in_features=12, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=256, bias=True)
    (5): ReLU()
    (6): Linear(in_features=256, out_features=6, bias=True)
    (7): Identity()
  )
  (mse_loss): MSELoss()
)
Training epoch 0
Time for epoch 0: 285.91. Rollout time: 83.09, Training time: 202.81
Evaluating epoch 0
Data_dir: data/05007bf/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|rolbatsiz:1|timsca:25,10|100
------------------------------------------------------
| epoch                     | 0                      |
| policy/steps              | 31250.0                |
| test/episodes             | 25.0                   |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -20.852108303730482    |
| test_1/avg_q              | -2.2632174434113006    |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 100.0                  |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -13.552007189887627    |
| train_0/current_q         | -8.460859697219135     |
| train_0/fw_bonus          | -0.9987608894705773    |
| train_0/fw_loss           | 0.037262948835268615   |
| train_0/mu_grads          | -0.02027765568345785   |
| train_0/mu_grads_std      | 0.17299544624984264    |
| train_0/mu_loss           | 8.249270321941605      |
| train_0/next_q            | -8.222965593329805     |
| train_0/q_grads           | -0.0016758031997596845 |
| train_0/q_grads_std       | 0.11928730625659227    |
| train_0/q_loss            | 0.9446744628081103     |
| train_0/reward            | -0.9164839920631493    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.02890625             |
| train_0/target_q          | -8.536148397713347     |
| train_1/avg_q             | -1.954016177264622     |
| train_1/current_q         | -2.5676955271528223    |
| train_1/fw_bonus          | -0.9998044520616531    |
| train_1/fw_loss           | 0.0008764324331423268  |
| train_1/mu_grads          | -0.02540176738984883   |
| train_1/mu_grads_std      | 0.18446896821260453    |
| train_1/mu_loss           | 1.7795210785712274     |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -1.7533646373158895    |
| train_1/q_grads           | 0.01568695467431098    |
| train_1/q_grads_std       | 0.11547720320522785    |
| train_1/q_loss            | 0.20226431894597696    |
| train_1/reward            | -1.1909116932045436    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0222412109375        |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -2.5667349201325522    |
------------------------------------------------------
Saving periodic policy to data/05007bf/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|rolbatsiz:1|timsca:25,10|100/policy_0.pkl ...
New best value for test/success_rate: 0.0. Saving policy to data/05007bf/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|rolbatsiz:1|timsca:25,10|100/policy_best.pkl ...
Training epoch 1
Time for epoch 1: 217.08. Rollout time: 78.39, Training time: 138.68
Evaluating epoch 1
Data_dir: data/05007bf/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|rolbatsiz:1|timsca:25,10|100
-----------------------------------------------------
| epoch                     | 1                     |
| policy/steps              | 62447.0               |
| test/episodes             | 50.0                  |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -20.962136024383238   |
| test_1/avg_q              | -2.1611282916090717   |
| test_1/n_subgoals         | 250.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 200.0                 |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -21.020287972646027   |
| train_0/current_q         | -8.977627579092694    |
| train_0/fw_bonus          | -0.9991472944617271   |
| train_0/fw_loss           | 0.02564355656504631   |
| train_0/mu_grads          | -0.029049756564199926 |
| train_0/mu_grads_std      | 0.21828569769859313   |
| train_0/mu_loss           | 8.756902883967696     |
| train_0/next_q            | -8.691126675243092    |
| train_0/q_grads           | -0.006878839153796434 |
| train_0/q_grads_std       | 0.13729985170066356   |
| train_0/q_loss            | 0.8162814186999711    |
| train_0/reward            | -0.9253065116688959   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0259765625          |
| train_0/target_q          | -9.041272515264874    |
| train_1/avg_q             | -2.850224096969073    |
| train_1/current_q         | -2.6870994033337774   |
| train_1/fw_bonus          | -0.9990398645401001   |
| train_1/fw_loss           | 0.002409711241489276  |
| train_1/mu_grads          | -0.0336932604201138   |
| train_1/mu_grads_std      | 0.22233715020120143   |
| train_1/mu_loss           | 1.9372538109240467    |
| train_1/n_subgoals        | 1000.0                |
| train_1/next_q            | -1.919204095129036    |
| train_1/q_grads           | 0.009985662112012506  |
| train_1/q_grads_std       | 0.1367058042436838    |
| train_1/q_loss            | 0.29333735901111063   |
| train_1/reward            | -1.175177115801489    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0266845703125       |
| train_1/reward_-10.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.004                 |
| train_1/target_q          | -2.713294196196967    |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/05007bf/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|rolbatsiz:1|timsca:25,10|100/policy_best.pkl ...
Training epoch 2
Time for epoch 2: 219.50. Rollout time: 75.46, Training time: 144.04
Evaluating epoch 2
Data_dir: data/05007bf/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|rolbatsiz:1|timsca:25,10|100
-----------------------------------------------------
| epoch                     | 2                     |
| policy/steps              | 93678.0               |
| test/episodes             | 75.0                  |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -20.29343906151491    |
| test_1/avg_q              | -1.3941508675591234   |
| test_1/n_subgoals         | 250.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 300.0                 |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -21.438944265817394   |
| train_0/current_q         | -9.689930436229705    |
| train_0/fw_bonus          | -0.9991994023323059   |
| train_0/fw_loss           | 0.02407625801861286   |
| train_0/mu_grads          | -0.03665833380073309  |
| train_0/mu_grads_std      | 0.2546000063419342    |
| train_0/mu_loss           | 9.411429169099497     |
| train_0/next_q            | -9.352512181081744    |
| train_0/q_grads           | -0.006356067780870944 |
| train_0/q_grads_std       | 0.1594216987490654    |
| train_0/q_loss            | 0.6598279343989599    |
| train_0/reward            | -0.9380409145174781   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0178466796875       |
| train_0/target_q          | -9.788566518155434    |
| train_1/avg_q             | -2.910875392971695    |
| train_1/current_q         | -1.8723262628181208   |
| train_1/fw_bonus          | -0.9973714426159859   |
| train_1/fw_loss           | 0.0057554977713152764 |
| train_1/mu_grads          | -0.034737242572009566 |
| train_1/mu_grads_std      | 0.21681939773261547   |
| train_1/mu_loss           | 0.8536731067849967    |
| train_1/n_subgoals        | 1000.0                |
| train_1/next_q            | -0.8772887273671979   |
| train_1/q_grads           | -0.016073005553334952 |
| train_1/q_grads_std       | 0.15828028991818427   |
| train_1/q_loss            | 0.148750889442627     |
| train_1/reward            | -1.180206311987422    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0171142578125       |
| train_1/reward_-10.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.002                 |
| train_1/target_q          | -1.8814188788709543   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/05007bf/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|rolbatsiz:1|timsca:25,10|100/policy_best.pkl ...
Training epoch 3
Time for epoch 3: 229.22. Rollout time: 75.09, Training time: 154.12
Evaluating epoch 3
Data_dir: data/05007bf/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|rolbatsiz:1|timsca:25,10|100
------------------------------------------------------
| epoch                     | 3                      |
| policy/steps              | 124829.0               |
| test/episodes             | 100.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -21.715066200297283    |
| test_1/avg_q              | -1.0864916876263497    |
| test_1/n_subgoals         | 256.0                  |
| test_1/subgoal_succ_rate  | 0.0234375              |
| train/episodes            | 400.0                  |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -21.411730182123552    |
| train_0/current_q         | -9.855413691809094     |
| train_0/fw_bonus          | -0.9992178797721862    |
| train_0/fw_loss           | 0.02352069285698235    |
| train_0/mu_grads          | -0.0415041234344244    |
| train_0/mu_grads_std      | 0.2810829132795334     |
| train_0/mu_loss           | 9.61527126076569       |
| train_0/next_q            | -9.482325200668399     |
| train_0/q_grads           | -0.0007609749576658942 |
| train_0/q_grads_std       | 0.18000963181257248    |
| train_0/q_loss            | 0.6852935398662505     |
| train_0/reward            | -0.9451226424556808    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.0173583984375        |
| train_0/target_q          | -9.902003291008914     |
| train_1/avg_q             | -2.9056072159045843    |
| train_1/current_q         | -1.8996327318354318    |
| train_1/fw_bonus          | -0.9956265389919281    |
| train_1/fw_loss           | 0.009254659351427108   |
| train_1/mu_grads          | -0.03223285637795925   |
| train_1/mu_grads_std      | 0.21802560612559319    |
| train_1/mu_loss           | 0.8805607430676801     |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -0.8993205394714409    |
| train_1/q_grads           | -0.028660801984369755  |
| train_1/q_grads_std       | 0.1759309984743595     |
| train_1/q_loss            | 0.06554359367166747    |
| train_1/reward            | -1.1803563697321806    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0128662109375        |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.013                  |
| train_1/target_q          | -1.9057776449837605    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/05007bf/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|rolbatsiz:1|timsca:25,10|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 4
Time for epoch 4: 5208.27. Rollout time: 1894.53, Training time: 3313.72
Evaluating epoch 4
Data_dir: data/05007bf/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|rolbatsiz:1|timsca:25,10|100
------------------------------------------------------
| epoch                     | 4                      |
| policy/steps              | 155840.0               |
| test/episodes             | 125.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -15.393666351208878    |
| test_1/avg_q              | -0.6935006117961655    |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 500.0                  |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -21.56711307403116     |
| train_0/current_q         | -10.321833926700549    |
| train_0/fw_bonus          | -0.9994020253419876    |
| train_0/fw_loss           | 0.01798338829539716    |
| train_0/mu_grads          | -0.048845740780234334  |
| train_0/mu_grads_std      | 0.30093863755464556    |
| train_0/mu_loss           | 10.00826205144696      |
| train_0/next_q            | -9.93601241266953      |
| train_0/q_grads           | -0.0014491223671939224 |
| train_0/q_grads_std       | 0.1907879773527384     |
| train_0/q_loss            | 0.598537947287683      |
| train_0/reward            | -0.9520844813465373    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.0255859375           |
| train_0/target_q          | -10.404740803693949    |
| train_1/avg_q             | -2.73603984730277      |
| train_1/current_q         | -1.7684436155263277    |
| train_1/fw_bonus          | -0.9948754623532295    |
| train_1/fw_loss           | 0.010760807886254043   |
| train_1/mu_grads          | -0.032256408222019674  |
| train_1/mu_grads_std      | 0.22777604572474958    |
| train_1/mu_loss           | 0.7045367140392385     |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -0.7257607053578526    |
| train_1/q_grads           | -0.03392416946589947   |
| train_1/q_grads_std       | 0.20584998577833175    |
| train_1/q_loss            | 0.060641244215141034   |
| train_1/reward            | -1.1865313524278462    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0072265625           |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.023                  |
| train_1/target_q          | -1.769708992200988     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/05007bf/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|rolbatsiz:1|timsca:25,10|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 5
Time for epoch 5: 233.56. Rollout time: 74.53, Training time: 159.02
Evaluating epoch 5
Data_dir: data/05007bf/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|rolbatsiz:1|timsca:25,10|100
------------------------------------------------------
| epoch                     | 5                      |
| policy/steps              | 186716.0               |
| test/episodes             | 150.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -20.747877112350857    |
| test_1/avg_q              | -0.918816470708551     |
| test_1/n_subgoals         | 263.0                  |
| test_1/subgoal_succ_rate  | 0.053231939163498096   |
| train/episodes            | 600.0                  |
| train/success_rate        | 0.01                   |
| train_0/avg_q             | -20.679367577855686    |
| train_0/current_q         | -10.131776595081107    |
| train_0/fw_bonus          | -0.9994747385382652    |
| train_0/fw_loss           | 0.015797098632901907   |
| train_0/mu_grads          | -0.05329954847693443   |
| train_0/mu_grads_std      | 0.3207655280828476     |
| train_0/mu_loss           | 9.801737210362656      |
| train_0/next_q            | -9.70164109267124      |
| train_0/q_grads           | 0.00039448951647500505 |
| train_0/q_grads_std       | 0.20226999782025815    |
| train_0/q_loss            | 0.4940300246644006     |
| train_0/reward            | -0.9569058563211001    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.022705078125         |
| train_0/target_q          | -10.222134257333334    |
| train_1/avg_q             | -2.900686814613325     |
| train_1/current_q         | -1.8379117493484487    |
| train_1/fw_bonus          | -0.9928553462028503    |
| train_1/fw_loss           | 0.014811835694126784   |
| train_1/mu_grads          | -0.03411498023197055   |
| train_1/mu_grads_std      | 0.2396444793790579     |
| train_1/mu_loss           | 0.8145576249947354     |
| train_1/n_subgoals        | 991.0                  |
| train_1/next_q            | -0.8209298770804242    |
| train_1/q_grads           | -0.0400476953946054    |
| train_1/q_grads_std       | 0.2328186087310314     |
| train_1/q_loss            | 0.051501276360900836   |
| train_1/reward            | -1.1808231570503267    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.00458984375          |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.016145307769929364   |
| train_1/target_q          | -1.8380692068292888    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/05007bf/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|rolbatsiz:1|timsca:25,10|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 6
Time for epoch 6: 224.72. Rollout time: 75.46, Training time: 149.25
Evaluating epoch 6
Data_dir: data/05007bf/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|rolbatsiz:1|timsca:25,10|100
------------------------------------------------------
| epoch                     | 6                      |
| policy/steps              | 217513.0               |
| test/episodes             | 175.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -21.160270671988698    |
| test_1/avg_q              | -1.9772546146770176    |
| test_1/n_subgoals         | 266.0                  |
| test_1/subgoal_succ_rate  | 0.07142857142857142    |
| train/episodes            | 700.0                  |
| train/success_rate        | 0.01                   |
| train_0/avg_q             | -21.53700334540872     |
| train_0/current_q         | -10.805809815808756    |
| train_0/fw_bonus          | -0.9994757264852524    |
| train_0/fw_loss           | 0.01576714471448213    |
| train_0/mu_grads          | -0.06522769015282393   |
| train_0/mu_grads_std      | 0.34055797681212424    |
| train_0/mu_loss           | 10.523657252218948     |
| train_0/next_q            | -10.399688904816571    |
| train_0/q_grads           | 0.00031472838982153916 |
| train_0/q_grads_std       | 0.21008460521697997    |
| train_0/q_loss            | 0.568130180375023      |
| train_0/reward            | -0.9602208124648314    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.0326171875           |
| train_0/target_q          | -10.896581846549278    |
| train_1/avg_q             | -2.9303146841125782    |
| train_1/current_q         | -2.06657222706284      |
| train_1/fw_bonus          | -0.9911071762442589    |
| train_1/fw_loss           | 0.018317534658126532   |
| train_1/mu_grads          | -0.034575813077390195  |
| train_1/mu_grads_std      | 0.25311894416809083    |
| train_1/mu_loss           | 1.0912382692686837     |
| train_1/n_subgoals        | 991.0                  |
| train_1/next_q            | -1.1092780740491814    |
| train_1/q_grads           | -0.04902733508497477   |
| train_1/q_grads_std       | 0.25880548283457755    |
| train_1/q_loss            | 0.03698149015451852    |
| train_1/reward            | -1.1812088058555674    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.00244140625          |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.029263370332996974   |
| train_1/target_q          | -2.064611131215198     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/05007bf/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|rolbatsiz:1|timsca:25,10|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 7
Time for epoch 7: 211.78. Rollout time: 75.49, Training time: 136.28
Evaluating epoch 7
Data_dir: data/05007bf/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|rolbatsiz:1|timsca:25,10|100
-----------------------------------------------------
| epoch                     | 7                     |
| policy/steps              | 248633.0              |
| test/episodes             | 200.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -21.6013329231428     |
| test_1/avg_q              | -1.6828820376851705   |
| test_1/n_subgoals         | 258.0                 |
| test_1/subgoal_succ_rate  | 0.03488372093023256   |
| train/episodes            | 800.0                 |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -22.297087317046465   |
| train_0/current_q         | -11.037200947269877   |
| train_0/fw_bonus          | -0.9994219079613685   |
| train_0/fw_loss           | 0.01738575447816402   |
| train_0/mu_grads          | -0.06793259922415018  |
| train_0/mu_grads_std      | 0.3630417473614216    |
| train_0/mu_loss           | 10.724270480953242    |
| train_0/next_q            | -10.616962679500812   |
| train_0/q_grads           | 0.0009291561727877706 |
| train_0/q_grads_std       | 0.21946142427623272   |
| train_0/q_loss            | 0.4967253192842911    |
| train_0/reward            | -0.9630185766669456   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0225830078125       |
| train_0/target_q          | -11.137633926494908   |
| train_1/avg_q             | -2.970619158754094    |
| train_1/current_q         | -2.1245478377948666   |
| train_1/fw_bonus          | -0.9917843922972679   |
| train_1/fw_loss           | 0.01695949980057776   |
| train_1/mu_grads          | -0.03913620291277766  |
| train_1/mu_grads_std      | 0.2671169966459274    |
| train_1/mu_loss           | 1.1643312147590366    |
| train_1/n_subgoals        | 1000.0                |
| train_1/next_q            | -1.183669387087778    |
| train_1/q_grads           | -0.05904186414554715  |
| train_1/q_grads_std       | 0.2767663337290287    |
| train_1/q_loss            | 0.052011474456964704  |
| train_1/reward            | -1.1841337018930063   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0020751953125       |
| train_1/reward_-10.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.021                 |
| train_1/target_q          | -2.1266209123312025   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/05007bf/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|rolbatsiz:1|timsca:25,10|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 8
Time for epoch 8: 226.32. Rollout time: 76.73, Training time: 149.59
Evaluating epoch 8
Data_dir: data/05007bf/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|rolbatsiz:1|timsca:25,10|100
-----------------------------------------------------
| epoch                     | 8                     |
| policy/steps              | 279219.0              |
| test/episodes             | 225.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -21.923376776029897   |
| test_1/avg_q              | -1.2940323056283443   |
| test_1/n_subgoals         | 269.0                 |
| test_1/subgoal_succ_rate  | 0.07806691449814127   |
| train/episodes            | 900.0                 |
| train/success_rate        | 0.02                  |
| train_0/avg_q             | -22.294906211027115   |
| train_0/current_q         | -11.017775689471174   |
| train_0/fw_bonus          | -0.9994279354810714   |
| train_0/fw_loss           | 0.01720415907911956   |
| train_0/mu_grads          | -0.07394050769507884  |
| train_0/mu_grads_std      | 0.37990112155675887   |
| train_0/mu_loss           | 10.726589724608662    |
| train_0/next_q            | -10.615713718703697   |
| train_0/q_grads           | 0.0008927488379413262 |
| train_0/q_grads_std       | 0.2284933567047119    |
| train_0/q_loss            | 0.5828410862800611    |
| train_0/reward            | -0.9621564066270366   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0205322265625       |
| train_0/target_q          | -11.106331612890747   |
| train_1/avg_q             | -2.940851278198099    |
| train_1/current_q         | -2.169390836232582    |
| train_1/fw_bonus          | -0.9929186686873436   |
| train_1/fw_loss           | 0.014684886089526117  |
| train_1/mu_grads          | -0.04341253461316228  |
| train_1/mu_grads_std      | 0.28001019805669786   |
| train_1/mu_loss           | 1.2170174760563355    |
| train_1/n_subgoals        | 986.0                 |
| train_1/next_q            | -1.2422124944904902   |
| train_1/q_grads           | -0.06544222068041564  |
| train_1/q_grads_std       | 0.30214789882302284   |
| train_1/q_loss            | 0.2869971011995201    |
| train_1/reward            | -1.1698598934875917   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0013916015625       |
| train_1/reward_-10.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.04056795131845842   |
| train_1/target_q          | -2.1822373931546992   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/05007bf/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|rolbatsiz:1|timsca:25,10|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 9
Time for epoch 9: 233.54. Rollout time: 81.19, Training time: 152.33
Evaluating epoch 9
Data_dir: data/05007bf/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|rolbatsiz:1|timsca:25,10|100
------------------------------------------------------
| epoch                     | 9                      |
| policy/steps              | 309987.0               |
| test/episodes             | 250.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -22.490219655179345    |
| test_1/avg_q              | -2.2728698280124378    |
| test_1/n_subgoals         | 278.0                  |
| test_1/subgoal_succ_rate  | 0.1223021582733813     |
| train/episodes            | 1000.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -22.14792852459121     |
| train_0/current_q         | -11.519443523409803    |
| train_0/fw_bonus          | -0.99950662702322      |
| train_0/fw_loss           | 0.014838070864789188   |
| train_0/mu_grads          | -0.08143288679420949   |
| train_0/mu_grads_std      | 0.3998511105775833     |
| train_0/mu_loss           | 11.252138376998127     |
| train_0/next_q            | -11.14205009695109     |
| train_0/q_grads           | 0.00040441299788653853 |
| train_0/q_grads_std       | 0.23638904169201852    |
| train_0/q_loss            | 0.636241050759936      |
| train_0/reward            | -0.9630027739505749    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.0534912109375        |
| train_0/target_q          | -11.62205366181924     |
| train_1/avg_q             | -2.7830411266938073    |
| train_1/current_q         | -2.0559059939824107    |
| train_1/fw_bonus          | -0.9933287352323532    |
| train_1/fw_loss           | 0.013862544693984092   |
| train_1/mu_grads          | -0.045948174223303793  |
| train_1/mu_grads_std      | 0.28413415774703027    |
| train_1/mu_loss           | 1.106809077381806      |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -1.112546966476091     |
| train_1/q_grads           | -0.07206203397363424   |
| train_1/q_grads_std       | 0.3212926536798477     |
| train_1/q_loss            | 0.037408091875336245   |
| train_1/reward            | -1.173634245400899     |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0017822265625        |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.056                  |
| train_1/target_q          | -2.065118154264579     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/05007bf/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|rolbatsiz:1|timsca:25,10|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 10
Time for epoch 10: 235.86. Rollout time: 81.18, Training time: 154.67
Evaluating epoch 10
Data_dir: data/05007bf/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|rolbatsiz:1|timsca:25,10|100
-----------------------------------------------------
| epoch                     | 10                    |
| policy/steps              | 340826.0              |
| test/episodes             | 275.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -21.930944920923114   |
| test_1/avg_q              | -1.4120525339471242   |
| test_1/n_subgoals         | 260.0                 |
| test_1/subgoal_succ_rate  | 0.06153846153846154   |
| train/episodes            | 1100.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -22.615356718237358   |
| train_0/current_q         | -11.731639570222772   |
| train_0/fw_bonus          | -0.99951042085886     |
| train_0/fw_loss           | 0.014724023919552565  |
| train_0/mu_grads          | -0.08657920341938734  |
| train_0/mu_grads_std      | 0.4174599565565586    |
| train_0/mu_loss           | 11.491896113962913    |
| train_0/next_q            | -11.376269542173802   |
| train_0/q_grads           | 0.0015710037085227668 |
| train_0/q_grads_std       | 0.24638027213513852   |
| train_0/q_loss            | 0.7206290204048285    |
| train_0/reward            | -0.9629018588195322   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0372802734375       |
| train_0/target_q          | -11.840560906274131   |
| train_1/avg_q             | -3.02506497988457     |
| train_1/current_q         | -1.8290087659166812   |
| train_1/fw_bonus          | -0.9950268343091011   |
| train_1/fw_loss           | 0.010457240173127502  |
| train_1/mu_grads          | -0.04878189843147993  |
| train_1/mu_grads_std      | 0.28839573860168455   |
| train_1/mu_loss           | 0.8199296683328899    |
| train_1/n_subgoals        | 1000.0                |
| train_1/next_q            | -0.7980399236051149   |
| train_1/q_grads           | -0.08494510799646378  |
| train_1/q_grads_std       | 0.32746000960469246   |
| train_1/q_loss            | 0.0567684090310589    |
| train_1/reward            | -1.179186390353425    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0035400390625       |
| train_1/reward_-10.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.054                 |
| train_1/target_q          | -1.8260468576882816   |
-----------------------------------------------------
Saving periodic policy to data/05007bf/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|rolbatsiz:1|timsca:25,10|100/policy_10.pkl ...
New best value for test/success_rate: 0.0. Saving policy to data/05007bf/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|rolbatsiz:1|timsca:25,10|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 11
Time for epoch 11: 237.03. Rollout time: 79.10, Training time: 157.93
Evaluating epoch 11
Data_dir: data/05007bf/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|rolbatsiz:1|timsca:25,10|100
-----------------------------------------------------
| epoch                     | 11                    |
| policy/steps              | 371710.0              |
| test/episodes             | 300.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -21.740237073240372   |
| test_1/avg_q              | -0.9560828471413525   |
| test_1/n_subgoals         | 265.0                 |
| test_1/subgoal_succ_rate  | 0.0830188679245283    |
| train/episodes            | 1200.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -22.604137957997025   |
| train_0/current_q         | -11.518077026149774   |
| train_0/fw_bonus          | -0.9993825659155846   |
| train_0/fw_loss           | 0.018569024745374918  |
| train_0/mu_grads          | -0.09270699489861726  |
| train_0/mu_grads_std      | 0.4308054484426975    |
| train_0/mu_loss           | 11.24965787368104     |
| train_0/next_q            | -11.147231208744643   |
| train_0/q_grads           | 0.0017633937357459217 |
| train_0/q_grads_std       | 0.2528138071298599    |
| train_0/q_loss            | 0.6906253147515988    |
| train_0/reward            | -0.9623481950286077   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.023291015625        |
| train_0/target_q          | -11.626993093683627   |
| train_1/avg_q             | -2.8542352486014284   |
| train_1/current_q         | -1.9593948986177891   |
| train_1/fw_bonus          | -0.9940905332565307   |
| train_1/fw_loss           | 0.012334876437671482  |
| train_1/mu_grads          | -0.05110980533063412  |
| train_1/mu_grads_std      | 0.29591466039419173   |
| train_1/mu_loss           | 0.9853385787175248    |
| train_1/n_subgoals        | 1000.0                |
| train_1/next_q            | -0.9396023226704209   |
| train_1/q_grads           | -0.09177537355571985  |
| train_1/q_grads_std       | 0.3398785084486008    |
| train_1/q_loss            | 0.12439992570040834   |
| train_1/reward            | -1.1787160900683376   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.003515625           |
| train_1/reward_-10.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.048                 |
| train_1/target_q          | -1.9575101257614507   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/05007bf/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|rolbatsiz:1|timsca:25,10|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 12
Time for epoch 12: 236.16. Rollout time: 77.93, Training time: 158.22
Evaluating epoch 12
Data_dir: data/05007bf/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|rolbatsiz:1|timsca:25,10|100
-----------------------------------------------------
| epoch                     | 12                    |
| policy/steps              | 402594.0              |
| test/episodes             | 325.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -21.85828436779291    |
| test_1/avg_q              | -1.8182809512182356   |
| test_1/n_subgoals         | 275.0                 |
| test_1/subgoal_succ_rate  | 0.10909090909090909   |
| train/episodes            | 1300.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -22.363651724541466   |
| train_0/current_q         | -11.279855163845124   |
| train_0/fw_bonus          | -0.9995022803544998   |
| train_0/fw_loss           | 0.014968829206191003  |
| train_0/mu_grads          | -0.09390103872865438  |
| train_0/mu_grads_std      | 0.44009889513254163   |
| train_0/mu_loss           | 11.003876098546362    |
| train_0/next_q            | -10.87511768470508    |
| train_0/q_grads           | 0.0009406618075445295 |
| train_0/q_grads_std       | 0.2589753806591034    |
| train_0/q_loss            | 0.6186958859624674    |
| train_0/reward            | -0.9651784677727846   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0378173828125       |
| train_0/target_q          | -11.381523034172059   |
| train_1/avg_q             | -2.911707063325094    |
| train_1/current_q         | -2.0401759547985763   |
| train_1/fw_bonus          | -0.9961930274963379   |
| train_1/fw_loss           | 0.008118623401969671  |
| train_1/mu_grads          | -0.050229508243501184 |
| train_1/mu_grads_std      | 0.29633193612098696   |
| train_1/mu_loss           | 1.0588380093514633    |
| train_1/n_subgoals        | 1000.0                |
| train_1/next_q            | -1.0897461880695887   |
| train_1/q_grads           | -0.0921647286042571   |
| train_1/q_grads_std       | 0.3486017227172852    |
| train_1/q_loss            | 0.03325487189537089   |
| train_1/reward            | -1.1654675251964364   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.005224609375        |
| train_1/reward_-10.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.047                 |
| train_1/target_q          | -2.0399188788942695   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/05007bf/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|rolbatsiz:1|timsca:25,10|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 13
Time for epoch 13: 238.33. Rollout time: 81.89, Training time: 156.43
Evaluating epoch 13
Data_dir: data/05007bf/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|rolbatsiz:1|timsca:25,10|100
------------------------------------------------------
| epoch                     | 13                     |
| policy/steps              | 433190.0               |
| test/episodes             | 350.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -21.69106514309572     |
| test_1/avg_q              | -1.378301444387552     |
| test_1/n_subgoals         | 264.0                  |
| test_1/subgoal_succ_rate  | 0.056818181818181816   |
| train/episodes            | 1400.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -22.36890530790384     |
| train_0/current_q         | -11.862753427534598    |
| train_0/fw_bonus          | -0.9994613975286484    |
| train_0/fw_loss           | 0.016198236611671746   |
| train_0/mu_grads          | -0.09609015621244907   |
| train_0/mu_grads_std      | 0.4465926721692085     |
| train_0/mu_loss           | 11.598172566598157     |
| train_0/next_q            | -11.469834867519996    |
| train_0/q_grads           | 0.00020845336057391252 |
| train_0/q_grads_std       | 0.26284986436367036    |
| train_0/q_loss            | 0.6883265902146574     |
| train_0/reward            | -0.9694080584187759    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.04072265625          |
| train_0/target_q          | -11.97220281092056     |
| train_1/avg_q             | -2.916805776528362     |
| train_1/current_q         | -1.96470055962725      |
| train_1/fw_bonus          | -0.9952864214777947    |
| train_1/fw_loss           | 0.009936682181432843   |
| train_1/mu_grads          | -0.052829400077462195  |
| train_1/mu_grads_std      | 0.3030470199882984     |
| train_1/mu_loss           | 0.9572807736573381     |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -0.9817229190018516    |
| train_1/q_grads           | -0.0949720835313201    |
| train_1/q_grads_std       | 0.3590498946607113     |
| train_1/q_loss            | 0.02474673044492499    |
| train_1/reward            | -1.17249631309096      |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.004150390625         |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.069                  |
| train_1/target_q          | -1.9639481764987068    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/05007bf/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|rolbatsiz:1|timsca:25,10|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 14
