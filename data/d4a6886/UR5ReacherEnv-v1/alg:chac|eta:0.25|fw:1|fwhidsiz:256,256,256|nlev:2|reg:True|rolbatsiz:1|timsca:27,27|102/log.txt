Starting process id: 50118
T: 100
alg: chac
algorithm: src.chac
atomic_noise: 0.2
base_logdir: data
batch_size: 1024
bind_core: 0
buffer_size: 500
chac_params: {}
early_stop_data_column: test/success_rate
early_stop_threshold: 100.0
env_name: UR5ReacherEnv-v1
eta: 0.25
fw: 1
fw_hidden_size: 256,256,256
fw_lr: 0.001
gamma: 0.99
graph: 1
info: 
make_env: <function prepare_params.<locals>.make_env at 0x7fd568b8bb90>
max_try_idx: 199
mu_hidden_size: 64
mu_lr: 0.001
n_episodes: 100
n_levels: 2
n_pre_episodes: 30
n_test_rollouts: 25
n_train_batches: 40
n_train_rollouts: 100
num_threads: 1
q_hidden_size: 64
q_lr: 0.001
random_action_perc: 0.3
regularization: True
render: 0
rollout_batch_size: 1
subgoal_noise: 0.2
subgoal_test_perc: 0.3
time_scales: 27,27
try_start_idx: 100
use_mpi: False
verbose: False

*** Warning ***
You are running src.chac with just a single MPI worker. This will work, but the HER experiments that we report in Plappert et al. (2018, https://arxiv.org/abs/1802.09464) were obtained with --num_cpu 19. This makes a significant difference and if you are looking to reproduce those results, be aware of this. Please also refer to https://github.com/openai/baselines/issues/314 for further details.
****************

dims: action = 3, subgoal = 6, end_goal = 3
subgoal_bounds: symmetric [6.28318531 6.28318531 6.28318531 4.         4.         4.        ], offset [0. 0. 0. 0. 0. 0.]
Running on CPU ...
Creating a CHAC agent

Hierarchy Level 0 with time scale 27
Actor(
  (fc1): Linear(in_features=12, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
  (fc4): Linear(in_features=64, out_features=3, bias=True)
)
Critic(
  (fc1): Linear(in_features=15, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
  (fc4): Linear(in_features=64, out_features=1, bias=True)
  (mse_loss): MSELoss()
)
ForwardModel(
  (mlp): Sequential(
    (0): Linear(in_features=9, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=256, bias=True)
    (5): ReLU()
    (6): Linear(in_features=256, out_features=6, bias=True)
    (7): Identity()
  )
  (mse_loss): MSELoss()
)

Hierarchy Level 1 with time scale 27
Actor(
  (fc1): Linear(in_features=9, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
  (fc4): Linear(in_features=64, out_features=6, bias=True)
)
Critic(
  (fc1): Linear(in_features=15, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
  (fc4): Linear(in_features=64, out_features=1, bias=True)
  (mse_loss): MSELoss()
)
ForwardModel(
  (mlp): Sequential(
    (0): Linear(in_features=12, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=256, bias=True)
    (5): ReLU()
    (6): Linear(in_features=256, out_features=6, bias=True)
    (7): Identity()
  )
  (mse_loss): MSELoss()
)
Training epoch 0
Time for epoch 0: 327.44. Rollout time: 100.33, Training time: 227.08
Evaluating epoch 0
Data_dir: data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102
------------------------------------------------------
| epoch                     | 0                      |
| policy/steps              | 90489.0                |
| test/episodes             | 25.0                   |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -27.0                  |
| test_1/avg_q              | -6.326706633185527     |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 100.0                  |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -16.45503022123032     |
| train_0/current_q         | -7.666125567177656     |
| train_0/fw_bonus          | -0.9987393513321876    |
| train_0/fw_loss           | 0.005818259634543211   |
| train_0/mu_grads          | -0.009463810943998396  |
| train_0/mu_grads_std      | 0.16785626597702502    |
| train_0/mu_loss           | 7.514767311994595      |
| train_0/next_q            | -7.528874807136001     |
| train_0/q_grads           | 0.007565098872873932   |
| train_0/q_grads_std       | 0.13257980197668076    |
| train_0/q_loss            | 0.3077557940060382     |
| train_0/reward            | -0.877045233809622     |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.02109375             |
| train_0/target_q          | -7.966754943308516     |
| train_1/avg_q             | -5.017789400887019     |
| train_1/current_q         | -5.412777007637441     |
| train_1/fw_bonus          | -0.9999421194195748    |
| train_1/fw_loss           | 0.00019900176412193104 |
| train_1/mu_grads          | 0.01664830599911511    |
| train_1/mu_grads_std      | 0.21472565606236457    |
| train_1/mu_loss           | 5.661319479096159      |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -4.676600626400441     |
| train_1/q_grads           | 0.01691357367672026    |
| train_1/q_grads_std       | 0.1091374684125185     |
| train_1/q_loss            | 0.6973194783086321     |
| train_1/reward            | -1.5581780270746095    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.3935302734375        |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.011851851851851851   |
| train_1/target_q          | -5.411053987782092     |
------------------------------------------------------
Saving periodic policy to data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102/policy_0.pkl ...
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102/policy_best.pkl ...
Training epoch 1
Time for epoch 1: 266.86. Rollout time: 98.03, Training time: 168.77
Evaluating epoch 1
Data_dir: data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102
------------------------------------------------------
| epoch                     | 1                      |
| policy/steps              | 181614.0               |
| test/episodes             | 50.0                   |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -26.999999999994444    |
| test_1/avg_q              | -26.999985004829004    |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 200.0                  |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -26.99999999999123     |
| train_0/current_q         | -8.303263983025069     |
| train_0/fw_bonus          | -0.9991758406162262    |
| train_0/fw_loss           | 0.0038042694970499722  |
| train_0/mu_grads          | -0.011879257066175342  |
| train_0/mu_grads_std      | 0.1869585584849119     |
| train_0/mu_loss           | 8.16421715478292       |
| train_0/next_q            | -8.1844110572283       |
| train_0/q_grads           | 0.0002838531308952952  |
| train_0/q_grads_std       | 0.13352326229214667    |
| train_0/q_loss            | 0.30366542291620136    |
| train_0/reward            | -0.8762190359382658    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.0678955078125        |
| train_0/target_q          | -8.59237315582421      |
| train_1/avg_q             | -17.735267373238734    |
| train_1/current_q         | -26.999884229766884    |
| train_1/fw_bonus          | -0.9999370351433754    |
| train_1/fw_loss           | 0.00021428493146231632 |
| train_1/mu_grads          | 0.01252285682130605    |
| train_1/mu_grads_std      | 0.2200022242963314     |
| train_1/mu_loss           | 27.999993368432552     |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -26.999993196066203    |
| train_1/q_grads           | 0.0006410609552403912  |
| train_1/q_grads_std       | 0.13170779906213284    |
| train_1/q_loss            | 73.64762252353717      |
| train_1/reward            | -1.564491339889355     |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.3482666015625        |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -24.147216833495985    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102/policy_best.pkl ...
Training epoch 2
Time for epoch 2: 271.01. Rollout time: 95.35, Training time: 175.63
Evaluating epoch 2
Data_dir: data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102
-----------------------------------------------------
| epoch                     | 2                     |
| policy/steps              | 272739.0              |
| test/episodes             | 75.0                  |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -26.85486735183828    |
| test_1/avg_q              | -8.16374651621902     |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 300.0                 |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.947367630272268   |
| train_0/current_q         | -9.05311291935391     |
| train_0/fw_bonus          | -0.9991445556282997   |
| train_0/fw_loss           | 0.003948564198799432  |
| train_0/mu_grads          | -0.02378350906074047  |
| train_0/mu_grads_std      | 0.234006180241704     |
| train_0/mu_loss           | 8.9119285898172       |
| train_0/next_q            | -8.904807269257892    |
| train_0/q_grads           | -0.011504689999856054 |
| train_0/q_grads_std       | 0.15333107560873033   |
| train_0/q_loss            | 0.23153603424226965   |
| train_0/reward            | -0.8750542241192306   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.066748046875        |
| train_0/target_q          | -9.174178201862642    |
| train_1/avg_q             | -19.063170260769354   |
| train_1/current_q         | -7.058589294029666    |
| train_1/fw_bonus          | -0.9998641014099121   |
| train_1/fw_loss           | 0.0004328055933001451 |
| train_1/mu_grads          | 0.01215703405905515   |
| train_1/mu_grads_std      | 0.2228774804621935    |
| train_1/mu_loss           | 7.598489147053206     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -6.623528488195198    |
| train_1/q_grads           | -0.030597746185958386 |
| train_1/q_grads_std       | 0.14649868458509446   |
| train_1/q_loss            | 2.315268115220223     |
| train_1/reward            | -1.559273069316987    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.5285888671875       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -7.063718012207043    |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102/policy_best.pkl ...
Training epoch 3
Time for epoch 3: 349.93. Rollout time: 132.14, Training time: 217.76
Evaluating epoch 3
Data_dir: data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102
-----------------------------------------------------
| epoch                     | 3                     |
| policy/steps              | 363864.0              |
| test/episodes             | 100.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -26.91480580829624    |
| test_1/avg_q              | -0.013183756221740237 |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 400.0                 |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.93566937419906    |
| train_0/current_q         | -9.443806259887221    |
| train_0/fw_bonus          | -0.9991398066282272   |
| train_0/fw_loss           | 0.003970513725653291  |
| train_0/mu_grads          | -0.02883356702513993  |
| train_0/mu_grads_std      | 0.2584629714488983    |
| train_0/mu_loss           | 9.345683207628717     |
| train_0/next_q            | -9.333964418222767    |
| train_0/q_grads           | -0.019943638425320386 |
| train_0/q_grads_std       | 0.16124679669737815   |
| train_0/q_loss            | 0.3022620239304651    |
| train_0/reward            | -0.8770404023031005   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0917724609375       |
| train_0/target_q          | -9.554922240576072    |
| train_1/avg_q             | -6.933813628756477    |
| train_1/current_q         | -0.017074436520872954 |
| train_1/fw_bonus          | -0.9998323440551757   |
| train_1/fw_loss           | 0.0005279766013700282 |
| train_1/mu_grads          | 0.00218288071337156   |
| train_1/mu_grads_std      | 0.2311222054064274    |
| train_1/mu_loss           | 1.0000607164505344    |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -5.24506058186618e-05 |
| train_1/q_grads           | -0.031577412504702806 |
| train_1/q_grads_std       | 0.1735490534454584    |
| train_1/q_loss            | 5.841834528404706     |
| train_1/reward            | -1.5650048632916878   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.4669189453125       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -1.565045822127584    |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 4
Time for epoch 4: 340.79. Rollout time: 125.14, Training time: 215.60
Evaluating epoch 4
Data_dir: data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102
-------------------------------------------------------
| epoch                     | 4                       |
| policy/steps              | 454989.0                |
| test/episodes             | 125.0                   |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -26.53213061732394      |
| test_1/avg_q              | -1.809373382334878e-05  |
| test_1/n_subgoals         | 675.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 500.0                   |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -26.78308954909262      |
| train_0/current_q         | -9.665639111461394      |
| train_0/fw_bonus          | -0.9979457333683968     |
| train_0/fw_loss           | 0.009480047528631986    |
| train_0/mu_grads          | -0.039989339839667085   |
| train_0/mu_grads_std      | 0.28842427432537077     |
| train_0/mu_loss           | 9.54918301815328        |
| train_0/next_q            | -9.545326832346111      |
| train_0/q_grads           | -0.021725705452263357   |
| train_0/q_grads_std       | 0.1688555095344782      |
| train_0/q_loss            | 0.3339808552085647      |
| train_0/reward            | -0.8796665766189108     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.074755859375          |
| train_0/target_q          | -9.790968300081568      |
| train_1/avg_q             | -0.2734531771368011     |
| train_1/current_q         | -0.00013305738943555703 |
| train_1/fw_bonus          | -0.998897272348404      |
| train_1/fw_loss           | 0.0033297524583758785   |
| train_1/mu_grads          | 0.0062577318516559895   |
| train_1/mu_grads_std      | 0.22887368500232697     |
| train_1/mu_loss           | 1.0000004043207258      |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -3.150626292877319e-07  |
| train_1/q_grads           | -0.026721546053886415   |
| train_1/q_grads_std       | 0.1869945764541626      |
| train_1/q_loss            | 6.098950698407211       |
| train_1/reward            | -1.5883758339710767     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.2311767578125         |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -1.5883760979948103     |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 5
Time for epoch 5: 364.79. Rollout time: 144.67, Training time: 220.07
Evaluating epoch 5
Data_dir: data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102
-------------------------------------------------------
| epoch                     | 5                       |
| policy/steps              | 546110.0                |
| test/episodes             | 150.0                   |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -26.728778188394386     |
| test_1/avg_q              | -7.429430368561129e-05  |
| test_1/n_subgoals         | 675.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 600.0                   |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -26.59705073781769      |
| train_0/current_q         | -10.095653558541173     |
| train_0/fw_bonus          | -0.9972433358430862     |
| train_0/fw_loss           | 0.012720946059562265    |
| train_0/mu_grads          | -0.039890353474766015   |
| train_0/mu_grads_std      | 0.30916708409786225     |
| train_0/mu_loss           | 9.993617631204831       |
| train_0/next_q            | -9.983794295170034      |
| train_0/q_grads           | -0.020848781056702136   |
| train_0/q_grads_std       | 0.17422082126140595     |
| train_0/q_loss            | 0.3673971978111764      |
| train_0/reward            | -0.8826431986963144     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.0322509765625         |
| train_0/target_q          | -10.232294063622668     |
| train_1/avg_q             | -1.53964160791648e-05   |
| train_1/current_q         | -0.00026555116676111234 |
| train_1/fw_bonus          | -0.9991927444934845     |
| train_1/fw_loss           | 0.0024444045877316965   |
| train_1/mu_grads          | 0.00625832088990137     |
| train_1/mu_grads_std      | 0.22887348160147666     |
| train_1/mu_loss           | 1.0000010247122024      |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -7.602934602997889e-07  |
| train_1/q_grads           | -0.02842044746503234    |
| train_1/q_grads_std       | 0.1863479170948267      |
| train_1/q_loss            | 6.121856234314511       |
| train_1/reward            | -1.5920093052845914     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.1625732421875         |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.00037037037037037035  |
| train_1/target_q          | -1.5920099465089126     |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 6
Time for epoch 6: 365.47. Rollout time: 139.27, Training time: 226.15
Evaluating epoch 6
Data_dir: data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102
-----------------------------------------------------
| epoch                     | 6                     |
| policy/steps              | 637235.0              |
| test/episodes             | 175.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -26.0092247585294     |
| test_1/avg_q              | -18.91936438723559    |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 700.0                 |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.2104237527998     |
| train_0/current_q         | -10.238646724734409   |
| train_0/fw_bonus          | -0.9963156610727311   |
| train_0/fw_loss           | 0.017001326894387603  |
| train_0/mu_grads          | -0.044307015277445315 |
| train_0/mu_grads_std      | 0.3246036611497402    |
| train_0/mu_loss           | 10.116092270331768    |
| train_0/next_q            | -10.098607432645801   |
| train_0/q_grads           | -0.020422337111085653 |
| train_0/q_grads_std       | 0.18054376132786273   |
| train_0/q_loss            | 0.3692062825260847    |
| train_0/reward            | -0.8883134058705764   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0241943359375       |
| train_0/target_q          | -10.375892913927865   |
| train_1/avg_q             | -1.7541545116145318   |
| train_1/current_q         | -4.195921148287948    |
| train_1/fw_bonus          | -0.9991704508662224   |
| train_1/fw_loss           | 0.0025111938011832535 |
| train_1/mu_grads          | 0.01664221491664648   |
| train_1/mu_grads_std      | 0.22846913821995257   |
| train_1/mu_loss           | 7.592422386380683     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -6.394647422864358    |
| train_1/q_grads           | -0.030514250323176385 |
| train_1/q_grads_std       | 0.22599808387458326   |
| train_1/q_loss            | 39.63271885707415     |
| train_1/reward            | -1.5896549404744291   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.1183837890625       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -6.976708671547942    |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 7
Time for epoch 7: 363.06. Rollout time: 140.53, Training time: 222.48
Evaluating epoch 7
Data_dir: data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102
------------------------------------------------------
| epoch                     | 7                      |
| policy/steps              | 728360.0               |
| test/episodes             | 200.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -26.99986920892261     |
| test_1/avg_q              | -0.13069654945945497   |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 800.0                  |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -26.347522406565602    |
| train_0/current_q         | -10.5572970229348      |
| train_0/fw_bonus          | -0.9966204956173896    |
| train_0/fw_loss           | 0.015594793530181051   |
| train_0/mu_grads          | -0.047018260881304744  |
| train_0/mu_grads_std      | 0.3389511860907078     |
| train_0/mu_loss           | 10.425288700431453     |
| train_0/next_q            | -10.400152393679358    |
| train_0/q_grads           | -0.018899272894486785  |
| train_0/q_grads_std       | 0.1922017142176628     |
| train_0/q_loss            | 0.4060043120663613     |
| train_0/reward            | -0.8945906166089117    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.0166259765625        |
| train_0/target_q          | -10.694401942740578    |
| train_1/avg_q             | -9.599580197800346     |
| train_1/current_q         | -0.07715726620181107   |
| train_1/fw_bonus          | -0.9994088158011436    |
| train_1/fw_loss           | 0.001796968950657174   |
| train_1/mu_grads          | 0.03226445354521275    |
| train_1/mu_grads_std      | 0.23213127963244914    |
| train_1/mu_loss           | 1.000117286364192      |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -0.0001164574479777782 |
| train_1/q_grads           | -0.042161803040653464  |
| train_1/q_grads_std       | 0.24299141354858875    |
| train_1/q_loss            | 5.7524898008259715     |
| train_1/reward            | -1.5675009500642774    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.09013671875          |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -1.5676103957921093    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 8
Time for epoch 8: 380.27. Rollout time: 147.85, Training time: 232.38
Evaluating epoch 8
Data_dir: data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102
------------------------------------------------------
| epoch                     | 8                      |
| policy/steps              | 819485.0               |
| test/episodes             | 225.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -26.999999999437886    |
| test_1/avg_q              | -0.038689842190416     |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 900.0                  |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -26.999693280838002    |
| train_0/current_q         | -10.302113386193799    |
| train_0/fw_bonus          | -0.9966759234666824    |
| train_0/fw_loss           | 0.015338989510200917   |
| train_0/mu_grads          | -0.04660076592117548   |
| train_0/mu_grads_std      | 0.35446291118860246    |
| train_0/mu_loss           | 10.13408184712948      |
| train_0/next_q            | -10.106783701549588    |
| train_0/q_grads           | -0.019080272177234292  |
| train_0/q_grads_std       | 0.20142137669026852    |
| train_0/q_loss            | 0.30600022106653035    |
| train_0/reward            | -0.8944979189393052    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.007958984375         |
| train_0/target_q          | -10.439391972600712    |
| train_1/avg_q             | -3.5487088631388843    |
| train_1/current_q         | -0.002180322845807236  |
| train_1/fw_bonus          | -0.9995528802275657    |
| train_1/fw_loss           | 0.0013652788707986473  |
| train_1/mu_grads          | 0.03560431255027652    |
| train_1/mu_grads_std      | 0.2371826820075512     |
| train_1/mu_loss           | 1.0000000000000058     |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -1.632855736947646e-15 |
| train_1/q_grads           | -0.03537654457613826   |
| train_1/q_grads_std       | 0.2597057081758976     |
| train_1/q_loss            | 5.806313889840881      |
| train_1/reward            | -1.5568973338173238    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.076025390625         |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -1.556897333817325     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 9
Time for epoch 9: 371.12. Rollout time: 145.92, Training time: 225.14
Evaluating epoch 9
Data_dir: data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102
-------------------------------------------------------
| epoch                     | 9                       |
| policy/steps              | 910610.0                |
| test/episodes             | 250.0                   |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -26.999999999994404     |
| test_1/avg_q              | -4.8877648308432254e-08 |
| test_1/n_subgoals         | 675.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 1000.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -26.999999996537614     |
| train_0/current_q         | -10.348402102054067     |
| train_0/fw_bonus          | -0.9973818510770798     |
| train_0/fw_loss           | 0.012081851228140294    |
| train_0/mu_grads          | -0.04780365861952305    |
| train_0/mu_grads_std      | 0.36172036826610565     |
| train_0/mu_loss           | 10.19631653188929       |
| train_0/next_q            | -10.17087442099239      |
| train_0/q_grads           | -0.01974941003136337    |
| train_0/q_grads_std       | 0.20606486462056636     |
| train_0/q_loss            | 0.27130587825956953     |
| train_0/reward            | -0.8940813098408398     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.0171875               |
| train_0/target_q          | -10.489895244566881     |
| train_1/avg_q             | -2.650380520599001      |
| train_1/current_q         | -1.4270548924589215e-06 |
| train_1/fw_bonus          | -0.9994724497199059     |
| train_1/fw_loss           | 0.0016063061382737942   |
| train_1/mu_grads          | 0.03812306746840477     |
| train_1/mu_grads_std      | 0.23683276772499084     |
| train_1/mu_loss           | 1.0                     |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -3.043593784213423e-20  |
| train_1/q_grads           | -0.03115393645130098    |
| train_1/q_grads_std       | 0.27847378328442574     |
| train_1/q_loss            | 6.035156720119671       |
| train_1/reward            | -1.5835765061172424     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.082763671875          |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -1.5835765061172424     |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 10
Time for epoch 10: 408.47. Rollout time: 156.16, Training time: 252.26
Evaluating epoch 10
Data_dir: data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102
-------------------------------------------------------
| epoch                     | 10                      |
| policy/steps              | 1001735.0               |
| test/episodes             | 275.0                   |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -26.99999999376895      |
| test_1/avg_q              | -1.8940660031456448e-07 |
| test_1/n_subgoals         | 675.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 1100.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -26.99999999971995      |
| train_0/current_q         | -10.217275239060646     |
| train_0/fw_bonus          | -0.9977877393364907     |
| train_0/fw_loss           | 0.01020901493029669     |
| train_0/mu_grads          | -0.05022564940154552    |
| train_0/mu_grads_std      | 0.36905195787549017     |
| train_0/mu_loss           | 10.081171039950139      |
| train_0/next_q            | -10.068629341159525     |
| train_0/q_grads           | -0.01996211800724268    |
| train_0/q_grads_std       | 0.21060942485928535     |
| train_0/q_loss            | 0.3259542451207954      |
| train_0/reward            | -0.8932077539902821     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.0323974609375         |
| train_0/target_q          | -10.358265479416541     |
| train_1/avg_q             | -7.555551523949591e-06  |
| train_1/current_q         | -2.4920889890849803e-06 |
| train_1/fw_bonus          | -0.9994838297367096     |
| train_1/fw_loss           | 0.0015722399737569503   |
| train_1/mu_grads          | 0.03812306746840477     |
| train_1/mu_grads_std      | 0.23683276772499084     |
| train_1/mu_loss           | 1.0                     |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -2.8571686049159533e-20 |
| train_1/q_grads           | -0.03127910392358899    |
| train_1/q_grads_std       | 0.2785403273999691      |
| train_1/q_loss            | 5.984935840008994       |
| train_1/reward            | -1.5773899643449112     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.0370361328125         |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -1.5773899643449112     |
-------------------------------------------------------
Saving periodic policy to data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102/policy_10.pkl ...
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 11
Time for epoch 11: 391.02. Rollout time: 144.58, Training time: 246.39
Evaluating epoch 11
Data_dir: data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102
-------------------------------------------------------
| epoch                     | 11                      |
| policy/steps              | 1092860.0               |
| test/episodes             | 300.0                   |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -26.99999999741197      |
| test_1/avg_q              | -1.7204747068999118e-06 |
| test_1/n_subgoals         | 675.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 1200.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -26.999999995680795     |
| train_0/current_q         | -10.423605898038149     |
| train_0/fw_bonus          | -0.9978802740573883     |
| train_0/fw_loss           | 0.009782095625996589    |
| train_0/mu_grads          | -0.051212995871901514   |
| train_0/mu_grads_std      | 0.37712018489837645     |
| train_0/mu_loss           | 10.348372809888025      |
| train_0/next_q            | -10.324596716221285     |
| train_0/q_grads           | -0.02061257096938789    |
| train_0/q_grads_std       | 0.21354371607303618     |
| train_0/q_loss            | 0.6065683019351645      |
| train_0/reward            | -0.8917329585165135     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.0434326171875         |
| train_0/target_q          | -10.597820258714775     |
| train_1/avg_q             | -7.956409931735133e-06  |
| train_1/current_q         | -4.9037078785788735e-06 |
| train_1/fw_bonus          | -0.9988001361489296     |
| train_1/fw_loss           | 0.0036208364414051174   |
| train_1/mu_grads          | 0.03812306746840477     |
| train_1/mu_grads_std      | 0.23683276772499084     |
| train_1/mu_loss           | 1.0                     |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -1.0960987695951344e-19 |
| train_1/q_grads           | -0.03163322145119309    |
| train_1/q_grads_std       | 0.2790420211851597      |
| train_1/q_loss            | 5.877174907244219       |
| train_1/reward            | -1.5645358844994917     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.0871337890625         |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -1.5645358844994917     |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 12
Time for epoch 12: 412.72. Rollout time: 153.78, Training time: 258.89
Evaluating epoch 12
Data_dir: data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102
-------------------------------------------------------
| epoch                     | 12                      |
| policy/steps              | 1183985.0               |
| test/episodes             | 325.0                   |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -27.0                   |
| test_1/avg_q              | -1.0085884212256182e-07 |
| test_1/n_subgoals         | 675.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 1300.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -26.999999999957016     |
| train_0/current_q         | -10.910598935661692     |
| train_0/fw_bonus          | -0.9978398740291595     |
| train_0/fw_loss           | 0.009968498698435723    |
| train_0/mu_grads          | -0.056654781196266414   |
| train_0/mu_grads_std      | 0.38398503586649896     |
| train_0/mu_loss           | 11.187188124973705      |
| train_0/next_q            | -11.154140608388314     |
| train_0/q_grads           | -0.020038066571578382   |
| train_0/q_grads_std       | 0.22176007069647313     |
| train_0/q_loss            | 1.9379120270241592      |
| train_0/reward            | -0.8908298567192106     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.0207275390625         |
| train_0/target_q          | -11.039713975750601     |
| train_1/avg_q             | -6.714650998257099e-06  |
| train_1/current_q         | -8.158524117078436e-06  |
| train_1/fw_bonus          | -0.9990380272269249     |
| train_1/fw_loss           | 0.0029079914587782697   |
| train_1/mu_grads          | 0.03812306746840477     |
| train_1/mu_grads_std      | 0.23683276772499084     |
| train_1/mu_loss           | 1.0                     |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -2.6231053243119838e-21 |
| train_1/q_grads           | -0.03232109881937504    |
| train_1/q_grads_std       | 0.28024930134415627     |
| train_1/q_loss            | 5.865704473048192       |
| train_1/reward            | -1.5640795464190886     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.0998779296875         |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -1.5640795464190886     |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 13
Time for epoch 13: 412.87. Rollout time: 152.94, Training time: 259.89
Evaluating epoch 13
Data_dir: data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102
-----------------------------------------------------
| epoch                     | 13                    |
| policy/steps              | 1275110.0             |
| test/episodes             | 350.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -4.759777061855313    |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 1400.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.999999999783185   |
| train_0/current_q         | -10.758534377763985   |
| train_0/fw_bonus          | -0.9973207399249077   |
| train_0/fw_loss           | 0.012363809847738595  |
| train_0/mu_grads          | -0.058985089976340534 |
| train_0/mu_grads_std      | 0.39148879796266556   |
| train_0/mu_loss           | 10.650024506744932    |
| train_0/next_q            | -10.628563551685142   |
| train_0/q_grads           | -0.020232336316257715 |
| train_0/q_grads_std       | 0.2233770240098238    |
| train_0/q_loss            | 0.7566955047626591    |
| train_0/reward            | -0.8968553025246365   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0108154296875       |
| train_0/target_q          | -10.919844096318199   |
| train_1/avg_q             | -1.0582626022564863   |
| train_1/current_q         | -25.204780705881937   |
| train_1/fw_bonus          | -0.996019434928894    |
| train_1/fw_loss           | 0.011952797276899218  |
| train_1/mu_grads          | 0.05720947030931711   |
| train_1/mu_grads_std      | 0.25042532458901406   |
| train_1/mu_loss           | 27.99962343422098     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -26.999990325311256   |
| train_1/q_grads           | -0.02577755698002875  |
| train_1/q_grads_std       | 0.29022674411535265   |
| train_1/q_loss            | 36.17429277714982     |
| train_1/reward            | -1.585088304519013    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0448486328125       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -24.461092860895807   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 14
Time for epoch 14: 380.49. Rollout time: 152.59, Training time: 227.86
Evaluating epoch 14
Data_dir: data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102
-------------------------------------------------------
| epoch                     | 14                      |
| policy/steps              | 1366235.0               |
| test/episodes             | 375.0                   |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -27.0                   |
| test_1/avg_q              | -7.290357584024325e-05  |
| test_1/n_subgoals         | 675.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 1500.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -27.0                   |
| train_0/current_q         | -12.032218314107165     |
| train_0/fw_bonus          | -0.9945355802774429     |
| train_0/fw_loss           | 0.02521472005173564     |
| train_0/mu_grads          | -0.0629881713539362     |
| train_0/mu_grads_std      | 0.3967910006642342      |
| train_0/mu_loss           | 12.058411527334213      |
| train_0/next_q            | -12.037274156956576     |
| train_0/q_grads           | -0.02020060704089701    |
| train_0/q_grads_std       | 0.226275472342968       |
| train_0/q_loss            | 2.653789534119297       |
| train_0/reward            | -0.9060579392578803     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.0201416015625         |
| train_0/target_q          | -12.17566539442168      |
| train_1/avg_q             | -6.583847335286676      |
| train_1/current_q         | -0.00013377207326302915 |
| train_1/fw_bonus          | -0.9944017708301545     |
| train_1/fw_loss           | 0.0167998758610338      |
| train_1/mu_grads          | 0.06494994461536407     |
| train_1/mu_grads_std      | 0.25743192434310913     |
| train_1/mu_loss           | 1.0                     |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -3.1950783176894975e-29 |
| train_1/q_grads           | -0.024855108046904207   |
| train_1/q_grads_std       | 0.3038554355502129      |
| train_1/q_loss            | 5.917033328279716       |
| train_1/reward            | -1.5704656389294542     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.0157958984375         |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -1.5704656389294542     |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 15
Time for epoch 15: 387.64. Rollout time: 152.63, Training time: 234.95
Evaluating epoch 15
Data_dir: data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102
-------------------------------------------------------
| epoch                     | 15                      |
| policy/steps              | 1457360.0               |
| test/episodes             | 400.0                   |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -7.819817608873831e-11  |
| test_1/avg_q              | -0.00017135590852700972 |
| test_1/n_subgoals         | 675.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 1600.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -17.550000000209664     |
| train_0/current_q         | -3.4826536108936055     |
| train_0/fw_bonus          | -0.9928695678710937     |
| train_0/fw_loss           | 0.03290186584927142     |
| train_0/mu_grads          | -0.06514882929623127    |
| train_0/mu_grads_std      | 0.4033444680273533      |
| train_0/mu_loss           | 3.6492661186381747      |
| train_0/next_q            | -3.6105525545332013     |
| train_0/q_grads           | -0.02005994520150125    |
| train_0/q_grads_std       | 0.23195642419159412     |
| train_0/q_loss            | 3.634136730679505       |
| train_0/reward            | -0.9114795652552857     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.0498291015625         |
| train_0/target_q          | -3.819929199200968      |
| train_1/avg_q             | -0.00036769041149932365 |
| train_1/current_q         | -0.00021605697052155159 |
| train_1/fw_bonus          | -0.9944895759224892     |
| train_1/fw_loss           | 0.016536768153309823    |
| train_1/mu_grads          | 0.06494994461536407     |
| train_1/mu_grads_std      | 0.25743192434310913     |
| train_1/mu_loss           | 1.0                     |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -2.942584693344286e-26  |
| train_1/q_grads           | -0.025175843620672822   |
| train_1/q_grads_std       | 0.30399183854460715     |
| train_1/q_loss            | 5.839898324694242       |
| train_1/reward            | -1.562023194876383      |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.0178466796875         |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -1.562023194876383      |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 16
Time for epoch 16: 396.16. Rollout time: 150.05, Training time: 246.06
Evaluating epoch 16
Data_dir: data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102
-------------------------------------------------------
| epoch                     | 16                      |
| policy/steps              | 1548485.0               |
| test/episodes             | 425.0                   |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -1.9182737950439976e-05 |
| test_1/avg_q              | -0.0017756678139114182  |
| test_1/n_subgoals         | 675.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 1700.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -7.283253748688032e-07  |
| train_0/current_q         | -3.9382009217253597     |
| train_0/fw_bonus          | -0.9926292181015015     |
| train_0/fw_loss           | 0.034010786470025776    |
| train_0/mu_grads          | -0.06711522117257118    |
| train_0/mu_grads_std      | 0.41024748235940933     |
| train_0/mu_loss           | 4.260987407709267       |
| train_0/next_q            | -4.218610237670586      |
| train_0/q_grads           | -0.020513685932382942   |
| train_0/q_grads_std       | 0.23728887140750884     |
| train_0/q_loss            | 4.482242186267281       |
| train_0/reward            | -0.9179582499506068     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.0194091796875         |
| train_0/target_q          | -4.295729171537049      |
| train_1/avg_q             | -1.8140591813142468     |
| train_1/current_q         | -6.551899990919913e-05  |
| train_1/fw_bonus          | -0.9949273273348809     |
| train_1/fw_loss           | 0.015225102100521326    |
| train_1/mu_grads          | 0.08041630685329437     |
| train_1/mu_grads_std      | 0.27062517404556274     |
| train_1/mu_loss           | 1.0000000000000002      |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -5.508333909159612e-18  |
| train_1/q_grads           | -0.02712722383439541    |
| train_1/q_grads_std       | 0.3143507957458496      |
| train_1/q_loss            | 5.9696093233766065      |
| train_1/reward            | -1.5769999474156067     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.0076416015625         |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -1.5769999474156067     |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 17
Time for epoch 17: 396.36. Rollout time: 150.86, Training time: 245.45
Evaluating epoch 17
Data_dir: data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102
-------------------------------------------------------
| epoch                     | 17                      |
| policy/steps              | 1639610.0               |
| test/episodes             | 450.0                   |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -2.5602329951414908e-05 |
| test_1/avg_q              | -0.0018005346005540402  |
| test_1/n_subgoals         | 675.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 1800.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -0.16063464669178404    |
| train_0/current_q         | -2.6077962963274413     |
| train_0/fw_bonus          | -0.9919183760881424     |
| train_0/fw_loss           | 0.03729072189889848     |
| train_0/mu_grads          | -0.07036469671875238    |
| train_0/mu_grads_std      | 0.4201963260769844      |
| train_0/mu_loss           | 2.478336738357127       |
| train_0/next_q            | -2.4721168768490847     |
| train_0/q_grads           | -0.02118874415755272    |
| train_0/q_grads_std       | 0.24121445827186108     |
| train_0/q_loss            | 1.5327486238712127      |
| train_0/reward            | -0.9273494252247474     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.009765625             |
| train_0/target_q          | -2.9506004836393718     |
| train_1/avg_q             | -0.00020167269618012003 |
| train_1/current_q         | -0.0034632529864606345  |
| train_1/fw_bonus          | -0.9956093415617943     |
| train_1/fw_loss           | 0.013181633944623173    |
| train_1/mu_grads          | 0.08041630685329437     |
| train_1/mu_grads_std      | 0.27062517404556274     |
| train_1/mu_loss           | 1.0000000000000384      |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -1.6408722150056354e-15 |
| train_1/q_grads           | -0.028587293205782772   |
| train_1/q_grads_std       | 0.31643287315964697     |
| train_1/q_loss            | 5.87862565028334        |
| train_1/reward            | -1.567654257104732      |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.0088134765625         |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -1.5676542571047336     |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 18
Time for epoch 18: 458.59. Rollout time: 195.00, Training time: 263.49
Evaluating epoch 18
Data_dir: data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102
-------------------------------------------------------
| epoch                     | 18                      |
| policy/steps              | 1730735.0               |
| test/episodes             | 475.0                   |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -0.0005343777933752286  |
| test_1/avg_q              | -1.0321786155225335e-05 |
| test_1/n_subgoals         | 675.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 1900.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -0.00012635289666035177 |
| train_0/current_q         | -2.6545368253525807     |
| train_0/fw_bonus          | -0.9910810574889183     |
| train_0/fw_loss           | 0.041154127940535544    |
| train_0/mu_grads          | -0.07390621919184923    |
| train_0/mu_grads_std      | 0.4269556112587452      |
| train_0/mu_loss           | 2.507347618584029       |
| train_0/next_q            | -2.480674734865493      |
| train_0/q_grads           | -0.02162120006978512    |
| train_0/q_grads_std       | 0.24283753894269466     |
| train_0/q_loss            | 1.2573063677007141      |
| train_0/reward            | -0.9299929271055589     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.0035888671875         |
| train_0/target_q          | -2.9705449855075656     |
| train_1/avg_q             | -9.618439599391962      |
| train_1/current_q         | -8.195324925351709e-06  |
| train_1/fw_bonus          | -0.9960996299982071     |
| train_1/fw_loss           | 0.011712497158441693    |
| train_1/mu_grads          | 0.08060014247894287     |
| train_1/mu_grads_std      | 0.272603303194046       |
| train_1/mu_loss           | 1.0                     |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -9.486449736332166e-20  |
| train_1/q_grads           | -0.03092625131830573    |
| train_1/q_grads_std       | 0.3271924555301666      |
| train_1/q_loss            | 6.021501001515991       |
| train_1/reward            | -1.5835371243039844     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.003515625             |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -1.5835371243039844     |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 19
Time for epoch 19: 383.78. Rollout time: 158.03, Training time: 225.69
Evaluating epoch 19
Data_dir: data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102
------------------------------------------------------
| epoch                     | 19                     |
| policy/steps              | 1821860.0              |
| test/episodes             | 500.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -9.845718325668812e-07 |
| test_1/avg_q              | -7.412374684212976e-06 |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 2000.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -0.0005391673892524409 |
| train_0/current_q         | -2.657912470146843     |
| train_0/fw_bonus          | -0.9917395204305649    |
| train_0/fw_loss           | 0.038115923292934895   |
| train_0/mu_grads          | -0.07826261930167674   |
| train_0/mu_grads_std      | 0.4322431206703186     |
| train_0/mu_loss           | 2.5129462452515976     |
| train_0/next_q            | -2.475106870493289     |
| train_0/q_grads           | -0.02147094705142081   |
| train_0/q_grads_std       | 0.24448129981756211    |
| train_0/q_loss            | 1.048432174345925      |
| train_0/reward            | -0.926434041743778     |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.0024658203125        |
| train_0/target_q          | -2.972264099216305     |
| train_1/avg_q             | -5.498884212817234e-06 |
| train_1/current_q         | -8.290215627441289e-06 |
| train_1/fw_bonus          | -0.9974029704928398    |
| train_1/fw_loss           | 0.007807235990185291   |
| train_1/mu_grads          | 0.08060014247894287    |
| train_1/mu_grads_std      | 0.272603303194046      |
| train_1/mu_loss           | 1.0                    |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -7.86602472184361e-20  |
| train_1/q_grads           | -0.030935621401295067  |
| train_1/q_grads_std       | 0.32719151824712756    |
| train_1/q_loss            | 5.96664507479637       |
| train_1/reward            | -1.5772758558698114    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0042236328125        |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -1.5772758558698114    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 20
Time for epoch 20: 360.03. Rollout time: 141.24, Training time: 218.73
Evaluating epoch 20
Data_dir: data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102
-------------------------------------------------------
| epoch                     | 20                      |
| policy/steps              | 1912985.0               |
| test/episodes             | 525.0                   |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -0.15054956669411865    |
| test_1/avg_q              | -1.1929975196509798e-05 |
| test_1/n_subgoals         | 675.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 2100.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -0.04309211699020977    |
| train_0/current_q         | -2.6883879370894617     |
| train_0/fw_bonus          | -0.9914872005581856     |
| train_0/fw_loss           | 0.039280205685645345    |
| train_0/mu_grads          | -0.07891360130161047    |
| train_0/mu_grads_std      | 0.44133404865860937     |
| train_0/mu_loss           | 2.5364170474575607      |
| train_0/next_q            | -2.490155405545476      |
| train_0/q_grads           | -0.021228132164105774   |
| train_0/q_grads_std       | 0.24639990516006946     |
| train_0/q_loss            | 1.000399304653444       |
| train_0/reward            | -0.9293453780635901     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.002685546875          |
| train_0/target_q          | -3.0172156244199164     |
| train_1/avg_q             | -1.939754274043063e-06  |
| train_1/current_q         | -9.51357354535092e-06   |
| train_1/fw_bonus          | -0.9973845139145852     |
| train_1/fw_loss           | 0.007862493291031569    |
| train_1/mu_grads          | 0.08060014247894287     |
| train_1/mu_grads_std      | 0.272603303194046       |
| train_1/mu_loss           | 1.0                     |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -1.9415778001992394e-20 |
| train_1/q_grads           | -0.03095455840229988    |
| train_1/q_grads_std       | 0.3271862640976906      |
| train_1/q_loss            | 6.0021444657328145      |
| train_1/reward            | -1.581839661381673      |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.00849609375           |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -1.581839661381673      |
-------------------------------------------------------
Saving periodic policy to data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102/policy_20.pkl ...
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 21
Time for epoch 21: 20000.66. Rollout time: 2417.23, Training time: 17583.21
Evaluating epoch 21
Data_dir: data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102
-----------------------------------------------------
| epoch                     | 21                    |
| policy/steps              | 2004110.0             |
| test/episodes             | 550.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -26.99999200703027    |
| test_1/avg_q              | -4.733832361995114    |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 2200.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -16.80874049792018    |
| train_0/current_q         | -11.52698069136151    |
| train_0/fw_bonus          | -0.9914846524596215   |
| train_0/fw_loss           | 0.03929191771894693   |
| train_0/mu_grads          | -0.08081995230168104  |
| train_0/mu_grads_std      | 0.4458409704267979    |
| train_0/mu_loss           | 11.355235385468411    |
| train_0/next_q            | -11.332304100284617   |
| train_0/q_grads           | -0.021817447617650032 |
| train_0/q_grads_std       | 0.24933290369808675   |
| train_0/q_loss            | 0.7512542893984323    |
| train_0/reward            | -0.9284035327247693   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.004443359375        |
| train_0/target_q          | -11.675053599805963   |
| train_1/avg_q             | -1.894889288245603    |
| train_1/current_q         | -25.695293927198726   |
| train_1/fw_bonus          | -0.9978138163685799   |
| train_1/fw_loss           | 0.006576160027179867  |
| train_1/mu_grads          | 0.08247094247490168   |
| train_1/mu_grads_std      | 0.27304320111870767   |
| train_1/mu_loss           | 27.999999475402888    |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -26.99999999694891    |
| train_1/q_grads           | -0.02329660556279123  |
| train_1/q_grads_std       | 0.330400437861681     |
| train_1/q_loss            | 46.89618041968814     |
| train_1/reward            | -1.5745383442525054   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0165771484375       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -24.656624767085027   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 22
Time for epoch 22: 343.35. Rollout time: 129.31, Training time: 213.99
Evaluating epoch 22
Data_dir: data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102
-------------------------------------------------------
| epoch                     | 22                      |
| policy/steps              | 2095235.0               |
| test/episodes             | 575.0                   |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -26.999994592791033     |
| test_1/avg_q              | -1.4096345973058684e-07 |
| test_1/n_subgoals         | 675.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 2300.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -26.99998263751297      |
| train_0/current_q         | -11.238589361536018     |
| train_0/fw_bonus          | -0.9914139553904533     |
| train_0/fw_loss           | 0.03961819978430867     |
| train_0/mu_grads          | -0.0803185360506177     |
| train_0/mu_grads_std      | 0.4516639165580273      |
| train_0/mu_loss           | 11.084320178646792      |
| train_0/next_q            | -11.04786735042672      |
| train_0/q_grads           | -0.022540511190891267   |
| train_0/q_grads_std       | 0.2505370810627937      |
| train_0/q_loss            | 0.6292596560378685      |
| train_0/reward            | -0.9214230065415905     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.0026123046875         |
| train_0/target_q          | -11.380627559596709     |
| train_1/avg_q             | -2.6617831358886024     |
| train_1/current_q         | -2.0719373457031457e-07 |
| train_1/fw_bonus          | -0.997945387661457      |
| train_1/fw_loss           | 0.006181965745054185    |
| train_1/mu_grads          | 0.07963290065526962     |
| train_1/mu_grads_std      | 0.26973283290863037     |
| train_1/mu_loss           | 1.0                     |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -4.777297344596951e-44  |
| train_1/q_grads           | -0.01858607246540487    |
| train_1/q_grads_std       | 0.3364504374563694      |
| train_1/q_loss            | 6.210655633326603       |
| train_1/reward            | -1.605789356151945      |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.0185546875            |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -1.605789356151945      |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 23
Time for epoch 23: 348.13. Rollout time: 130.87, Training time: 217.20
Evaluating epoch 23
Data_dir: data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102
-------------------------------------------------------
| epoch                     | 23                      |
| policy/steps              | 2186360.0               |
| test/episodes             | 600.0                   |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -26.999984446117026     |
| test_1/avg_q              | -0.00012298696359075475 |
| test_1/n_subgoals         | 675.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 2400.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -26.999980037885063     |
| train_0/current_q         | -11.21457962800777      |
| train_0/fw_bonus          | -0.9930936723947525     |
| train_0/fw_loss           | 0.031867778254672886    |
| train_0/mu_grads          | -0.07975826356559992    |
| train_0/mu_grads_std      | 0.45971240401268004     |
| train_0/mu_loss           | 11.062900335379577      |
| train_0/next_q            | -11.042479915454908     |
| train_0/q_grads           | -0.02200419451110065    |
| train_0/q_grads_std       | 0.2534347765147686      |
| train_0/q_loss            | 0.6209524075495685      |
| train_0/reward            | -0.915950630644511      |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.012548828125          |
| train_0/target_q          | -11.350829130015608     |
| train_1/avg_q             | -8.337181876661553e-07  |
| train_1/current_q         | -2.256622100768011e-06  |
| train_1/fw_bonus          | -0.9986966133117676     |
| train_1/fw_loss           | 0.0039310272200964395   |
| train_1/mu_grads          | 0.07963290065526962     |
| train_1/mu_grads_std      | 0.26973283290863037     |
| train_1/mu_loss           | 1.0                     |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -4.239651735092707e-37  |
| train_1/q_grads           | -0.01862924457527697    |
| train_1/q_grads_std       | 0.33646785393357276     |
| train_1/q_loss            | 6.087117250508325       |
| train_1/reward            | -1.591209951446217      |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.069775390625          |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -1.591209951446217      |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 24
Time for epoch 24: 5158.06. Rollout time: 1804.35, Training time: 3353.54
Evaluating epoch 24
Data_dir: data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102
-------------------------------------------------------
| epoch                     | 24                      |
| policy/steps              | 2277485.0               |
| test/episodes             | 625.0                   |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -26.999935858649415     |
| test_1/avg_q              | -0.4705833256196573     |
| test_1/n_subgoals         | 675.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 2500.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -26.99995260149532      |
| train_0/current_q         | -10.951951581647396     |
| train_0/fw_bonus          | -0.9948489814996719     |
| train_0/fw_loss           | 0.023768685944378375    |
| train_0/mu_grads          | -0.08335987161844968    |
| train_0/mu_grads_std      | 0.4647460140287876      |
| train_0/mu_loss           | 10.804835466004281      |
| train_0/next_q            | -10.78216088225918      |
| train_0/q_grads           | -0.021658005053177477   |
| train_0/q_grads_std       | 0.2561922617256641      |
| train_0/q_loss            | 0.5091278562125171      |
| train_0/reward            | -0.9109499690850498     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.0374267578125         |
| train_0/target_q          | -11.087560728664574     |
| train_1/avg_q             | -0.014763170013393223   |
| train_1/current_q         | -0.020052491795318993   |
| train_1/fw_bonus          | -0.9985695660114289     |
| train_1/fw_loss           | 0.004311667470028624    |
| train_1/mu_grads          | 0.07963526993989944     |
| train_1/mu_grads_std      | 0.2697382867336273      |
| train_1/mu_loss           | 1.0000000000212315      |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -3.0297964525986356e-11 |
| train_1/q_grads           | -0.021732585225254297   |
| train_1/q_grads_std       | 0.3411867909133434      |
| train_1/q_loss            | 5.926382182023154       |
| train_1/reward            | -1.5795790315954945     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.0552490234375         |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -1.5795790316221434     |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 25
Time for epoch 25: 1223.67. Rollout time: 519.47, Training time: 703.95
Evaluating epoch 25
Data_dir: data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102
-----------------------------------------------------
| epoch                     | 25                    |
| policy/steps              | 2368610.0             |
| test/episodes             | 650.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -26.999981252168713   |
| test_1/avg_q              | -26.999999928193315   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 2600.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.999974181584943   |
| train_0/current_q         | -10.870935286885015   |
| train_0/fw_bonus          | -0.9965384498238563   |
| train_0/fw_loss           | 0.015973345190286637  |
| train_0/mu_grads          | -0.08972385246306658  |
| train_0/mu_grads_std      | 0.4672513358294964    |
| train_0/mu_loss           | 10.708351198972093    |
| train_0/next_q            | -10.681794325472588   |
| train_0/q_grads           | -0.020796804595738648 |
| train_0/q_grads_std       | 0.2595041677355766    |
| train_0/q_loss            | 0.4076736387009001    |
| train_0/reward            | -0.9085122386859439   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0388427734375       |
| train_0/target_q          | -11.016775894276725   |
| train_1/avg_q             | -16.15736923775501    |
| train_1/current_q         | -26.999999986258064   |
| train_1/fw_bonus          | -0.9990906193852425   |
| train_1/fw_loss           | 0.002750383110833354  |
| train_1/mu_grads          | 0.0803707093000412    |
| train_1/mu_grads_std      | 0.26956838369369507   |
| train_1/mu_loss           | 28.0                  |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -27.0                 |
| train_1/q_grads           | -0.02209104588255286  |
| train_1/q_grads_std       | 0.34910867288708686   |
| train_1/q_loss            | 63.42257059822045     |
| train_1/reward            | -1.5813492210072582   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.1164794921875       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -24.53294443585103    |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 26
Time for epoch 26: 1398.76. Rollout time: 707.71, Training time: 690.75
Evaluating epoch 26
Data_dir: data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102
-----------------------------------------------------
| epoch                     | 26                    |
| policy/steps              | 2459735.0             |
| test/episodes             | 675.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -26.999998107702623   |
| test_1/avg_q              | -26.999999681906445   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 2700.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.99999936254609    |
| train_0/current_q         | -10.500098844701167   |
| train_0/fw_bonus          | -0.9985762163996696   |
| train_0/fw_loss           | 0.006571006507147104  |
| train_0/mu_grads          | -0.09033305272459984  |
| train_0/mu_grads_std      | 0.4716402642428875    |
| train_0/mu_loss           | 10.34206879714037     |
| train_0/next_q            | -10.31182257128194    |
| train_0/q_grads           | -0.02031472292728722  |
| train_0/q_grads_std       | 0.2608064390718937    |
| train_0/q_loss            | 0.30324121780814794   |
| train_0/reward            | -0.9003664812509669   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.126171875           |
| train_0/target_q          | -10.641495359543788   |
| train_1/avg_q             | -26.99999988747448    |
| train_1/current_q         | -26.99999998256372    |
| train_1/fw_bonus          | -0.9993998587131501   |
| train_1/fw_loss           | 0.0018237938667880372 |
| train_1/mu_grads          | 0.0803707093000412    |
| train_1/mu_grads_std      | 0.26956838369369507   |
| train_1/mu_loss           | 28.0                  |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -27.0                 |
| train_1/q_grads           | -0.0220387349370867   |
| train_1/q_grads_std       | 0.34913185313344003   |
| train_1/q_loss            | 65.91858398939374     |
| train_1/reward            | -1.5894241671907365   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.08359375            |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -24.429262057815755   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 27
Time for epoch 27: 400.60. Rollout time: 162.15, Training time: 238.39
Evaluating epoch 27
Data_dir: data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102
-----------------------------------------------------
| epoch                     | 27                    |
| policy/steps              | 2550860.0             |
| test/episodes             | 700.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -26.999999984123836   |
| test_1/avg_q              | -26.999999946985422   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 2800.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.999999121195174   |
| train_0/current_q         | -10.430216569704829   |
| train_0/fw_bonus          | -0.9994666308164597   |
| train_0/fw_loss           | 0.0024626063619507476 |
| train_0/mu_grads          | -0.09000933412462472  |
| train_0/mu_grads_std      | 0.4752878829836845    |
| train_0/mu_loss           | 10.261607095413002    |
| train_0/next_q            | -10.234181022798786   |
| train_0/q_grads           | -0.019640140887349843 |
| train_0/q_grads_std       | 0.2623773507773876    |
| train_0/q_loss            | 0.2227818637412588    |
| train_0/reward            | -0.8962918546283618   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.19599609375         |
| train_0/target_q          | -10.574232183932612   |
| train_1/avg_q             | -26.999999880292506   |
| train_1/current_q         | -26.99999997927022    |
| train_1/fw_bonus          | -0.9996815279126168   |
| train_1/fw_loss           | 0.0009798167113331147 |
| train_1/mu_grads          | 0.0803707093000412    |
| train_1/mu_grads_std      | 0.26956838369369507   |
| train_1/mu_loss           | 28.0                  |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -27.0                 |
| train_1/q_grads           | -0.02190437326207757  |
| train_1/q_grads_std       | 0.3492430210113525    |
| train_1/q_loss            | 66.47301533557854     |
| train_1/reward            | -1.593321941686736    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.453662109375        |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -24.404736004186752   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 28
Time for epoch 28: 378.04. Rollout time: 148.84, Training time: 229.13
Evaluating epoch 28
Data_dir: data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102
------------------------------------------------------
| epoch                     | 28                     |
| policy/steps              | 2641985.0              |
| test/episodes             | 725.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -27.0                  |
| test_1/avg_q              | -1.419614884391746e-09 |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 2900.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -26.999999996952734    |
| train_0/current_q         | -10.583293136170216    |
| train_0/fw_bonus          | -0.9994446471333504    |
| train_0/fw_loss           | 0.0025639436556957663  |
| train_0/mu_grads          | -0.09228779133409262   |
| train_0/mu_grads_std      | 0.48221008479595184    |
| train_0/mu_loss           | 10.43915421257527      |
| train_0/next_q            | -10.416461817632936    |
| train_0/q_grads           | -0.019972074031829833  |
| train_0/q_grads_std       | 0.26482839807868       |
| train_0/q_loss            | 0.28784163434148874    |
| train_0/reward            | -0.8957565839242306    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.194384765625         |
| train_0/target_q          | -10.716225788703744    |
| train_1/avg_q             | -24.839864553128976    |
| train_1/current_q         | -2.717631304788906e-11 |
| train_1/fw_bonus          | -0.9996205732226372    |
| train_1/fw_loss           | 0.0011624349368503316  |
| train_1/mu_grads          | 0.07927997410297394    |
| train_1/mu_grads_std      | 0.2680293321609497     |
| train_1/mu_loss           | 1.0                    |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -9.266492051222774e-47 |
| train_1/q_grads           | -0.02645868994295597   |
| train_1/q_grads_std       | 0.3509773313999176     |
| train_1/q_loss            | 6.0105148190316005     |
| train_1/reward            | -1.5805259874337936    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.2295654296875        |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -1.5805259874337936    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 29
Time for epoch 29: 386.38. Rollout time: 151.67, Training time: 234.64
Evaluating epoch 29
Data_dir: data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102
-------------------------------------------------------
| epoch                     | 29                      |
| policy/steps              | 2733110.0               |
| test/episodes             | 750.0                   |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -26.999999999997502     |
| test_1/avg_q              | -6.248623005164998e-10  |
| test_1/n_subgoals         | 675.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 3000.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -26.99999999996739      |
| train_0/current_q         | -10.379989464726119     |
| train_0/fw_bonus          | -0.9993885263800621     |
| train_0/fw_loss           | 0.0028228926326846703   |
| train_0/mu_grads          | -0.09346006456762553    |
| train_0/mu_grads_std      | 0.4874297440052032      |
| train_0/mu_loss           | 10.229211252849684      |
| train_0/next_q            | -10.207713909409359     |
| train_0/q_grads           | -0.02103509372100234    |
| train_0/q_grads_std       | 0.26580042764544487     |
| train_0/q_loss            | 0.22935683739514828     |
| train_0/reward            | -0.8941580962215084     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.2059814453125         |
| train_0/target_q          | -10.525938195196616     |
| train_1/avg_q             | -7.34866639529697e-11   |
| train_1/current_q         | -8.47144726405472e-12   |
| train_1/fw_bonus          | -0.9998020112514496     |
| train_1/fw_loss           | 0.0006187875849718693   |
| train_1/mu_grads          | 0.07927997410297394     |
| train_1/mu_grads_std      | 0.2680293321609497      |
| train_1/mu_loss           | 1.0                     |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -1.8313718486766563e-48 |
| train_1/q_grads           | -0.02645868994295597    |
| train_1/q_grads_std       | 0.3509773313999176      |
| train_1/q_loss            | 6.142060680575954       |
| train_1/reward            | -1.5960734416148625     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.510986328125          |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -1.5960734416148625     |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 30
Time for epoch 30: 372.98. Rollout time: 149.81, Training time: 223.09
Evaluating epoch 30
Data_dir: data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102
-------------------------------------------------------
| epoch                     | 30                      |
| policy/steps              | 2824235.0               |
| test/episodes             | 775.0                   |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -27.0                   |
| test_1/avg_q              | -9.176479465345513e-10  |
| test_1/n_subgoals         | 675.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 3100.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -26.99999999961801      |
| train_0/current_q         | -10.173905793418959     |
| train_0/fw_bonus          | -0.9995177313685417     |
| train_0/fw_loss           | 0.002226773020811379    |
| train_0/mu_grads          | -0.09532593376934528    |
| train_0/mu_grads_std      | 0.4903296932578087      |
| train_0/mu_loss           | 10.017166067888686      |
| train_0/next_q            | -9.998814883669466      |
| train_0/q_grads           | -0.02148737097159028    |
| train_0/q_grads_std       | 0.2678641349077225      |
| train_0/q_loss            | 0.20815642081817382     |
| train_0/reward            | -0.8869002570572775     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.2429443359375         |
| train_0/target_q          | -10.320977967374954     |
| train_1/avg_q             | -2.3180374261916084e-10 |
| train_1/current_q         | -4.415993334963758e-12  |
| train_1/fw_bonus          | -0.999893669784069      |
| train_1/fw_loss           | 0.0003441852099058451   |
| train_1/mu_grads          | 0.07927997410297394     |
| train_1/mu_grads_std      | 0.2680293321609497      |
| train_1/mu_loss           | 1.0                     |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -9.270232265079599e-74  |
| train_1/q_grads           | -0.02645869553089142    |
| train_1/q_grads_std       | 0.3509773313999176      |
| train_1/q_loss            | 5.877959782402139       |
| train_1/reward            | -1.5640926145919365     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.4505126953125         |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -1.5640926145919365     |
-------------------------------------------------------
Saving periodic policy to data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102/policy_30.pkl ...
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 31
Time for epoch 31: 423.93. Rollout time: 166.46, Training time: 257.39
Evaluating epoch 31
Data_dir: data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102
-------------------------------------------------------
| epoch                     | 31                      |
| policy/steps              | 2915360.0               |
| test/episodes             | 800.0                   |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -27.0                   |
| test_1/avg_q              | -4.0480706577272413e-10 |
| test_1/n_subgoals         | 675.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 3200.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -26.999999999999858     |
| train_0/current_q         | -10.223738840707572     |
| train_0/fw_bonus          | -0.9994855344295501     |
| train_0/fw_loss           | 0.0023752410837914796   |
| train_0/mu_grads          | -0.09752447940409184    |
| train_0/mu_grads_std      | 0.49154315441846846     |
| train_0/mu_loss           | 10.059192860501597      |
| train_0/next_q            | -10.045665833196114     |
| train_0/q_grads           | -0.022305890265852214   |
| train_0/q_grads_std       | 0.2690261617302895      |
| train_0/q_loss            | 0.20241528692887836     |
| train_0/reward            | -0.8875414070047555     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.2212158203125         |
| train_0/target_q          | -10.37587919843187      |
| train_1/avg_q             | -7.548808271963535e-11  |
| train_1/current_q         | -1.850322177692975e-11  |
| train_1/fw_bonus          | -0.999932411313057      |
| train_1/fw_loss           | 0.00022810313930676784  |
| train_1/mu_grads          | 0.07927997410297394     |
| train_1/mu_grads_std      | 0.2680293321609497      |
| train_1/mu_loss           | 1.0                     |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -1.631541092020774e-77  |
| train_1/q_grads           | -0.026458721607923508   |
| train_1/q_grads_std       | 0.35097736120224        |
| train_1/q_loss            | 5.8028300760438185      |
| train_1/reward            | -1.55524105548684       |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.5758056640625         |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -1.55524105548684       |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 32
Time for epoch 32: 457.36. Rollout time: 193.48, Training time: 263.79
Evaluating epoch 32
Data_dir: data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102
-------------------------------------------------------
| epoch                     | 32                      |
| policy/steps              | 3006485.0               |
| test/episodes             | 825.0                   |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -26.99999999999834      |
| test_1/avg_q              | -8.23663122102671e-10   |
| test_1/n_subgoals         | 675.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 3300.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -26.99999999999997      |
| train_0/current_q         | -10.142864703716963     |
| train_0/fw_bonus          | -0.9994192913174629     |
| train_0/fw_loss           | 0.0026809311180841177   |
| train_0/mu_grads          | -0.09844450373202562    |
| train_0/mu_grads_std      | 0.4944388955831528      |
| train_0/mu_loss           | 9.982073985747105       |
| train_0/next_q            | -9.96537077419064       |
| train_0/q_grads           | -0.02264117761515081    |
| train_0/q_grads_std       | 0.2696030095219612      |
| train_0/q_loss            | 0.19322588016711573     |
| train_0/reward            | -0.8871804392823833     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.1509765625            |
| train_0/target_q          | -10.294403548859261     |
| train_1/avg_q             | -2.8846134293885105e-10 |
| train_1/current_q         | -2.836951355455224e-11  |
| train_1/fw_bonus          | -0.9999398127198219     |
| train_1/fw_loss           | 0.00020591413722286233  |
| train_1/mu_grads          | 0.07927997410297394     |
| train_1/mu_grads_std      | 0.2680293321609497      |
| train_1/mu_loss           | 1.0                     |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -1.6248245795820729e-77 |
| train_1/q_grads           | -0.02645879713818431    |
| train_1/q_grads_std       | 0.35097751021385193     |
| train_1/q_loss            | 5.889243723784628       |
| train_1/reward            | -1.5648596461804118     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.657958984375          |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -1.5648596461804118     |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 33
Time for epoch 33: 443.54. Rollout time: 186.73, Training time: 256.72
Evaluating epoch 33
Data_dir: data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102
-------------------------------------------------------
| epoch                     | 33                      |
| policy/steps              | 3097610.0               |
| test/episodes             | 850.0                   |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -26.99999999955655      |
| test_1/avg_q              | -3.8138812051638806e-09 |
| test_1/n_subgoals         | 675.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 3400.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -26.999999967784845     |
| train_0/current_q         | -10.181784837607788     |
| train_0/fw_bonus          | -0.9995159655809402     |
| train_0/fw_loss           | 0.0022349213220877575   |
| train_0/mu_grads          | -0.10107034724205732    |
| train_0/mu_grads_std      | 0.4968505546450615      |
| train_0/mu_loss           | 9.992867189352562       |
| train_0/next_q            | -9.977642629234087      |
| train_0/q_grads           | -0.022915939707309006   |
| train_0/q_grads_std       | 0.2717506378889084      |
| train_0/q_loss            | 0.1784363858364496      |
| train_0/reward            | -0.8887299871144932     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.2354248046875         |
| train_0/target_q          | -10.33293052286327      |
| train_1/avg_q             | -5.594807724852328e-11  |
| train_1/current_q         | -2.4893548264907626e-11 |
| train_1/fw_bonus          | -0.9997926473617553     |
| train_1/fw_loss           | 0.0006468679719546344   |
| train_1/mu_grads          | 0.07927997410297394     |
| train_1/mu_grads_std      | 0.2680293321609497      |
| train_1/mu_loss           | 1.0                     |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -1.1349250640788795e-78 |
| train_1/q_grads           | -0.026459085196256636   |
| train_1/q_grads_std       | 0.35097792744636536     |
| train_1/q_loss            | 5.896286880077793       |
| train_1/reward            | -1.565957769897068      |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.533837890625          |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -1.565957769897068      |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 34
Time for epoch 34: 422.63. Rollout time: 171.35, Training time: 251.21
Evaluating epoch 34
Data_dir: data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102
-------------------------------------------------------
| epoch                     | 34                      |
| policy/steps              | 3188735.0               |
| test/episodes             | 875.0                   |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -26.999999857469938     |
| test_1/avg_q              | -6.339140216564351e-10  |
| test_1/n_subgoals         | 675.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 3500.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -26.999999998725894     |
| train_0/current_q         | -10.073304078042465     |
| train_0/fw_bonus          | -0.9996153771877289     |
| train_0/fw_loss           | 0.0017762743489583954   |
| train_0/mu_grads          | -0.09913075659424067    |
| train_0/mu_grads_std      | 0.5023529887199402      |
| train_0/mu_loss           | 9.917067616771456       |
| train_0/next_q            | -9.897439988413252      |
| train_0/q_grads           | -0.023796136816963555   |
| train_0/q_grads_std       | 0.2730694442987442      |
| train_0/q_loss            | 0.16543127908579874     |
| train_0/reward            | -0.8880293919966789     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.276708984375          |
| train_0/target_q          | -10.221514640035883     |
| train_1/avg_q             | -3.3564941653704786e-11 |
| train_1/current_q         | -1.00101732346484e-11   |
| train_1/fw_bonus          | -0.9999165087938309     |
| train_1/fw_loss           | 0.0002757345762802288   |
| train_1/mu_grads          | 0.07927997410297394     |
| train_1/mu_grads_std      | 0.2680293321609497      |
| train_1/mu_loss           | 1.0                     |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -9.254387274033037e-74  |
| train_1/q_grads           | -0.026459870859980583   |
| train_1/q_grads_std       | 0.35097894072532654     |
| train_1/q_loss            | 5.847825509441812       |
| train_1/reward            | -1.5596297676573159     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.544482421875          |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -1.5596297676573159     |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 35
Time for epoch 35: 518.93. Rollout time: 228.85, Training time: 289.98
Evaluating epoch 35
Data_dir: data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102
-------------------------------------------------------
| epoch                     | 35                      |
| policy/steps              | 3279860.0               |
| test/episodes             | 900.0                   |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -26.999998938482413     |
| test_1/avg_q              | -2.2056067362209833e-09 |
| test_1/n_subgoals         | 675.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 3600.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -26.999999836090467     |
| train_0/current_q         | -10.122294886175828     |
| train_0/fw_bonus          | -0.9995618671178818     |
| train_0/fw_loss           | 0.0020230580965289848   |
| train_0/mu_grads          | -0.09827194716781378    |
| train_0/mu_grads_std      | 0.5054370313882828      |
| train_0/mu_loss           | 9.935636632095441       |
| train_0/next_q            | -9.925205299102938      |
| train_0/q_grads           | -0.024630641331896186   |
| train_0/q_grads_std       | 0.27446695417165756     |
| train_0/q_loss            | 0.1522093857498527      |
| train_0/reward            | -0.8886972860025708     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.228125                |
| train_0/target_q          | -10.275983745747835     |
| train_1/avg_q             | -9.814611764480381e-11  |
| train_1/current_q         | -1.1184751430041943e-11 |
| train_1/fw_bonus          | -0.9999572619795799     |
| train_1/fw_loss           | 0.0001536402856800123   |
| train_1/mu_grads          | 0.07927997410297394     |
| train_1/mu_grads_std      | 0.2680293321609497      |
| train_1/mu_loss           | 1.0                     |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -2.9114829259762584e-79 |
| train_1/q_grads           | -0.026460468536242843   |
| train_1/q_grads_std       | 0.3509796433150768      |
| train_1/q_loss            | 5.976958144238587       |
| train_1/reward            | -1.5757491960932384     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.7070556640625         |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -1.5757491960932384     |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 36
Time for epoch 36: 497.26. Rollout time: 231.07, Training time: 266.08
Evaluating epoch 36
Data_dir: data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102
-------------------------------------------------------
| epoch                     | 36                      |
| policy/steps              | 3370985.0               |
| test/episodes             | 925.0                   |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -27.0                   |
| test_1/avg_q              | -3.830869858169668e-09  |
| test_1/n_subgoals         | 675.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 3700.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -26.99999999955213      |
| train_0/current_q         | -10.203595833285023     |
| train_0/fw_bonus          | -0.9984360828995704     |
| train_0/fw_loss           | 0.007217579934513196    |
| train_0/mu_grads          | -0.09965762980282307    |
| train_0/mu_grads_std      | 0.5074851155281067      |
| train_0/mu_loss           | 10.052202266616254      |
| train_0/next_q            | -10.0341383716106       |
| train_0/q_grads           | -0.024623970268294215   |
| train_0/q_grads_std       | 0.2769636273384094      |
| train_0/q_loss            | 0.21890688918468076     |
| train_0/reward            | -0.892802886878053      |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.2128662109375         |
| train_0/target_q          | -10.383901856121776     |
| train_1/avg_q             | -1.1314899622135691e-10 |
| train_1/current_q         | -1.7744156012287803e-11 |
| train_1/fw_bonus          | -0.9982462763786316     |
| train_1/fw_loss           | 0.005280384520301595    |
| train_1/mu_grads          | 0.07927997410297394     |
| train_1/mu_grads_std      | 0.2680293321609497      |
| train_1/mu_loss           | 1.0                     |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -1.4618428720174706e-76 |
| train_1/q_grads           | -0.0264624145347625     |
| train_1/q_grads_std       | 0.350981080532074       |
| train_1/q_loss            | 5.972044561921932       |
| train_1/reward            | -1.5759674179586        |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.0289794921875         |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -1.5759674179586        |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 37
Time for epoch 37: 413.20. Rollout time: 182.27, Training time: 230.78
Evaluating epoch 37
Data_dir: data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102
-------------------------------------------------------
| epoch                     | 37                      |
| policy/steps              | 3462110.0               |
| test/episodes             | 950.0                   |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -5.506900253792628e-16  |
| test_1/avg_q              | -5.57911491324078e-10   |
| test_1/n_subgoals         | 675.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 3800.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -8.91                   |
| train_0/current_q         | -1.383038939508766      |
| train_0/fw_bonus          | -0.9964007511734962     |
| train_0/fw_loss           | 0.016608671215362848    |
| train_0/mu_grads          | -0.09698652811348438    |
| train_0/mu_grads_std      | 0.5121912568807602      |
| train_0/mu_loss           | 1.2207591578380441      |
| train_0/next_q            | -1.2108692434326636     |
| train_0/q_grads           | -0.024768853653222324   |
| train_0/q_grads_std       | 0.2796003259718418      |
| train_0/q_loss            | 0.7573126670300944      |
| train_0/reward            | -0.899308627116261      |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.14833984375           |
| train_0/target_q          | -1.7460167646058848     |
| train_1/avg_q             | -2.3252658735048785e-10 |
| train_1/current_q         | -1.9296879113338357e-11 |
| train_1/fw_bonus          | -0.9984864547848702     |
| train_1/fw_loss           | 0.0045607068808749315   |
| train_1/mu_grads          | 0.07927997410297394     |
| train_1/mu_grads_std      | 0.2680293321609497      |
| train_1/mu_loss           | 1.0                     |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -6.960929484989113e-70  |
| train_1/q_grads           | -0.026469424832612276   |
| train_1/q_grads_std       | 0.3509819507598877      |
| train_1/q_loss            | 5.792857102086041       |
| train_1/reward            | -1.5552430149022256     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.2224365234375         |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -1.5552430149022256     |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 38
Time for epoch 38: 476.70. Rollout time: 231.65, Training time: 244.94
Evaluating epoch 38
Data_dir: data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102
-------------------------------------------------------
| epoch                     | 38                      |
| policy/steps              | 3553235.0               |
| test/episodes             | 975.0                   |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -27.0                   |
| test_1/avg_q              | -3.1195002613881155e-09 |
| test_1/n_subgoals         | 675.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 3900.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -10.260000076795016     |
| train_0/current_q         | -11.080815203895668     |
| train_0/fw_bonus          | -0.9944975808262825     |
| train_0/fw_loss           | 0.02539013563655317     |
| train_0/mu_grads          | -0.09617346115410327    |
| train_0/mu_grads_std      | 0.5167809024453163      |
| train_0/mu_loss           | 10.952632575074432      |
| train_0/next_q            | -10.939585978257565     |
| train_0/q_grads           | -0.027398473164066673   |
| train_0/q_grads_std       | 0.28120787292718885     |
| train_0/q_loss            | 0.7853648783944399      |
| train_0/reward            | -0.906542121397797      |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.0901123046875         |
| train_0/target_q          | -11.22321219048693      |
| train_1/avg_q             | -2.89489442024555e-10   |
| train_1/current_q         | -5.230641398549369e-11  |
| train_1/fw_bonus          | -0.9975156545639038     |
| train_1/fw_loss           | 0.00746960326214321     |
| train_1/mu_grads          | 0.07927997410297394     |
| train_1/mu_grads_std      | 0.2680293321609497      |
| train_1/mu_loss           | 1.0                     |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -3.894635318184913e-70  |
| train_1/q_grads           | -0.02649280889891088    |
| train_1/q_grads_std       | 0.35097571834921837     |
| train_1/q_loss            | 6.0675914752287285      |
| train_1/reward            | -1.5882551417831565     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.1798583984375         |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -1.5882551417831565     |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 39
Time for epoch 39: 651.42. Rollout time: 311.81, Training time: 339.51
Evaluating epoch 39
Data_dir: data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102
-----------------------------------------------------
| epoch                     | 39                    |
| policy/steps              | 3644360.0             |
| test/episodes             | 1000.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -26.99999999901888    |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 4000.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -27.0                 |
| train_0/current_q         | -11.02642728576753    |
| train_0/fw_bonus          | -0.992777806520462    |
| train_0/fw_loss           | 0.033325216034427285  |
| train_0/mu_grads          | -0.0971984026953578   |
| train_0/mu_grads_std      | 0.5209002137184143    |
| train_0/mu_loss           | 10.896406136212352    |
| train_0/next_q            | -10.89141620202382    |
| train_0/q_grads           | -0.026987539092078804 |
| train_0/q_grads_std       | 0.28229261562228203   |
| train_0/q_loss            | 0.849726123545205     |
| train_0/reward            | -0.9117647550861875   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0487548828125       |
| train_0/target_q          | -11.185277920843728   |
| train_1/avg_q             | -3.24052094591853     |
| train_1/current_q         | -26.999999920913144   |
| train_1/fw_bonus          | -0.9966441124677659   |
| train_1/fw_loss           | 0.010081041406374424  |
| train_1/mu_grads          | 0.07875271141529083   |
| train_1/mu_grads_std      | 0.2675131559371948    |
| train_1/mu_loss           | 28.0                  |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -27.0                 |
| train_1/q_grads           | -0.01899715345352888  |
| train_1/q_grads_std       | 0.35863795801997184   |
| train_1/q_loss            | 62.69143872906774     |
| train_1/reward            | -1.5851478758311714   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.10908203125         |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -24.55741496567494    |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 40
Time for epoch 40: 724.51. Rollout time: 335.89, Training time: 388.45
Evaluating epoch 40
Data_dir: data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102
------------------------------------------------------
| epoch                     | 40                     |
| policy/steps              | 3735485.0              |
| test/episodes             | 1025.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -27.0                  |
| test_1/avg_q              | -6.156307183751102e-07 |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 4100.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -27.0                  |
| train_0/current_q         | -11.267634020664193    |
| train_0/fw_bonus          | -0.9924254417419434    |
| train_0/fw_loss           | 0.0349510851316154     |
| train_0/mu_grads          | -0.10116203408688307   |
| train_0/mu_grads_std      | 0.5275142654776573     |
| train_0/mu_loss           | 11.11299120059612      |
| train_0/next_q            | -11.092450309763029    |
| train_0/q_grads           | -0.02774350824765861   |
| train_0/q_grads_std       | 0.2831858530640602     |
| train_0/q_loss            | 0.7886723539938438     |
| train_0/reward            | -0.9153395874294802    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.0622314453125        |
| train_0/target_q          | -11.412141033811352    |
| train_1/avg_q             | -9.804637886829488     |
| train_1/current_q         | -9.930874338799199e-09 |
| train_1/fw_bonus          | -0.9961239546537399    |
| train_1/fw_loss           | 0.011639588174875825   |
| train_1/mu_grads          | 0.07656017690896988    |
| train_1/mu_grads_std      | 0.26612988114356995    |
| train_1/mu_loss           | 1.0                    |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -2.276972618103296e-56 |
| train_1/q_grads           | -0.02507012099958956   |
| train_1/q_grads_std       | 0.35864509269595146    |
| train_1/q_loss            | 5.8967299718580195     |
| train_1/reward            | -1.5688031763071195    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0632568359375        |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -1.5688031763071195    |
------------------------------------------------------
Saving periodic policy to data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102/policy_40.pkl ...
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 41
Time for epoch 41: 701.27. Rollout time: 316.90, Training time: 384.24
Evaluating epoch 41
Data_dir: data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102
-------------------------------------------------------
| epoch                     | 41                      |
| policy/steps              | 3826610.0               |
| test/episodes             | 1050.0                  |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -26.999999999957463     |
| test_1/avg_q              | -1.8478459595807276e-07 |
| test_1/n_subgoals         | 675.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 4200.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -26.99999999999999      |
| train_0/current_q         | -11.399765099449182     |
| train_0/fw_bonus          | -0.9925049409270287     |
| train_0/fw_loss           | 0.034584278054535386    |
| train_0/mu_grads          | -0.10373068954795599    |
| train_0/mu_grads_std      | 0.5345659047365189      |
| train_0/mu_loss           | 11.24350292328521       |
| train_0/next_q            | -11.229764703476757     |
| train_0/q_grads           | -0.028675349475815893   |
| train_0/q_grads_std       | 0.28499611988663676     |
| train_0/q_loss            | 0.9421815833519604      |
| train_0/reward            | -0.9189087321763509     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.02431640625           |
| train_0/target_q          | -11.538613504853254     |
| train_1/avg_q             | -6.833345003146437e-07  |
| train_1/current_q         | -2.7233675845166074e-07 |
| train_1/fw_bonus          | -0.9946192935109138     |
| train_1/fw_loss           | 0.016148130339570344    |
| train_1/mu_grads          | 0.07656017690896988     |
| train_1/mu_grads_std      | 0.26612988114356995     |
| train_1/mu_loss           | 1.0                     |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -3.904421085479483e-43  |
| train_1/q_grads           | -0.025774925760924817   |
| train_1/q_grads_std       | 0.35896937921643257     |
| train_1/q_loss            | 6.043317413843208       |
| train_1/reward            | -1.586351429966453      |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.01923828125           |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -1.586351429966453      |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 42
Time for epoch 42: 875.75. Rollout time: 383.66, Training time: 491.83
Evaluating epoch 42
Data_dir: data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102
-------------------------------------------------------
| epoch                     | 42                      |
| policy/steps              | 3917735.0               |
| test/episodes             | 1075.0                  |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -26.999999999972736     |
| test_1/avg_q              | -4.484000124245728e-07  |
| test_1/n_subgoals         | 675.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 4300.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -26.99999999999511      |
| train_0/current_q         | -10.979787758738611     |
| train_0/fw_bonus          | -0.993966481089592      |
| train_0/fw_loss           | 0.027840573480352758    |
| train_0/mu_grads          | -0.10152467153966427    |
| train_0/mu_grads_std      | 0.54160146266222        |
| train_0/mu_loss           | 10.832120409627608      |
| train_0/next_q            | -10.814677818342215     |
| train_0/q_grads           | -0.02970893173478544    |
| train_0/q_grads_std       | 0.28631972074508666     |
| train_0/q_loss            | 0.6631731759983703      |
| train_0/reward            | -0.9098486461174616     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.06123046875           |
| train_0/target_q          | -11.128948861182431     |
| train_1/avg_q             | -4.757432599313197e-06  |
| train_1/current_q         | -3.6869437672371763e-06 |
| train_1/fw_bonus          | -0.9966110035777092     |
| train_1/fw_loss           | 0.01018025609664619     |
| train_1/mu_grads          | 0.07656017690896988     |
| train_1/mu_grads_std      | 0.26612988114356995     |
| train_1/mu_loss           | 1.0                     |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -3.2905217654946524e-35 |
| train_1/q_grads           | -0.026948132924735545   |
| train_1/q_grads_std       | 0.3597785755991936      |
| train_1/q_loss            | 6.062781286720336       |
| train_1/reward            | -1.5865512652977487     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.02978515625           |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -1.5865512652977487     |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 43
Time for epoch 43: 663.17. Rollout time: 308.16, Training time: 354.82
Evaluating epoch 43
Data_dir: data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102
-------------------------------------------------------
| epoch                     | 43                      |
| policy/steps              | 4008860.0               |
| test/episodes             | 1100.0                  |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -26.999999952296363     |
| test_1/avg_q              | -1.2482917482514592e-08 |
| test_1/n_subgoals         | 675.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 4400.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -26.99999999834974      |
| train_0/current_q         | -10.808951959795186     |
| train_0/fw_bonus          | -0.9949815213680268     |
| train_0/fw_loss           | 0.02315713339485228     |
| train_0/mu_grads          | -0.10093437619507313    |
| train_0/mu_grads_std      | 0.5466040879487991      |
| train_0/mu_loss           | 10.69586489416838       |
| train_0/next_q            | -10.677804975369689     |
| train_0/q_grads           | -0.030821697600185872   |
| train_0/q_grads_std       | 0.28709410354495046     |
| train_0/q_loss            | 0.5561199977197635      |
| train_0/reward            | -0.9021626560992445     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.0753662109375         |
| train_0/target_q          | -10.957401459183531     |
| train_1/avg_q             | -1.3787110343431512e-07 |
| train_1/current_q         | -6.478924349292896e-08  |
| train_1/fw_bonus          | -0.9960765093564987     |
| train_1/fw_loss           | 0.011781756777781992    |
| train_1/mu_grads          | 0.07656017690896988     |
| train_1/mu_grads_std      | 0.26612988114356995     |
| train_1/mu_loss           | 1.0                     |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -6.68480894887995e-40   |
| train_1/q_grads           | -0.028708114475011825   |
| train_1/q_grads_std       | 0.35984979197382927     |
| train_1/q_loss            | 6.0114006981559704      |
| train_1/reward            | -1.5794914682999661     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.046826171875          |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -1.5794914682999661     |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 44
Time for epoch 44: 612.61. Rollout time: 256.42, Training time: 356.01
Evaluating epoch 44
Data_dir: data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102
-------------------------------------------------------
| epoch                     | 44                      |
| policy/steps              | 4099985.0               |
| test/episodes             | 1125.0                  |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -26.999999999999318     |
| test_1/avg_q              | -1.0595389433929683e-07 |
| test_1/n_subgoals         | 675.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 4500.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -26.99999998488367      |
| train_0/current_q         | -10.434913774006493     |
| train_0/fw_bonus          | -0.9968481436371803     |
| train_0/fw_loss           | 0.014544399688020349    |
| train_0/mu_grads          | -0.10208409484475851    |
| train_0/mu_grads_std      | 0.549525935947895       |
| train_0/mu_loss           | 10.307701829074244      |
| train_0/next_q            | -10.292617163263714     |
| train_0/q_grads           | -0.03154691327363253    |
| train_0/q_grads_std       | 0.2882621519267559      |
| train_0/q_loss            | 0.4351862532879235      |
| train_0/reward            | -0.8928448502527317     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.1090576171875         |
| train_0/target_q          | -10.587621360474476     |
| train_1/avg_q             | -4.728152530433464e-09  |
| train_1/current_q         | -5.752117832384891e-08  |
| train_1/fw_bonus          | -0.9984872847795486     |
| train_1/fw_loss           | 0.004558220820035786    |
| train_1/mu_grads          | 0.07656017690896988     |
| train_1/mu_grads_std      | 0.26612988114356995     |
| train_1/mu_loss           | 1.0                     |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -1.2993882661968797e-39 |
| train_1/q_grads           | -0.02882375749759376    |
| train_1/q_grads_std       | 0.36010748967528344     |
| train_1/q_loss            | 6.054420193927974       |
| train_1/reward            | -1.5836005709148595     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.144921875             |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -1.5836005709148595     |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 45
Time for epoch 45: 2521.26. Rollout time: 323.28, Training time: 2197.86
Evaluating epoch 45
Data_dir: data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102
-------------------------------------------------------
| epoch                     | 45                      |
| policy/steps              | 4191110.0               |
| test/episodes             | 1150.0                  |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -27.0                   |
| test_1/avg_q              | -3.6417863828372767e-07 |
| test_1/n_subgoals         | 675.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 4600.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -26.999999999999922     |
| train_0/current_q         | -10.267401816890656     |
| train_0/fw_bonus          | -0.9971651762723923     |
| train_0/fw_loss           | 0.013081516581587493    |
| train_0/mu_grads          | -0.10536606255918742    |
| train_0/mu_grads_std      | 0.5553834348917007      |
| train_0/mu_loss           | 10.141743072337071      |
| train_0/next_q            | -10.125786769486883     |
| train_0/q_grads           | -0.032112876605242494   |
| train_0/q_grads_std       | 0.28973421081900597     |
| train_0/q_loss            | 0.37267432818415447     |
| train_0/reward            | -0.8893350777718296     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.0968505859375         |
| train_0/target_q          | -10.419208821428404     |
| train_1/avg_q             | -1.2349675316162416e-08 |
| train_1/current_q         | -1.7674100017150792e-06 |
| train_1/fw_bonus          | -0.9974930956959724     |
| train_1/fw_loss           | 0.0075371382990852      |
| train_1/mu_grads          | 0.07656017690896988     |
| train_1/mu_grads_std      | 0.26612988114356995     |
| train_1/mu_loss           | 1.0                     |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -1.4546282237280947e-25 |
| train_1/q_grads           | -0.030331827327609064   |
| train_1/q_grads_std       | 0.3629160299897194      |
| train_1/q_loss            | 5.889905195935006       |
| train_1/reward            | -1.5628048875150853     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.1073974609375         |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -1.5628048875150853     |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 46
Time for epoch 46: 788.21. Rollout time: 344.52, Training time: 443.55
Evaluating epoch 46
Data_dir: data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102
-------------------------------------------------------
| epoch                     | 46                      |
| policy/steps              | 4281525.0               |
| test/episodes             | 1175.0                  |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -26.999999999803844     |
| test_1/avg_q              | -8.419349938580651e-10  |
| test_1/n_subgoals         | 675.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 4700.0                  |
| train/success_rate        | 0.01                    |
| train_0/avg_q             | -26.999999999981135     |
| train_0/current_q         | -10.206890135821816     |
| train_0/fw_bonus          | -0.9977510392665863     |
| train_0/fw_loss           | 0.010378417710307985    |
| train_0/mu_grads          | -0.1073627568781376     |
| train_0/mu_grads_std      | 0.5587125793099403      |
| train_0/mu_loss           | 10.093936311128513      |
| train_0/next_q            | -10.085550763154602     |
| train_0/q_grads           | -0.03288569832220674    |
| train_0/q_grads_std       | 0.29173502773046495     |
| train_0/q_loss            | 0.39856793938967977     |
| train_0/reward            | -0.8873879819824652     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.0822265625            |
| train_0/target_q          | -10.352779831179578     |
| train_1/avg_q             | -2.6204875477821546e-07 |
| train_1/current_q         | -1.707927236117728e-08  |
| train_1/fw_bonus          | -0.9967630282044411     |
| train_1/fw_loss           | 0.009724742011167109    |
| train_1/mu_grads          | 0.07656017690896988     |
| train_1/mu_grads_std      | 0.26612988114356995     |
| train_1/mu_loss           | 1.0                     |
| train_1/n_subgoals        | 2674.0                  |
| train_1/next_q            | -3.895447815547855e-26  |
| train_1/q_grads           | -0.030020474968478084   |
| train_1/q_grads_std       | 0.36409653574228285     |
| train_1/q_loss            | 5.913183819384034       |
| train_1/reward            | -1.5667617173021426     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.1385986328125         |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -1.5667617173021426     |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 47
Time for epoch 47: 746.91. Rollout time: 307.84, Training time: 438.80
Evaluating epoch 47
Data_dir: data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102
-------------------------------------------------------
| epoch                     | 47                      |
| policy/steps              | 4372650.0               |
| test/episodes             | 1200.0                  |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -27.0                   |
| test_1/avg_q              | -6.813221185182601e-09  |
| test_1/n_subgoals         | 675.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 4800.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -26.999999999527596     |
| train_0/current_q         | -10.662134538024059     |
| train_0/fw_bonus          | -0.9974068209528923     |
| train_0/fw_loss           | 0.011966618523001672    |
| train_0/mu_grads          | -0.11104636248201132    |
| train_0/mu_grads_std      | 0.5610251873731613      |
| train_0/mu_loss           | 10.546656074194559      |
| train_0/next_q            | -10.52997877653183      |
| train_0/q_grads           | -0.031818043906241655   |
| train_0/q_grads_std       | 0.2932440310716629      |
| train_0/q_loss            | 0.4784034509140692      |
| train_0/reward            | -0.8975729409845371     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.0716064453125         |
| train_0/target_q          | -10.807059362758434     |
| train_1/avg_q             | -1.0708376889952824e-08 |
| train_1/current_q         | -7.22688419158851e-05   |
| train_1/fw_bonus          | -0.9958685234189033     |
| train_1/fw_loss           | 0.012404968519695104    |
| train_1/mu_grads          | 0.07656017690896988     |
| train_1/mu_grads_std      | 0.26612988114356995     |
| train_1/mu_loss           | 1.0                     |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -2.8088914406386293e-25 |
| train_1/q_grads           | -0.0302457423415035     |
| train_1/q_grads_std       | 0.3643029898405075      |
| train_1/q_loss            | 5.798722256061009       |
| train_1/reward            | -1.5550068623022526     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.094384765625          |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -1.5550068623022526     |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 48
Time for epoch 48: 614.20. Rollout time: 277.92, Training time: 336.16
Evaluating epoch 48
Data_dir: data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102
-------------------------------------------------------
| epoch                     | 48                      |
| policy/steps              | 4463775.0               |
| test/episodes             | 1225.0                  |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -26.999999999993534     |
| test_1/avg_q              | -1.1575559098454371e-10 |
| test_1/n_subgoals         | 675.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 4900.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -26.99999999955402      |
| train_0/current_q         | -11.012250462345275     |
| train_0/fw_bonus          | -0.9982994496822357     |
| train_0/fw_loss           | 0.007847947452683001    |
| train_0/mu_grads          | -0.11109861973673105    |
| train_0/mu_grads_std      | 0.565381395816803       |
| train_0/mu_loss           | 10.876904010415874      |
| train_0/next_q            | -10.844388052068044     |
| train_0/q_grads           | -0.031916594691574575   |
| train_0/q_grads_std       | 0.2949649080634117      |
| train_0/q_loss            | 0.45103427397151136     |
| train_0/reward            | -0.9114138022167027     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.07255859375           |
| train_0/target_q          | -11.156601384658694     |
| train_1/avg_q             | -2.787808138398677e-07  |
| train_1/current_q         | -7.68233418987384e-11   |
| train_1/fw_bonus          | -0.995900672674179      |
| train_1/fw_loss           | 0.012308672489598393    |
| train_1/mu_grads          | 0.07656017690896988     |
| train_1/mu_grads_std      | 0.26612988114356995     |
| train_1/mu_loss           | 1.0                     |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -1.077824093259596e-30  |
| train_1/q_grads           | -0.03047027736902237    |
| train_1/q_grads_std       | 0.3643206968903542      |
| train_1/q_loss            | 5.834702759039276       |
| train_1/reward            | -1.5604814056874603     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.0921142578125         |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -1.5604814056874603     |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 49
Time for epoch 49: 633.12. Rollout time: 270.37, Training time: 362.63
Evaluating epoch 49
Data_dir: data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102
-------------------------------------------------------
| epoch                     | 49                      |
| policy/steps              | 4554900.0               |
| test/episodes             | 1250.0                  |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -26.999999996876163     |
| test_1/avg_q              | -1.0909176182066293e-10 |
| test_1/n_subgoals         | 675.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 5000.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -26.999999999889912     |
| train_0/current_q         | -11.213615202010843     |
| train_0/fw_bonus          | -0.9984397381544113     |
| train_0/fw_loss           | 0.0072007363894954326   |
| train_0/mu_grads          | -0.11157794147729874    |
| train_0/mu_grads_std      | 0.5678841203451157      |
| train_0/mu_loss           | 11.042391053262746      |
| train_0/next_q            | -10.992216458635104     |
| train_0/q_grads           | -0.03206557473167777    |
| train_0/q_grads_std       | 0.296444234251976       |
| train_0/q_loss            | 0.4069257370504154      |
| train_0/reward            | -0.9200456152670086     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.0605224609375         |
| train_0/target_q          | -11.35439597846729      |
| train_1/avg_q             | -1.3760620792502232e-10 |
| train_1/current_q         | -7.533737626260028e-11  |
| train_1/fw_bonus          | -0.9969955950975418     |
| train_1/fw_loss           | 0.009027826553210615    |
| train_1/mu_grads          | 0.07656017690896988     |
| train_1/mu_grads_std      | 0.26612988114356995     |
| train_1/mu_loss           | 1.0                     |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -2.1712515240286628e-30 |
| train_1/q_grads           | -0.03047218415886164    |
| train_1/q_grads_std       | 0.36432253047823904     |
| train_1/q_loss            | 5.957194998370673       |
| train_1/reward            | -1.5761280523671304     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.0679443359375         |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -1.5761280523671304     |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 50
Time for epoch 50: 627.33. Rollout time: 270.65, Training time: 356.54
Evaluating epoch 50
Data_dir: data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102
-------------------------------------------------------
| epoch                     | 50                      |
| policy/steps              | 4646025.0               |
| test/episodes             | 1275.0                  |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -26.999984516825133     |
| test_1/avg_q              | -4.410866573907019e-11  |
| test_1/n_subgoals         | 675.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 5100.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -26.999990908947872     |
| train_0/current_q         | -11.403592151160359     |
| train_0/fw_bonus          | -0.998800340294838      |
| train_0/fw_loss           | 0.005536820890847593    |
| train_0/mu_grads          | -0.11656866576522588    |
| train_0/mu_grads_std      | 0.5705413773655892      |
| train_0/mu_loss           | 11.226169515999763      |
| train_0/next_q            | -11.165529491539402     |
| train_0/q_grads           | -0.03285727333277464    |
| train_0/q_grads_std       | 0.29742823615670205     |
| train_0/q_loss            | 0.3301892889972412      |
| train_0/reward            | -0.9256688617460895     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.0987548828125         |
| train_0/target_q          | -11.55149270769241      |
| train_1/avg_q             | -1.3861420965570886e-10 |
| train_1/current_q         | -5.2485284544039444e-11 |
| train_1/fw_bonus          | -0.9973058849573135     |
| train_1/fw_loss           | 0.00809813067317009     |
| train_1/mu_grads          | 0.07656017690896988     |
| train_1/mu_grads_std      | 0.26612988114356995     |
| train_1/mu_loss           | 1.0                     |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -3.353601797547219e-30  |
| train_1/q_grads           | -0.03047753358259797    |
| train_1/q_grads_std       | 0.3643273085355759      |
| train_1/q_loss            | 5.998994155811698       |
| train_1/reward            | -1.5821407878262108     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.0552978515625         |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -1.5821407878262108     |
-------------------------------------------------------
Saving periodic policy to data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102/policy_50.pkl ...
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 51
Time for epoch 51: 706.32. Rollout time: 300.45, Training time: 405.71
Evaluating epoch 51
Data_dir: data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102
-------------------------------------------------------
| epoch                     | 51                      |
| policy/steps              | 4737150.0               |
| test/episodes             | 1300.0                  |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -26.999999999514344     |
| test_1/avg_q              | -3.19841415474327e-10   |
| test_1/n_subgoals         | 675.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 5200.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -26.999995425722606     |
| train_0/current_q         | -11.06138151394357      |
| train_0/fw_bonus          | -0.9990195453166961     |
| train_0/fw_loss           | 0.004525384661974385    |
| train_0/mu_grads          | -0.11978911347687245    |
| train_0/mu_grads_std      | 0.568192757666111       |
| train_0/mu_loss           | 10.86370932575348       |
| train_0/next_q            | -10.815693448265476     |
| train_0/q_grads           | -0.03380288621410728    |
| train_0/q_grads_std       | 0.29896202832460406     |
| train_0/q_loss            | 0.2550393162842402      |
| train_0/reward            | -0.9190827784623252     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.1084228515625         |
| train_0/target_q          | -11.208845038531598     |
| train_1/avg_q             | -1.1292118701244336e-10 |
| train_1/current_q         | -6.846604305356286e-11  |
| train_1/fw_bonus          | -0.9982905074954033     |
| train_1/fw_loss           | 0.005147817690158263    |
| train_1/mu_grads          | 0.07656017690896988     |
| train_1/mu_grads_std      | 0.26612988114356995     |
| train_1/mu_loss           | 1.0                     |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -1.29247314156621e-30   |
| train_1/q_grads           | -0.03049133182503283    |
| train_1/q_grads_std       | 0.36433623880147936     |
| train_1/q_loss            | 5.821809415185443       |
| train_1/reward            | -1.5607112927697018     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.116259765625          |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -1.5607112927697018     |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 52
Time for epoch 52: 781.68. Rollout time: 343.62, Training time: 437.94
Evaluating epoch 52
Data_dir: data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102
-------------------------------------------------------
| epoch                     | 52                      |
| policy/steps              | 4828275.0               |
| test/episodes             | 1325.0                  |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -26.99999999999993      |
| test_1/avg_q              | -4.501259207288984e-10  |
| test_1/n_subgoals         | 675.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 5300.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -26.999999998910756     |
| train_0/current_q         | -10.834759268737189     |
| train_0/fw_bonus          | -0.9994798794388771     |
| train_0/fw_loss           | 0.002401363561511971    |
| train_0/mu_grads          | -0.12102946694940328    |
| train_0/mu_grads_std      | 0.5689125418663025      |
| train_0/mu_loss           | 10.639771061491805      |
| train_0/next_q            | -10.587886436013784     |
| train_0/q_grads           | -0.03390940371900797    |
| train_0/q_grads_std       | 0.30099601447582247     |
| train_0/q_loss            | 0.2019008504307965      |
| train_0/reward            | -0.9119817411905388     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.1858154296875         |
| train_0/target_q          | -10.976821841046982     |
| train_1/avg_q             | -1.1235626542681653e-10 |
| train_1/current_q         | -8.00025980741738e-11   |
| train_1/fw_bonus          | -0.9987957745790481     |
| train_1/fw_loss           | 0.00363384778611362     |
| train_1/mu_grads          | 0.07656017690896988     |
| train_1/mu_grads_std      | 0.26612988114356995     |
| train_1/mu_loss           | 1.0                     |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -1.0918986884036304e-32 |
| train_1/q_grads           | -0.030533595383167265   |
| train_1/q_grads_std       | 0.36434349939227106     |
| train_1/q_loss            | 5.9041076693163905      |
| train_1/reward            | -1.5698609968298114     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.1876708984375         |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -1.5698609968298114     |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 53
Time for epoch 53: 623.39. Rollout time: 279.45, Training time: 343.83
Evaluating epoch 53
Data_dir: data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102
-------------------------------------------------------
| epoch                     | 53                      |
| policy/steps              | 4919400.0               |
| test/episodes             | 1350.0                  |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -27.0                   |
| test_1/avg_q              | -5.96871446521834e-10   |
| test_1/n_subgoals         | 675.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 5400.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -26.99999999990487      |
| train_0/current_q         | -10.40367440167438      |
| train_0/fw_bonus          | -0.9995658949017525     |
| train_0/fw_loss           | 0.0020045658078743146   |
| train_0/mu_grads          | -0.12332584373652936    |
| train_0/mu_grads_std      | 0.5711692243814468      |
| train_0/mu_loss           | 10.216642665820524      |
| train_0/next_q            | -10.187356039910645     |
| train_0/q_grads           | -0.03339464878663421    |
| train_0/q_grads_std       | 0.3051400236785412      |
| train_0/q_loss            | 0.18990329351379526     |
| train_0/reward            | -0.897184178463067      |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.2099365234375         |
| train_0/target_q          | -10.551524028080113     |
| train_1/avg_q             | -1.7488453422476913e-10 |
| train_1/current_q         | -1.2362296901827163e-10 |
| train_1/fw_bonus          | -0.9993003487586976     |
| train_1/fw_loss           | 0.0021219924892648123   |
| train_1/mu_grads          | 0.07656017690896988     |
| train_1/mu_grads_std      | 0.26612988114356995     |
| train_1/mu_loss           | 1.0                     |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -3.4775553094367994e-33 |
| train_1/q_grads           | -0.030640684347599745   |
| train_1/q_grads_std       | 0.3643277928233147      |
| train_1/q_loss            | 6.045219847957464       |
| train_1/reward            | -1.585760660644155      |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.41005859375           |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -1.585760660644155      |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 54
Time for epoch 54: 5831.95. Rollout time: 4327.20, Training time: 1504.61
Evaluating epoch 54
Data_dir: data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102
-------------------------------------------------------
| epoch                     | 54                      |
| policy/steps              | 5010525.0               |
| test/episodes             | 1375.0                  |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -26.99999999999999      |
| test_1/avg_q              | -4.1855271048976386e-07 |
| test_1/n_subgoals         | 675.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 5500.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -27.0                   |
| train_0/current_q         | -10.130654528931558     |
| train_0/fw_bonus          | -0.9995437785983086     |
| train_0/fw_loss           | 0.0021066023473395035   |
| train_0/mu_grads          | -0.12440257761627435    |
| train_0/mu_grads_std      | 0.5729978963732719      |
| train_0/mu_loss           | 9.938593025369235       |
| train_0/next_q            | -9.914939001353932      |
| train_0/q_grads           | -0.032950880285352466   |
| train_0/q_grads_std       | 0.30670093595981596     |
| train_0/q_loss            | 0.14896567940416336     |
| train_0/reward            | -0.888402944340487      |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.20732421875           |
| train_0/target_q          | -10.283023279474005     |
| train_1/avg_q             | -5.366766852073599e-05  |
| train_1/current_q         | -1.1066987522515676e-07 |
| train_1/fw_bonus          | -0.9997062027454376     |
| train_1/fw_loss           | 0.0009059118572622537   |
| train_1/mu_grads          | 0.07692602276802063     |
| train_1/mu_grads_std      | 0.2664904296398163      |
| train_1/mu_loss           | 1.0                     |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -5.599795679468605e-20  |
| train_1/q_grads           | -0.03187712840735912    |
| train_1/q_grads_std       | 0.3661253906786442      |
| train_1/q_loss            | 5.85409644711951        |
| train_1/reward            | -1.5624993761011865     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.469384765625          |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -1.5624993761011865     |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 55
Time for epoch 55: 757.47. Rollout time: 324.10, Training time: 433.15
Evaluating epoch 55
Data_dir: data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102
-------------------------------------------------------
| epoch                     | 55                      |
| policy/steps              | 5101650.0               |
| test/episodes             | 1400.0                  |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -26.999999999999684     |
| test_1/avg_q              | -2.778143389328229e-07  |
| test_1/n_subgoals         | 675.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 5600.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -26.999999999999876     |
| train_0/current_q         | -10.040882160778915     |
| train_0/fw_bonus          | -0.9995246052742004     |
| train_0/fw_loss           | 0.002195042392122559    |
| train_0/mu_grads          | -0.12548636384308337    |
| train_0/mu_grads_std      | 0.577069553732872       |
| train_0/mu_loss           | 9.84047699767596        |
| train_0/next_q            | -9.825325180036527      |
| train_0/q_grads           | -0.032357157487422225   |
| train_0/q_grads_std       | 0.3089795529842377      |
| train_0/q_loss            | 0.1293257482189186      |
| train_0/reward            | -0.881302387159667      |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.1964111328125         |
| train_0/target_q          | -10.195521871313238     |
| train_1/avg_q             | -1.020332232569197e-06  |
| train_1/current_q         | -1.3526486353342953e-07 |
| train_1/fw_bonus          | -0.9997890457510948     |
| train_1/fw_loss           | 0.0006576503685209901   |
| train_1/mu_grads          | 0.07692602276802063     |
| train_1/mu_grads_std      | 0.2664904296398163      |
| train_1/mu_loss           | 1.0                     |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -9.722096996205779e-20  |
| train_1/q_grads           | -0.032036497723311186   |
| train_1/q_grads_std       | 0.3662868276238441      |
| train_1/q_loss            | 5.9463241886464875      |
| train_1/reward            | -1.5725407642792562     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.5271240234375         |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -1.5725407642792562     |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 56
Time for epoch 56: 748.10. Rollout time: 339.50, Training time: 408.39
Evaluating epoch 56
Data_dir: data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102
-------------------------------------------------------
| epoch                     | 56                      |
| policy/steps              | 5192775.0               |
| test/episodes             | 1425.0                  |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -26.99999999999994      |
| test_1/avg_q              | -6.994478915156099e-07  |
| test_1/n_subgoals         | 675.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 5700.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -26.99999999999982      |
| train_0/current_q         | -9.876352355047736      |
| train_0/fw_bonus          | -0.9994854122400284     |
| train_0/fw_loss           | 0.002375851944088936    |
| train_0/mu_grads          | -0.12443257048726082    |
| train_0/mu_grads_std      | 0.5801811322569848      |
| train_0/mu_loss           | 9.684557357118965       |
| train_0/next_q            | -9.674999518975278      |
| train_0/q_grads           | -0.03297847919166088    |
| train_0/q_grads_std       | 0.3120125688612461      |
| train_0/q_loss            | 0.12172414361587088     |
| train_0/reward            | -0.8772690912228427     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.19033203125           |
| train_0/target_q          | -10.026663912915343     |
| train_1/avg_q             | -3.941948964293128e-07  |
| train_1/current_q         | -2.646223764017684e-07  |
| train_1/fw_bonus          | -0.9998501598834991     |
| train_1/fw_loss           | 0.0004745344780531013   |
| train_1/mu_grads          | 0.07692602276802063     |
| train_1/mu_grads_std      | 0.2664904296398163      |
| train_1/mu_loss           | 1.0                     |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -1.1734912030081086e-18 |
| train_1/q_grads           | -0.032028420735150574   |
| train_1/q_grads_std       | 0.3672186478972435      |
| train_1/q_loss            | 6.096461895777396       |
| train_1/reward            | -1.5894261992099927     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.52177734375           |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -1.5894261992099927     |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 57
Time for epoch 57: 751.38. Rollout time: 330.59, Training time: 420.65
Evaluating epoch 57
Data_dir: data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102
-------------------------------------------------------
| epoch                     | 57                      |
| policy/steps              | 5283900.0               |
| test/episodes             | 1450.0                  |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -27.0                   |
| test_1/avg_q              | -1.245182303146975e-07  |
| test_1/n_subgoals         | 675.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 5800.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -26.99999999997166      |
| train_0/current_q         | -9.73010177763846       |
| train_0/fw_bonus          | -0.9995557487010955     |
| train_0/fw_loss           | 0.002051373428548686    |
| train_0/mu_grads          | -0.12288724202662707    |
| train_0/mu_grads_std      | 0.5830291584134102      |
| train_0/mu_loss           | 9.551119610399171       |
| train_0/next_q            | -9.544191868395965      |
| train_0/q_grads           | -0.033479817863553765   |
| train_0/q_grads_std       | 0.3147138148546219      |
| train_0/q_loss            | 0.12071300909625464     |
| train_0/reward            | -0.8726587480428861     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.207666015625          |
| train_0/target_q          | -9.883521915738926      |
| train_1/avg_q             | -1.0751310041887486e-05 |
| train_1/current_q         | -1.0379772695705977e-06 |
| train_1/fw_bonus          | -0.9999500632286071     |
| train_1/fw_loss           | 0.0001752106069943693   |
| train_1/mu_grads          | 0.07692602276802063     |
| train_1/mu_grads_std      | 0.2664904296398163      |
| train_1/mu_loss           | 1.0                     |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -1.2270072557623495e-21 |
| train_1/q_grads           | -0.033181232213973996   |
| train_1/q_grads_std       | 0.37132914140820505     |
| train_1/q_loss            | 6.168918709154988       |
| train_1/reward            | -1.5973502418026329     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.7029052734375         |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -1.5973502418026329     |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 58
Time for epoch 58: 665.35. Rollout time: 280.60, Training time: 384.57
Evaluating epoch 58
Data_dir: data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102
-----------------------------------------------------
| epoch                     | 58                    |
| policy/steps              | 5375025.0             |
| test/episodes             | 1475.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -26.999999999999456   |
| test_1/avg_q              | -24.226870049018338   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 5900.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.99999999997431    |
| train_0/current_q         | -9.804605691469414    |
| train_0/fw_bonus          | -0.9995120570063591   |
| train_0/fw_loss           | 0.002252963330829516  |
| train_0/mu_grads          | -0.12130863890051842  |
| train_0/mu_grads_std      | 0.5850010260939598    |
| train_0/mu_loss           | 9.627474906649326     |
| train_0/next_q            | -9.620756803096457    |
| train_0/q_grads           | -0.03466921765357256  |
| train_0/q_grads_std       | 0.31660408824682235   |
| train_0/q_loss            | 0.11429060036530399   |
| train_0/reward            | -0.8751040078685037   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.187158203125        |
| train_0/target_q          | -9.955210739945787    |
| train_1/avg_q             | -0.19935264914290896  |
| train_1/current_q         | -25.478718151537006   |
| train_1/fw_bonus          | -0.9994718566536903   |
| train_1/fw_loss           | 0.0016081377129012254 |
| train_1/mu_grads          | 0.07822014186531305   |
| train_1/mu_grads_std      | 0.26730935275554657   |
| train_1/mu_loss           | 27.67970429804466     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -26.402705518260813   |
| train_1/q_grads           | -0.04533348651602864  |
| train_1/q_grads_std       | 0.37894278168678286   |
| train_1/q_loss            | 62.35129635420778     |
| train_1/reward            | -1.5821539284821484   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.451318359375        |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -23.74102320039068    |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 59
Time for epoch 59: 759.95. Rollout time: 359.40, Training time: 400.32
Evaluating epoch 59
Data_dir: data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102
-------------------------------------------------------
| epoch                     | 59                      |
| policy/steps              | 5466150.0               |
| test/episodes             | 1500.0                  |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -27.0                   |
| test_1/avg_q              | -1.0522862503429456e-05 |
| test_1/n_subgoals         | 675.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 6000.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -27.0                   |
| train_0/current_q         | -10.050422690764517     |
| train_0/fw_bonus          | -0.9994547188282012     |
| train_0/fw_loss           | 0.0025175495335133745   |
| train_0/mu_grads          | -0.12082051672041416    |
| train_0/mu_grads_std      | 0.587613770365715       |
| train_0/mu_loss           | 9.856065540157825       |
| train_0/next_q            | -9.83684124585898       |
| train_0/q_grads           | -0.03478396097198129    |
| train_0/q_grads_std       | 0.3175675220787525      |
| train_0/q_loss            | 0.1403772279167821      |
| train_0/reward            | -0.8869201308276388     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.1805908203125         |
| train_0/target_q          | -10.203166208307044     |
| train_1/avg_q             | -7.125136353383013      |
| train_1/current_q         | -3.9802960594122735e-05 |
| train_1/fw_bonus          | -0.9985310643911361     |
| train_1/fw_loss           | 0.004427035737899132    |
| train_1/mu_grads          | 0.0770668014883995      |
| train_1/mu_grads_std      | 0.26845499873161316     |
| train_1/mu_loss           | 1.0                     |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -4.139942930074149e-27  |
| train_1/q_grads           | -0.039988563023507596   |
| train_1/q_grads_std       | 0.3977887131273746      |
| train_1/q_loss            | 6.016542780991547       |
| train_1/reward            | -1.5811614830818144     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.5811279296875         |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -1.5811614830818144     |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 60
Time for epoch 60: 764.88. Rollout time: 344.48, Training time: 420.18
Evaluating epoch 60
Data_dir: data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102
-------------------------------------------------------
| epoch                     | 60                      |
| policy/steps              | 5557275.0               |
| test/episodes             | 1525.0                  |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -26.999999999999993     |
| test_1/avg_q              | -5.157307225507903e-05  |
| test_1/n_subgoals         | 675.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 6100.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -26.99999999887747      |
| train_0/current_q         | -10.537752625892498     |
| train_0/fw_bonus          | -0.9993511945009231     |
| train_0/fw_loss           | 0.002995230208034627    |
| train_0/mu_grads          | -0.1246650880202651     |
| train_0/mu_grads_std      | 0.5911466538906097      |
| train_0/mu_loss           | 10.342247936416713      |
| train_0/next_q            | -10.304331661395674     |
| train_0/q_grads           | -0.034996317233890295   |
| train_0/q_grads_std       | 0.3190939299762249      |
| train_0/q_loss            | 0.2171211362381548      |
| train_0/reward            | -0.9035417044382484     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.182275390625          |
| train_0/target_q          | -10.684751281514293     |
| train_1/avg_q             | -1.8061186663887833e-05 |
| train_1/current_q         | -3.4563722435604016e-05 |
| train_1/fw_bonus          | -0.9974119529128075     |
| train_1/fw_loss           | 0.007780322071630508    |
| train_1/mu_grads          | 0.0770668014883995      |
| train_1/mu_grads_std      | 0.26845499873161316     |
| train_1/mu_loss           | 1.0                     |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -8.079662053909237e-27  |
| train_1/q_grads           | -0.04020361891016364    |
| train_1/q_grads_std       | 0.39805879518389703     |
| train_1/q_loss            | 5.925393153091547       |
| train_1/reward            | -1.5712218011482038     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.3443115234375         |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -1.5712218011482038     |
-------------------------------------------------------
Saving periodic policy to data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102/policy_60.pkl ...
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 61
Time for epoch 61: 711.87. Rollout time: 312.67, Training time: 399.06
Evaluating epoch 61
Data_dir: data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102
------------------------------------------------------
| epoch                     | 61                     |
| policy/steps              | 5648400.0              |
| test/episodes             | 1550.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -26.999999999999957    |
| test_1/avg_q              | -8.084335244480317e-05 |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 6200.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -26.999999999999986    |
| train_0/current_q         | -10.817105315704868    |
| train_0/fw_bonus          | -0.9993126302957535    |
| train_0/fw_loss           | 0.00317309636157006    |
| train_0/mu_grads          | -0.126886098459363     |
| train_0/mu_grads_std      | 0.594754533469677      |
| train_0/mu_loss           | 10.592218271659766     |
| train_0/next_q            | -10.54749722208941     |
| train_0/q_grads           | -0.03509008698165417   |
| train_0/q_grads_std       | 0.3206807650625706     |
| train_0/q_loss            | 0.20134184661018728    |
| train_0/reward            | -0.9154756835712761    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.18642578125          |
| train_0/target_q          | -10.968818681709788    |
| train_1/avg_q             | -2.071000841515285e-05 |
| train_1/current_q         | -5.265814462282634e-05 |
| train_1/fw_bonus          | -0.9978048115968704    |
| train_1/fw_loss           | 0.006603114935569465   |
| train_1/mu_grads          | 0.0770668014883995     |
| train_1/mu_grads_std      | 0.26845499873161316    |
| train_1/mu_loss           | 1.0                    |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -1.686414339421425e-26 |
| train_1/q_grads           | -0.04048424353823066   |
| train_1/q_grads_std       | 0.3986541822552681     |
| train_1/q_loss            | 6.186383802046725      |
| train_1/reward            | -1.6029922907495346    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.2867431640625        |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -1.6029922907495346    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 62
Time for epoch 62: 720.54. Rollout time: 341.77, Training time: 378.65
Evaluating epoch 62
Data_dir: data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102
-------------------------------------------------------
| epoch                     | 62                      |
| policy/steps              | 5739525.0               |
| test/episodes             | 1575.0                  |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -27.0                   |
| test_1/avg_q              | -0.0002548181890116328  |
| test_1/n_subgoals         | 675.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 6300.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -27.0                   |
| train_0/current_q         | -10.815166204493838     |
| train_0/fw_bonus          | -0.9995089754462242     |
| train_0/fw_loss           | 0.002267153578577563    |
| train_0/mu_grads          | -0.12735122330486776    |
| train_0/mu_grads_std      | 0.5963888078927994      |
| train_0/mu_loss           | 10.582150726619941      |
| train_0/next_q            | -10.520952206609502     |
| train_0/q_grads           | -0.03515793606638908    |
| train_0/q_grads_std       | 0.32176450937986373     |
| train_0/q_loss            | 0.198322713912199       |
| train_0/reward            | -0.924625626867055      |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.19833984375           |
| train_0/target_q          | -10.957076063267545     |
| train_1/avg_q             | -0.9285249522113908     |
| train_1/current_q         | -1.5903407965573263e-05 |
| train_1/fw_bonus          | -0.9976103574037551     |
| train_1/fw_loss           | 0.007185832114191726    |
| train_1/mu_grads          | 0.08532164245843887     |
| train_1/mu_grads_std      | 0.2774612009525299      |
| train_1/mu_loss           | 1.0                     |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -1.0818565419133476e-19 |
| train_1/q_grads           | -0.03413531444966793    |
| train_1/q_grads_std       | 0.40601233169436457     |
| train_1/q_loss            | 5.951567518835733       |
| train_1/reward            | -1.5755576156698226     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.1072265625            |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -1.5755576156698226     |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 63
Time for epoch 63: 580.11. Rollout time: 261.72, Training time: 318.27
Evaluating epoch 63
Data_dir: data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102
-------------------------------------------------------
| epoch                     | 63                      |
| policy/steps              | 5830650.0               |
| test/episodes             | 1600.0                  |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -27.0                   |
| test_1/avg_q              | -0.0001451746234482324  |
| test_1/n_subgoals         | 675.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 6400.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -27.0                   |
| train_0/current_q         | -11.114059831389213     |
| train_0/fw_bonus          | -0.9993833065032959     |
| train_0/fw_loss           | 0.0028469590266468003   |
| train_0/mu_grads          | -0.12901236042380332    |
| train_0/mu_grads_std      | 0.5998999163508415      |
| train_0/mu_loss           | 10.877399610646702      |
| train_0/next_q            | -10.800958826278166     |
| train_0/q_grads           | -0.034886823035776615   |
| train_0/q_grads_std       | 0.3235553875565529      |
| train_0/q_loss            | 0.2130863238409506      |
| train_0/reward            | -0.932722299555462      |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.27734375              |
| train_0/target_q          | -11.25778465128919      |
| train_1/avg_q             | -4.983357469465537e-05  |
| train_1/current_q         | -1.910641732258756e-05  |
| train_1/fw_bonus          | -0.9954445943236351     |
| train_1/fw_loss           | 0.013675240450538695    |
| train_1/mu_grads          | 0.08532164245843887     |
| train_1/mu_grads_std      | 0.2774612009525299      |
| train_1/mu_loss           | 1.0                     |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -1.5496400161955585e-18 |
| train_1/q_grads           | -0.03417410245165229    |
| train_1/q_grads_std       | 0.40600442960858346     |
| train_1/q_loss            | 6.276664880228983       |
| train_1/reward            | -1.614692570923944      |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.025537109375          |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -1.614692570923944      |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 64
Time for epoch 64: 1602.81. Rollout time: 261.94, Training time: 1340.71
Evaluating epoch 64
Data_dir: data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102
-------------------------------------------------------
| epoch                     | 64                      |
| policy/steps              | 5921775.0               |
| test/episodes             | 1625.0                  |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -27.0                   |
| test_1/avg_q              | -0.0003620600575640585  |
| test_1/n_subgoals         | 675.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 6500.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -26.999999999999915     |
| train_0/current_q         | -11.200302097259359     |
| train_0/fw_bonus          | -0.9991467103362084     |
| train_0/fw_loss           | 0.003938682313309983    |
| train_0/mu_grads          | -0.1306760400533676     |
| train_0/mu_grads_std      | 0.6041693061590194      |
| train_0/mu_loss           | 10.979522520173749      |
| train_0/next_q            | -10.906232871361402     |
| train_0/q_grads           | -0.03602090161293745    |
| train_0/q_grads_std       | 0.32631404548883436     |
| train_0/q_loss            | 0.27970040458858286     |
| train_0/reward            | -0.9322701858385699     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.1978759765625         |
| train_0/target_q          | -11.352153264911184     |
| train_1/avg_q             | -2.5471261473556498e-05 |
| train_1/current_q         | -3.988317082020893e-05  |
| train_1/fw_bonus          | -0.9927743375301361     |
| train_1/fw_loss           | 0.021676301746629177    |
| train_1/mu_grads          | 0.08532164245843887     |
| train_1/mu_grads_std      | 0.2774612009525299      |
| train_1/mu_loss           | 1.0                     |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -2.2826214999298844e-17 |
| train_1/q_grads           | -0.03445133259519935    |
| train_1/q_grads_std       | 0.4059862449765205      |
| train_1/q_loss            | 6.021238531646796       |
| train_1/reward            | -1.5851264002289098     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.0255859375            |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -1.5851264002289098     |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 65
Time for epoch 65: 622.73. Rollout time: 285.58, Training time: 337.04
Evaluating epoch 65
Data_dir: data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102
-------------------------------------------------------
| epoch                     | 65                      |
| policy/steps              | 6012900.0               |
| test/episodes             | 1650.0                  |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -27.0                   |
| test_1/avg_q              | -0.00021619107503325285 |
| test_1/n_subgoals         | 675.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 6600.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -27.0                   |
| train_0/current_q         | -11.251646692953237     |
| train_0/fw_bonus          | -0.9986967116594314     |
| train_0/fw_loss           | 0.006014993943972513    |
| train_0/mu_grads          | -0.1337314423173666     |
| train_0/mu_grads_std      | 0.6099922940135002      |
| train_0/mu_loss           | 11.041108051595051      |
| train_0/next_q            | -10.979520217450448     |
| train_0/q_grads           | -0.036570845637470484   |
| train_0/q_grads_std       | 0.32992162704467776     |
| train_0/q_loss            | 0.3055915535718931      |
| train_0/reward            | -0.9313650622643763     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.1175537109375         |
| train_0/target_q          | -11.398437622165714     |
| train_1/avg_q             | -0.005536637251722786   |
| train_1/current_q         | -0.0012331361776712403  |
| train_1/fw_bonus          | -0.9916697829961777     |
| train_1/fw_loss           | 0.024985941220074893    |
| train_1/mu_grads          | 0.07940721046179533     |
| train_1/mu_grads_std      | 0.29027870297431946     |
| train_1/mu_loss           | 1.0000011863702252      |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -6.545641959471256e-07  |
| train_1/q_grads           | -0.035518509428948165   |
| train_1/q_grads_std       | 0.40755818262696264     |
| train_1/q_loss            | 5.956166408931441       |
| train_1/reward            | -1.5787804076397152     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.0279541015625         |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -1.578781049112596      |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 66
Time for epoch 66: 697.33. Rollout time: 308.89, Training time: 388.34
Evaluating epoch 66
Data_dir: data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102
-------------------------------------------------------
| epoch                     | 66                      |
| policy/steps              | 6104025.0               |
| test/episodes             | 1675.0                  |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -26.999999999999023     |
| test_1/avg_q              | -1.1395804138985906e-07 |
| test_1/n_subgoals         | 675.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 6700.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -26.999999999999574     |
| train_0/current_q         | -11.160382122135635     |
| train_0/fw_bonus          | -0.9983879461884498     |
| train_0/fw_loss           | 0.007439668872393668    |
| train_0/mu_grads          | -0.13522772192955018    |
| train_0/mu_grads_std      | 0.6166123151779175      |
| train_0/mu_loss           | 10.983498621206854      |
| train_0/next_q            | -10.919371003000496     |
| train_0/q_grads           | -0.03672362640500069    |
| train_0/q_grads_std       | 0.33339668735861777     |
| train_0/q_loss            | 0.3483243335198581      |
| train_0/reward            | -0.927672793250531      |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.0738037109375         |
| train_0/target_q          | -11.29964078759086      |
| train_1/avg_q             | -1.0747914336578348     |
| train_1/current_q         | -1.3155330237106321e-07 |
| train_1/fw_bonus          | -0.9913516148924828     |
| train_1/fw_loss           | 0.025939252972602845    |
| train_1/mu_grads          | 0.08132550865411758     |
| train_1/mu_grads_std      | 0.2931478023529053      |
| train_1/mu_loss           | 1.0000000000000053      |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -1.36112371079703e-14   |
| train_1/q_grads           | -0.04140732605010271    |
| train_1/q_grads_std       | 0.4121774137020111      |
| train_1/q_loss            | 5.830012764591135       |
| train_1/reward            | -1.5637285572825932     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.013330078125          |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -1.5637285572826065     |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 67
Time for epoch 67: 695.88. Rollout time: 325.15, Training time: 370.56
Evaluating epoch 67
Data_dir: data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102
-------------------------------------------------------
| epoch                     | 67                      |
| policy/steps              | 6195150.0               |
| test/episodes             | 1700.0                  |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -27.0                   |
| test_1/avg_q              | -4.772200716825927e-08  |
| test_1/n_subgoals         | 675.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 6800.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -27.0                   |
| train_0/current_q         | -11.105335155343854     |
| train_0/fw_bonus          | -0.998424744606018      |
| train_0/fw_loss           | 0.007269860233645886    |
| train_0/mu_grads          | -0.13818864114582538    |
| train_0/mu_grads_std      | 0.621870020031929       |
| train_0/mu_loss           | 10.95699133165768       |
| train_0/next_q            | -10.906395324458689     |
| train_0/q_grads           | -0.03761089788749814    |
| train_0/q_grads_std       | 0.33688501492142675     |
| train_0/q_loss            | 0.40824753128465013     |
| train_0/reward            | -0.9210231488490536     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.086572265625          |
| train_0/target_q          | -11.24045379103002      |
| train_1/avg_q             | -5.4808428132775916e-08 |
| train_1/current_q         | -1.4011160615907607e-07 |
| train_1/fw_bonus          | -0.992049190402031      |
| train_1/fw_loss           | 0.02384906318038702     |
| train_1/mu_grads          | 0.08132550865411758     |
| train_1/mu_grads_std      | 0.2931478023529053      |
| train_1/mu_loss           | 1.0000000000000147      |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -6.921762187729815e-15  |
| train_1/q_grads           | -0.04140886794775724    |
| train_1/q_grads_std       | 0.41217729449272156     |
| train_1/q_loss            | 6.015768611489063       |
| train_1/reward            | -1.5849365538771962     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.0188232421875         |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -1.5849365538772031     |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 68
Time for epoch 68: 971.94. Rollout time: 442.71, Training time: 528.98
Evaluating epoch 68
Data_dir: data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102
------------------------------------------------------
| epoch                     | 68                     |
| policy/steps              | 6286275.0              |
| test/episodes             | 1725.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -27.0                  |
| test_1/avg_q              | -0.0025266610317650953 |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 6900.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -26.999999999959083    |
| train_0/current_q         | -11.036741563805046    |
| train_0/fw_bonus          | -0.997381180524826     |
| train_0/fw_loss           | 0.01208487362600863    |
| train_0/mu_grads          | -0.13726293072104453   |
| train_0/mu_grads_std      | 0.6273778200149536     |
| train_0/mu_loss           | 10.904690803874159     |
| train_0/next_q            | -10.868000794581494    |
| train_0/q_grads           | -0.038227396365255115  |
| train_0/q_grads_std       | 0.3411157391965389     |
| train_0/q_loss            | 0.4544366892701916     |
| train_0/reward            | -0.9152844317177369    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.05126953125          |
| train_0/target_q          | -11.166748544860699    |
| train_1/avg_q             | -4.534323759688024e-05 |
| train_1/current_q         | -0.0032331041444016383 |
| train_1/fw_bonus          | -0.9894062116742134    |
| train_1/fw_loss           | 0.031768405437469484   |
| train_1/mu_grads          | 0.0813258757814765     |
| train_1/mu_grads_std      | 0.29314789175987244    |
| train_1/mu_loss           | 1.0000000419606547     |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -4.124114431759148e-08 |
| train_1/q_grads           | -0.03948746435344219   |
| train_1/q_grads_std       | 0.4147020414471626     |
| train_1/q_loss            | 5.955339107283361      |
| train_1/reward            | -1.5796340011889698    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0097900390625        |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -1.5796340416052908    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 69
Time for epoch 69: 838.43. Rollout time: 400.63, Training time: 437.65
Evaluating epoch 69
Data_dir: data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102
------------------------------------------------------
| epoch                     | 69                     |
| policy/steps              | 6377400.0              |
| test/episodes             | 1750.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -27.0                  |
| test_1/avg_q              | -5.611029364047797e-05 |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 7000.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -27.0                  |
| train_0/current_q         | -10.596965924004243    |
| train_0/fw_bonus          | -0.997817775607109     |
| train_0/fw_loss           | 0.010070432699285448   |
| train_0/mu_grads          | -0.13649058006703854   |
| train_0/mu_grads_std      | 0.6316003143787384     |
| train_0/mu_loss           | 10.457520376592063     |
| train_0/next_q            | -10.428716665186027    |
| train_0/q_grads           | -0.038615946937352416  |
| train_0/q_grads_std       | 0.3446116462349892     |
| train_0/q_loss            | 0.3947681603076426     |
| train_0/reward            | -0.9050649967684876    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.0710205078125        |
| train_0/target_q          | -10.730023165120118    |
| train_1/avg_q             | -1.1400740549110684    |
| train_1/current_q         | -0.0001049159205310212 |
| train_1/fw_bonus          | -0.9932490572333336    |
| train_1/fw_loss           | 0.020253827283158898   |
| train_1/mu_grads          | 0.0832328125834465     |
| train_1/mu_grads_std      | 0.292264461517334      |
| train_1/mu_loss           | 1.0000000014360269     |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -5.902157629210566e-10 |
| train_1/q_grads           | -0.04421033775433898   |
| train_1/q_grads_std       | 0.4179015226662159     |
| train_1/q_loss            | 5.9981205925806105     |
| train_1/reward            | -1.5812344035235584    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.02294921875          |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -1.5812344041019695    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 70
Time for epoch 70: 925.94. Rollout time: 420.68, Training time: 505.08
Evaluating epoch 70
Data_dir: data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102
------------------------------------------------------
| epoch                     | 70                     |
| policy/steps              | 6468525.0              |
| test/episodes             | 1775.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -27.0                  |
| test_1/avg_q              | -0.0010756405776213868 |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 7100.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -27.0                  |
| train_0/current_q         | -10.449297893505912    |
| train_0/fw_bonus          | -0.9979018911719322    |
| train_0/fw_loss           | 0.00968236691551283    |
| train_0/mu_grads          | -0.13867859840393065   |
| train_0/mu_grads_std      | 0.6358594298362732     |
| train_0/mu_loss           | 10.307661941222412     |
| train_0/next_q            | -10.290947498802765    |
| train_0/q_grads           | -0.0390156202018261    |
| train_0/q_grads_std       | 0.3489998124539852     |
| train_0/q_loss            | 0.34631846819687295    |
| train_0/reward            | -0.8953380364073382    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.0965576171875        |
| train_0/target_q          | -10.589427682921368    |
| train_1/avg_q             | -0.7087727114797242    |
| train_1/current_q         | -0.0007556506206336849 |
| train_1/fw_bonus          | -0.9963074341416359    |
| train_1/fw_loss           | 0.011089809227269144   |
| train_1/mu_grads          | 0.09803213179111481    |
| train_1/mu_grads_std      | 0.3133913576602936     |
| train_1/mu_loss           | 1.0000000000000016     |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -4.496251410946475e-18 |
| train_1/q_grads           | -0.045609393157064915  |
| train_1/q_grads_std       | 0.42333145886659623    |
| train_1/q_loss            | 5.87653198986077       |
| train_1/reward            | -1.5657730633145548    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.025244140625         |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -1.5657730633145548    |
------------------------------------------------------
Saving periodic policy to data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102/policy_70.pkl ...
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 71
Time for epoch 71: 1186.12. Rollout time: 563.92, Training time: 621.86
Evaluating epoch 71
Data_dir: data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102
------------------------------------------------------
| epoch                     | 71                     |
| policy/steps              | 6559650.0              |
| test/episodes             | 1800.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -27.0                  |
| test_1/avg_q              | -4.565405439540265     |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 7200.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -26.999999895249722    |
| train_0/current_q         | -10.310680755825114    |
| train_0/fw_bonus          | -0.9985079601407051    |
| train_0/fw_loss           | 0.006885919091291726   |
| train_0/mu_grads          | -0.13841153793036937   |
| train_0/mu_grads_std      | 0.6390930041670799     |
| train_0/mu_loss           | 10.154203246562492     |
| train_0/next_q            | -10.141218430771909    |
| train_0/q_grads           | -0.03981065982952714   |
| train_0/q_grads_std       | 0.35187444612383845    |
| train_0/q_loss            | 0.3039462524472055     |
| train_0/reward            | -0.8916299906617496    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.1762451171875        |
| train_0/target_q          | -10.463048542615788    |
| train_1/avg_q             | -0.0030608022625838944 |
| train_1/current_q         | -0.8788021479998152    |
| train_1/fw_bonus          | -0.9973123848438263    |
| train_1/fw_loss           | 0.00807863074587658    |
| train_1/mu_grads          | 0.09803877118974924    |
| train_1/mu_grads_std      | 0.3134023144841194     |
| train_1/mu_loss           | 1.0078301367271407     |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -0.004160789718839824  |
| train_1/q_grads           | -0.04441086072474718   |
| train_1/q_grads_std       | 0.42444428205490115    |
| train_1/q_loss            | 3.988035091167373      |
| train_1/reward            | -1.5651778952567839    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0504638671875        |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -1.5692554155846543    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 72
Time for epoch 72: 1110.82. Rollout time: 535.04, Training time: 575.44
Evaluating epoch 72
Data_dir: data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102
-----------------------------------------------------
| epoch                     | 72                    |
| policy/steps              | 6650775.0             |
| test/episodes             | 1825.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -26.99999999998992    |
| test_1/avg_q              | -6.94712070608376     |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 7300.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.999999999994355   |
| train_0/current_q         | -10.349313680534163   |
| train_0/fw_bonus          | -0.9977479472756385   |
| train_0/fw_loss           | 0.010392654477618635  |
| train_0/mu_grads          | -0.1401852171868086   |
| train_0/mu_grads_std      | 0.642426386475563     |
| train_0/mu_loss           | 10.19754908460941     |
| train_0/next_q            | -10.177874272842825   |
| train_0/q_grads           | -0.03990605194121599  |
| train_0/q_grads_std       | 0.3542029678821564    |
| train_0/q_loss            | 0.353050076419798     |
| train_0/reward            | -0.8929825913866807   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.161572265625        |
| train_0/target_q          | -10.502282943502923   |
| train_1/avg_q             | -4.308708503689209    |
| train_1/current_q         | -1.5574981990700962   |
| train_1/fw_bonus          | -0.9963053241372108   |
| train_1/fw_loss           | 0.01109616799512878   |
| train_1/mu_grads          | 0.0990920839831233    |
| train_1/mu_grads_std      | 0.3418846748769283    |
| train_1/mu_loss           | 1.004762125000743     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -0.002345437831819842 |
| train_1/q_grads           | -0.04376808675006032  |
| train_1/q_grads_std       | 0.43134092539548874   |
| train_1/q_loss            | 0.2765489413152662    |
| train_1/reward            | -1.5628478880986223   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0243896484375       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -1.5648415466800842   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/UR5ReacherEnv-v1/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|102/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 73
