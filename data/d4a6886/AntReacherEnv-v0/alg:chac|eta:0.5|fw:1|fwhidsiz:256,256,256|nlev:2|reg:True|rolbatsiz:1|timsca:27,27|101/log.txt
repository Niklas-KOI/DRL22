Starting process id: 9004
T: 700
alg: chac
algorithm: src.chac
atomic_noise: 0.2
base_logdir: data
batch_size: 1024
bind_core: 0
buffer_size: 500
chac_params: {}
early_stop_data_column: test/success_rate
early_stop_threshold: 100.0
env_name: AntReacherEnv-v0
eta: 0.5
fw: 1
fw_hidden_size: 256,256,256
fw_lr: 0.001
gamma: 0.9985714285714286
graph: 1
info: 
make_env: <function prepare_params.<locals>.make_env at 0x7fa588e223b0>
max_try_idx: 199
mu_hidden_size: 64
mu_lr: 0.001
n_episodes: 100
n_levels: 2
n_pre_episodes: 30
n_test_rollouts: 25
n_train_batches: 40
n_train_rollouts: 100
num_threads: 1
q_hidden_size: 64
q_lr: 0.001
random_action_perc: 0.3
regularization: True
render: 0
rollout_batch_size: 1
subgoal_noise: 0.2
subgoal_test_perc: 0.3
time_scales: 27,27
try_start_idx: 100
use_mpi: False
verbose: False

*** Warning ***
You are running src.chac with just a single MPI worker. This will work, but the HER experiments that we report in Plappert et al. (2018, https://arxiv.org/abs/1802.09464) were obtained with --num_cpu 19. This makes a significant difference and if you are looking to reproduce those results, be aware of this. Please also refer to https://github.com/openai/baselines/issues/314 for further details.
****************

dims: action = 8, subgoal = 5, end_goal = 3
subgoal_bounds: symmetric [11.75 11.75  0.5   3.    3.  ], offset [0.  0.  0.5 0.  0. ]
Running on CPU ...
Creating a CHAC agent

Hierarchy Level 0 with time scale 27
Actor(
  (fc1): Linear(in_features=34, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
  (fc4): Linear(in_features=64, out_features=8, bias=True)
)
Critic(
  (fc1): Linear(in_features=42, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
  (fc4): Linear(in_features=64, out_features=1, bias=True)
  (mse_loss): MSELoss()
)
ForwardModel(
  (mlp): Sequential(
    (0): Linear(in_features=37, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=256, bias=True)
    (5): ReLU()
    (6): Linear(in_features=256, out_features=29, bias=True)
    (7): Identity()
  )
  (mse_loss): MSELoss()
)

Hierarchy Level 1 with time scale 27
Actor(
  (fc1): Linear(in_features=32, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
  (fc4): Linear(in_features=64, out_features=5, bias=True)
)
Critic(
  (fc1): Linear(in_features=37, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
  (fc4): Linear(in_features=64, out_features=1, bias=True)
  (mse_loss): MSELoss()
)
ForwardModel(
  (mlp): Sequential(
    (0): Linear(in_features=34, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=256, bias=True)
    (5): ReLU()
    (6): Linear(in_features=256, out_features=29, bias=True)
    (7): Identity()
  )
  (mse_loss): MSELoss()
)
Training epoch 0
Time for epoch 0: 626.56. Rollout time: 321.71, Training time: 304.78
Evaluating epoch 0
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 0                     |
| policy/steps              | 91125.0               |
| test/episodes             | 25.0                  |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -5.052986065300981    |
| test_1/avg_q              | -11.48327708975845    |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 100.0                 |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -5.012121845659217    |
| train_0/current_q         | -1.5861172928479117   |
| train_0/fw_bonus          | -0.9965754583477974   |
| train_0/fw_loss           | 0.0320427009370178    |
| train_0/mu_grads          | -0.014034394710324704 |
| train_0/mu_grads_std      | 0.15338456854224206   |
| train_0/mu_loss           | 1.4086149412559306    |
| train_0/next_q            | -1.4005471576042094   |
| train_0/q_grads           | -0.004129016911610961 |
| train_0/q_grads_std       | 0.11860734820365906   |
| train_0/q_loss            | 0.5425063342028432    |
| train_0/reward            | -0.7549649776919978   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0005859375          |
| train_0/target_q          | -1.6915956132414025   |
| train_1/avg_q             | -7.3542250708914665   |
| train_1/current_q         | -7.91587787528537     |
| train_1/fw_bonus          | -0.9945215493440628   |
| train_1/fw_loss           | 0.06503210151568055   |
| train_1/mu_grads          | -0.026278658397495748 |
| train_1/mu_grads_std      | 0.14455009512603284   |
| train_1/mu_loss           | 5.99220025926172      |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -7.64822180640726     |
| train_1/q_grads           | 0.0025062220520339906 |
| train_1/q_grads_std       | 0.10842520222067834   |
| train_1/q_loss            | 2.4627300858123276    |
| train_1/reward            | -2.0761181951609613   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0057861328125       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -7.8992017512914945   |
-----------------------------------------------------
Saving periodic policy to data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_0.pkl ...
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Training epoch 1
Time for epoch 1: 560.48. Rollout time: 311.05, Training time: 249.37
Evaluating epoch 1
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 1                     |
| policy/steps              | 182250.0              |
| test/episodes             | 50.0                  |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -26.999995552495168   |
| test_1/avg_q              | -12.620871795233521   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 200.0                 |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -13.154222078477659   |
| train_0/current_q         | -9.278342916503846    |
| train_0/fw_bonus          | -0.9982167318463325   |
| train_0/fw_loss           | 0.01686308770440519   |
| train_0/mu_grads          | -0.02148301820270717  |
| train_0/mu_grads_std      | 0.19083036109805107   |
| train_0/mu_loss           | 9.253700878266418     |
| train_0/next_q            | -9.238368824273314    |
| train_0/q_grads           | -0.004227137740235776 |
| train_0/q_grads_std       | 0.14270802550017833   |
| train_0/q_loss            | 0.4354023635049777    |
| train_0/reward            | -0.7539911778651003   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.004541015625        |
| train_0/target_q          | -9.356094122751614    |
| train_1/avg_q             | -12.78159104227637    |
| train_1/current_q         | -9.086331843643036    |
| train_1/fw_bonus          | -0.9929600834846497   |
| train_1/fw_loss           | 0.07930857297033071   |
| train_1/mu_grads          | -0.04539453536272049  |
| train_1/mu_grads_std      | 0.17858834564685822   |
| train_1/mu_loss           | 6.304968998151423     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -8.819597336580715    |
| train_1/q_grads           | -0.011024166294373573 |
| train_1/q_grads_std       | 0.12988534308969973   |
| train_1/q_loss            | 1.6949262912191394    |
| train_1/reward            | -2.1252398085111053   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.00361328125         |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -9.046855392118156    |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Training epoch 2
Time for epoch 2: 559.22. Rollout time: 317.88, Training time: 241.27
Evaluating epoch 2
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 2                     |
| policy/steps              | 273375.0              |
| test/episodes             | 75.0                  |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -25.78873725613227    |
| test_1/avg_q              | -13.154066569731947   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 300.0                 |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -23.800369765222015   |
| train_0/current_q         | -9.2127705980289      |
| train_0/fw_bonus          | -0.9985987693071365   |
| train_0/fw_loss           | 0.013329700566828251  |
| train_0/mu_grads          | -0.022284635109826922 |
| train_0/mu_grads_std      | 0.21477207504212856   |
| train_0/mu_loss           | 9.196449359417606     |
| train_0/next_q            | -9.200047282158213    |
| train_0/q_grads           | -0.004587202589027584 |
| train_0/q_grads_std       | 0.16189654245972634   |
| train_0/q_loss            | 0.3570589008839593    |
| train_0/reward            | -0.7502759485301794   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.009423828125        |
| train_0/target_q          | -9.359013174578305    |
| train_1/avg_q             | -13.36381367995415    |
| train_1/current_q         | -8.515081158733555    |
| train_1/fw_bonus          | -0.992411395907402    |
| train_1/fw_loss           | 0.08432532604783774   |
| train_1/mu_grads          | -0.04628324415534735  |
| train_1/mu_grads_std      | 0.21058058068156243   |
| train_1/mu_loss           | 6.156422304188983     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -8.130295002516874    |
| train_1/q_grads           | -0.014007216738536953 |
| train_1/q_grads_std       | 0.14453167840838432   |
| train_1/q_loss            | 0.7624749463940942    |
| train_1/reward            | -2.115468389275338    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.00263671875         |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -8.501560440354856    |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Training epoch 3
Time for epoch 3: 535.67. Rollout time: 294.37, Training time: 241.23
Evaluating epoch 3
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
----------------------------------------------------
| epoch                     | 3                    |
| policy/steps              | 364500.0             |
| test/episodes             | 100.0                |
| test/success_rate         | 0.0                  |
| test_0/avg_q              | -26.958403274537517  |
| test_1/avg_q              | -12.706297934855982  |
| test_1/n_subgoals         | 675.0                |
| test_1/subgoal_succ_rate  | 0.0                  |
| train/episodes            | 400.0                |
| train/success_rate        | 0.0                  |
| train_0/avg_q             | -23.29612498335379   |
| train_0/current_q         | -9.390314239821265   |
| train_0/fw_bonus          | -0.998706167936325   |
| train_0/fw_loss           | 0.012336471606977284 |
| train_0/mu_grads          | -0.01952634765766561 |
| train_0/mu_grads_std      | 0.22919817566871642  |
| train_0/mu_loss           | 9.330289898638114    |
| train_0/next_q            | -9.333150499478126   |
| train_0/q_grads           | -0.00579804580193013 |
| train_0/q_grads_std       | 0.17193811386823654  |
| train_0/q_loss            | 0.31993951055866965  |
| train_0/reward            | -0.7496948782965773  |
| train_0/reward_-0.0_frac  | 0.0                  |
| train_0/reward_-1.0_frac  | 0.0072998046875      |
| train_0/target_q          | -9.525667194989234   |
| train_1/avg_q             | -13.739852661654623  |
| train_1/current_q         | -8.053141492692099   |
| train_1/fw_bonus          | -0.9915685281157494  |
| train_1/fw_loss           | 0.0920317279174924   |
| train_1/mu_grads          | -0.05131389247253537 |
| train_1/mu_grads_std      | 0.23190404437482356  |
| train_1/mu_loss           | 5.935034982286066    |
| train_1/n_subgoals        | 2700.0               |
| train_1/next_q            | -7.564501485298581   |
| train_1/q_grads           | -0.01906187692657113 |
| train_1/q_grads_std       | 0.15819815695285797  |
| train_1/q_loss            | 0.8850363948421176   |
| train_1/reward            | -2.095674394749949   |
| train_1/reward_-0.0_frac  | 0.0                  |
| train_1/reward_-1.0_frac  | 0.0025390625         |
| train_1/reward_-27.0_frac | 0.0                  |
| train_1/subgoal_succ_rate | 0.0                  |
| train_1/target_q          | -8.096168338133984   |
----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 4
Time for epoch 4: 558.34. Rollout time: 313.96, Training time: 244.31
Evaluating epoch 4
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
------------------------------------------------------
| epoch                     | 4                      |
| policy/steps              | 455615.0               |
| test/episodes             | 125.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -26.637936250991963    |
| test_1/avg_q              | -13.87697554421484     |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 500.0                  |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -25.77832779207016     |
| train_0/current_q         | -9.60593608760853      |
| train_0/fw_bonus          | -0.998863185942173     |
| train_0/fw_loss           | 0.010884128254838287   |
| train_0/mu_grads          | -0.019033067300915717  |
| train_0/mu_grads_std      | 0.2600613057613373     |
| train_0/mu_loss           | 9.552016788366926      |
| train_0/next_q            | -9.553673339932278     |
| train_0/q_grads           | -0.005883206706494093  |
| train_0/q_grads_std       | 0.17488648369908333    |
| train_0/q_loss            | 0.38871726482679225    |
| train_0/reward            | -0.7515067928019562    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.020556640625         |
| train_0/target_q          | -9.761171254104257     |
| train_1/avg_q             | -13.6630593523209      |
| train_1/current_q         | -7.829835058500907     |
| train_1/fw_bonus          | -0.9908212676644326    |
| train_1/fw_loss           | 0.09886382557451726    |
| train_1/mu_grads          | -0.05263277729973197   |
| train_1/mu_grads_std      | 0.24298993982374667    |
| train_1/mu_loss           | 5.848391731887764      |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -7.251979364225514     |
| train_1/q_grads           | -0.02862542779184878   |
| train_1/q_grads_std       | 0.17531764768064023    |
| train_1/q_loss            | 0.5073370621368646     |
| train_1/reward            | -2.0928593648801326    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0022705078125        |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.00037037037037037035 |
| train_1/target_q          | -7.83224779655375      |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 5
Time for epoch 5: 543.73. Rollout time: 303.36, Training time: 240.29
Evaluating epoch 5
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
------------------------------------------------------
| epoch                     | 5                      |
| policy/steps              | 546714.0               |
| test/episodes             | 150.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -26.920643410434387    |
| test_1/avg_q              | -13.479677602183976    |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 600.0                  |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -26.862405367426906    |
| train_0/current_q         | -9.516212335405575     |
| train_0/fw_bonus          | -0.9989254981279373    |
| train_0/fw_loss           | 0.010307940538041294   |
| train_0/mu_grads          | -0.018788445740938187  |
| train_0/mu_grads_std      | 0.2775484211742878     |
| train_0/mu_loss           | 9.445796626914802      |
| train_0/next_q            | -9.445197318323242     |
| train_0/q_grads           | -0.006983624340500682  |
| train_0/q_grads_std       | 0.1789074070751667     |
| train_0/q_loss            | 0.3103957264613755     |
| train_0/reward            | -0.7467283722158754    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.035205078125         |
| train_0/target_q          | -9.666642228208968     |
| train_1/avg_q             | -13.965607513051687    |
| train_1/current_q         | -7.81210482986595      |
| train_1/fw_bonus          | -0.9900862976908684    |
| train_1/fw_loss           | 0.10558359790593386    |
| train_1/mu_grads          | -0.0523783047683537    |
| train_1/mu_grads_std      | 0.2541673518717289     |
| train_1/mu_loss           | 5.823135233445975      |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -7.251623019817369     |
| train_1/q_grads           | -0.03069950742647052   |
| train_1/q_grads_std       | 0.19150602184236049    |
| train_1/q_loss            | 0.4219165023651607     |
| train_1/reward            | -2.144383974037919     |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.002685546875         |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.00037037037037037035 |
| train_1/target_q          | -7.82221386358264      |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 6
Time for epoch 6: 737.74. Rollout time: 391.76, Training time: 345.87
Evaluating epoch 6
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 6                     |
| policy/steps              | 637839.0              |
| test/episodes             | 175.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -26.234666950096322   |
| test_1/avg_q              | -11.208782727397963   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 700.0                 |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.65566946629518    |
| train_0/current_q         | -9.529265859102011    |
| train_0/fw_bonus          | -0.9989495977759362   |
| train_0/fw_loss           | 0.010085065150633454  |
| train_0/mu_grads          | -0.018949497118592264 |
| train_0/mu_grads_std      | 0.28393415436148645   |
| train_0/mu_loss           | 9.433071558448571     |
| train_0/next_q            | -9.433554704689286    |
| train_0/q_grads           | -0.006340374262072146 |
| train_0/q_grads_std       | 0.18464600704610348   |
| train_0/q_loss            | 0.29142309328149957   |
| train_0/reward            | -0.7498043860003236   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0467041015625       |
| train_0/target_q          | -9.677356254483238    |
| train_1/avg_q             | -13.817613978410348   |
| train_1/current_q         | -9.506445606031985    |
| train_1/fw_bonus          | -0.9900201201438904   |
| train_1/fw_loss           | 0.10618862099945545   |
| train_1/mu_grads          | -0.05518177449703217  |
| train_1/mu_grads_std      | 0.2593634434044361    |
| train_1/mu_loss           | 5.4123984225018855    |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -9.349395743170868    |
| train_1/q_grads           | -0.041450966708362104 |
| train_1/q_grads_std       | 0.20518625974655152   |
| train_1/q_loss            | 3.2281354734506538    |
| train_1/reward            | -2.111208797524887    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0027099609375       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -9.568029394375163    |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 7
Time for epoch 7: 535.82. Rollout time: 300.06, Training time: 235.69
Evaluating epoch 7
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 7                     |
| policy/steps              | 728964.0              |
| test/episodes             | 200.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -26.996375634028688   |
| test_1/avg_q              | -13.33849383046516    |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 800.0                 |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -25.920894573777918   |
| train_0/current_q         | -9.491867330337744    |
| train_0/fw_bonus          | -0.9989383831620217   |
| train_0/fw_loss           | 0.010188776045106351  |
| train_0/mu_grads          | -0.019993544928729533 |
| train_0/mu_grads_std      | 0.2911834686994553    |
| train_0/mu_loss           | 9.400822527807447     |
| train_0/next_q            | -9.400477389468254    |
| train_0/q_grads           | -0.005068431177642196 |
| train_0/q_grads_std       | 0.19449688196182252   |
| train_0/q_loss            | 0.2832971311801246    |
| train_0/reward            | -0.7506654782839177   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0328125             |
| train_0/target_q          | -9.635578462520774    |
| train_1/avg_q             | -13.792542069531178   |
| train_1/current_q         | -8.070994234169735    |
| train_1/fw_bonus          | -0.990254656970501    |
| train_1/fw_loss           | 0.1040443830192089    |
| train_1/mu_grads          | -0.05472089434042573  |
| train_1/mu_grads_std      | 0.26476135179400445   |
| train_1/mu_loss           | 5.513651356613974     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -7.555264636553889    |
| train_1/q_grads           | -0.04194987704977393  |
| train_1/q_grads_std       | 0.21628154814243317   |
| train_1/q_loss            | 0.587144865771883     |
| train_1/reward            | -2.12450965199605     |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0021728515625       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -8.080271092295195    |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 8
Time for epoch 8: 553.89. Rollout time: 304.26, Training time: 249.51
Evaluating epoch 8
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 8                     |
| policy/steps              | 820089.0              |
| test/episodes             | 225.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -26.992016666650827   |
| test_1/avg_q              | -14.078884828726974   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 900.0                 |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.699774165481276   |
| train_0/current_q         | -9.625433365691727    |
| train_0/fw_bonus          | -0.9989316746592521   |
| train_0/fw_loss           | 0.010250676632858812  |
| train_0/mu_grads          | -0.02229467947036028  |
| train_0/mu_grads_std      | 0.30018370151519774   |
| train_0/mu_loss           | 9.545211985491258     |
| train_0/next_q            | -9.545717361181776    |
| train_0/q_grads           | -0.005799438792746514 |
| train_0/q_grads_std       | 0.201025103777647     |
| train_0/q_loss            | 0.3066695216258292    |
| train_0/reward            | -0.7514922100999684   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.02216796875         |
| train_0/target_q          | -9.761677068370833    |
| train_1/avg_q             | -13.97910975899716    |
| train_1/current_q         | -8.800092962716965    |
| train_1/fw_bonus          | -0.9902285620570183   |
| train_1/fw_loss           | 0.10428291503340006   |
| train_1/mu_grads          | -0.055369903519749644 |
| train_1/mu_grads_std      | 0.27218544855713844   |
| train_1/mu_loss           | 5.663899894250688     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -8.363163609835762    |
| train_1/q_grads           | -0.04692673645913601  |
| train_1/q_grads_std       | 0.22561900541186333   |
| train_1/q_loss            | 0.9766328279557062    |
| train_1/reward            | -2.1060273694238276   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0016845703125       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -8.816923654930866    |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 9
Time for epoch 9: 741.80. Rollout time: 412.16, Training time: 329.52
Evaluating epoch 9
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 9                     |
| policy/steps              | 911211.0              |
| test/episodes             | 250.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -26.999998386613157   |
| test_1/avg_q              | -13.758613649152583   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 1000.0                |
| train/success_rate        | 0.01                  |
| train_0/avg_q             | -26.972577595612666   |
| train_0/current_q         | -9.486827917768206    |
| train_0/fw_bonus          | -0.998982948064804    |
| train_0/fw_loss           | 0.009776625130325556  |
| train_0/mu_grads          | -0.02425819877535105  |
| train_0/mu_grads_std      | 0.30976256653666495   |
| train_0/mu_loss           | 9.3968558891455       |
| train_0/next_q            | -9.397660556850585    |
| train_0/q_grads           | -0.005212613346520812 |
| train_0/q_grads_std       | 0.2074868131428957    |
| train_0/q_loss            | 0.2834542307063207    |
| train_0/reward            | -0.7484538731703652   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0414794921875       |
| train_0/target_q          | -9.6205782903574      |
| train_1/avg_q             | -14.025059440954625   |
| train_1/current_q         | -9.06220094513864     |
| train_1/fw_bonus          | -0.9905543819069862   |
| train_1/fw_loss           | 0.1013040479272604    |
| train_1/mu_grads          | -0.05757259931415319  |
| train_1/mu_grads_std      | 0.27471868619322776   |
| train_1/mu_loss           | 6.011654319271506     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -8.679254759885664    |
| train_1/q_grads           | -0.048926559276878834 |
| train_1/q_grads_std       | 0.23756188824772834   |
| train_1/q_loss            | 0.8810837209970366    |
| train_1/reward            | -2.1100354878537475   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.001220703125        |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -9.059975115187655    |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 10
Time for epoch 10: 655.55. Rollout time: 385.05, Training time: 270.39
Evaluating epoch 10
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 10                    |
| policy/steps              | 1002336.0             |
| test/episodes             | 275.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -26.9972297972908     |
| test_1/avg_q              | -8.938627396450284    |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 1100.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.623012949446267   |
| train_0/current_q         | -9.615176472585388    |
| train_0/fw_bonus          | -0.998957522213459    |
| train_0/fw_loss           | 0.010011671483516693  |
| train_0/mu_grads          | -0.02492635096423328  |
| train_0/mu_grads_std      | 0.32259825319051744   |
| train_0/mu_loss           | 9.559966202595595     |
| train_0/next_q            | -9.553954210808204    |
| train_0/q_grads           | -0.007661897328216582 |
| train_0/q_grads_std       | 0.21528648883104323   |
| train_0/q_loss            | 0.34032373073185906   |
| train_0/reward            | -0.7501867716142442   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.026611328125        |
| train_0/target_q          | -9.757932674999282    |
| train_1/avg_q             | -13.310657439605162   |
| train_1/current_q         | -5.028256685964731    |
| train_1/fw_bonus          | -0.9904006257653236   |
| train_1/fw_loss           | 0.10270975809544325   |
| train_1/mu_grads          | -0.06118894135579467  |
| train_1/mu_grads_std      | 0.28011215254664423   |
| train_1/mu_loss           | 4.011652105372699     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -4.272416198180023    |
| train_1/q_grads           | -0.049688890017569064 |
| train_1/q_grads_std       | 0.24245042204856873   |
| train_1/q_loss            | 1.7147862714738715    |
| train_1/reward            | -2.11323009912885     |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0010986328125       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -5.382119256750287    |
-----------------------------------------------------
Saving periodic policy to data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_10.pkl ...
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 11
Time for epoch 11: 709.21. Rollout time: 437.63, Training time: 271.48
Evaluating epoch 11
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 11                    |
| policy/steps              | 1092574.0             |
| test/episodes             | 300.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -25.33383644860139    |
| test_1/avg_q              | -13.82670972914745    |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 1200.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -24.88507118772647    |
| train_0/current_q         | -9.692721927269606    |
| train_0/fw_bonus          | -0.9989988133311272   |
| train_0/fw_loss           | 0.009629798005335033  |
| train_0/mu_grads          | -0.026916521694511177 |
| train_0/mu_grads_std      | 0.3309340350329876    |
| train_0/mu_loss           | 9.633252216255016     |
| train_0/next_q            | -9.633649010383973    |
| train_0/q_grads           | -0.006703481427393854 |
| train_0/q_grads_std       | 0.22196823805570604   |
| train_0/q_loss            | 0.36364291443771857   |
| train_0/reward            | -0.7529295840904524   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.059130859375        |
| train_0/target_q          | -9.872956431696446    |
| train_1/avg_q             | -13.382359910237277   |
| train_1/current_q         | -9.61823022676738     |
| train_1/fw_bonus          | -0.9905500754714012   |
| train_1/fw_loss           | 0.10134335365146399   |
| train_1/mu_grads          | -0.06070450423285365  |
| train_1/mu_grads_std      | 0.2873482510447502    |
| train_1/mu_loss           | 6.643223838521183     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -9.318746941764749    |
| train_1/q_grads           | -0.04991962164640427  |
| train_1/q_grads_std       | 0.24387888461351395   |
| train_1/q_loss            | 1.4813835640250872    |
| train_1/reward            | -2.1078360730221903   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.001220703125        |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.014444444444444444  |
| train_1/target_q          | -9.621042632868932    |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 12
Time for epoch 12: 645.07. Rollout time: 399.49, Training time: 245.48
Evaluating epoch 12
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 12                    |
| policy/steps              | 1183510.0             |
| test/episodes             | 325.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -25.662560989568018   |
| test_1/avg_q              | -13.526167737601535   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 1300.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -25.61511100953073    |
| train_0/current_q         | -9.505228720995538    |
| train_0/fw_bonus          | -0.9990171730518341   |
| train_0/fw_loss           | 0.009460108238272369  |
| train_0/mu_grads          | -0.028958668978884817 |
| train_0/mu_grads_std      | 0.34176974669098853   |
| train_0/mu_loss           | 9.425710583687776     |
| train_0/next_q            | -9.425811893191726    |
| train_0/q_grads           | -0.007010598655324429 |
| train_0/q_grads_std       | 0.2248984720557928    |
| train_0/q_loss            | 0.33968011242135365   |
| train_0/reward            | -0.7509856330449111   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0406982421875       |
| train_0/target_q          | -9.645349033325328    |
| train_1/avg_q             | -14.038413516371241   |
| train_1/current_q         | -7.404268711046521    |
| train_1/fw_bonus          | -0.9907502576708793   |
| train_1/fw_loss           | 0.09951295685023069   |
| train_1/mu_grads          | -0.062213454768061636 |
| train_1/mu_grads_std      | 0.2941277541220188    |
| train_1/mu_loss           | 5.604988208492569     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -7.19009686057768     |
| train_1/q_grads           | -0.05059330994263291  |
| train_1/q_grads_std       | 0.24709734953939916   |
| train_1/q_loss            | 1.7901328171717856    |
| train_1/reward            | -2.1071671282101305   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0010498046875       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.004814814814814815  |
| train_1/target_q          | -7.7086832386765565   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 13
Time for epoch 13: 642.68. Rollout time: 402.93, Training time: 239.65
Evaluating epoch 13
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 13                    |
| policy/steps              | 1274587.0             |
| test/episodes             | 350.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -26.92036709031148    |
| test_1/avg_q              | -13.762424285820348   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 1400.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.56372598246526    |
| train_0/current_q         | -9.424266804940162    |
| train_0/fw_bonus          | -0.9989996537566185   |
| train_0/fw_loss           | 0.009622038551606238  |
| train_0/mu_grads          | -0.032795894984155895 |
| train_0/mu_grads_std      | 0.3502553038299084    |
| train_0/mu_loss           | 9.368107934078427     |
| train_0/next_q            | -9.351036065953272    |
| train_0/q_grads           | -0.006613629451021552 |
| train_0/q_grads_std       | 0.22941202633082866   |
| train_0/q_loss            | 0.46014888307061785   |
| train_0/reward            | -0.7547854725402431   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0384521484375       |
| train_0/target_q          | -9.530166534344854    |
| train_1/avg_q             | -14.259626423961755   |
| train_1/current_q         | -10.563318375267121   |
| train_1/fw_bonus          | -0.9903401419520378   |
| train_1/fw_loss           | 0.1032627260312438    |
| train_1/mu_grads          | -0.06399311888962984  |
| train_1/mu_grads_std      | 0.2961264573037624    |
| train_1/mu_loss           | 6.55115281613257      |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -10.678289593465484   |
| train_1/q_grads           | -0.0473732384853065   |
| train_1/q_grads_std       | 0.2511068589985371    |
| train_1/q_loss            | 1.7984712487067256    |
| train_1/reward            | -2.1217936087952696   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.001416015625        |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0007407407407407407 |
| train_1/target_q          | -10.599694710351638   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 14
Time for epoch 14: 640.05. Rollout time: 402.39, Training time: 237.59
Evaluating epoch 14
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
------------------------------------------------------
| epoch                     | 14                     |
| policy/steps              | 1365712.0              |
| test/episodes             | 375.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -26.99772412666764     |
| test_1/avg_q              | -12.621044705520054    |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 1500.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -26.684644864072535    |
| train_0/current_q         | -9.552860900807973     |
| train_0/fw_bonus          | -0.9989997819066048    |
| train_0/fw_loss           | 0.009620841895230114   |
| train_0/mu_grads          | -0.03351273415610194   |
| train_0/mu_grads_std      | 0.35996642112731936    |
| train_0/mu_loss           | 9.472032711695169      |
| train_0/next_q            | -9.466833997687583     |
| train_0/q_grads           | -0.0067653591511771085 |
| train_0/q_grads_std       | 0.23344216421246528    |
| train_0/q_loss            | 0.35776379795478674    |
| train_0/reward            | -0.7580326075541961    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.0588623046875        |
| train_0/target_q          | -9.690227303238895     |
| train_1/avg_q             | -14.112056356319943    |
| train_1/current_q         | -9.267583067818148     |
| train_1/fw_bonus          | -0.9905830949544907    |
| train_1/fw_loss           | 0.10104148089885712    |
| train_1/mu_grads          | -0.06362306009978055   |
| train_1/mu_grads_std      | 0.3020305514335632     |
| train_1/mu_loss           | 6.049012313686367      |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -8.956326189110735     |
| train_1/q_grads           | -0.04665928585454822   |
| train_1/q_grads_std       | 0.2523297518491745     |
| train_1/q_loss            | 1.296897337346602      |
| train_1/reward            | -2.12339213183659      |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.001513671875         |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -9.262740993138067     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 15
Time for epoch 15: 662.75. Rollout time: 411.61, Training time: 251.05
Evaluating epoch 15
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 15                    |
| policy/steps              | 1456837.0             |
| test/episodes             | 400.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -26.977536026649986   |
| test_1/avg_q              | -12.987816486363617   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 1600.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.839530380685726   |
| train_0/current_q         | -9.627069865108325    |
| train_0/fw_bonus          | -0.9989934027194977   |
| train_0/fw_loss           | 0.009679744951426983  |
| train_0/mu_grads          | -0.03039588490501046  |
| train_0/mu_grads_std      | 0.37292083352804184   |
| train_0/mu_loss           | 9.550159917308568     |
| train_0/next_q            | -9.545242788110134    |
| train_0/q_grads           | -0.005210213758982718 |
| train_0/q_grads_std       | 0.24016491137444973   |
| train_0/q_loss            | 0.4166626812665353    |
| train_0/reward            | -0.7605806926098012   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.044384765625        |
| train_0/target_q          | -9.755265365980245    |
| train_1/avg_q             | -13.735109125869661   |
| train_1/current_q         | -9.426727251357704    |
| train_1/fw_bonus          | -0.990695284307003    |
| train_1/fw_loss           | 0.10001573525369167   |
| train_1/mu_grads          | -0.06295693460851907  |
| train_1/mu_grads_std      | 0.3126705139875412    |
| train_1/mu_loss           | 5.90103657167016      |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -9.181714824270827    |
| train_1/q_grads           | -0.04684571530669927  |
| train_1/q_grads_std       | 0.2568890519440174    |
| train_1/q_loss            | 1.6213250050986125    |
| train_1/reward            | -2.1315781097502624   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0011474609375       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -9.444707125653293    |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 16
Time for epoch 16: 713.19. Rollout time: 444.62, Training time: 268.47
Evaluating epoch 16
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 16                    |
| policy/steps              | 1547962.0             |
| test/episodes             | 425.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -26.99842637487234    |
| test_1/avg_q              | -14.949874758367164   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 1700.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.98860045705703    |
| train_0/current_q         | -9.654720256072894    |
| train_0/fw_bonus          | -0.9990499690175056   |
| train_0/fw_loss           | 0.009156762482598424  |
| train_0/mu_grads          | -0.033623731788247825 |
| train_0/mu_grads_std      | 0.38177543580532075   |
| train_0/mu_loss           | 9.58149723349608      |
| train_0/next_q            | -9.573946276039583    |
| train_0/q_grads           | -0.005153939337469637 |
| train_0/q_grads_std       | 0.24367072619497776   |
| train_0/q_loss            | 0.36511501263641666   |
| train_0/reward            | -0.7621470518388378   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.089990234375        |
| train_0/target_q          | -9.795948132317822    |
| train_1/avg_q             | -13.903086917281058   |
| train_1/current_q         | -10.005903171751708   |
| train_1/fw_bonus          | -0.9911733239889144   |
| train_1/fw_loss           | 0.09564495105296374   |
| train_1/mu_grads          | -0.06431911829859019  |
| train_1/mu_grads_std      | 0.3144785687327385    |
| train_1/mu_loss           | 6.937347267906714     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -9.886953959079264    |
| train_1/q_grads           | -0.049835398141294715 |
| train_1/q_grads_std       | 0.26368706300854683   |
| train_1/q_loss            | 2.23043558990745      |
| train_1/reward            | -2.091558343076758    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0016845703125       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -9.998918567286584    |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 17
Time for epoch 17: 698.31. Rollout time: 439.24, Training time: 258.96
Evaluating epoch 17
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 17                    |
| policy/steps              | 1639087.0             |
| test/episodes             | 450.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -26.92831527944427    |
| test_1/avg_q              | -13.649792484203008   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 1800.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.908297036688435   |
| train_0/current_q         | -9.901490076543677    |
| train_0/fw_bonus          | -0.999018007516861    |
| train_0/fw_loss           | 0.009452330460771919  |
| train_0/mu_grads          | -0.03256903877481818  |
| train_0/mu_grads_std      | 0.3945888005197048    |
| train_0/mu_loss           | 9.87379519005191      |
| train_0/next_q            | -9.865304069678638    |
| train_0/q_grads           | -0.004952410282567143 |
| train_0/q_grads_std       | 0.2508533962070942    |
| train_0/q_loss            | 0.5075019708662852    |
| train_0/reward            | -0.7654635924845934   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0701416015625       |
| train_0/target_q          | -10.057772350861999   |
| train_1/avg_q             | -14.711819200637983   |
| train_1/current_q         | -9.788997439865247    |
| train_1/fw_bonus          | -0.9912940561771393   |
| train_1/fw_loss           | 0.0945411253720522    |
| train_1/mu_grads          | -0.0635991657152772   |
| train_1/mu_grads_std      | 0.3216504126787186    |
| train_1/mu_loss           | 7.423585985782276     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -9.621476242837502    |
| train_1/q_grads           | -0.04874216774478555  |
| train_1/q_grads_std       | 0.2637503981590271    |
| train_1/q_loss            | 1.523521492939005     |
| train_1/reward            | -2.110637262398086    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0019775390625       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -9.740531255746443    |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 18
Time for epoch 18: 710.46. Rollout time: 446.05, Training time: 264.30
Evaluating epoch 18
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 18                    |
| policy/steps              | 1730212.0             |
| test/episodes             | 475.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -26.99732371551139    |
| test_1/avg_q              | -13.056030931577526   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 1900.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.92596802168671    |
| train_0/current_q         | -10.01007645957219    |
| train_0/fw_bonus          | -0.9989640220999718   |
| train_0/fw_loss           | 0.009951685043051838  |
| train_0/mu_grads          | -0.029638563096523286 |
| train_0/mu_grads_std      | 0.4046840950846672    |
| train_0/mu_loss           | 9.94850163096741      |
| train_0/next_q            | -9.94087786730653     |
| train_0/q_grads           | -0.00535513071808964  |
| train_0/q_grads_std       | 0.25422787144780157   |
| train_0/q_loss            | 0.46235282967347613   |
| train_0/reward            | -0.7726021241403942   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.060546875           |
| train_0/target_q          | -10.169468130648358   |
| train_1/avg_q             | -14.045451004875906   |
| train_1/current_q         | -10.346980105472827   |
| train_1/fw_bonus          | -0.9906067222356796   |
| train_1/fw_loss           | 0.10082533732056617   |
| train_1/mu_grads          | -0.06097168866544962  |
| train_1/mu_grads_std      | 0.33041265308856965   |
| train_1/mu_loss           | 6.597742320110146     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -10.086601234800868   |
| train_1/q_grads           | -0.05092141916975379  |
| train_1/q_grads_std       | 0.2666598066687584    |
| train_1/q_loss            | 1.5377005862782407    |
| train_1/reward            | -2.112942526543338    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0015869140625       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -10.32747008862684    |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 19
Time for epoch 19: 729.47. Rollout time: 462.41, Training time: 266.96
Evaluating epoch 19
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 19                    |
| policy/steps              | 1821196.0             |
| test/episodes             | 500.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -26.992643841154585   |
| test_1/avg_q              | -13.275799332440279   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 2000.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.878540154029302   |
| train_0/current_q         | -10.010673056315127   |
| train_0/fw_bonus          | -0.9990081712603569   |
| train_0/fw_loss           | 0.00954323667101562   |
| train_0/mu_grads          | -0.03092317469418049  |
| train_0/mu_grads_std      | 0.41136572808027266   |
| train_0/mu_loss           | 9.93982473508804      |
| train_0/next_q            | -9.936422916378186    |
| train_0/q_grads           | -0.005000864109024406 |
| train_0/q_grads_std       | 0.2571793906390667    |
| train_0/q_loss            | 0.4597683139168992    |
| train_0/reward            | -0.7703659045459063   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0774169921875       |
| train_0/target_q          | -10.170848938054178   |
| train_1/avg_q             | -13.810046216290301   |
| train_1/current_q         | -9.368426653464683    |
| train_1/fw_bonus          | -0.9910859555006027   |
| train_1/fw_loss           | 0.09644377864897251   |
| train_1/mu_grads          | -0.06047877604141831  |
| train_1/mu_grads_std      | 0.33262980431318284   |
| train_1/mu_loss           | 6.243174340782015     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -9.047346118330571    |
| train_1/q_grads           | -0.05162540366873145  |
| train_1/q_grads_std       | 0.2695181183516979    |
| train_1/q_loss            | 1.1882893381179485    |
| train_1/reward            | -2.06177832932226     |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0018798828125       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0022222222222222222 |
| train_1/target_q          | -9.355741400227071    |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 20
Time for epoch 20: 720.60. Rollout time: 461.60, Training time: 258.91
Evaluating epoch 20
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 20                    |
| policy/steps              | 1912184.0             |
| test/episodes             | 525.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -24.560493219467098   |
| test_1/avg_q              | -13.19137602048423    |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 2100.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -25.301883753474804   |
| train_0/current_q         | -9.925382595452485    |
| train_0/fw_bonus          | -0.9990335792303086   |
| train_0/fw_loss           | 0.009308280097320676  |
| train_0/mu_grads          | -0.03169138636440039  |
| train_0/mu_grads_std      | 0.42230005413293836   |
| train_0/mu_loss           | 9.881657288665698     |
| train_0/next_q            | -9.867806728437916    |
| train_0/q_grads           | -0.004013638338074088 |
| train_0/q_grads_std       | 0.26121543943881986   |
| train_0/q_loss            | 0.5428305949066459    |
| train_0/reward            | -0.7715871028820402   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0895751953125       |
| train_0/target_q          | -10.07347739897396    |
| train_1/avg_q             | -13.919918175855683   |
| train_1/current_q         | -9.211694427038134    |
| train_1/fw_bonus          | -0.9908251166343689   |
| train_1/fw_loss           | 0.09882853496819735   |
| train_1/mu_grads          | -0.05996587574481964  |
| train_1/mu_grads_std      | 0.3388960547745228    |
| train_1/mu_loss           | 5.8332983394674525    |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -8.789203884665067    |
| train_1/q_grads           | -0.054244814161211255 |
| train_1/q_grads_std       | 0.27314490601420405   |
| train_1/q_loss            | 0.8329305084479273    |
| train_1/reward            | -2.065468716235773    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.002001953125        |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0025925925925925925 |
| train_1/target_q          | -9.225875562636944    |
-----------------------------------------------------
Saving periodic policy to data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_20.pkl ...
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 21
Time for epoch 21: 749.12. Rollout time: 471.28, Training time: 277.75
Evaluating epoch 21
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
------------------------------------------------------
| epoch                     | 21                     |
| policy/steps              | 2003298.0              |
| test/episodes             | 550.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -26.979262734516137    |
| test_1/avg_q              | -14.427871048866681    |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 2200.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -22.678527784851852    |
| train_0/current_q         | -10.025961520217043    |
| train_0/fw_bonus          | -0.998932284116745     |
| train_0/fw_loss           | 0.010245003108866512   |
| train_0/mu_grads          | -0.031649918667972086  |
| train_0/mu_grads_std      | 0.4364433065056801     |
| train_0/mu_loss           | 9.956604151459223      |
| train_0/next_q            | -9.952197758240393     |
| train_0/q_grads           | -0.0021671548078302295 |
| train_0/q_grads_std       | 0.26903700828552246    |
| train_0/q_loss            | 0.6420942850454371     |
| train_0/reward            | -0.77997152071257      |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.074853515625         |
| train_0/target_q          | -10.18213805691727     |
| train_1/avg_q             | -13.516206826411901    |
| train_1/current_q         | -11.660450621145015    |
| train_1/fw_bonus          | -0.9888268932700157    |
| train_1/fw_loss           | 0.11709845196455718    |
| train_1/mu_grads          | -0.06306544784456491   |
| train_1/mu_grads_std      | 0.3409819580614567     |
| train_1/mu_loss           | 7.8608801206654615     |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -11.590513709634774    |
| train_1/q_grads           | -0.0505000957287848    |
| train_1/q_grads_std       | 0.2764835447072983     |
| train_1/q_loss            | 2.670509483648441      |
| train_1/reward            | -2.1184368835442       |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.002197265625         |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.00037037037037037035 |
| train_1/target_q          | -11.602224462854938    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 22
Time for epoch 22: 761.90. Rollout time: 488.80, Training time: 273.00
Evaluating epoch 22
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 22                    |
| policy/steps              | 2093804.0             |
| test/episodes             | 575.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -16.212134978179463   |
| test_1/avg_q              | -13.553893544856951   |
| test_1/n_subgoals         | 4092.0                |
| test_1/subgoal_succ_rate  | 0.8677908113391984    |
| train/episodes            | 2300.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.51515178920077    |
| train_0/current_q         | -9.516323999492302    |
| train_0/fw_bonus          | -0.9988935783505439   |
| train_0/fw_loss           | 0.010603006673045456  |
| train_0/mu_grads          | -0.03263786621391773  |
| train_0/mu_grads_std      | 0.44616314619779585   |
| train_0/mu_loss           | 9.416449655378033     |
| train_0/next_q            | -9.40730328251405     |
| train_0/q_grads           | -0.002398457226809114 |
| train_0/q_grads_std       | 0.2733917377889156    |
| train_0/q_loss            | 1.2302151864795001    |
| train_0/reward            | -0.7900840023285127   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0696044921875       |
| train_0/target_q          | -9.645749760871452    |
| train_1/avg_q             | -14.527820415411146   |
| train_1/current_q         | -9.306709860350354    |
| train_1/fw_bonus          | -0.987583051621914    |
| train_1/fw_loss           | 0.12847088016569613   |
| train_1/mu_grads          | -0.06376392487436533  |
| train_1/mu_grads_std      | 0.34375392869114874   |
| train_1/mu_loss           | 7.137130788246054     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -9.119390227686807    |
| train_1/q_grads           | -0.05067200157791376  |
| train_1/q_grads_std       | 0.27751070335507394   |
| train_1/q_loss            | 1.6763716587119226    |
| train_1/reward            | -2.125334496429423    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.001611328125        |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.011111111111111112  |
| train_1/target_q          | -9.327124220109642    |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 23
Time for epoch 23: 661.46. Rollout time: 407.73, Training time: 253.63
Evaluating epoch 23
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
------------------------------------------------------
| epoch                     | 23                     |
| policy/steps              | 2182163.0              |
| test/episodes             | 600.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -26.999937636818686    |
| test_1/avg_q              | -13.907219147388947    |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 2400.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -23.673785382498746    |
| train_0/current_q         | -9.196098612170122     |
| train_0/fw_bonus          | -0.9988999471068383    |
| train_0/fw_loss           | 0.01054424319881946    |
| train_0/mu_grads          | -0.031575378589332105  |
| train_0/mu_grads_std      | 0.4539199940860271     |
| train_0/mu_loss           | 9.09669297240183       |
| train_0/next_q            | -9.04677885441526      |
| train_0/q_grads           | -0.0014942691806936637 |
| train_0/q_grads_std       | 0.2796210445463657     |
| train_0/q_loss            | 0.8786791321392198     |
| train_0/reward            | -0.7958855192111514    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.0677978515625        |
| train_0/target_q          | -9.306394290582864     |
| train_1/avg_q             | -13.901005264272465    |
| train_1/current_q         | -10.653287985074886    |
| train_1/fw_bonus          | -0.9870044678449631    |
| train_1/fw_loss           | 0.13376099839806557    |
| train_1/mu_grads          | -0.06488805580884219   |
| train_1/mu_grads_std      | 0.3481817603111267     |
| train_1/mu_loss           | 7.229804522976815      |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -10.660617202517319    |
| train_1/q_grads           | -0.05117830950766802   |
| train_1/q_grads_std       | 0.27677899822592733    |
| train_1/q_loss            | 2.095794583355057      |
| train_1/reward            | -2.0946496908080006    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0011474609375        |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.04666666666666667    |
| train_1/target_q          | -10.641399068998972    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 24
Time for epoch 24: 657.03. Rollout time: 408.95, Training time: 247.98
Evaluating epoch 24
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 24                    |
| policy/steps              | 2272330.0             |
| test/episodes             | 625.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -25.29450358127243    |
| test_1/avg_q              | -14.271125269956562   |
| test_1/n_subgoals         | 686.0                 |
| test_1/subgoal_succ_rate  | 0.016034985422740525  |
| train/episodes            | 2500.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.072445549630533   |
| train_0/current_q         | -10.295057967879831   |
| train_0/fw_bonus          | -0.9986948311328888   |
| train_0/fw_loss           | 0.012441267422400416  |
| train_0/mu_grads          | -0.03185797529295087  |
| train_0/mu_grads_std      | 0.4613273687660694    |
| train_0/mu_loss           | 10.173984508973998    |
| train_0/next_q            | -10.161773946299457   |
| train_0/q_grads           | 0.0004868609823461156 |
| train_0/q_grads_std       | 0.287276803702116     |
| train_0/q_loss            | 0.8336607043063363    |
| train_0/reward            | -0.8222844276715477   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0521240234375       |
| train_0/target_q          | -10.44152732032106    |
| train_1/avg_q             | -14.75626041324773    |
| train_1/current_q         | -10.343507147134677   |
| train_1/fw_bonus          | -0.9830797746777534   |
| train_1/fw_loss           | 0.16964448280632496   |
| train_1/mu_grads          | -0.06476707011461258  |
| train_1/mu_grads_std      | 0.35196530967950823   |
| train_1/mu_loss           | 7.0804133957221085    |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -10.08702792747569    |
| train_1/q_grads           | -0.05411117784678936  |
| train_1/q_grads_std       | 0.27925369814038276   |
| train_1/q_loss            | 1.9485753803600443    |
| train_1/reward            | -2.0984246529464143   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.00068359375         |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.01814814814814815   |
| train_1/target_q          | -10.326253498705645   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 25
Time for epoch 25: 637.23. Rollout time: 393.21, Training time: 243.93
Evaluating epoch 25
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
------------------------------------------------------
| epoch                     | 25                     |
| policy/steps              | 2362384.0              |
| test/episodes             | 650.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -26.97547886663786     |
| test_1/avg_q              | -12.699471047241794    |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 2600.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -26.29681154529911     |
| train_0/current_q         | -10.385977765581108    |
| train_0/fw_bonus          | -0.9985109910368919    |
| train_0/fw_loss           | 0.014141537551768123   |
| train_0/mu_grads          | -0.036459595803171395  |
| train_0/mu_grads_std      | 0.4682160161435604     |
| train_0/mu_loss           | 10.252131064034193     |
| train_0/next_q            | -10.23129709826082     |
| train_0/q_grads           | 1.9994534227407712e-05 |
| train_0/q_grads_std       | 0.29114352390170095    |
| train_0/q_loss            | 0.8940464888357582     |
| train_0/reward            | -0.8504151645051025    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.0373779296875        |
| train_0/target_q          | -10.537279939621037    |
| train_1/avg_q             | -14.501828817039163    |
| train_1/current_q         | -9.273377251231533     |
| train_1/fw_bonus          | -0.9792773604393006    |
| train_1/fw_loss           | 0.2044101431965828     |
| train_1/mu_grads          | -0.06532070245593787   |
| train_1/mu_grads_std      | 0.35718610137701035    |
| train_1/mu_loss           | 5.877947049668736      |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -8.820698605670222     |
| train_1/q_grads           | -0.05514224348589778   |
| train_1/q_grads_std       | 0.2815908908843994     |
| train_1/q_loss            | 1.577447796884838      |
| train_1/reward            | -2.1489404329535318    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0006103515625        |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.01814814814814815    |
| train_1/target_q          | -9.281445693963253     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 26
Time for epoch 26: 661.42. Rollout time: 411.72, Training time: 249.61
Evaluating epoch 26
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-------------------------------------------------------
| epoch                     | 26                      |
| policy/steps              | 2452774.0               |
| test/episodes             | 675.0                   |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -26.758335320025786     |
| test_1/avg_q              | -10.943379009385227     |
| test_1/n_subgoals         | 675.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 2700.0                  |
| train/success_rate        | 0.02                    |
| train_0/avg_q             | -26.846150405226645     |
| train_0/current_q         | -10.224201198113452     |
| train_0/fw_bonus          | -0.9983574181795121     |
| train_0/fw_loss           | 0.015561906271614134    |
| train_0/mu_grads          | -0.03847721815109253    |
| train_0/mu_grads_std      | 0.47359792366623876     |
| train_0/mu_loss           | 10.006967662612842      |
| train_0/next_q            | -9.983068625706121      |
| train_0/q_grads           | -0.00012311765294725774 |
| train_0/q_grads_std       | 0.2949044942855835      |
| train_0/q_loss            | 0.9567599000172204      |
| train_0/reward            | -0.8756923812936293     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.0257080078125         |
| train_0/target_q          | -10.353774692451248     |
| train_1/avg_q             | -13.508113225906257     |
| train_1/current_q         | -7.905148483683044      |
| train_1/fw_bonus          | -0.9763558372855187     |
| train_1/fw_loss           | 0.2311216000467539      |
| train_1/mu_grads          | -0.06567221507430077    |
| train_1/mu_grads_std      | 0.36099463850259783     |
| train_1/mu_loss           | 5.271818893185265       |
| train_1/n_subgoals        | 2678.0                  |
| train_1/next_q            | -7.2542668167467355     |
| train_1/q_grads           | -0.05540034109726548    |
| train_1/q_grads_std       | 0.2850634068250656      |
| train_1/q_loss            | 1.627070894023824       |
| train_1/reward            | -2.135712098971271      |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.000439453125          |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0022404779686333084   |
| train_1/target_q          | -7.925805918214996      |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 27
Time for epoch 27: 657.18. Rollout time: 408.34, Training time: 248.75
Evaluating epoch 27
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
------------------------------------------------------
| epoch                     | 27                     |
| policy/steps              | 2543777.0              |
| test/episodes             | 700.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -27.0                  |
| test_1/avg_q              | -11.73994680851412     |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 2800.0                 |
| train/success_rate        | 0.01                   |
| train_0/avg_q             | -26.755120905459613    |
| train_0/current_q         | -10.892778209510631    |
| train_0/fw_bonus          | -0.9981680110096931    |
| train_0/fw_loss           | 0.017313704080879688   |
| train_0/mu_grads          | -0.04130176296457648   |
| train_0/mu_grads_std      | 0.4777135714888573     |
| train_0/mu_loss           | 10.707614838172308     |
| train_0/next_q            | -10.678030033770636    |
| train_0/q_grads           | -0.0006003918897476979 |
| train_0/q_grads_std       | 0.29784934520721434    |
| train_0/q_loss            | 1.1422188149609542     |
| train_0/reward            | -0.8926997270464199    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.0170654296875        |
| train_0/target_q          | -11.032521282910608    |
| train_1/avg_q             | -12.837810583773853    |
| train_1/current_q         | -8.584453006903505     |
| train_1/fw_bonus          | -0.9741826221346855    |
| train_1/fw_loss           | 0.2509913433343172     |
| train_1/mu_grads          | -0.06686527691781521   |
| train_1/mu_grads_std      | 0.3631108023226261     |
| train_1/mu_loss           | 5.6096583338184205     |
| train_1/n_subgoals        | 2698.0                 |
| train_1/next_q            | -8.069911542271758     |
| train_1/q_grads           | -0.05756900580599904   |
| train_1/q_grads_std       | 0.2903829075396061     |
| train_1/q_loss            | 1.3963262445501559     |
| train_1/reward            | -2.127862797123089     |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 2.44140625e-05         |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0014825796886582653  |
| train_1/target_q          | -8.612692207397796     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 28
Time for epoch 28: 665.43. Rollout time: 413.28, Training time: 252.04
Evaluating epoch 28
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
------------------------------------------------------
| epoch                     | 28                     |
| policy/steps              | 2634847.0              |
| test/episodes             | 725.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -27.0                  |
| test_1/avg_q              | -13.486963279677083    |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 2900.0                 |
| train/success_rate        | 0.01                   |
| train_0/avg_q             | -27.0                  |
| train_0/current_q         | -11.368760450659341    |
| train_0/fw_bonus          | -0.9980474054813385    |
| train_0/fw_loss           | 0.018429186567664145   |
| train_0/mu_grads          | -0.040611616987735036  |
| train_0/mu_grads_std      | 0.48204947263002396    |
| train_0/mu_loss           | 11.179551787023717     |
| train_0/next_q            | -11.141155369021176    |
| train_0/q_grads           | -0.0013919514749431983 |
| train_0/q_grads_std       | 0.30067403987050056    |
| train_0/q_loss            | 1.2927367503477174     |
| train_0/reward            | -0.9125735042747692    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.0064697265625        |
| train_0/target_q          | -11.506710681505567    |
| train_1/avg_q             | -13.797020343900007    |
| train_1/current_q         | -11.205157237287029    |
| train_1/fw_bonus          | -0.9729629516601562    |
| train_1/fw_loss           | 0.26214276552200316    |
| train_1/mu_grads          | -0.06763955857604742   |
| train_1/mu_grads_std      | 0.36487927809357645    |
| train_1/mu_loss           | 5.824438976108924      |
| train_1/n_subgoals        | 2698.0                 |
| train_1/next_q            | -11.148687505349798    |
| train_1/q_grads           | -0.06001754403114319   |
| train_1/q_grads_std       | 0.2960916168987751     |
| train_1/q_loss            | 1.038563250511363      |
| train_1/reward            | -2.1357734729186633    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0                    |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -11.199780228123865    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 29
Time for epoch 29: 726.63. Rollout time: 453.16, Training time: 273.37
Evaluating epoch 29
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
------------------------------------------------------
| epoch                     | 29                     |
| policy/steps              | 2725972.0              |
| test/episodes             | 750.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -27.0                  |
| test_1/avg_q              | -13.052926553313478    |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 3000.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -27.0                  |
| train_0/current_q         | -11.48320014201057     |
| train_0/fw_bonus          | -0.9980884447693825    |
| train_0/fw_loss           | 0.018049634294584394   |
| train_0/mu_grads          | -0.03924193186685443   |
| train_0/mu_grads_std      | 0.4884458102285862     |
| train_0/mu_loss           | 11.286294173561227     |
| train_0/next_q            | -11.248688502972191    |
| train_0/q_grads           | -0.0016140971682034432 |
| train_0/q_grads_std       | 0.3046525098383427     |
| train_0/q_loss            | 1.2489512350523904     |
| train_0/reward            | -0.9194299608869188    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.0029052734375        |
| train_0/target_q          | -11.630785317586094    |
| train_1/avg_q             | -13.86365124226703     |
| train_1/current_q         | -12.129369931394148    |
| train_1/fw_bonus          | -0.9732033386826515    |
| train_1/fw_loss           | 0.25994490534067155    |
| train_1/mu_grads          | -0.0684089906513691    |
| train_1/mu_grads_std      | 0.36592419892549516    |
| train_1/mu_loss           | 5.671380592396238      |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -12.268126480570928    |
| train_1/q_grads           | -0.06153498915955424   |
| train_1/q_grads_std       | 0.30455304533243177    |
| train_1/q_loss            | 1.4185698042118886     |
| train_1/reward            | -2.134538685709413     |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0                    |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -12.169113791864282    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 30
Time for epoch 30: 672.28. Rollout time: 414.94, Training time: 257.20
Evaluating epoch 30
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
------------------------------------------------------
| epoch                     | 30                     |
| policy/steps              | 2817097.0              |
| test/episodes             | 775.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -27.0                  |
| test_1/avg_q              | -11.517038865534486    |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 3100.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -27.0                  |
| train_0/current_q         | -11.522405245097243    |
| train_0/fw_bonus          | -0.9982527136802674    |
| train_0/fw_loss           | 0.016530317394062875   |
| train_0/mu_grads          | -0.039600944798439744  |
| train_0/mu_grads_std      | 0.4925308831036091     |
| train_0/mu_loss           | 11.302266153511187     |
| train_0/next_q            | -11.269748976697187    |
| train_0/q_grads           | -0.0016739779501222074 |
| train_0/q_grads_std       | 0.30667439475655556    |
| train_0/q_loss            | 1.1265423333558815     |
| train_0/reward            | -0.9199149297477561    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.0053466796875        |
| train_0/target_q          | -11.679855472515815    |
| train_1/avg_q             | -13.98516181636923     |
| train_1/current_q         | -13.291543052130944    |
| train_1/fw_bonus          | -0.9750883519649506    |
| train_1/fw_loss           | 0.24271025881171227    |
| train_1/mu_grads          | -0.06899008825421334   |
| train_1/mu_grads_std      | 0.3676242984831333     |
| train_1/mu_loss           | 5.325738143570641      |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -13.709321196265545    |
| train_1/q_grads           | -0.06243438879027963   |
| train_1/q_grads_std       | 0.31465511694550513    |
| train_1/q_loss            | 1.0480186716224462     |
| train_1/reward            | -2.126179430133925     |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 9.765625e-05           |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -13.342331681514725    |
------------------------------------------------------
Saving periodic policy to data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_30.pkl ...
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 31
Time for epoch 31: 667.36. Rollout time: 409.92, Training time: 257.35
Evaluating epoch 31
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
------------------------------------------------------
| epoch                     | 31                     |
| policy/steps              | 2908178.0              |
| test/episodes             | 800.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -27.0                  |
| test_1/avg_q              | -13.371441539642449    |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 3200.0                 |
| train/success_rate        | 0.01                   |
| train_0/avg_q             | -27.0                  |
| train_0/current_q         | -11.078438363083322    |
| train_0/fw_bonus          | -0.9984553575515747    |
| train_0/fw_loss           | 0.014656076696701348   |
| train_0/mu_grads          | -0.04005141369998455   |
| train_0/mu_grads_std      | 0.49778021648526194    |
| train_0/mu_loss           | 10.827842755184896     |
| train_0/next_q            | -10.800509375255812    |
| train_0/q_grads           | -0.0019479530194075779 |
| train_0/q_grads_std       | 0.30758405327796934    |
| train_0/q_loss            | 0.8863880883074339     |
| train_0/reward            | -0.9158780704710807    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.010400390625         |
| train_0/target_q          | -11.230934469185632    |
| train_1/avg_q             | -13.950064765729305    |
| train_1/current_q         | -13.522498666832345    |
| train_1/fw_bonus          | -0.9775099575519561    |
| train_1/fw_loss           | 0.22056947387754916    |
| train_1/mu_grads          | -0.06833089403808117   |
| train_1/mu_grads_std      | 0.36771259307861326    |
| train_1/mu_loss           | 4.928607245544472      |
| train_1/n_subgoals        | 2699.0                 |
| train_1/next_q            | -14.078293036077223    |
| train_1/q_grads           | -0.06399768218398094   |
| train_1/q_grads_std       | 0.3194218769669533     |
| train_1/q_loss            | 0.8025998217761746     |
| train_1/reward            | -2.13185800522042      |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.000146484375         |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -13.57321007268251     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 32
Time for epoch 32: 652.38. Rollout time: 401.29, Training time: 250.99
Evaluating epoch 32
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 32                    |
| policy/steps              | 2999303.0             |
| test/episodes             | 825.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -13.031470400301238   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 3300.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -27.0                 |
| train_0/current_q         | -10.97695916580378    |
| train_0/fw_bonus          | -0.9986608266830445   |
| train_0/fw_loss           | 0.012755729025229812  |
| train_0/mu_grads          | -0.0414926839992404   |
| train_0/mu_grads_std      | 0.5033559560775757    |
| train_0/mu_loss           | 10.726214131018011    |
| train_0/next_q            | -10.702944970861742   |
| train_0/q_grads           | -0.002020856412127614 |
| train_0/q_grads_std       | 0.3078631415963173    |
| train_0/q_loss            | 0.7997439236042844    |
| train_0/reward            | -0.9090105915034655   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0162353515625       |
| train_0/target_q          | -11.129291884832442   |
| train_1/avg_q             | -14.107759886460343   |
| train_1/current_q         | -13.448590898827922   |
| train_1/fw_bonus          | -0.979417371749878    |
| train_1/fw_loss           | 0.2031299076974392    |
| train_1/mu_grads          | -0.06839376743882894  |
| train_1/mu_grads_std      | 0.36866487711668017   |
| train_1/mu_loss           | 5.257016234751935     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -14.053258315986813   |
| train_1/q_grads           | -0.0660176295787096   |
| train_1/q_grads_std       | 0.32729349508881567   |
| train_1/q_loss            | 0.5488781182438575    |
| train_1/reward            | -2.089098335075687    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.00029296875         |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -13.491904247242928   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 33
Time for epoch 33: 652.30. Rollout time: 402.75, Training time: 249.45
Evaluating epoch 33
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 33                    |
| policy/steps              | 3089683.0             |
| test/episodes             | 850.0                 |
| test/success_rate         | 0.08                  |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -12.179385734744264   |
| test_1/n_subgoals         | 670.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 3400.0                |
| train/success_rate        | 0.02                  |
| train_0/avg_q             | -27.0                 |
| train_0/current_q         | -10.816469295978129   |
| train_0/fw_bonus          | -0.9988277599215507   |
| train_0/fw_loss           | 0.011211828980594873  |
| train_0/mu_grads          | -0.0407007853500545   |
| train_0/mu_grads_std      | 0.508114056289196     |
| train_0/mu_loss           | 10.587518945996248    |
| train_0/next_q            | -10.563775599439992   |
| train_0/q_grads           | -0.002588350517908111 |
| train_0/q_grads_std       | 0.30861783400177956   |
| train_0/q_loss            | 0.7025698552018695    |
| train_0/reward            | -0.8967649556769175   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0218994140625       |
| train_0/target_q          | -10.971247661475308   |
| train_1/avg_q             | -14.062611564841644   |
| train_1/current_q         | -13.378363127157224   |
| train_1/fw_bonus          | -0.9811442360281944   |
| train_1/fw_loss           | 0.1873411674052477    |
| train_1/mu_grads          | -0.06852237917482853  |
| train_1/mu_grads_std      | 0.36934570521116256   |
| train_1/mu_loss           | 5.689457145202386     |
| train_1/n_subgoals        | 2680.0                |
| train_1/next_q            | -14.031435270798417   |
| train_1/q_grads           | -0.06794738359749317  |
| train_1/q_grads_std       | 0.33408455848693847   |
| train_1/q_loss            | 0.7488212150063214    |
| train_1/reward            | -2.144441746568191    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0002197265625       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -13.387297743960128   |
-----------------------------------------------------
New best value for test/success_rate: 0.08. Saving policy to data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.02
Training epoch 34
Time for epoch 34: 652.26. Rollout time: 404.71, Training time: 247.46
Evaluating epoch 34
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
------------------------------------------------------
| epoch                     | 34                     |
| policy/steps              | 3180302.0              |
| test/episodes             | 875.0                  |
| test/success_rate         | 0.04                   |
| test_0/avg_q              | -27.0                  |
| test_1/avg_q              | -12.364176350560456    |
| test_1/n_subgoals         | 657.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 3500.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -27.0                  |
| train_0/current_q         | -10.638782557897944    |
| train_0/fw_bonus          | -0.9988883197307586    |
| train_0/fw_loss           | 0.010651767905801534   |
| train_0/mu_grads          | -0.04202145403251052   |
| train_0/mu_grads_std      | 0.5123641565442085     |
| train_0/mu_loss           | 10.41059018608279      |
| train_0/next_q            | -10.3945424462269      |
| train_0/q_grads           | -0.0029097781400196253 |
| train_0/q_grads_std       | 0.3104813575744629     |
| train_0/q_loss            | 0.678561290162708      |
| train_0/reward            | -0.8864436709831353    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.027392578125         |
| train_0/target_q          | -10.791431592587495    |
| train_1/avg_q             | -14.017707473573738    |
| train_1/current_q         | -13.555785130726651    |
| train_1/fw_bonus          | -0.9816890671849251    |
| train_1/fw_loss           | 0.18235972970724107    |
| train_1/mu_grads          | -0.0683338949456811    |
| train_1/mu_grads_std      | 0.3707603618502617     |
| train_1/mu_loss           | 5.7749204993520005     |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -14.255015946037611    |
| train_1/q_grads           | -0.07060666475445032   |
| train_1/q_grads_std       | 0.3417647436261177     |
| train_1/q_loss            | 1.0141724306628732     |
| train_1/reward            | -2.1406370148899443    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0005615234375        |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -13.555165688725822    |
------------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.03
Training epoch 35
Time for epoch 35: 655.81. Rollout time: 404.91, Training time: 250.79
Evaluating epoch 35
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 35                    |
| policy/steps              | 3271346.0             |
| test/episodes             | 900.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -14.092351230179698   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 3600.0                |
| train/success_rate        | 0.01                  |
| train_0/avg_q             | -27.0                 |
| train_0/current_q         | -10.603248309605224   |
| train_0/fw_bonus          | -0.9989420711994171   |
| train_0/fw_loss           | 0.010154580511152745  |
| train_0/mu_grads          | -0.041550721414387225 |
| train_0/mu_grads_std      | 0.5147074177861214    |
| train_0/mu_loss           | 10.389436188679444    |
| train_0/next_q            | -10.369395867529102   |
| train_0/q_grads           | -0.003842238360084593 |
| train_0/q_grads_std       | 0.3118617370724678    |
| train_0/q_loss            | 0.6028926376134406    |
| train_0/reward            | -0.8736287528037792   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0376708984375       |
| train_0/target_q          | -10.758442848066219   |
| train_1/avg_q             | -13.893088375928244   |
| train_1/current_q         | -13.401233294553624   |
| train_1/fw_bonus          | -0.9834210753440857   |
| train_1/fw_loss           | 0.16652401760220528   |
| train_1/mu_grads          | -0.06866239346563816  |
| train_1/mu_grads_std      | 0.37171055376529694   |
| train_1/mu_loss           | 6.414646094442565     |
| train_1/n_subgoals        | 2697.0                |
| train_1/next_q            | -14.042523309076682   |
| train_1/q_grads           | -0.07288674842566252  |
| train_1/q_grads_std       | 0.3482539735734463    |
| train_1/q_loss            | 0.6666599673487618    |
| train_1/reward            | -2.1477163938427113   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0009033203125       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -13.391224234352965   |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.03
Training epoch 36
Time for epoch 36: 662.78. Rollout time: 407.46, Training time: 255.19
Evaluating epoch 36
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 36                    |
| policy/steps              | 3362471.0             |
| test/episodes             | 925.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -12.835135917978391   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 3700.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -27.0                 |
| train_0/current_q         | -10.41007718741275    |
| train_0/fw_bonus          | -0.9989910170435905   |
| train_0/fw_loss           | 0.009701968170702457  |
| train_0/mu_grads          | -0.04020312111824751  |
| train_0/mu_grads_std      | 0.5165586784482002    |
| train_0/mu_loss           | 10.212992753061958    |
| train_0/next_q            | -10.193891483182266   |
| train_0/q_grads           | -0.004818636598065496 |
| train_0/q_grads_std       | 0.3126347564160824    |
| train_0/q_loss            | 0.5551253306497662    |
| train_0/reward            | -0.8533529287713464   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0481689453125       |
| train_0/target_q          | -10.56965232366845    |
| train_1/avg_q             | -14.02090417686942    |
| train_1/current_q         | -13.464181366561883   |
| train_1/fw_bonus          | -0.9851524144411087   |
| train_1/fw_loss           | 0.15069434605538845   |
| train_1/mu_grads          | -0.06897979788482189  |
| train_1/mu_grads_std      | 0.37230951637029647   |
| train_1/mu_loss           | 6.394058558864627     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -14.243380288246442   |
| train_1/q_grads           | -0.07514031585305929  |
| train_1/q_grads_std       | 0.354811392724514     |
| train_1/q_loss            | 0.7644781593761713    |
| train_1/reward            | -2.147983919602848    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0009521484375       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -13.466687630042804   |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.03
Training epoch 37
Time for epoch 37: 664.40. Rollout time: 408.87, Training time: 255.42
Evaluating epoch 37
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 37                    |
| policy/steps              | 3453596.0             |
| test/episodes             | 950.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -13.766205916919555   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 3800.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -27.0                 |
| train_0/current_q         | -10.22730542072404    |
| train_0/fw_bonus          | -0.9990508660674096   |
| train_0/fw_loss           | 0.009148406400345265  |
| train_0/mu_grads          | -0.04197035795077682  |
| train_0/mu_grads_std      | 0.5180853381752968    |
| train_0/mu_loss           | 10.032433066568512    |
| train_0/next_q            | -10.017282778478005   |
| train_0/q_grads           | -0.004965895786881447 |
| train_0/q_grads_std       | 0.3136394090950489    |
| train_0/q_loss            | 0.5036096903755104    |
| train_0/reward            | -0.8395889099294436   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.06025390625         |
| train_0/target_q          | -10.385410338394449   |
| train_1/avg_q             | -13.9273748114007     |
| train_1/current_q         | -13.287942838362193   |
| train_1/fw_bonus          | -0.9859261572360992   |
| train_1/fw_loss           | 0.1436200313270092    |
| train_1/mu_grads          | -0.06902820579707622  |
| train_1/mu_grads_std      | 0.3729731716215611    |
| train_1/mu_loss           | 6.588378665327946     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -14.026521320552447   |
| train_1/q_grads           | -0.07646372672170401  |
| train_1/q_grads_std       | 0.3602643221616745    |
| train_1/q_loss            | 0.5447812812559778    |
| train_1/reward            | -2.1909122922668756   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0018798828125       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -13.306030317556267   |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.01
Training epoch 38
Time for epoch 38: 562.11. Rollout time: 335.69, Training time: 226.36
Evaluating epoch 38
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 38                    |
| policy/steps              | 3544721.0             |
| test/episodes             | 975.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -13.378909404947771   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 3900.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -27.0                 |
| train_0/current_q         | -10.287953878895994   |
| train_0/fw_bonus          | -0.9990789353847503   |
| train_0/fw_loss           | 0.008888796716928483  |
| train_0/mu_grads          | -0.043387964460998775 |
| train_0/mu_grads_std      | 0.5193711131811142    |
| train_0/mu_loss           | 10.098468151543765    |
| train_0/next_q            | -10.084444498261437   |
| train_0/q_grads           | -0.00569085426395759  |
| train_0/q_grads_std       | 0.3147270053625107    |
| train_0/q_loss            | 0.49672049328903045   |
| train_0/reward            | -0.8330477689109103   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.06513671875         |
| train_0/target_q          | -10.442013240004144   |
| train_1/avg_q             | -14.14365819505999    |
| train_1/current_q         | -13.243585253902172   |
| train_1/fw_bonus          | -0.9860193744301796   |
| train_1/fw_loss           | 0.14276765529066324   |
| train_1/mu_grads          | -0.069346890039742    |
| train_1/mu_grads_std      | 0.37297165766358376   |
| train_1/mu_loss           | 6.457085229204023     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -14.005298656970695   |
| train_1/q_grads           | -0.07836153768002987  |
| train_1/q_grads_std       | 0.3654660157859325    |
| train_1/q_loss            | 0.4780949827748053    |
| train_1/reward            | -2.109867597038101    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0023193359375       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -13.271500606021078   |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 39
Time for epoch 39: 543.35. Rollout time: 318.27, Training time: 225.02
Evaluating epoch 39
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 39                    |
| policy/steps              | 3635760.0             |
| test/episodes             | 1000.0                |
| test/success_rate         | 0.04                  |
| test_0/avg_q              | -26.999999999999936   |
| test_1/avg_q              | -14.041118747572844   |
| test_1/n_subgoals         | 672.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 4000.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.999999999950788   |
| train_0/current_q         | -10.225289939390953   |
| train_0/fw_bonus          | -0.9990815922617913   |
| train_0/fw_loss           | 0.008864193758927285  |
| train_0/mu_grads          | -0.04355941005051136  |
| train_0/mu_grads_std      | 0.5208494290709496    |
| train_0/mu_loss           | 10.040103450909474    |
| train_0/next_q            | -10.027537266816088   |
| train_0/q_grads           | -0.006895736267324537 |
| train_0/q_grads_std       | 0.3166377454996109    |
| train_0/q_loss            | 0.47370092082799964   |
| train_0/reward            | -0.8247849387109454   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0708984375          |
| train_0/target_q          | -10.379663128932155   |
| train_1/avg_q             | -14.037234675358562   |
| train_1/current_q         | -13.212425909002349   |
| train_1/fw_bonus          | -0.986282679438591    |
| train_1/fw_loss           | 0.14036039020866156   |
| train_1/mu_grads          | -0.06957535129040479  |
| train_1/mu_grads_std      | 0.37284638583660124   |
| train_1/mu_loss           | 6.352384209931306     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -14.041792332045432   |
| train_1/q_grads           | -0.07957750428467988  |
| train_1/q_grads_std       | 0.3701208718121052    |
| train_1/q_loss            | 0.566441968088378     |
| train_1/reward            | -2.1902238513917838   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.003369140625        |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -13.248079698156232   |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.01
Training epoch 40
Time for epoch 40: 560.46. Rollout time: 329.81, Training time: 230.56
Evaluating epoch 40
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 40                    |
| policy/steps              | 3726822.0             |
| test/episodes             | 1025.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -13.953321932558847   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 4100.0                |
| train/success_rate        | 0.01                  |
| train_0/avg_q             | -26.999999999609933   |
| train_0/current_q         | -10.280945506967994   |
| train_0/fw_bonus          | -0.9990560099482536   |
| train_0/fw_loss           | 0.009100886271335184  |
| train_0/mu_grads          | -0.04348699655383825  |
| train_0/mu_grads_std      | 0.521099953353405     |
| train_0/mu_loss           | 10.109223950485486    |
| train_0/next_q            | -10.089683018947678   |
| train_0/q_grads           | -0.007544181670527905 |
| train_0/q_grads_std       | 0.31872569397091866   |
| train_0/q_loss            | 0.4857856354770805    |
| train_0/reward            | -0.8224650140931772   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0769775390625       |
| train_0/target_q          | -10.434826458303652   |
| train_1/avg_q             | -14.186103659034744   |
| train_1/current_q         | -13.209293780777319   |
| train_1/fw_bonus          | -0.9856601119041443   |
| train_1/fw_loss           | 0.14605235792696475   |
| train_1/mu_grads          | -0.0699737412855029   |
| train_1/mu_grads_std      | 0.3727982394397259    |
| train_1/mu_loss           | 6.401829004752111     |
| train_1/n_subgoals        | 2698.0                |
| train_1/next_q            | -14.024298965436284   |
| train_1/q_grads           | -0.08022626657038927  |
| train_1/q_grads_std       | 0.37466615736484526   |
| train_1/q_loss            | 0.5530347837044381    |
| train_1/reward            | -2.142915506726422    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0031494140625       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -13.23651335919494    |
-----------------------------------------------------
Saving periodic policy to data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_40.pkl ...
Mean of test/success_rate of last 4 epochs: 0.01
Training epoch 41
Time for epoch 41: 686.06. Rollout time: 434.61, Training time: 251.37
Evaluating epoch 41
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
----------------------------------------------------
| epoch                     | 41                   |
| policy/steps              | 3817729.0            |
| test/episodes             | 1050.0               |
| test/success_rate         | 0.0                  |
| test_0/avg_q              | -26.999999992785174  |
| test_1/avg_q              | -13.261239338011995  |
| test_1/n_subgoals         | 675.0                |
| test_1/subgoal_succ_rate  | 0.0                  |
| train/episodes            | 4200.0               |
| train/success_rate        | 0.01                 |
| train_0/avg_q             | -26.999974617375706  |
| train_0/current_q         | -10.33136832632984   |
| train_0/fw_bonus          | -0.9990257248282433  |
| train_0/fw_loss           | 0.009381015505641698 |
| train_0/mu_grads          | -0.0434464012272656  |
| train_0/mu_grads_std      | 0.5221503540873528   |
| train_0/mu_loss           | 10.161550386638748   |
| train_0/next_q            | -10.14195734713108   |
| train_0/q_grads           | -0.00839873100630939 |
| train_0/q_grads_std       | 0.3212647572159767   |
| train_0/q_loss            | 0.47142780110897287  |
| train_0/reward            | -0.8195934243929515  |
| train_0/reward_-0.0_frac  | 0.0                  |
| train_0/reward_-1.0_frac  | 0.0725341796875      |
| train_0/target_q          | -10.485314821239049  |
| train_1/avg_q             | -14.05458242720706   |
| train_1/current_q         | -13.181924237077592  |
| train_1/fw_bonus          | -0.9849939048290253  |
| train_1/fw_loss           | 0.15214347392320632  |
| train_1/mu_grads          | -0.07030283883213997 |
| train_1/mu_grads_std      | 0.37344668582081797  |
| train_1/mu_loss           | 6.0953270909093105   |
| train_1/n_subgoals        | 2692.0               |
| train_1/next_q            | -13.986174102121671  |
| train_1/q_grads           | -0.08141233865171671 |
| train_1/q_grads_std       | 0.38023523315787316  |
| train_1/q_loss            | 0.6269965140799474   |
| train_1/reward            | -2.0747479336176182  |
| train_1/reward_-0.0_frac  | 0.0                  |
| train_1/reward_-1.0_frac  | 0.0023681640625      |
| train_1/reward_-27.0_frac | 0.0                  |
| train_1/subgoal_succ_rate | 0.0                  |
| train_1/target_q          | -13.203337387425131  |
----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.01
Training epoch 42
Time for epoch 42: 542.42. Rollout time: 317.92, Training time: 224.44
Evaluating epoch 42
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 42                    |
| policy/steps              | 3908854.0             |
| test/episodes             | 1075.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -26.999999999999968   |
| test_1/avg_q              | -12.564963803923494   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 4300.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.99999999995766    |
| train_0/current_q         | -10.164784134669695   |
| train_0/fw_bonus          | -0.9990403801202774   |
| train_0/fw_loss           | 0.009245368232950569  |
| train_0/mu_grads          | -0.04226419208571315  |
| train_0/mu_grads_std      | 0.5239334270358086    |
| train_0/mu_loss           | 9.998612762897283     |
| train_0/next_q            | -9.980288017408691    |
| train_0/q_grads           | -0.009327737661078573 |
| train_0/q_grads_std       | 0.3241527728736401    |
| train_0/q_loss            | 0.43877725136884094   |
| train_0/reward            | -0.8129479257637285   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0755126953125       |
| train_0/target_q          | -10.324962854944733   |
| train_1/avg_q             | -14.01719012353107    |
| train_1/current_q         | -13.120621027355906   |
| train_1/fw_bonus          | -0.9845280155539513   |
| train_1/fw_loss           | 0.15640320479869843   |
| train_1/mu_grads          | -0.07053864225745202  |
| train_1/mu_grads_std      | 0.3746260441839695    |
| train_1/mu_loss           | 5.955073141983251     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -14.039166607707717   |
| train_1/q_grads           | -0.08221090231090784  |
| train_1/q_grads_std       | 0.38549245744943617   |
| train_1/q_loss            | 0.5819069219045956    |
| train_1/reward            | -2.0701326586320645   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.002294921875        |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -13.137691552575745   |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.01
Training epoch 43
Time for epoch 43: 547.56. Rollout time: 323.00, Training time: 224.48
Evaluating epoch 43
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 43                    |
| policy/steps              | 3999979.0             |
| test/episodes             | 1100.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -17.115487503506206   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 4400.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.999999999989527   |
| train_0/current_q         | -10.086289984168591   |
| train_0/fw_bonus          | -0.9990848258137703   |
| train_0/fw_loss           | 0.008834282914176584  |
| train_0/mu_grads          | -0.04239941351115704  |
| train_0/mu_grads_std      | 0.5256763949990273    |
| train_0/mu_loss           | 9.917338852883244     |
| train_0/next_q            | -9.904639929967429    |
| train_0/q_grads           | -0.010114721534773708 |
| train_0/q_grads_std       | 0.3269741743803024    |
| train_0/q_loss            | 0.39431874128009003   |
| train_0/reward            | -0.8036317706406407   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.087353515625        |
| train_0/target_q          | -10.236124610029659   |
| train_1/avg_q             | -13.954854562592406   |
| train_1/current_q         | -13.036318319813736   |
| train_1/fw_bonus          | -0.9849764809012413   |
| train_1/fw_loss           | 0.15230288319289684   |
| train_1/mu_grads          | -0.0704064980149269   |
| train_1/mu_grads_std      | 0.37573786824941635   |
| train_1/mu_loss           | 6.1841705852339635    |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -14.09321271124063    |
| train_1/q_grads           | -0.08348325993865728  |
| train_1/q_grads_std       | 0.3906347297132015    |
| train_1/q_loss            | 0.8472655109570354    |
| train_1/reward            | -2.0871659741846087   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.00263671875         |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -13.088448001565222   |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 44
Time for epoch 44: 547.45. Rollout time: 320.28, Training time: 227.10
Evaluating epoch 44
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 44                    |
| policy/steps              | 4091104.0             |
| test/episodes             | 1125.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -13.564387493686523   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 4500.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.999999999872152   |
| train_0/current_q         | -9.93084869682978     |
| train_0/fw_bonus          | -0.999126310646534    |
| train_0/fw_loss           | 0.008450698875822126  |
| train_0/mu_grads          | -0.04303132127970457  |
| train_0/mu_grads_std      | 0.5280692473053932    |
| train_0/mu_loss           | 9.775190293966663     |
| train_0/next_q            | -9.764660732123044    |
| train_0/q_grads           | -0.009772430406883359 |
| train_0/q_grads_std       | 0.32905188873410224   |
| train_0/q_loss            | 0.36458315223977555   |
| train_0/reward            | -0.7898829767284041   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0848388671875       |
| train_0/target_q          | -10.086821252986013   |
| train_1/avg_q             | -15.174006604004584   |
| train_1/current_q         | -12.973523487049746   |
| train_1/fw_bonus          | -0.9856255754828454   |
| train_1/fw_loss           | 0.14636821635067462   |
| train_1/mu_grads          | -0.07063996121287346  |
| train_1/mu_grads_std      | 0.3764542996883392    |
| train_1/mu_loss           | 5.963893150961196     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -14.017004197785713   |
| train_1/q_grads           | -0.08527901396155357  |
| train_1/q_grads_std       | 0.39436451345682144   |
| train_1/q_loss            | 0.3614693710780073    |
| train_1/reward            | -2.1029202072822955   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.002294921875        |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -12.97964129437662    |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 45
Time for epoch 45: 542.82. Rollout time: 317.28, Training time: 225.47
Evaluating epoch 45
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 45                    |
| policy/steps              | 4182229.0             |
| test/episodes             | 1150.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -12.666927509486495   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 4600.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -27.0                 |
| train_0/current_q         | -9.788658683134276    |
| train_0/fw_bonus          | -0.9991916194558144   |
| train_0/fw_loss           | 0.00784652285510674   |
| train_0/mu_grads          | -0.04293337529525161  |
| train_0/mu_grads_std      | 0.5299994140863419    |
| train_0/mu_loss           | 9.626173485492131     |
| train_0/next_q            | -9.614156024733848    |
| train_0/q_grads           | -0.009428067714907229 |
| train_0/q_grads_std       | 0.3307400159537792    |
| train_0/q_loss            | 0.2861803679753668    |
| train_0/reward            | -0.7789171043565147   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.1082275390625       |
| train_0/target_q          | -9.939717281045821    |
| train_1/avg_q             | -14.212504508169022   |
| train_1/current_q         | -12.913198642128407   |
| train_1/fw_bonus          | -0.9869665965437889   |
| train_1/fw_loss           | 0.13410718087106943   |
| train_1/mu_grads          | -0.07102095670998096  |
| train_1/mu_grads_std      | 0.37694034203886984   |
| train_1/mu_loss           | 6.084765515561691     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -13.991631406509049   |
| train_1/q_grads           | -0.08678918443620205  |
| train_1/q_grads_std       | 0.39842781201004984   |
| train_1/q_loss            | 0.3464354796925574    |
| train_1/reward            | -2.0721209938797984   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0029296875          |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -12.906526271107808   |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 46
Time for epoch 46: 543.14. Rollout time: 321.10, Training time: 221.99
Evaluating epoch 46
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 46                    |
| policy/steps              | 4273354.0             |
| test/episodes             | 1175.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -13.65806213871558    |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 4700.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -27.0                 |
| train_0/current_q         | -9.78963827478186     |
| train_0/fw_bonus          | -0.9992617160081864   |
| train_0/fw_loss           | 0.007198321307078004  |
| train_0/mu_grads          | -0.04274984672665596  |
| train_0/mu_grads_std      | 0.5323292449116707    |
| train_0/mu_loss           | 9.629645141297274     |
| train_0/next_q            | -9.619679920377703    |
| train_0/q_grads           | -0.009104392561130225 |
| train_0/q_grads_std       | 0.3327353335916996    |
| train_0/q_loss            | 0.2755515834093111    |
| train_0/reward            | -0.7740815416720579   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.107861328125        |
| train_0/target_q          | -9.939145108087605    |
| train_1/avg_q             | -13.943594171771919   |
| train_1/current_q         | -12.885104137027264   |
| train_1/fw_bonus          | -0.987188683450222    |
| train_1/fw_loss           | 0.1320766242220998    |
| train_1/mu_grads          | -0.07119814585894346  |
| train_1/mu_grads_std      | 0.37710063681006434   |
| train_1/mu_loss           | 6.109877649507695     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -14.008273846505313   |
| train_1/q_grads           | -0.08766958378255367  |
| train_1/q_grads_std       | 0.40368272811174394   |
| train_1/q_loss            | 0.32897066987332246   |
| train_1/reward            | -2.119754010588076    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.003369140625        |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -12.878303069395207   |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 47
Time for epoch 47: 553.98. Rollout time: 326.53, Training time: 227.38
Evaluating epoch 47
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 47                    |
| policy/steps              | 4364479.0             |
| test/episodes             | 1200.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -14.074945206031197   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 4800.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.9999999880793     |
| train_0/current_q         | -9.679372283132258    |
| train_0/fw_bonus          | -0.9992602422833443   |
| train_0/fw_loss           | 0.0072119363583624365 |
| train_0/mu_grads          | -0.043275348376482725 |
| train_0/mu_grads_std      | 0.5359147295355797    |
| train_0/mu_loss           | 9.519436208209765     |
| train_0/next_q            | -9.510137428794733    |
| train_0/q_grads           | -0.009135763766244054 |
| train_0/q_grads_std       | 0.3354731373488903    |
| train_0/q_loss            | 0.26077092624442244   |
| train_0/reward            | -0.7716149853222305   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.095654296875        |
| train_0/target_q          | -9.83313245901402     |
| train_1/avg_q             | -14.061987450238055   |
| train_1/current_q         | -12.818368136513277   |
| train_1/fw_bonus          | -0.9873378142714501   |
| train_1/fw_loss           | 0.13071312792599202   |
| train_1/mu_grads          | -0.07164522930979729  |
| train_1/mu_grads_std      | 0.37694039940834045   |
| train_1/mu_loss           | 6.022764027300019     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -13.98972094340568    |
| train_1/q_grads           | -0.08913286309689283  |
| train_1/q_grads_std       | 0.4080826729536057    |
| train_1/q_loss            | 0.49496833372299864   |
| train_1/reward            | -2.1289594333517017   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.00380859375         |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -12.81648686030906    |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 48
Time for epoch 48: 552.84. Rollout time: 322.06, Training time: 230.72
Evaluating epoch 48
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 48                    |
| policy/steps              | 4455604.0             |
| test/episodes             | 1225.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -12.675896478256425   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 4900.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.999999999999996   |
| train_0/current_q         | -9.911208050478734    |
| train_0/fw_bonus          | -0.9992084056138992   |
| train_0/fw_loss           | 0.007691354455891997  |
| train_0/mu_grads          | -0.042620182130485774 |
| train_0/mu_grads_std      | 0.5394864290952682    |
| train_0/mu_loss           | 9.761170728234305     |
| train_0/next_q            | -9.75289902268169     |
| train_0/q_grads           | -0.008994428860023617 |
| train_0/q_grads_std       | 0.33778421953320503   |
| train_0/q_loss            | 0.2775202641121656    |
| train_0/reward            | -0.7742257858830272   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.1135986328125       |
| train_0/target_q          | -10.064093201515062   |
| train_1/avg_q             | -14.069499432193659   |
| train_1/current_q         | -12.900882574346777   |
| train_1/fw_bonus          | -0.9870186433196068   |
| train_1/fw_loss           | 0.1336313348263502    |
| train_1/mu_grads          | -0.07240822501480579  |
| train_1/mu_grads_std      | 0.37730348855257034   |
| train_1/mu_loss           | 6.020641069393075     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -14.03436712010859    |
| train_1/q_grads           | -0.09032123070210218  |
| train_1/q_grads_std       | 0.4113080374896526    |
| train_1/q_loss            | 0.413083377667785     |
| train_1/reward            | -2.109537152959092    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.003759765625        |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -12.890178581696244   |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 49
Time for epoch 49: 545.85. Rollout time: 321.97, Training time: 223.79
Evaluating epoch 49
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 49                    |
| policy/steps              | 4546729.0             |
| test/episodes             | 1250.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -13.094233425476661   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 5000.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -27.0                 |
| train_0/current_q         | -9.948435807427698    |
| train_0/fw_bonus          | -0.9992046609520913   |
| train_0/fw_loss           | 0.007725975092034787  |
| train_0/mu_grads          | -0.042760849371552466 |
| train_0/mu_grads_std      | 0.5439418002963066    |
| train_0/mu_loss           | 9.805657944305867     |
| train_0/next_q            | -9.795878521381415    |
| train_0/q_grads           | -0.008615582622587681 |
| train_0/q_grads_std       | 0.33999999761581423   |
| train_0/q_loss            | 0.27539020107128165   |
| train_0/reward            | -0.7741995000724273   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.105322265625        |
| train_0/target_q          | -10.10478947418766    |
| train_1/avg_q             | -13.81228880694081    |
| train_1/current_q         | -12.880111883369505   |
| train_1/fw_bonus          | -0.9873593717813491   |
| train_1/fw_loss           | 0.13051608223468064   |
| train_1/mu_grads          | -0.07288134731352329  |
| train_1/mu_grads_std      | 0.3777215577661991    |
| train_1/mu_loss           | 6.046879224326237     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -13.969878683246787   |
| train_1/q_grads           | -0.09203388150781393  |
| train_1/q_grads_std       | 0.41621184572577474   |
| train_1/q_loss            | 0.456201604061619     |
| train_1/reward            | -2.100649898503616    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.003564453125        |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -12.882329665462965   |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 50
Time for epoch 50: 544.09. Rollout time: 320.17, Training time: 223.85
Evaluating epoch 50
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 50                    |
| policy/steps              | 4637854.0             |
| test/episodes             | 1275.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -13.018089054528321   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 5100.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.999999999999964   |
| train_0/current_q         | -9.773104004925381    |
| train_0/fw_bonus          | -0.9992068529129028   |
| train_0/fw_loss           | 0.00770581002580002   |
| train_0/mu_grads          | -0.04197127260267734  |
| train_0/mu_grads_std      | 0.5481748178601265    |
| train_0/mu_loss           | 9.626514577157844     |
| train_0/next_q            | -9.618634091418798    |
| train_0/q_grads           | -0.009379429067485035 |
| train_0/q_grads_std       | 0.3423894979059696    |
| train_0/q_loss            | 0.2970301792156585    |
| train_0/reward            | -0.7712709029037796   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.1124755859375       |
| train_0/target_q          | -9.926303045053265    |
| train_1/avg_q             | -14.006312897289527   |
| train_1/current_q         | -12.874184596448952   |
| train_1/fw_bonus          | -0.9873517274856567   |
| train_1/fw_loss           | 0.13058591205626727   |
| train_1/mu_grads          | -0.07317860387265682  |
| train_1/mu_grads_std      | 0.3776682086288929    |
| train_1/mu_loss           | 5.96777757028533      |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -14.026095307300059   |
| train_1/q_grads           | -0.09359434954822063  |
| train_1/q_grads_std       | 0.42140410616993906   |
| train_1/q_loss            | 0.5080838541224233    |
| train_1/reward            | -2.141100180975627    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.003466796875        |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -12.872761992383056   |
-----------------------------------------------------
Saving periodic policy to data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_50.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 51
Time for epoch 51: 551.65. Rollout time: 324.16, Training time: 227.42
Evaluating epoch 51
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 51                    |
| policy/steps              | 4728979.0             |
| test/episodes             | 1300.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -13.877875072571653   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 5200.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.99999996557948    |
| train_0/current_q         | -9.821060665878337    |
| train_0/fw_bonus          | -0.9991891458630562   |
| train_0/fw_loss           | 0.007869551866315305  |
| train_0/mu_grads          | -0.04173895213752985  |
| train_0/mu_grads_std      | 0.5513853609561921    |
| train_0/mu_loss           | 9.671055680636865     |
| train_0/next_q            | -9.661529759502733    |
| train_0/q_grads           | -0.010149515233933926 |
| train_0/q_grads_std       | 0.3440912663936615    |
| train_0/q_loss            | 0.28600365429385766   |
| train_0/reward            | -0.7735935934913869   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.1096923828125       |
| train_0/target_q          | -9.975181975389969    |
| train_1/avg_q             | -13.947601286016816   |
| train_1/current_q         | -12.884955868832582   |
| train_1/fw_bonus          | -0.9872474566102027   |
| train_1/fw_loss           | 0.1315392566844821    |
| train_1/mu_grads          | -0.0729312801733613   |
| train_1/mu_grads_std      | 0.3781114086508751    |
| train_1/mu_loss           | 6.2052220675548755    |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -13.985433885253144   |
| train_1/q_grads           | -0.09430230408906937  |
| train_1/q_grads_std       | 0.42697752863168714   |
| train_1/q_loss            | 0.5691710070593615    |
| train_1/reward            | -2.0940143539060956   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.00341796875         |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -12.89636702769813    |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 52
Time for epoch 52: 542.66. Rollout time: 318.40, Training time: 224.21
Evaluating epoch 52
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 52                    |
| policy/steps              | 4820104.0             |
| test/episodes             | 1325.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -13.650947549201767   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 5300.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.999999999500677   |
| train_0/current_q         | -9.910062795289896    |
| train_0/fw_bonus          | -0.9992307394742965   |
| train_0/fw_loss           | 0.0074847838724963365 |
| train_0/mu_grads          | -0.04274335503578186  |
| train_0/mu_grads_std      | 0.554075887799263     |
| train_0/mu_loss           | 9.763059760997319     |
| train_0/next_q            | -9.755985114648587    |
| train_0/q_grads           | -0.010498184966854751 |
| train_0/q_grads_std       | 0.34590291380882265   |
| train_0/q_loss            | 0.2883293741793551    |
| train_0/reward            | -0.7736388091325352   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.10478515625         |
| train_0/target_q          | -10.064856904765273   |
| train_1/avg_q             | -13.968636271199616   |
| train_1/current_q         | -12.946941184433166   |
| train_1/fw_bonus          | -0.9868806511163711   |
| train_1/fw_loss           | 0.13489300832152368   |
| train_1/mu_grads          | -0.07289335615932942  |
| train_1/mu_grads_std      | 0.3787694193422794    |
| train_1/mu_loss           | 6.327513678553329     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -14.012831666670355   |
| train_1/q_grads           | -0.09570314139127731  |
| train_1/q_grads_std       | 0.4306890845298767    |
| train_1/q_loss            | 0.49105267196888125   |
| train_1/reward            | -2.0892655190989897   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.003173828125        |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -12.942201736959003   |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 53
Time for epoch 53: 541.04. Rollout time: 324.49, Training time: 216.49
Evaluating epoch 53
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 53                    |
| policy/steps              | 4911229.0             |
| test/episodes             | 1350.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -13.480335779239955   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 5400.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.999999999998618   |
| train_0/current_q         | -9.922847601270107    |
| train_0/fw_bonus          | -0.9992700845003128   |
| train_0/fw_loss           | 0.0071208686218597    |
| train_0/mu_grads          | -0.0417279283516109   |
| train_0/mu_grads_std      | 0.5565615490078926    |
| train_0/mu_loss           | 9.777761197622642     |
| train_0/next_q            | -9.767800094593595    |
| train_0/q_grads           | -0.011360580753535032 |
| train_0/q_grads_std       | 0.34733682721853254   |
| train_0/q_loss            | 0.2801821516769881    |
| train_0/reward            | -0.7715588730534364   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.1057373046875       |
| train_0/target_q          | -10.07779129864471    |
| train_1/avg_q             | -14.129042277760233   |
| train_1/current_q         | -12.924273607060224   |
| train_1/fw_bonus          | -0.9873305097222328   |
| train_1/fw_loss           | 0.13078010343015195   |
| train_1/mu_grads          | -0.07283379472792148  |
| train_1/mu_grads_std      | 0.37835763692855834   |
| train_1/mu_loss           | 6.114986677270396     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -13.996447419927346   |
| train_1/q_grads           | -0.09701159168034793  |
| train_1/q_grads_std       | 0.4359628401696682    |
| train_1/q_loss            | 0.7202806145513925    |
| train_1/reward            | -2.095654991029005    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0035888671875       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -12.932646323016689   |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 54
Time for epoch 54: 537.21. Rollout time: 318.61, Training time: 218.53
Evaluating epoch 54
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 54                    |
| policy/steps              | 5002354.0             |
| test/episodes             | 1375.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -13.660485593817636   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 5500.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.99999999999998    |
| train_0/current_q         | -9.767187818890967    |
| train_0/fw_bonus          | -0.9992916822433472   |
| train_0/fw_loss           | 0.006921154051087797  |
| train_0/mu_grads          | -0.042910554073750974 |
| train_0/mu_grads_std      | 0.5582886964082718    |
| train_0/mu_loss           | 9.614672310682796     |
| train_0/next_q            | -9.60770883408657     |
| train_0/q_grads           | -0.011764629976823926 |
| train_0/q_grads_std       | 0.34985294491052626   |
| train_0/q_loss            | 0.24729901921988873   |
| train_0/reward            | -0.7654783615485939   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.1118896484375       |
| train_0/target_q          | -9.919176555141366    |
| train_1/avg_q             | -13.973786919503748   |
| train_1/current_q         | -12.899530036214049   |
| train_1/fw_bonus          | -0.9878092646598816   |
| train_1/fw_loss           | 0.1264027263969183    |
| train_1/mu_grads          | -0.07330568395555019  |
| train_1/mu_grads_std      | 0.3782822847366333    |
| train_1/mu_loss           | 5.898903345898384     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -13.989390409689516   |
| train_1/q_grads           | -0.09830574821680785  |
| train_1/q_grads_std       | 0.4398368291556835    |
| train_1/q_loss            | 0.33880421217652024   |
| train_1/reward            | -2.0925984756060645   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0037841796875       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -12.896369705394651   |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 55
Time for epoch 55: 547.15. Rollout time: 323.86, Training time: 223.22
Evaluating epoch 55
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 55                    |
| policy/steps              | 5093479.0             |
| test/episodes             | 1400.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -12.302807127678632   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 5600.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -27.0                 |
| train_0/current_q         | -9.656980622638418    |
| train_0/fw_bonus          | -0.9993262588977814   |
| train_0/fw_loss           | 0.00660143633140251   |
| train_0/mu_grads          | -0.0427952229976654   |
| train_0/mu_grads_std      | 0.560989984869957     |
| train_0/mu_loss           | 9.515843181788792     |
| train_0/next_q            | -9.504315738982296    |
| train_0/q_grads           | -0.012276973528787493 |
| train_0/q_grads_std       | 0.3524935141205788    |
| train_0/q_loss            | 0.2465895561152595    |
| train_0/reward            | -0.7610156658593041   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0902099609375       |
| train_0/target_q          | -9.80675742935808     |
| train_1/avg_q             | -14.016950595690998   |
| train_1/current_q         | -12.869696965948544   |
| train_1/fw_bonus          | -0.9887551754713059   |
| train_1/fw_loss           | 0.117754165828228     |
| train_1/mu_grads          | -0.07379537764936686  |
| train_1/mu_grads_std      | 0.3788456432521343    |
| train_1/mu_loss           | 5.892464442057933     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -14.0251139215834     |
| train_1/q_grads           | -0.09898067563772202  |
| train_1/q_grads_std       | 0.4433013454079628    |
| train_1/q_loss            | 0.3040007346180643    |
| train_1/reward            | -2.0891895990069314   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0026611328125       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -12.86907741278019    |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 56
Time for epoch 56: 540.32. Rollout time: 321.20, Training time: 219.05
Evaluating epoch 56
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 56                    |
| policy/steps              | 5184604.0             |
| test/episodes             | 1425.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -14.04709091896161    |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 5700.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.999999999999392   |
| train_0/current_q         | -9.754410515013161    |
| train_0/fw_bonus          | -0.9994117677211761   |
| train_0/fw_loss           | 0.005810499680228532  |
| train_0/mu_grads          | -0.043582456279546024 |
| train_0/mu_grads_std      | 0.5639614447951317    |
| train_0/mu_loss           | 9.615205566173238     |
| train_0/next_q            | -9.609982106529628    |
| train_0/q_grads           | -0.012093718955293297 |
| train_0/q_grads_std       | 0.35477420687675476   |
| train_0/q_loss            | 0.23892791321170578   |
| train_0/reward            | -0.7548206320068858   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0730712890625       |
| train_0/target_q          | -9.90566980409093     |
| train_1/avg_q             | -14.012255002929885   |
| train_1/current_q         | -12.806330534866209   |
| train_1/fw_bonus          | -0.9898118749260902   |
| train_1/fw_loss           | 0.10809274353086948   |
| train_1/mu_grads          | -0.07394415587186813  |
| train_1/mu_grads_std      | 0.3787969544529915    |
| train_1/mu_loss           | 5.95133275497111      |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -14.011652953467642   |
| train_1/q_grads           | -0.10089646298438311  |
| train_1/q_grads_std       | 0.4463993966579437    |
| train_1/q_loss            | 0.36880604296428005   |
| train_1/reward            | -2.1201256638807537   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.00390625            |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -12.810541054174342   |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 57
Time for epoch 57: 546.55. Rollout time: 325.74, Training time: 220.74
Evaluating epoch 57
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 57                    |
| policy/steps              | 5275729.0             |
| test/episodes             | 1450.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -13.974074383379675   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 5800.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.99999999999987    |
| train_0/current_q         | -9.684406803618064    |
| train_0/fw_bonus          | -0.9994501680135727   |
| train_0/fw_loss           | 0.005455419805366546  |
| train_0/mu_grads          | -0.04383967723697424  |
| train_0/mu_grads_std      | 0.5672190189361572    |
| train_0/mu_loss           | 9.542902674144258     |
| train_0/next_q            | -9.537228230350026    |
| train_0/q_grads           | -0.012612864165566862 |
| train_0/q_grads_std       | 0.35755386725068095   |
| train_0/q_loss            | 0.21461763585876187   |
| train_0/reward            | -0.753777816240472    |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0742919921875       |
| train_0/target_q          | -9.83691148104453     |
| train_1/avg_q             | -14.038971111229372   |
| train_1/current_q         | -12.75953223282362    |
| train_1/fw_bonus          | -0.9907617971301079   |
| train_1/fw_loss           | 0.09940755404531956   |
| train_1/mu_grads          | -0.0744313731789589   |
| train_1/mu_grads_std      | 0.3786918751895428    |
| train_1/mu_loss           | 6.003297218081333     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -14.005120248600708   |
| train_1/q_grads           | -0.10162788722664118  |
| train_1/q_grads_std       | 0.4493807017803192    |
| train_1/q_loss            | 0.44848674623814483   |
| train_1/reward            | -2.100983927623747    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.003955078125        |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -12.7627787826116     |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 58
Time for epoch 58: 549.06. Rollout time: 319.99, Training time: 229.00
Evaluating epoch 58
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 58                    |
| policy/steps              | 5366854.0             |
| test/episodes             | 1475.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -13.894997126165975   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 5900.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.999999999997986   |
| train_0/current_q         | -9.672541638968204    |
| train_0/fw_bonus          | -0.9994181349873543   |
| train_0/fw_loss           | 0.005751622875686735  |
| train_0/mu_grads          | -0.044697878137230876 |
| train_0/mu_grads_std      | 0.5699603676795959    |
| train_0/mu_loss           | 9.522536432243763     |
| train_0/next_q            | -9.518563286386243    |
| train_0/q_grads           | -0.012943498441018164 |
| train_0/q_grads_std       | 0.36017258316278455   |
| train_0/q_loss            | 0.19933699446240763   |
| train_0/reward            | -0.7551656118637766   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.08984375            |
| train_0/target_q          | -9.826582316160387    |
| train_1/avg_q             | -13.965461440758588   |
| train_1/current_q         | -12.767812944699983   |
| train_1/fw_bonus          | -0.9904684841632843   |
| train_1/fw_loss           | 0.10208928622305394   |
| train_1/mu_grads          | -0.07495716921985149  |
| train_1/mu_grads_std      | 0.3787758134305477    |
| train_1/mu_loss           | 5.910500732283717     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -14.111928606301607   |
| train_1/q_grads           | -0.10267496760934591  |
| train_1/q_grads_std       | 0.45312109813094137   |
| train_1/q_loss            | 0.47314775429346306   |
| train_1/reward            | -2.088666861942329    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.00458984375         |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -12.759823478397834   |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 59
Time for epoch 59: 550.41. Rollout time: 323.04, Training time: 227.30
Evaluating epoch 59
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 59                    |
| policy/steps              | 5457979.0             |
| test/episodes             | 1500.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -13.52735015939136    |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 6000.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.99999999999911    |
| train_0/current_q         | -9.716212301219233    |
| train_0/fw_bonus          | -0.9994021683931351   |
| train_0/fw_loss           | 0.0058992690872401    |
| train_0/mu_grads          | -0.044189215451478955 |
| train_0/mu_grads_std      | 0.5728141069412231    |
| train_0/mu_loss           | 9.565225716137913     |
| train_0/next_q            | -9.556493284235055    |
| train_0/q_grads           | -0.012649752269499003 |
| train_0/q_grads_std       | 0.36294636949896814   |
| train_0/q_loss            | 0.2178147587785031    |
| train_0/reward            | -0.7589784307376248   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0949951171875       |
| train_0/target_q          | -9.868349849714088    |
| train_1/avg_q             | -13.989301246544287   |
| train_1/current_q         | -12.722478807726926   |
| train_1/fw_bonus          | -0.9901043638586998   |
| train_1/fw_loss           | 0.10541855040937662   |
| train_1/mu_grads          | -0.07490145415067673  |
| train_1/mu_grads_std      | 0.37925679311156274   |
| train_1/mu_loss           | 5.973026879515656     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -13.992654381240234   |
| train_1/q_grads           | -0.10389605760574341  |
| train_1/q_grads_std       | 0.45497915148735046   |
| train_1/q_loss            | 0.42420785724323523   |
| train_1/reward            | -2.097872003747034    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0035400390625       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -12.719669498343041   |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 60
Time for epoch 60: 545.27. Rollout time: 319.31, Training time: 225.90
Evaluating epoch 60
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 60                    |
| policy/steps              | 5549104.0             |
| test/episodes             | 1525.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -14.094301206191707   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 6100.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.99999999999615    |
| train_0/current_q         | -9.808574600520206    |
| train_0/fw_bonus          | -0.999368117749691    |
| train_0/fw_loss           | 0.006214280019048601  |
| train_0/mu_grads          | -0.04380754940211773  |
| train_0/mu_grads_std      | 0.5763750731945038    |
| train_0/mu_loss           | 9.651738211686487     |
| train_0/next_q            | -9.643371706262974    |
| train_0/q_grads           | -0.013393318466842174 |
| train_0/q_grads_std       | 0.36457172185182574   |
| train_0/q_loss            | 0.22496373806298958   |
| train_0/reward            | -0.7659926506807097   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.101708984375        |
| train_0/target_q          | -9.96280529008381     |
| train_1/avg_q             | -14.080515855025977   |
| train_1/current_q         | -12.804330059684233   |
| train_1/fw_bonus          | -0.9892463818192482   |
| train_1/fw_loss           | 0.11326302289962768   |
| train_1/mu_grads          | -0.0749363100156188   |
| train_1/mu_grads_std      | 0.38017708286643026   |
| train_1/mu_loss           | 6.111172035217766     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -13.990004847412148   |
| train_1/q_grads           | -0.1051219454035163   |
| train_1/q_grads_std       | 0.4574503056704998    |
| train_1/q_loss            | 0.29904835812496355   |
| train_1/reward            | -2.092242821621767    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0035400390625       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -12.80432219960798    |
-----------------------------------------------------
Saving periodic policy to data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_60.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 61
Time for epoch 61: 549.40. Rollout time: 325.54, Training time: 223.81
Evaluating epoch 61
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 61                    |
| policy/steps              | 5639694.0             |
| test/episodes             | 1550.0                |
| test/success_rate         | 0.04                  |
| test_0/avg_q              | -26.999999999968438   |
| test_1/avg_q              | -13.883064737811576   |
| test_1/n_subgoals         | 656.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 6200.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.999998764444147   |
| train_0/current_q         | -9.930918999944467    |
| train_0/fw_bonus          | -0.9993118464946746   |
| train_0/fw_loss           | 0.006734643341042101  |
| train_0/mu_grads          | -0.04400889165699482  |
| train_0/mu_grads_std      | 0.5794301748275756    |
| train_0/mu_loss           | 9.772752033201929     |
| train_0/next_q            | -9.76402203921015     |
| train_0/q_grads           | -0.014989277976565064 |
| train_0/q_grads_std       | 0.3678730607032776    |
| train_0/q_loss            | 0.25273540316249493   |
| train_0/reward            | -0.7734474291784863   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.099072265625        |
| train_0/target_q          | -10.08620693078478    |
| train_1/avg_q             | -14.05022002013826    |
| train_1/current_q         | -12.961199076826833   |
| train_1/fw_bonus          | -0.9884013265371323   |
| train_1/fw_loss           | 0.12098944410681725   |
| train_1/mu_grads          | -0.07469555158168077  |
| train_1/mu_grads_std      | 0.3809992797672749    |
| train_1/mu_loss           | 6.026093170127111     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -14.01416688051454    |
| train_1/q_grads           | -0.1063495947048068   |
| train_1/q_grads_std       | 0.4605938382446766    |
| train_1/q_loss            | 0.2697975851352286    |
| train_1/reward            | -2.0778432393897672   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0031005859375       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -12.958529107367411   |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.01
Training epoch 62
Time for epoch 62: 508.67. Rollout time: 293.15, Training time: 215.46
Evaluating epoch 62
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 62                    |
| policy/steps              | 5730653.0             |
| test/episodes             | 1575.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -26.999999872792035   |
| test_1/avg_q              | -13.978616674644824   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 6300.0                |
| train/success_rate        | 0.01                  |
| train_0/avg_q             | -26.999999897050206   |
| train_0/current_q         | -9.827909765506401    |
| train_0/fw_bonus          | -0.9993043258786202   |
| train_0/fw_loss           | 0.00680419490672648   |
| train_0/mu_grads          | -0.04453877061605453  |
| train_0/mu_grads_std      | 0.5827599570155144    |
| train_0/mu_loss           | 9.669579893099558     |
| train_0/next_q            | -9.658915136590476    |
| train_0/q_grads           | -0.014739087712951005 |
| train_0/q_grads_std       | 0.370046366751194     |
| train_0/q_loss            | 0.25799616402822656   |
| train_0/reward            | -0.7757385908334982   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.09365234375         |
| train_0/target_q          | -9.98169068864895     |
| train_1/avg_q             | -14.008877613162321   |
| train_1/current_q         | -13.030501741995414   |
| train_1/fw_bonus          | -0.9879642784595489   |
| train_1/fw_loss           | 0.12498537879437208   |
| train_1/mu_grads          | -0.07460421361029149  |
| train_1/mu_grads_std      | 0.38142512664198874   |
| train_1/mu_loss           | 6.271720920175572     |
| train_1/n_subgoals        | 2694.0                |
| train_1/next_q            | -14.02618539517914    |
| train_1/q_grads           | -0.10664683561772108  |
| train_1/q_grads_std       | 0.4622444033622742    |
| train_1/q_loss            | 0.36491774687109074   |
| train_1/reward            | -2.1416261804377426   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0037841796875       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -13.033401184626541   |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.01
Training epoch 63
Time for epoch 63: 471.28. Rollout time: 263.02, Training time: 208.20
Evaluating epoch 63
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 63                    |
| policy/steps              | 5821778.0             |
| test/episodes             | 1600.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -14.096819717378368   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 6400.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.99999909101857    |
| train_0/current_q         | -9.844769665784897    |
| train_0/fw_bonus          | -0.9992777824401855   |
| train_0/fw_loss           | 0.0070496491738595065 |
| train_0/mu_grads          | -0.04526884704828262  |
| train_0/mu_grads_std      | 0.5860847711563111    |
| train_0/mu_loss           | 9.684561632629876     |
| train_0/next_q            | -9.669911134314237    |
| train_0/q_grads           | -0.014893738436512649 |
| train_0/q_grads_std       | 0.372565370798111     |
| train_0/q_loss            | 0.2622575600846782    |
| train_0/reward            | -0.779118876020948    |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.1012939453125       |
| train_0/target_q          | -9.99704927921977     |
| train_1/avg_q             | -14.016859475829524   |
| train_1/current_q         | -13.042062634834846   |
| train_1/fw_bonus          | -0.9878247767686844   |
| train_1/fw_loss           | 0.12626076135784386   |
| train_1/mu_grads          | -0.07464376986026763  |
| train_1/mu_grads_std      | 0.3818334609270096    |
| train_1/mu_loss           | 6.020807196243944     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -13.985466843326062   |
| train_1/q_grads           | -0.10716471020132304  |
| train_1/q_grads_std       | 0.46519755125045775   |
| train_1/q_loss            | 0.38450779800593926   |
| train_1/reward            | -2.0992712783030583   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.003515625           |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -13.047308530009076   |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.01
Training epoch 64
Time for epoch 64: 475.33. Rollout time: 264.52, Training time: 210.76
Evaluating epoch 64
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 64                    |
| policy/steps              | 5912903.0             |
| test/episodes             | 1625.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -26.999999999999748   |
| test_1/avg_q              | -13.39534882425183    |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 6500.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.999999999805283   |
| train_0/current_q         | -9.80733846016858     |
| train_0/fw_bonus          | -0.9992866188287735   |
| train_0/fw_loss           | 0.0069679618231020864 |
| train_0/mu_grads          | -0.04667052896693349  |
| train_0/mu_grads_std      | 0.5892353773117065    |
| train_0/mu_loss           | 9.644232974649896     |
| train_0/next_q            | -9.635065992546533    |
| train_0/q_grads           | -0.015531941782683134 |
| train_0/q_grads_std       | 0.37436018139123917   |
| train_0/q_loss            | 0.28801339877881693   |
| train_0/reward            | -0.7778049325970642   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0961181640625       |
| train_0/target_q          | -9.957202073317077    |
| train_1/avg_q             | -14.05071630283891    |
| train_1/current_q         | -13.07729444004659    |
| train_1/fw_bonus          | -0.9881968826055527   |
| train_1/fw_loss           | 0.12285863272845746   |
| train_1/mu_grads          | -0.07480024285614491  |
| train_1/mu_grads_std      | 0.3823457844555378    |
| train_1/mu_loss           | 5.803766347161381     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -14.010357227166029   |
| train_1/q_grads           | -0.10753352101892233  |
| train_1/q_grads_std       | 0.4674509257078171    |
| train_1/q_loss            | 0.3527159526422767    |
| train_1/reward            | -2.083771293643804    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0033935546875       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -13.081845189642214   |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.01
Training epoch 65
Time for epoch 65: 474.06. Rollout time: 261.42, Training time: 212.59
Evaluating epoch 65
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 65                    |
| policy/steps              | 6004028.0             |
| test/episodes             | 1650.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -26.999999999999993   |
| test_1/avg_q              | -11.608561714609653   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 6600.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.999999992579788   |
| train_0/current_q         | -9.828302297032323    |
| train_0/fw_bonus          | -0.9993102893233299   |
| train_0/fw_loss           | 0.006749031157232821  |
| train_0/mu_grads          | -0.04845819044858217  |
| train_0/mu_grads_std      | 0.5914126038551331    |
| train_0/mu_loss           | 9.668830996763807     |
| train_0/next_q            | -9.660060799578549    |
| train_0/q_grads           | -0.015604474279098213 |
| train_0/q_grads_std       | 0.37643342912197114   |
| train_0/q_loss            | 0.28761850774822983   |
| train_0/reward            | -0.7766815144757857   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.08017578125         |
| train_0/target_q          | -9.978287800426509    |
| train_1/avg_q             | -14.009166483452141   |
| train_1/current_q         | -13.023253018197295   |
| train_1/fw_bonus          | -0.9886192262172699   |
| train_1/fw_loss           | 0.11899704206734896   |
| train_1/mu_grads          | -0.07486096657812595  |
| train_1/mu_grads_std      | 0.3826842062175274    |
| train_1/mu_loss           | 5.930065592075827     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -14.009906722139595   |
| train_1/q_grads           | -0.10877079870551824  |
| train_1/q_grads_std       | 0.4698335736989975    |
| train_1/q_loss            | 0.5030638586714966    |
| train_1/reward            | -2.1194561030752084   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.003271484375        |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -13.02308699700642    |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 66
Time for epoch 66: 469.90. Rollout time: 259.37, Training time: 210.49
Evaluating epoch 66
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 66                    |
| policy/steps              | 6095153.0             |
| test/episodes             | 1675.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -26.99999999999996    |
| test_1/avg_q              | -13.098080766864161   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 6700.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.999999999630273   |
| train_0/current_q         | -9.890044929544667    |
| train_0/fw_bonus          | -0.9993394374847412   |
| train_0/fw_loss           | 0.006479442212730646  |
| train_0/mu_grads          | -0.049435594398528335 |
| train_0/mu_grads_std      | 0.5937025949358941    |
| train_0/mu_loss           | 9.737028763473896     |
| train_0/next_q            | -9.7315592787056      |
| train_0/q_grads           | -0.016133024543523788 |
| train_0/q_grads_std       | 0.37854073494672774   |
| train_0/q_loss            | 0.25511863086872033   |
| train_0/reward            | -0.7728752882947447   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.100048828125        |
| train_0/target_q          | -10.043672566802277   |
| train_1/avg_q             | -14.029622018573683   |
| train_1/current_q         | -13.002170870209929   |
| train_1/fw_bonus          | -0.9893005281686783   |
| train_1/fw_loss           | 0.11276795454323292   |
| train_1/mu_grads          | -0.07510653305798769  |
| train_1/mu_grads_std      | 0.38316195607185366   |
| train_1/mu_loss           | 5.99121437861927      |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -14.00063460718648    |
| train_1/q_grads           | -0.11035925075411797  |
| train_1/q_grads_std       | 0.4727682687342167    |
| train_1/q_loss            | 0.25761154202546177   |
| train_1/reward            | -2.1228607148317677   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.003955078125        |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -13.003416513180236   |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 67
Time for epoch 67: 471.20. Rollout time: 261.99, Training time: 209.15
Evaluating epoch 67
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 67                    |
| policy/steps              | 6186091.0             |
| test/episodes             | 1700.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -26.999999999999776   |
| test_1/avg_q              | -14.044937950599818   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 6800.0                |
| train/success_rate        | 0.01                  |
| train_0/avg_q             | -26.99999999997717    |
| train_0/current_q         | -9.799944326667973    |
| train_0/fw_bonus          | -0.9993334725499153   |
| train_0/fw_loss           | 0.0065346516901627185 |
| train_0/mu_grads          | -0.05019209189340472  |
| train_0/mu_grads_std      | 0.596348337829113     |
| train_0/mu_loss           | 9.644768677381464     |
| train_0/next_q            | -9.63821542604865     |
| train_0/q_grads           | -0.01710548340342939  |
| train_0/q_grads_std       | 0.38033449947834014   |
| train_0/q_loss            | 0.2587251128268204    |
| train_0/reward            | -0.7725793346980936   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0859375             |
| train_0/target_q          | -9.955125936466949    |
| train_1/avg_q             | -14.036978616325891   |
| train_1/current_q         | -13.087196087248838   |
| train_1/fw_bonus          | -0.9895508468151093   |
| train_1/fw_loss           | 0.11047933604568243   |
| train_1/mu_grads          | -0.07517457902431487  |
| train_1/mu_grads_std      | 0.3835317775607109    |
| train_1/mu_loss           | 6.178822674481535     |
| train_1/n_subgoals        | 2694.0                |
| train_1/next_q            | -14.033742168746667   |
| train_1/q_grads           | -0.1113222660496831   |
| train_1/q_grads_std       | 0.4756717137992382    |
| train_1/q_loss            | 0.4917535100433167    |
| train_1/reward            | -2.123364289922756    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.004541015625        |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -13.094788700568506   |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 68
Time for epoch 68: 477.71. Rollout time: 262.66, Training time: 214.99
Evaluating epoch 68
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 68                    |
| policy/steps              | 6277216.0             |
| test/episodes             | 1725.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -26.999999999999858   |
| test_1/avg_q              | -14.165583446162888   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 6900.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.999999990294622   |
| train_0/current_q         | -9.866857699257162    |
| train_0/fw_bonus          | -0.9993278861045838   |
| train_0/fw_loss           | 0.0065862952847965065 |
| train_0/mu_grads          | -0.049931714311242104 |
| train_0/mu_grads_std      | 0.600107592344284     |
| train_0/mu_loss           | 9.709702380014509     |
| train_0/next_q            | -9.699782493247813    |
| train_0/q_grads           | -0.01767397583462298  |
| train_0/q_grads_std       | 0.38248310014605524   |
| train_0/q_loss            | 0.2532634798932939    |
| train_0/reward            | -0.7755550583111471   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0937744140625       |
| train_0/target_q          | -10.021119736950435   |
| train_1/avg_q             | -14.032162761237183   |
| train_1/current_q         | -13.063737051585639   |
| train_1/fw_bonus          | -0.9892650127410889   |
| train_1/fw_loss           | 0.11309278067201375   |
| train_1/mu_grads          | -0.07519063577055932  |
| train_1/mu_grads_std      | 0.38406311720609665   |
| train_1/mu_loss           | 6.212285567271977     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -14.014080038826759   |
| train_1/q_grads           | -0.11232142504304647  |
| train_1/q_grads_std       | 0.4783323399722576    |
| train_1/q_loss            | 0.3622886041539736    |
| train_1/reward            | -2.16766717018254     |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0043701171875       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -13.068715029059803   |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 69
Time for epoch 69: 476.06. Rollout time: 262.38, Training time: 213.63
Evaluating epoch 69
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 69                    |
| policy/steps              | 6368341.0             |
| test/episodes             | 1750.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -13.30159297250365    |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 7000.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.999999997957477   |
| train_0/current_q         | -9.827087653016198    |
| train_0/fw_bonus          | -0.9993001028895379   |
| train_0/fw_loss           | 0.00684332954697311   |
| train_0/mu_grads          | -0.049831059761345387 |
| train_0/mu_grads_std      | 0.6040918976068497    |
| train_0/mu_loss           | 9.667524084336652     |
| train_0/next_q            | -9.658701919493897    |
| train_0/q_grads           | -0.018695946037769317 |
| train_0/q_grads_std       | 0.3842277802526951    |
| train_0/q_loss            | 0.2549444238664103    |
| train_0/reward            | -0.7773844245559303   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0875244140625       |
| train_0/target_q          | -9.981089106805982    |
| train_1/avg_q             | -14.087132033781788   |
| train_1/current_q         | -13.133665184249722   |
| train_1/fw_bonus          | -0.9879818022251129   |
| train_1/fw_loss           | 0.12482512649148703   |
| train_1/mu_grads          | -0.07531395014375449  |
| train_1/mu_grads_std      | 0.3846732087433338    |
| train_1/mu_loss           | 6.204243589031595     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -13.991861686196836   |
| train_1/q_grads           | -0.11323354933410883  |
| train_1/q_grads_std       | 0.4831188730895519    |
| train_1/q_loss            | 0.30851423168109726   |
| train_1/reward            | -2.1283048661112844   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0040283203125       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -13.134552041638235   |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 70
Time for epoch 70: 475.40. Rollout time: 261.87, Training time: 213.48
Evaluating epoch 70
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 70                    |
| policy/steps              | 6459466.0             |
| test/episodes             | 1775.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -13.507072611740055   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 7100.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.999999997133393   |
| train_0/current_q         | -9.844055508869014    |
| train_0/fw_bonus          | -0.9992734014987945   |
| train_0/fw_loss           | 0.007090324605815113  |
| train_0/mu_grads          | -0.05089151756837964  |
| train_0/mu_grads_std      | 0.6075775861740113    |
| train_0/mu_loss           | 9.676891748244099     |
| train_0/next_q            | -9.668313253996136    |
| train_0/q_grads           | -0.018912394111976028 |
| train_0/q_grads_std       | 0.38585255965590476   |
| train_0/q_loss            | 0.26118921009729945   |
| train_0/reward            | -0.7808012471061374   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.093310546875        |
| train_0/target_q          | -9.99664144673219     |
| train_1/avg_q             | -13.96595805324638    |
| train_1/current_q         | -13.143029781205659   |
| train_1/fw_bonus          | -0.9875201731920242   |
| train_1/fw_loss           | 0.12904578614979983   |
| train_1/mu_grads          | -0.07513035871088505  |
| train_1/mu_grads_std      | 0.3857172645628452    |
| train_1/mu_loss           | 6.097430238039888     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -13.999562646628789   |
| train_1/q_grads           | -0.11379891131073236  |
| train_1/q_grads_std       | 0.4873869113624096    |
| train_1/q_loss            | 0.38571645556545464   |
| train_1/reward            | -2.1355203412487755   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.00302734375         |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -13.143793512681134   |
-----------------------------------------------------
Saving periodic policy to data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_70.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 71
Time for epoch 71: 434.23. Rollout time: 235.49, Training time: 198.71
Evaluating epoch 71
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
----------------------------------------------------
| epoch                     | 71                   |
| policy/steps              | 6550349.0            |
| test/episodes             | 1800.0               |
| test/success_rate         | 0.0                  |
| test_0/avg_q              | -26.99999999999998   |
| test_1/avg_q              | -13.458408908280445  |
| test_1/n_subgoals         | 675.0                |
| test_1/subgoal_succ_rate  | 0.0                  |
| train/episodes            | 7200.0               |
| train/success_rate        | 0.01                 |
| train_0/avg_q             | -26.99999999967229   |
| train_0/current_q         | -9.960426604698913   |
| train_0/fw_bonus          | -0.9992749392986298  |
| train_0/fw_loss           | 0.007075942296069115 |
| train_0/mu_grads          | -0.05187023123726249 |
| train_0/mu_grads_std      | 0.6113529950380325   |
| train_0/mu_loss           | 9.79492189195915     |
| train_0/next_q            | -9.787741018976135   |
| train_0/q_grads           | -0.01925490596331656 |
| train_0/q_grads_std       | 0.3881648577749729   |
| train_0/q_loss            | 0.28408046065389564  |
| train_0/reward            | -0.7829870834779286  |
| train_0/reward_-0.0_frac  | 0.0                  |
| train_0/reward_-1.0_frac  | 0.0729248046875      |
| train_0/target_q          | -10.113486277077413  |
| train_1/avg_q             | -14.026978164253357  |
| train_1/current_q         | -13.145301669646136  |
| train_1/fw_bonus          | -0.9872798919677734  |
| train_1/fw_loss           | 0.13124271370470525  |
| train_1/mu_grads          | -0.07530067358165979 |
| train_1/mu_grads_std      | 0.38603883758187296  |
| train_1/mu_loss           | 6.08187695863725     |
| train_1/n_subgoals        | 2692.0               |
| train_1/next_q            | -14.024933673519593  |
| train_1/q_grads           | -0.11446205247193575 |
| train_1/q_grads_std       | 0.4910743869841099   |
| train_1/q_loss            | 0.16417542100338173  |
| train_1/reward            | -2.1659299857426957  |
| train_1/reward_-0.0_frac  | 0.0                  |
| train_1/reward_-1.0_frac  | 0.003466796875       |
| train_1/reward_-27.0_frac | 0.0                  |
| train_1/subgoal_succ_rate | 0.0                  |
| train_1/target_q          | -13.145388013779629  |
----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 72
Time for epoch 72: 433.43. Rollout time: 231.74, Training time: 201.65
Evaluating epoch 72
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 72                    |
| policy/steps              | 6641474.0             |
| test/episodes             | 1825.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -26.999999999999993   |
| test_1/avg_q              | -14.003746768532626   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 7300.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.99999979642538    |
| train_0/current_q         | -9.938980491461958    |
| train_0/fw_bonus          | -0.9992286875844002   |
| train_0/fw_loss           | 0.007503745274152607  |
| train_0/mu_grads          | -0.05351688023656607  |
| train_0/mu_grads_std      | 0.6150311887264251    |
| train_0/mu_loss           | 9.775047030504641     |
| train_0/next_q            | -9.765119166713399    |
| train_0/q_grads           | -0.019984438456594943 |
| train_0/q_grads_std       | 0.3897220708429813    |
| train_0/q_loss            | 0.267618051924579     |
| train_0/reward            | -0.7847461821678735   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.071044921875        |
| train_0/target_q          | -10.094938651189292   |
| train_1/avg_q             | -14.016832006985364   |
| train_1/current_q         | -13.122020217644458   |
| train_1/fw_bonus          | -0.9866492062807083   |
| train_1/fw_loss           | 0.13700901586562395   |
| train_1/mu_grads          | -0.07553478833287955  |
| train_1/mu_grads_std      | 0.3868956819176674    |
| train_1/mu_loss           | 5.847282455469967     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -14.007610620613644   |
| train_1/q_grads           | -0.11574104651808739  |
| train_1/q_grads_std       | 0.4953734286129475    |
| train_1/q_loss            | 0.333275109748424     |
| train_1/reward            | -2.1189105456214747   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.002392578125        |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -13.132900590364764   |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 73
Time for epoch 73: 427.01. Rollout time: 228.35, Training time: 198.63
Evaluating epoch 73
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 73                    |
| policy/steps              | 6732599.0             |
| test/episodes             | 1850.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -13.704158897824016   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 7400.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.999999997868695   |
| train_0/current_q         | -9.839008966935651    |
| train_0/fw_bonus          | -0.9992370188236237   |
| train_0/fw_loss           | 0.00742686812300235   |
| train_0/mu_grads          | -0.054400058649480346 |
| train_0/mu_grads_std      | 0.6188574746251106    |
| train_0/mu_loss           | 9.668709341551738     |
| train_0/next_q            | -9.661057685287812    |
| train_0/q_grads           | -0.019751790910959244 |
| train_0/q_grads_std       | 0.39151213243603705   |
| train_0/q_loss            | 0.2744573277672627    |
| train_0/reward            | -0.7849944788533321   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0807861328125       |
| train_0/target_q          | -9.989222765490782    |
| train_1/avg_q             | -14.028944679202606   |
| train_1/current_q         | -13.135755051767973   |
| train_1/fw_bonus          | -0.9863471165299416   |
| train_1/fw_loss           | 0.13977122791111468   |
| train_1/mu_grads          | -0.07580644432455301  |
| train_1/mu_grads_std      | 0.3875392869114876    |
| train_1/mu_loss           | 6.04655057940238      |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -14.004022678052227   |
| train_1/q_grads           | -0.11747916769236326  |
| train_1/q_grads_std       | 0.4998787760734558    |
| train_1/q_loss            | 0.2635773135617545    |
| train_1/reward            | -2.1261203282600034   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.003662109375        |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -13.140076055221362   |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 74
Time for epoch 74: 426.57. Rollout time: 227.71, Training time: 198.82
Evaluating epoch 74
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 74                    |
| policy/steps              | 6823724.0             |
| test/episodes             | 1875.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -13.95067974312529    |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 7500.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.999999999999993   |
| train_0/current_q         | -9.917125499086595    |
| train_0/fw_bonus          | -0.9992745235562325   |
| train_0/fw_loss           | 0.007079922745469957  |
| train_0/mu_grads          | -0.05544356657192111  |
| train_0/mu_grads_std      | 0.6220189377665519    |
| train_0/mu_loss           | 9.74843618189088      |
| train_0/next_q            | -9.740287188356008    |
| train_0/q_grads           | -0.020603742357343434 |
| train_0/q_grads_std       | 0.3931622251868248    |
| train_0/q_loss            | 0.26494875073295326   |
| train_0/reward            | -0.7839634305622895   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.06669921875         |
| train_0/target_q          | -10.069881189509891   |
| train_1/avg_q             | -14.092038243923232   |
| train_1/current_q         | -13.09531994409087    |
| train_1/fw_bonus          | -0.9866431295871735   |
| train_1/fw_loss           | 0.13706465922296046   |
| train_1/mu_grads          | -0.07590353284031152  |
| train_1/mu_grads_std      | 0.3883435562252998    |
| train_1/mu_loss           | 6.012247793218236     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -14.027087553846354   |
| train_1/q_grads           | -0.11838100980967284  |
| train_1/q_grads_std       | 0.5038284912705422    |
| train_1/q_loss            | 0.3486436806762367    |
| train_1/reward            | -2.124595978720026    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0033935546875       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -13.092595314710678   |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 75
Time for epoch 75: 430.93. Rollout time: 230.22, Training time: 200.67
Evaluating epoch 75
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 75                    |
| policy/steps              | 6914849.0             |
| test/episodes             | 1900.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -13.889780885135142   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 7600.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.999999999999968   |
| train_0/current_q         | -9.900278315819486    |
| train_0/fw_bonus          | -0.9992646351456642   |
| train_0/fw_loss           | 0.007171254605054855  |
| train_0/mu_grads          | -0.05478123668581247  |
| train_0/mu_grads_std      | 0.6246474757790565    |
| train_0/mu_loss           | 9.735553811475928     |
| train_0/next_q            | -9.726240570768121    |
| train_0/q_grads           | -0.020544329285621644 |
| train_0/q_grads_std       | 0.3954003818333149    |
| train_0/q_loss            | 0.27154888012914047   |
| train_0/reward            | -0.7817368132658885   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0845947265625       |
| train_0/target_q          | -10.053840948726991   |
| train_1/avg_q             | -14.009632407793466   |
| train_1/current_q         | -12.968939047474048   |
| train_1/fw_bonus          | -0.9868625462055206   |
| train_1/fw_loss           | 0.13505854327231645   |
| train_1/mu_grads          | -0.07606177311390638  |
| train_1/mu_grads_std      | 0.38867864981293676   |
| train_1/mu_loss           | 6.046761311101895     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -14.005223520188219   |
| train_1/q_grads           | -0.11928615048527717  |
| train_1/q_grads_std       | 0.5079899519681931    |
| train_1/q_loss            | 0.2715835219944564    |
| train_1/reward            | -2.1012559784037874   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.00380859375         |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -12.969777492132215   |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 76
Time for epoch 76: 427.26. Rollout time: 227.41, Training time: 199.82
Evaluating epoch 76
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 76                    |
| policy/steps              | 7005974.0             |
| test/episodes             | 1925.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -26.99999999999942    |
| test_1/avg_q              | -13.998777594765583   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 7700.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.99999999988787    |
| train_0/current_q         | -9.828700024843995    |
| train_0/fw_bonus          | -0.9992938682436943   |
| train_0/fw_loss           | 0.006900940649211406  |
| train_0/mu_grads          | -0.053451371286064385 |
| train_0/mu_grads_std      | 0.6268658310174942    |
| train_0/mu_loss           | 9.672509802097114     |
| train_0/next_q            | -9.662483565913268    |
| train_0/q_grads           | -0.02111531109549105  |
| train_0/q_grads_std       | 0.3968506522476673    |
| train_0/q_loss            | 0.26197832523680453   |
| train_0/reward            | -0.7761483994814625   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0884765625          |
| train_0/target_q          | -9.98229267353341     |
| train_1/avg_q             | -13.989175957700876   |
| train_1/current_q         | -12.966719274384209   |
| train_1/fw_bonus          | -0.986228646337986    |
| train_1/fw_loss           | 0.14085427820682525   |
| train_1/mu_grads          | -0.07648269031196833  |
| train_1/mu_grads_std      | 0.3890905849635601    |
| train_1/mu_loss           | 5.933994262586431     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -13.99615246347887    |
| train_1/q_grads           | -0.12001731339842081  |
| train_1/q_grads_std       | 0.5118086248636246    |
| train_1/q_loss            | 0.40191883969537756   |
| train_1/reward            | -2.1045401037546982   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0040283203125       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -12.974494978812254   |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 77
Time for epoch 77: 424.77. Rollout time: 229.64, Training time: 195.10
Evaluating epoch 77
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 77                    |
| policy/steps              | 7097099.0             |
| test/episodes             | 1950.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -26.999999999999954   |
| test_1/avg_q              | -13.3542145536043     |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 7800.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.99999999999834    |
| train_0/current_q         | -9.876649817842104    |
| train_0/fw_bonus          | -0.999320924282074    |
| train_0/fw_loss           | 0.006650782120414078  |
| train_0/mu_grads          | -0.053509251307696105 |
| train_0/mu_grads_std      | 0.6301266193389893    |
| train_0/mu_loss           | 9.7248334058958       |
| train_0/next_q            | -9.716068707127045    |
| train_0/q_grads           | -0.021622328879311682 |
| train_0/q_grads_std       | 0.3989589288830757    |
| train_0/q_loss            | 0.25186520258587264   |
| train_0/reward            | -0.772499814350158    |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.081201171875        |
| train_0/target_q          | -10.03220640549739    |
| train_1/avg_q             | -14.018639598767983   |
| train_1/current_q         | -12.966498827677915   |
| train_1/fw_bonus          | -0.9869893968105317   |
| train_1/fw_loss           | 0.13389869313687086   |
| train_1/mu_grads          | -0.0764647476375103   |
| train_1/mu_grads_std      | 0.38951090052723886   |
| train_1/mu_loss           | 5.76488735037667      |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -14.070043226214187   |
| train_1/q_grads           | -0.12098426688462496  |
| train_1/q_grads_std       | 0.515026593208313     |
| train_1/q_loss            | 0.5385325820204068    |
| train_1/reward            | -2.1047495819620963   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.004638671875        |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -12.97273249296077    |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 78
Time for epoch 78: 427.42. Rollout time: 230.47, Training time: 196.92
Evaluating epoch 78
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
----------------------------------------------------
| epoch                     | 78                   |
| policy/steps              | 7188224.0            |
| test/episodes             | 1975.0               |
| test/success_rate         | 0.0                  |
| test_0/avg_q              | -26.99999999998491   |
| test_1/avg_q              | -14.066459940783803  |
| test_1/n_subgoals         | 675.0                |
| test_1/subgoal_succ_rate  | 0.0                  |
| train/episodes            | 7900.0               |
| train/success_rate        | 0.0                  |
| train_0/avg_q             | -26.99999999999458   |
| train_0/current_q         | -9.828631437679983   |
| train_0/fw_bonus          | -0.9993179187178611  |
| train_0/fw_loss           | 0.006678482168354094 |
| train_0/mu_grads          | -0.05549003854393959 |
| train_0/mu_grads_std      | 0.6338230639696121   |
| train_0/mu_loss           | 9.676951747836743    |
| train_0/next_q            | -9.670542196215667   |
| train_0/q_grads           | -0.02274110377766192 |
| train_0/q_grads_std       | 0.4008028574287891   |
| train_0/q_loss            | 0.28524739309492836  |
| train_0/reward            | -0.7698249769389804  |
| train_0/reward_-0.0_frac  | 0.0                  |
| train_0/reward_-1.0_frac  | 0.0939697265625      |
| train_0/target_q          | -9.979634182240549   |
| train_1/avg_q             | -14.000062035930874  |
| train_1/current_q         | -12.962895308191642  |
| train_1/fw_bonus          | -0.9867611050605773  |
| train_1/fw_loss           | 0.13598605003207923  |
| train_1/mu_grads          | -0.07655398361384869 |
| train_1/mu_grads_std      | 0.389944364130497    |
| train_1/mu_loss           | 6.008546081385546    |
| train_1/n_subgoals        | 2700.0               |
| train_1/next_q            | -14.039564944620967  |
| train_1/q_grads           | -0.12248300462961197 |
| train_1/q_grads_std       | 0.5184694230556488   |
| train_1/q_loss            | 0.35270673165568034  |
| train_1/reward            | -2.1337736159257474  |
| train_1/reward_-0.0_frac  | 0.0                  |
| train_1/reward_-1.0_frac  | 0.00390625           |
| train_1/reward_-27.0_frac | 0.0                  |
| train_1/subgoal_succ_rate | 0.0                  |
| train_1/target_q          | -12.963630892095296  |
----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 79
Time for epoch 79: 431.38. Rollout time: 231.04, Training time: 200.31
Evaluating epoch 79
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 79                    |
| policy/steps              | 7279139.0             |
| test/episodes             | 2000.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -26.99999999999999    |
| test_1/avg_q              | -13.764801204254136   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 8000.0                |
| train/success_rate        | 0.01                  |
| train_0/avg_q             | -26.9999999997914     |
| train_0/current_q         | -9.87834976742495     |
| train_0/fw_bonus          | -0.9993051022291184   |
| train_0/fw_loss           | 0.006797037879005074  |
| train_0/mu_grads          | -0.054543303325772285 |
| train_0/mu_grads_std      | 0.638176566362381     |
| train_0/mu_loss           | 9.721204188512093     |
| train_0/next_q            | -9.711599020172788    |
| train_0/q_grads           | -0.023346845386549832 |
| train_0/q_grads_std       | 0.40282009467482566   |
| train_0/q_loss            | 0.2665826201529589    |
| train_0/reward            | -0.7758366339941858   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0732177734375       |
| train_0/target_q          | -10.027963727270972   |
| train_1/avg_q             | -14.042201681188688   |
| train_1/current_q         | -12.956618349962394   |
| train_1/fw_bonus          | -0.9866532891988754   |
| train_1/fw_loss           | 0.13697167560458184   |
| train_1/mu_grads          | -0.07660947162657976  |
| train_1/mu_grads_std      | 0.390472749620676     |
| train_1/mu_loss           | 6.009898473294214     |
| train_1/n_subgoals        | 2693.0                |
| train_1/next_q            | -14.067738231111985   |
| train_1/q_grads           | -0.12302702367305755  |
| train_1/q_grads_std       | 0.5225355789065361    |
| train_1/q_loss            | 0.2756568488839911    |
| train_1/reward            | -2.1125161022879184   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.004443359375        |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -12.961161676674646   |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 80
Time for epoch 80: 429.27. Rollout time: 230.01, Training time: 199.23
Evaluating epoch 80
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
----------------------------------------------------
| epoch                     | 80                   |
| policy/steps              | 7370068.0            |
| test/episodes             | 2025.0               |
| test/success_rate         | 0.0                  |
| test_0/avg_q              | -26.999999999999872  |
| test_1/avg_q              | -13.480015943636042  |
| test_1/n_subgoals         | 675.0                |
| test_1/subgoal_succ_rate  | 0.0                  |
| train/episodes            | 8100.0               |
| train/success_rate        | 0.03                 |
| train_0/avg_q             | -26.999999721287185  |
| train_0/current_q         | -9.832240819785099   |
| train_0/fw_bonus          | -0.9992729961872101  |
| train_0/fw_loss           | 0.007093940454069525 |
| train_0/mu_grads          | -0.05547456154599786 |
| train_0/mu_grads_std      | 0.6415859818458557   |
| train_0/mu_loss           | 9.667143253163701    |
| train_0/next_q            | -9.653738016496614   |
| train_0/q_grads           | -0.0237653864081949  |
| train_0/q_grads_std       | 0.40457126647233965  |
| train_0/q_loss            | 0.28225055875579597  |
| train_0/reward            | -0.7803249774638971  |
| train_0/reward_-0.0_frac  | 0.0                  |
| train_0/reward_-1.0_frac  | 0.1062744140625      |
| train_0/target_q          | -9.98039166168493    |
| train_1/avg_q             | -14.118224485991108  |
| train_1/current_q         | -13.066242099287468  |
| train_1/fw_bonus          | -0.98603805154562    |
| train_1/fw_loss           | 0.14259688630700112  |
| train_1/mu_grads          | -0.07677791956812144 |
| train_1/mu_grads_std      | 0.39021722301840783  |
| train_1/mu_loss           | 6.094913054775556    |
| train_1/n_subgoals        | 2694.0               |
| train_1/next_q            | -14.034709947190748  |
| train_1/q_grads           | -0.12374998610466718 |
| train_1/q_grads_std       | 0.5269231706857681   |
| train_1/q_loss            | 0.42376267195716066  |
| train_1/reward            | -2.133875959913712   |
| train_1/reward_-0.0_frac  | 0.0                  |
| train_1/reward_-1.0_frac  | 0.004638671875       |
| train_1/reward_-27.0_frac | 0.0                  |
| train_1/subgoal_succ_rate | 0.0                  |
| train_1/target_q          | -13.07232351841634   |
----------------------------------------------------
Saving periodic policy to data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_80.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 81
Time for epoch 81: 428.89. Rollout time: 231.66, Training time: 197.20
Evaluating epoch 81
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 81                    |
| policy/steps              | 7461193.0             |
| test/episodes             | 2050.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -26.99999999997133    |
| test_1/avg_q              | -14.307689253644211   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 8200.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.999999987357096   |
| train_0/current_q         | -9.909615840022985    |
| train_0/fw_bonus          | -0.9992866069078445   |
| train_0/fw_loss           | 0.0069681605906225744 |
| train_0/mu_grads          | -0.05573981339111924  |
| train_0/mu_grads_std      | 0.6447393640875816    |
| train_0/mu_loss           | 9.74697643294041      |
| train_0/next_q            | -9.733811700886259    |
| train_0/q_grads           | -0.02398382811807096  |
| train_0/q_grads_std       | 0.40581731498241425   |
| train_0/q_loss            | 0.26444448712090873   |
| train_0/reward            | -0.7820773830993858   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.100244140625        |
| train_0/target_q          | -10.062352899354101   |
| train_1/avg_q             | -14.041501411496165   |
| train_1/current_q         | -13.065874892056058   |
| train_1/fw_bonus          | -0.9861415162682533   |
| train_1/fw_loss           | 0.14165084585547447   |
| train_1/mu_grads          | -0.07703146040439605  |
| train_1/mu_grads_std      | 0.39069713801145556   |
| train_1/mu_loss           | 6.519812861309726     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -14.008920670598474   |
| train_1/q_grads           | -0.12556656450033188  |
| train_1/q_grads_std       | 0.5318993151187896    |
| train_1/q_loss            | 0.47901512075667974   |
| train_1/reward            | -2.1730512299458495   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0040771484375       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -13.07102518971397    |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 82
Time for epoch 82: 434.94. Rollout time: 233.94, Training time: 200.97
Evaluating epoch 82
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 82                    |
| policy/steps              | 7552318.0             |
| test/episodes             | 2075.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -14.000376259414416   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 8300.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.999999997260698   |
| train_0/current_q         | -9.89306514042338     |
| train_0/fw_bonus          | -0.999300068616867    |
| train_0/fw_loss           | 0.006843522109556943  |
| train_0/mu_grads          | -0.0540281574241817   |
| train_0/mu_grads_std      | 0.6475669801235199    |
| train_0/mu_loss           | 9.726321764622492     |
| train_0/next_q            | -9.714471630279473    |
| train_0/q_grads           | -0.024204381555318833 |
| train_0/q_grads_std       | 0.40735533758997916   |
| train_0/q_loss            | 0.2893684726095067    |
| train_0/reward            | -0.7848429103447415   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.07919921875         |
| train_0/target_q          | -10.044051621884758   |
| train_1/avg_q             | -14.153618078651974   |
| train_1/current_q         | -13.14798657015973    |
| train_1/fw_bonus          | -0.9859642043709755   |
| train_1/fw_loss           | 0.14327203556895257   |
| train_1/mu_grads          | -0.07733943685889244  |
| train_1/mu_grads_std      | 0.39171319678425787   |
| train_1/mu_loss           | 6.341059530110529     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -14.030283899043088   |
| train_1/q_grads           | -0.12654325291514396  |
| train_1/q_grads_std       | 0.5362652018666267    |
| train_1/q_loss            | 0.2837799055190897    |
| train_1/reward            | -2.1034801511224943   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.00361328125         |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -13.146097881390347   |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 83
Time for epoch 83: 442.95. Rollout time: 233.87, Training time: 209.05
Evaluating epoch 83
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 83                    |
| policy/steps              | 7643443.0             |
| test/episodes             | 2100.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -26.999999999999993   |
| test_1/avg_q              | -13.631069312452231   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 8400.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.999999999427107   |
| train_0/current_q         | -9.965201143163622    |
| train_0/fw_bonus          | -0.9993145182728768   |
| train_0/fw_loss           | 0.006709914840757847  |
| train_0/mu_grads          | -0.05423390120267868  |
| train_0/mu_grads_std      | 0.650681784749031     |
| train_0/mu_loss           | 9.79524188894718      |
| train_0/next_q            | -9.788148050296888    |
| train_0/q_grads           | -0.024538984522223473 |
| train_0/q_grads_std       | 0.4091114796698093    |
| train_0/q_loss            | 0.2781543476333252    |
| train_0/reward            | -0.785423863476899    |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0697265625          |
| train_0/target_q          | -10.120771839834596   |
| train_1/avg_q             | -14.05478857518909    |
| train_1/current_q         | -13.19307047292583    |
| train_1/fw_bonus          | -0.9856980875134468   |
| train_1/fw_loss           | 0.14570522159337998   |
| train_1/mu_grads          | -0.07764695528894663  |
| train_1/mu_grads_std      | 0.3932445265352726    |
| train_1/mu_loss           | 5.978676931557571     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -14.039628159383136   |
| train_1/q_grads           | -0.1267982769757509   |
| train_1/q_grads_std       | 0.5397026091814041    |
| train_1/q_loss            | 0.3071868106021235    |
| train_1/reward            | -2.124160077160923    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.004150390625        |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -13.194070248335674   |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 84
Time for epoch 84: 443.74. Rollout time: 238.29, Training time: 205.41
Evaluating epoch 84
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 84                    |
| policy/steps              | 7734568.0             |
| test/episodes             | 2125.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -13.67170501895606    |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 8500.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.999999999993026   |
| train_0/current_q         | -9.879984293822012    |
| train_0/fw_bonus          | -0.9993193641304969   |
| train_0/fw_loss           | 0.006665158388204873  |
| train_0/mu_grads          | -0.0542776039801538   |
| train_0/mu_grads_std      | 0.6534386545419693    |
| train_0/mu_loss           | 9.715122916502242     |
| train_0/next_q            | -9.704262404803746    |
| train_0/q_grads           | -0.025396640971302988 |
| train_0/q_grads_std       | 0.41112301498651505   |
| train_0/q_loss            | 0.26127614915322483   |
| train_0/reward            | -0.7813979628641391   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.091015625           |
| train_0/target_q          | -10.034580971435798   |
| train_1/avg_q             | -14.113363783799178   |
| train_1/current_q         | -13.156707428311625   |
| train_1/fw_bonus          | -0.9859566822648048   |
| train_1/fw_loss           | 0.14334086440503596   |
| train_1/mu_grads          | -0.07764808256179094  |
| train_1/mu_grads_std      | 0.39435721188783646   |
| train_1/mu_loss           | 5.7468012232945345    |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -14.0220968604654     |
| train_1/q_grads           | -0.12672074399888517  |
| train_1/q_grads_std       | 0.5447310134768486    |
| train_1/q_loss            | 0.24368624922474486   |
| train_1/reward            | -2.077341133032314    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0036865234375       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -13.156446320818748   |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 85
Time for epoch 85: 475.93. Rollout time: 255.79, Training time: 220.10
Evaluating epoch 85
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
----------------------------------------------------
| epoch                     | 85                   |
| policy/steps              | 7825693.0            |
| test/episodes             | 2150.0               |
| test/success_rate         | 0.0                  |
| test_0/avg_q              | -27.0                |
| test_1/avg_q              | -13.632363610714458  |
| test_1/n_subgoals         | 675.0                |
| test_1/subgoal_succ_rate  | 0.0                  |
| train/episodes            | 8600.0               |
| train/success_rate        | 0.0                  |
| train_0/avg_q             | -26.99999999990548   |
| train_0/current_q         | -9.819300466936287   |
| train_0/fw_bonus          | -0.9993255406618118  |
| train_0/fw_loss           | 0.006607997405808419 |
| train_0/mu_grads          | -0.05401396220549941 |
| train_0/mu_grads_std      | 0.6562308311462403   |
| train_0/mu_loss           | 9.66024017090336     |
| train_0/next_q            | -9.654043289006008   |
| train_0/q_grads           | -0.02564824870787561 |
| train_0/q_grads_std       | 0.412877568602562    |
| train_0/q_loss            | 0.26481400172364905  |
| train_0/reward            | -0.7783615021733568  |
| train_0/reward_-0.0_frac  | 0.0                  |
| train_0/reward_-1.0_frac  | 0.1001953125         |
| train_0/target_q          | -9.972723832660622   |
| train_1/avg_q             | -13.99547194176438   |
| train_1/current_q         | -13.033569373295242  |
| train_1/fw_bonus          | -0.9858294978737832  |
| train_1/fw_loss           | 0.14450369626283646  |
| train_1/mu_grads          | -0.07779719941318035 |
| train_1/mu_grads_std      | 0.39446167051792147  |
| train_1/mu_loss           | 5.648941666840949    |
| train_1/n_subgoals        | 2700.0               |
| train_1/next_q            | -14.011173373048141  |
| train_1/q_grads           | -0.12663050666451453 |
| train_1/q_grads_std       | 0.5491293013095856   |
| train_1/q_loss            | 0.3425644384285297   |
| train_1/reward            | -2.109760452450428   |
| train_1/reward_-0.0_frac  | 0.0                  |
| train_1/reward_-1.0_frac  | 0.0033447265625      |
| train_1/reward_-27.0_frac | 0.0                  |
| train_1/subgoal_succ_rate | 0.0                  |
| train_1/target_q          | -13.040384960530659  |
----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 86
Time for epoch 86: 697.33. Rollout time: 408.51, Training time: 288.73
Evaluating epoch 86
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 86                    |
| policy/steps              | 7916818.0             |
| test/episodes             | 2175.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -13.636980326737305   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 8700.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.999999999861092   |
| train_0/current_q         | -9.92715546403848     |
| train_0/fw_bonus          | -0.9993302553892136   |
| train_0/fw_loss           | 0.006564494548365473  |
| train_0/mu_grads          | -0.05516867088153958  |
| train_0/mu_grads_std      | 0.6594691455364228    |
| train_0/mu_loss           | 9.768437445238732     |
| train_0/next_q            | -9.762777879278687    |
| train_0/q_grads           | -0.025740329083055256 |
| train_0/q_grads_std       | 0.41453164368867873   |
| train_0/q_loss            | 0.2691199267625243    |
| train_0/reward            | -0.7793160027707927   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0844482421875       |
| train_0/target_q          | -10.082527797422316   |
| train_1/avg_q             | -14.074989030603788   |
| train_1/current_q         | -13.107342983206603   |
| train_1/fw_bonus          | -0.9853286907076836   |
| train_1/fw_loss           | 0.14908257313072681   |
| train_1/mu_grads          | -0.07801956366747617  |
| train_1/mu_grads_std      | 0.39450708851218225   |
| train_1/mu_loss           | 6.005411439487871     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -14.035106776299624   |
| train_1/q_grads           | -0.12663553580641745  |
| train_1/q_grads_std       | 0.5530872419476509    |
| train_1/q_loss            | 0.42764658466610755   |
| train_1/reward            | -2.1163459868992502   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.00341796875         |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -13.108485572123083   |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 87
Time for epoch 87: 799.70. Rollout time: 489.56, Training time: 310.03
Evaluating epoch 87
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 87                    |
| policy/steps              | 8007943.0             |
| test/episodes             | 2200.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -13.482583094868309   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 8800.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.999999999958128   |
| train_0/current_q         | -9.788396258624939    |
| train_0/fw_bonus          | -0.9993225365877152   |
| train_0/fw_loss           | 0.006635843147523701  |
| train_0/mu_grads          | -0.05493164602667093  |
| train_0/mu_grads_std      | 0.6623284071683884    |
| train_0/mu_loss           | 9.625150584526585     |
| train_0/next_q            | -9.615820115529882    |
| train_0/q_grads           | -0.026867253612726925 |
| train_0/q_grads_std       | 0.41651639342308044   |
| train_0/q_loss            | 0.2551630719658271    |
| train_0/reward            | -0.7799102899582067   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.059326171875        |
| train_0/target_q          | -9.941900528565801    |
| train_1/avg_q             | -14.082589696716589   |
| train_1/current_q         | -13.043009734716799   |
| train_1/fw_bonus          | -0.9850897058844567   |
| train_1/fw_loss           | 0.15126772075891495   |
| train_1/mu_grads          | -0.07794979456812143  |
| train_1/mu_grads_std      | 0.39438931196928023   |
| train_1/mu_loss           | 6.13870554087606      |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -13.998729545873687   |
| train_1/q_grads           | -0.1276050578802824   |
| train_1/q_grads_std       | 0.5565239161252975    |
| train_1/q_loss            | 0.3570640886275057    |
| train_1/reward            | -2.1439974853426973   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0041015625          |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -13.046603136323023   |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 88
Time for epoch 88: 757.16. Rollout time: 478.67, Training time: 278.37
Evaluating epoch 88
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 88                    |
| policy/steps              | 8099068.0             |
| test/episodes             | 2225.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -13.950599451337098   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 8900.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.999999999980037   |
| train_0/current_q         | -9.840477285252058    |
| train_0/fw_bonus          | -0.9993256881833077   |
| train_0/fw_loss           | 0.006606660527177155  |
| train_0/mu_grads          | -0.054530500434339046 |
| train_0/mu_grads_std      | 0.6650468125939369    |
| train_0/mu_loss           | 9.68329546477224      |
| train_0/next_q            | -9.679244112102655    |
| train_0/q_grads           | -0.026615139748901128 |
| train_0/q_grads_std       | 0.41757668033242223   |
| train_0/q_loss            | 0.2788661820933679    |
| train_0/reward            | -0.7782272971882775   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0588623046875       |
| train_0/target_q          | -9.994953833050669    |
| train_1/avg_q             | -14.025069686682535   |
| train_1/current_q         | -13.108603456729465   |
| train_1/fw_bonus          | -0.9851129502058029   |
| train_1/fw_loss           | 0.15105508901178838   |
| train_1/mu_grads          | -0.07820389531552792  |
| train_1/mu_grads_std      | 0.39417831003665926   |
| train_1/mu_loss           | 6.128556071967692     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -14.053691674750757   |
| train_1/q_grads           | -0.12809469774365426  |
| train_1/q_grads_std       | 0.5606994524598121    |
| train_1/q_loss            | 0.3930215890363598    |
| train_1/reward            | -2.094167804872268    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.004443359375        |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -13.110012961667683   |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 89
Time for epoch 89: 793.26. Rollout time: 495.66, Training time: 297.49
Evaluating epoch 89
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 89                    |
| policy/steps              | 8190193.0             |
| test/episodes             | 2250.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -26.99999999999582    |
| test_1/avg_q              | -13.278952053946446   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 9000.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.999999993710908   |
| train_0/current_q         | -9.846122397791655    |
| train_0/fw_bonus          | -0.9993448942899704   |
| train_0/fw_loss           | 0.006429054064210504  |
| train_0/mu_grads          | -0.05368430465459824  |
| train_0/mu_grads_std      | 0.6679783433675766    |
| train_0/mu_loss           | 9.681188505188704     |
| train_0/next_q            | -9.672994811480386    |
| train_0/q_grads           | -0.026911140838637947 |
| train_0/q_grads_std       | 0.41974916830658915   |
| train_0/q_loss            | 0.2612508795224368    |
| train_0/reward            | -0.7800191255380924   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.075634765625        |
| train_0/target_q          | -9.997475862522117    |
| train_1/avg_q             | -14.055882117129915   |
| train_1/current_q         | -13.033464280779219   |
| train_1/fw_bonus          | -0.9851128906011581   |
| train_1/fw_loss           | 0.15105561651289462   |
| train_1/mu_grads          | -0.07814741339534521  |
| train_1/mu_grads_std      | 0.39451323375105857   |
| train_1/mu_loss           | 6.161340700493102     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -13.977540283875527   |
| train_1/q_grads           | -0.12918530516326426  |
| train_1/q_grads_std       | 0.5653245702385903    |
| train_1/q_loss            | 0.6424268649769896    |
| train_1/reward            | -2.1265042016828373   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.00478515625         |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -13.048296375649675   |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 90
Time for epoch 90: 871.01. Rollout time: 531.71, Training time: 339.17
Evaluating epoch 90
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 90                    |
| policy/steps              | 8281318.0             |
| test/episodes             | 2275.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -12.847747743019607   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 9100.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.999999997121684   |
| train_0/current_q         | -9.933416188440916    |
| train_0/fw_bonus          | -0.9993678539991379   |
| train_0/fw_loss           | 0.0062166373012587425 |
| train_0/mu_grads          | -0.05382334282621741  |
| train_0/mu_grads_std      | 0.6709990724921227    |
| train_0/mu_loss           | 9.771548066978639     |
| train_0/next_q            | -9.761411064915965    |
| train_0/q_grads           | -0.02680660868063569  |
| train_0/q_grads_std       | 0.4211845390498638    |
| train_0/q_loss            | 0.253369188592399     |
| train_0/reward            | -0.7784803159025614   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0736083984375       |
| train_0/target_q          | -10.089191935687504   |
| train_1/avg_q             | -14.043540920308091   |
| train_1/current_q         | -13.10634975775704    |
| train_1/fw_bonus          | -0.9861079171299935   |
| train_1/fw_loss           | 0.14195809960365297   |
| train_1/mu_grads          | -0.0782046090811491   |
| train_1/mu_grads_std      | 0.3951836206018925    |
| train_1/mu_loss           | 5.6254728585806015    |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -14.018967665792207   |
| train_1/q_grads           | -0.12990939877927304  |
| train_1/q_grads_std       | 0.5687658846378326    |
| train_1/q_loss            | 0.29620899179212923   |
| train_1/reward            | -2.1322507367280195   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0046875             |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -13.103207513652112   |
-----------------------------------------------------
Saving periodic policy to data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_90.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 91
Time for epoch 91: 840.68. Rollout time: 516.59, Training time: 323.98
Evaluating epoch 91
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 91                    |
| policy/steps              | 8372443.0             |
| test/episodes             | 2300.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -26.999999999996696   |
| test_1/avg_q              | -13.242819947740289   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 9200.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.99999999999325    |
| train_0/current_q         | -9.875235947436527    |
| train_0/fw_bonus          | -0.9993668526411057   |
| train_0/fw_loss           | 0.006225991214159876  |
| train_0/mu_grads          | -0.05408049300312996  |
| train_0/mu_grads_std      | 0.6747376516461372    |
| train_0/mu_loss           | 9.70624000302897      |
| train_0/next_q            | -9.699515410964827    |
| train_0/q_grads           | -0.027242412511259317 |
| train_0/q_grads_std       | 0.42274906039237975   |
| train_0/q_loss            | 0.26116204797895237   |
| train_0/reward            | -0.7790779550487059   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.10185546875         |
| train_0/target_q          | -10.030337779280964   |
| train_1/avg_q             | -13.980362842582679   |
| train_1/current_q         | -13.220284222940865   |
| train_1/fw_bonus          | -0.9858881711959839   |
| train_1/fw_loss           | 0.14396733827888966   |
| train_1/mu_grads          | -0.07833147123456001  |
| train_1/mu_grads_std      | 0.39564365446567534   |
| train_1/mu_loss           | 5.861216675671412     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -14.085378840046758   |
| train_1/q_grads           | -0.13075726814568042  |
| train_1/q_grads_std       | 0.5720337614417076    |
| train_1/q_loss            | 0.34565751141235534   |
| train_1/reward            | -2.1400021856046805   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.004150390625        |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -13.216421315084006   |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 92
Time for epoch 92: 822.92. Rollout time: 516.73, Training time: 306.06
Evaluating epoch 92
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 92                    |
| policy/steps              | 8463568.0             |
| test/episodes             | 2325.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -26.999999999999996   |
| test_1/avg_q              | -14.246520734831279   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 9300.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.999999999999623   |
| train_0/current_q         | -9.83870996953124     |
| train_0/fw_bonus          | -0.9993957877159119   |
| train_0/fw_loss           | 0.005958352901507169  |
| train_0/mu_grads          | -0.054649567045271394 |
| train_0/mu_grads_std      | 0.6788659259676934    |
| train_0/mu_loss           | 9.672433218764928     |
| train_0/next_q            | -9.6613678787734      |
| train_0/q_grads           | -0.027526682475581765 |
| train_0/q_grads_std       | 0.42475800588727      |
| train_0/q_loss            | 0.24056530019346853   |
| train_0/reward            | -0.7774688885809156   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0607421875          |
| train_0/target_q          | -9.992416519820562    |
| train_1/avg_q             | -13.942302578568627   |
| train_1/current_q         | -13.321664009925623   |
| train_1/fw_bonus          | -0.9859887763857842   |
| train_1/fw_loss           | 0.14304736144840718   |
| train_1/mu_grads          | -0.07846079263836145  |
| train_1/mu_grads_std      | 0.39607754349708557   |
| train_1/mu_loss           | 5.438175369355828     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -14.12299279476024    |
| train_1/q_grads           | -0.1322993364185095   |
| train_1/q_grads_std       | 0.5763609796762467    |
| train_1/q_loss            | 0.40639361994569984   |
| train_1/reward            | -2.1200817932200153   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.00400390625         |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -13.319142590137693   |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 93
Time for epoch 93: 774.38. Rollout time: 478.67, Training time: 295.59
Evaluating epoch 93
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 93                    |
| policy/steps              | 8554693.0             |
| test/episodes             | 2350.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -13.30169594161538    |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 9400.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.999999999966985   |
| train_0/current_q         | -9.874971136344552    |
| train_0/fw_bonus          | -0.999372273683548    |
| train_0/fw_loss           | 0.006175695173442364  |
| train_0/mu_grads          | -0.05558175491169095  |
| train_0/mu_grads_std      | 0.6828181400895119    |
| train_0/mu_loss           | 9.703841996601128     |
| train_0/next_q            | -9.695854350696369    |
| train_0/q_grads           | -0.027419293764978647 |
| train_0/q_grads_std       | 0.42669595330953597   |
| train_0/q_loss            | 0.2747395622919583    |
| train_0/reward            | -0.7790074317730614   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.07080078125         |
| train_0/target_q          | -10.026104553069441   |
| train_1/avg_q             | -14.114645666057573   |
| train_1/current_q         | -13.280230192255749   |
| train_1/fw_bonus          | -0.985439196228981    |
| train_1/fw_loss           | 0.14807226546108723   |
| train_1/mu_grads          | -0.0787618251517415   |
| train_1/mu_grads_std      | 0.3964451014995575    |
| train_1/mu_loss           | 5.912759812271801     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -14.025658159305422   |
| train_1/q_grads           | -0.13303123340010642  |
| train_1/q_grads_std       | 0.579228875041008     |
| train_1/q_loss            | 0.4488631759971202    |
| train_1/reward            | -2.125873892437812    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.003759765625        |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -13.280712187543411   |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 94
Time for epoch 94: 869.07. Rollout time: 524.88, Training time: 344.11
Evaluating epoch 94
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 94                    |
| policy/steps              | 8645818.0             |
| test/episodes             | 2375.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -26.999999999999993   |
| test_1/avg_q              | -12.298339576536662   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 9500.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.99999999935923    |
| train_0/current_q         | -9.945521676152866    |
| train_0/fw_bonus          | -0.9993674159049988   |
| train_0/fw_loss           | 0.006220762780867517  |
| train_0/mu_grads          | -0.05457905400544405  |
| train_0/mu_grads_std      | 0.686687932908535     |
| train_0/mu_loss           | 9.76607515133198      |
| train_0/next_q            | -9.757897909474817    |
| train_0/q_grads           | -0.028681635297834872 |
| train_0/q_grads_std       | 0.4287492923438549    |
| train_0/q_loss            | 0.27440772296526206   |
| train_0/reward            | -0.7828502102704078   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0829833984375       |
| train_0/target_q          | -10.10138918084499    |
| train_1/avg_q             | -13.928836738117342   |
| train_1/current_q         | -13.311492388231901   |
| train_1/fw_bonus          | -0.9852938860654831   |
| train_1/fw_loss           | 0.14940072000026702   |
| train_1/mu_grads          | -0.07899515889585018  |
| train_1/mu_grads_std      | 0.3964259631931782    |
| train_1/mu_loss           | 5.824590726717409     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -14.014142493579786   |
| train_1/q_grads           | -0.1338890228420496   |
| train_1/q_grads_std       | 0.5830771863460541    |
| train_1/q_loss            | 0.4093375529971315    |
| train_1/reward            | -2.151765600586077    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.003857421875        |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -13.313127280160563   |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 95
Time for epoch 95: 955.52. Rollout time: 580.75, Training time: 374.61
Evaluating epoch 95
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 95                    |
| policy/steps              | 8736860.0             |
| test/episodes             | 2400.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -26.99999999999975    |
| test_1/avg_q              | -13.136110956140147   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 9600.0                |
| train/success_rate        | 0.01                  |
| train_0/avg_q             | -26.999999999970896   |
| train_0/current_q         | -9.881401947111076    |
| train_0/fw_bonus          | -0.999369703233242    |
| train_0/fw_loss           | 0.006199535774067044  |
| train_0/mu_grads          | -0.05547704137861729  |
| train_0/mu_grads_std      | 0.6899101749062538    |
| train_0/mu_loss           | 9.701402808101156     |
| train_0/next_q            | -9.693586897788752    |
| train_0/q_grads           | -0.029308171663433313 |
| train_0/q_grads_std       | 0.4303782723844051    |
| train_0/q_loss            | 0.2868060251169612    |
| train_0/reward            | -0.7826804596057627   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0541015625          |
| train_0/target_q          | -10.034964703202709   |
| train_1/avg_q             | -14.04444329316       |
| train_1/current_q         | -13.367591001503433   |
| train_1/fw_bonus          | -0.9846855893731117   |
| train_1/fw_loss           | 0.15496255792677402   |
| train_1/mu_grads          | -0.07914477903395892  |
| train_1/mu_grads_std      | 0.3969738855957985    |
| train_1/mu_loss           | 5.938440535049793     |
| train_1/n_subgoals        | 2697.0                |
| train_1/next_q            | -13.996431070772939   |
| train_1/q_grads           | -0.13467224948108197  |
| train_1/q_grads_std       | 0.5856026172637939    |
| train_1/q_loss            | 0.45306746405320164   |
| train_1/reward            | -2.1651943530552673   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0035888671875       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -13.372721031156647   |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 96
Time for epoch 96: 1096.60. Rollout time: 680.39, Training time: 416.04
Evaluating epoch 96
Data_dir: data/eef7a77/AntReacherEnv-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
----------------------------------------------------
| epoch                     | 96                   |
| policy/steps              | 8827985.0            |
| test/episodes             | 2425.0               |
| test/success_rate         | 0.0                  |
| test_0/avg_q              | -26.99999999999993   |
| test_1/avg_q              | -12.928345572313438  |
| test_1/n_subgoals         | 675.0                |
| test_1/subgoal_succ_rate  | 0.0                  |
| train/episodes            | 9700.0               |
| train/success_rate        | 0.0                  |
| train_0/avg_q             | -26.999999999616414  |
| train_0/current_q         | -9.978129391615752   |
| train_0/fw_bonus          | -0.9993656024336814  |
| train_0/fw_loss           | 0.006237530731596053 |
| train_0/mu_grads          | -0.05631327917799354 |
| train_0/mu_grads_std      | 0.6930186942219734   |
| train_0/mu_loss           | 9.802497034535836    |
| train_0/next_q            | -9.791519684261004   |
| train_0/q_grads           | -0.02987591098062694 |
| train_0/q_grads_std       | 0.432237995415926    |
| train_0/q_loss            | 0.24943409766902996  |
| train_0/reward            | -0.7841188350394077  |
| train_0/reward_-0.0_frac  | 0.0                  |
| train_0/reward_-1.0_frac  | 0.0807861328125      |
| train_0/target_q          | -10.137065860729233  |
| train_1/avg_q             | -13.98884191212834   |
| train_1/current_q         | -13.375855549066689  |
| train_1/fw_bonus          | -0.9849471062421798  |
| train_1/fw_loss           | 0.15257142186164857  |
| train_1/mu_grads          | -0.07942640818655491 |
| train_1/mu_grads_std      | 0.3973034553229809   |
| train_1/mu_loss           | 6.245375186373141    |
| train_1/n_subgoals        | 2700.0               |
| train_1/next_q            | -14.052874748064445  |
| train_1/q_grads           | -0.13506103940308095 |
| train_1/q_grads_std       | 0.5889883622527122   |
| train_1/q_loss            | 0.3959871608544489   |
| train_1/reward            | -2.131023358657694   |
| train_1/reward_-0.0_frac  | 0.0                  |
| train_1/reward_-1.0_frac  | 0.004052734375       |
| train_1/reward_-27.0_frac | 0.0                  |
| train_1/subgoal_succ_rate | 0.0                  |
| train_1/target_q          | -13.374042537890812  |
----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 97
