Starting process id: 2247
T: 700
alg: chac
algorithm: src.chac
atomic_noise: 0.2
base_logdir: data
batch_size: 1024
bind_core: 0
buffer_size: 500
chac_params: {}
early_stop_data_column: test/success_rate
early_stop_threshold: 100.0
env_name: AntReacherEnv-v0
eta: 0.75
fw: 1
fw_hidden_size: 256,256,256
fw_lr: 0.001
gamma: 0.9985714285714286
graph: 1
info: 
make_env: <function prepare_params.<locals>.make_env at 0x7f7eb980f050>
max_try_idx: 199
mu_hidden_size: 64
mu_lr: 0.001
n_episodes: 100
n_levels: 2
n_pre_episodes: 30
n_test_rollouts: 25
n_train_batches: 40
n_train_rollouts: 100
num_threads: 1
q_hidden_size: 64
q_lr: 0.001
random_action_perc: 0.3
regularization: False
render: 0
rollout_batch_size: 1
subgoal_noise: 0.2
subgoal_test_perc: 0.3
time_scales: 27,27
try_start_idx: 100
use_mpi: False
verbose: False

*** Warning ***
You are running src.chac with just a single MPI worker. This will work, but the HER experiments that we report in Plappert et al. (2018, https://arxiv.org/abs/1802.09464) were obtained with --num_cpu 19. This makes a significant difference and if you are looking to reproduce those results, be aware of this. Please also refer to https://github.com/openai/baselines/issues/314 for further details.
****************

dims: action = 8, subgoal = 5, end_goal = 3
subgoal_bounds: symmetric [11.75 11.75  0.5   3.    3.  ], offset [0.  0.  0.5 0.  0. ]
Running on CPU ...
Creating a CHAC agent

Hierarchy Level 0 with time scale 27
Actor(
  (fc1): Linear(in_features=34, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
  (fc4): Linear(in_features=64, out_features=8, bias=True)
)
Critic(
  (fc1): Linear(in_features=42, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
  (fc4): Linear(in_features=64, out_features=1, bias=True)
  (mse_loss): MSELoss()
)
ForwardModel(
  (mlp): Sequential(
    (0): Linear(in_features=37, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=256, bias=True)
    (5): ReLU()
    (6): Linear(in_features=256, out_features=29, bias=True)
    (7): Identity()
  )
  (mse_loss): MSELoss()
)

Hierarchy Level 1 with time scale 27
Actor(
  (fc1): Linear(in_features=32, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
  (fc4): Linear(in_features=64, out_features=5, bias=True)
)
Critic(
  (fc1): Linear(in_features=37, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
  (fc4): Linear(in_features=64, out_features=1, bias=True)
  (mse_loss): MSELoss()
)
ForwardModel(
  (mlp): Sequential(
    (0): Linear(in_features=34, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=256, bias=True)
    (5): ReLU()
    (6): Linear(in_features=256, out_features=29, bias=True)
    (7): Identity()
  )
  (mse_loss): MSELoss()
)
Training epoch 0
Time for epoch 0: 507.77. Rollout time: 241.59, Training time: 266.14
Evaluating epoch 0
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 0                     |
| policy/steps              | 91063.0               |
| test/episodes             | 25.0                  |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -2.8898317898201045   |
| test_1/avg_q              | -5.092080467106321    |
| test_1/n_subgoals         | 6137.0                |
| test_1/subgoal_succ_rate  | 0.9242300798435718    |
| train/episodes            | 100.0                 |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -12.297677104043936   |
| train_0/current_q         | -9.229281278853438    |
| train_0/fw_bonus          | -0.9951434105634689   |
| train_0/fw_loss           | 0.0344445105176419    |
| train_0/mu_grads          | -0.01014575376175344  |
| train_0/mu_grads_std      | 0.1549884218722582    |
| train_0/mu_loss           | 9.190763282425923     |
| train_0/next_q            | -9.179946498617614    |
| train_0/q_grads           | 0.018720265291631222  |
| train_0/q_grads_std       | 0.11481472067534923   |
| train_0/q_loss            | 0.6098569712029485    |
| train_0/reward            | -0.6283451798644819   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0025390625          |
| train_0/target_q          | -9.372378224105685    |
| train_1/avg_q             | -9.719298822544149    |
| train_1/current_q         | -5.31589845936692     |
| train_1/fw_bonus          | -0.9912352830171585   |
| train_1/fw_loss           | 0.08440011274069548   |
| train_1/mu_grads          | 0.0013489010598277673 |
| train_1/mu_grads_std      | 0.11817348767071963   |
| train_1/mu_loss           | 3.4680426210611714    |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -3.4327823529679735   |
| train_1/q_grads           | 0.021550290239974856  |
| train_1/q_grads_std       | 0.11748228576034307   |
| train_1/q_loss            | 1.3520930351398968    |
| train_1/reward            | -2.6645446602396987   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0060302734375       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0022222222222222222 |
| train_1/target_q          | -5.291323999904945    |
-----------------------------------------------------
Saving periodic policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101/policy_0.pkl ...
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Training epoch 1
Time for epoch 1: 419.54. Rollout time: 228.66, Training time: 190.84
Evaluating epoch 1
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 1                     |
| policy/steps              | 181521.0              |
| test/episodes             | 50.0                  |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -4.233162530459949    |
| test_1/avg_q              | -9.559677267607027    |
| test_1/n_subgoals         | 2284.0                |
| test_1/subgoal_succ_rate  | 0.7324868651488616    |
| train/episodes            | 200.0                 |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -24.8304888265725     |
| train_0/current_q         | -9.153648637967667    |
| train_0/fw_bonus          | -0.9971611350774765   |
| train_0/fw_loss           | 0.020533978566527368  |
| train_0/mu_grads          | -0.017809404572471977 |
| train_0/mu_grads_std      | 0.20580598562955857   |
| train_0/mu_loss           | 9.084447128345639     |
| train_0/next_q            | -9.089117135715355    |
| train_0/q_grads           | 0.016508680861443283  |
| train_0/q_grads_std       | 0.12438043765723705   |
| train_0/q_loss            | 0.5178703299977686    |
| train_0/reward            | -0.6291975119962444   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0095458984375       |
| train_0/target_q          | -9.309028899975411    |
| train_1/avg_q             | -18.10572502052502    |
| train_1/current_q         | -6.308238552975461    |
| train_1/fw_bonus          | -0.9890916019678115   |
| train_1/fw_loss           | 0.10075795147567987   |
| train_1/mu_grads          | -0.000326513008621987 |
| train_1/mu_grads_std      | 0.1341935694217682    |
| train_1/mu_loss           | 4.738314365224746     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -4.714459896278188    |
| train_1/q_grads           | 0.019012814620509744  |
| train_1/q_grads_std       | 0.12910457141697407   |
| train_1/q_loss            | 0.8165473986641263    |
| train_1/reward            | -2.691492869827198    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0019287109375       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.012222222222222223  |
| train_1/target_q          | -6.278481345511048    |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Training epoch 2
Time for epoch 2: 420.26. Rollout time: 228.32, Training time: 191.91
Evaluating epoch 2
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
------------------------------------------------------
| epoch                     | 2                      |
| policy/steps              | 271933.0               |
| test/episodes             | 75.0                   |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -4.502315399743174     |
| test_1/avg_q              | -11.633734778460886    |
| test_1/n_subgoals         | 4538.0                 |
| test_1/subgoal_succ_rate  | 0.884310268840899      |
| train/episodes            | 300.0                  |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -25.20625242269886     |
| train_0/current_q         | -9.293482024391519     |
| train_0/fw_bonus          | -0.9975417330861092    |
| train_0/fw_loss           | 0.01791009595617652    |
| train_0/mu_grads          | -0.022465462004765867  |
| train_0/mu_grads_std      | 0.2510934993624687     |
| train_0/mu_loss           | 9.233082921598577      |
| train_0/next_q            | -9.221218353474413     |
| train_0/q_grads           | 0.013348981062881649   |
| train_0/q_grads_std       | 0.1330224506556988     |
| train_0/q_loss            | 0.6662502316304773     |
| train_0/reward            | -0.6461200533132796    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.01494140625          |
| train_0/target_q          | -9.406695176829121     |
| train_1/avg_q             | -18.932302883551746    |
| train_1/current_q         | -6.832491494228085     |
| train_1/fw_bonus          | -0.9860529690980911    |
| train_1/fw_loss           | 0.12394482176750898    |
| train_1/mu_grads          | -0.0019737929105758667 |
| train_1/mu_grads_std      | 0.13825843371450902    |
| train_1/mu_loss           | 5.417213989702975      |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -5.402162832229164     |
| train_1/q_grads           | 0.015373942931182683   |
| train_1/q_grads_std       | 0.13964783400297165    |
| train_1/q_loss            | 0.7363433276190111     |
| train_1/reward            | -2.650010553635366     |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.00048828125          |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.014444444444444444   |
| train_1/target_q          | -6.8088470465938995    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Training epoch 3
Time for epoch 3: 415.02. Rollout time: 221.01, Training time: 193.98
Evaluating epoch 3
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 3                     |
| policy/steps              | 360760.0              |
| test/episodes             | 100.0                 |
| test/success_rate         | 0.04                  |
| test_0/avg_q              | -7.892195599833942    |
| test_1/avg_q              | -10.962186606142756   |
| test_1/n_subgoals         | 3900.0                |
| test_1/subgoal_succ_rate  | 0.8733333333333333    |
| train/episodes            | 400.0                 |
| train/success_rate        | 0.02                  |
| train_0/avg_q             | -24.647975809467077   |
| train_0/current_q         | -8.277710720543135    |
| train_0/fw_bonus          | -0.997412234544754    |
| train_0/fw_loss           | 0.018802879378199577  |
| train_0/mu_grads          | -0.02634142842143774  |
| train_0/mu_grads_std      | 0.2901573941111565    |
| train_0/mu_loss           | 8.117917614190569     |
| train_0/next_q            | -8.144691310031503    |
| train_0/q_grads           | 0.009136963216587902  |
| train_0/q_grads_std       | 0.14494547471404076   |
| train_0/q_loss            | 0.6620598145919963    |
| train_0/reward            | -0.6742309891607874   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0187255859375       |
| train_0/target_q          | -8.427305888931754    |
| train_1/avg_q             | -18.169855701249936   |
| train_1/current_q         | -6.287480261128841    |
| train_1/fw_bonus          | -0.9817031770944595   |
| train_1/fw_loss           | 0.15713665783405303   |
| train_1/mu_grads          | -0.003720535174943507 |
| train_1/mu_grads_std      | 0.1422008778899908    |
| train_1/mu_loss           | 4.62633556821657      |
| train_1/n_subgoals        | 2671.0                |
| train_1/next_q            | -4.611211932218279    |
| train_1/q_grads           | 0.011779989395290614  |
| train_1/q_grads_std       | 0.15716008208692073   |
| train_1/q_loss            | 1.174571688496423     |
| train_1/reward            | -2.7081943477667663   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.000244140625        |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.020965930363159864  |
| train_1/target_q          | -6.265695943520972    |
-----------------------------------------------------
New best value for test/success_rate: 0.04. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.01
Training epoch 4
Time for epoch 4: 435.13. Rollout time: 228.43, Training time: 206.67
Evaluating epoch 4
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 4                     |
| policy/steps              | 449316.0              |
| test/episodes             | 125.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -7.226577455622211    |
| test_1/avg_q              | -6.409759058624278    |
| test_1/n_subgoals         | 2709.0                |
| test_1/subgoal_succ_rate  | 0.7818383167220376    |
| train/episodes            | 500.0                 |
| train/success_rate        | 0.05                  |
| train_0/avg_q             | -23.058309025977653   |
| train_0/current_q         | -9.423361723687133    |
| train_0/fw_bonus          | -0.9973412424325943   |
| train_0/fw_loss           | 0.019292385876178743  |
| train_0/mu_grads          | -0.030936249950900673 |
| train_0/mu_grads_std      | 0.3202270992100239    |
| train_0/mu_loss           | 9.28311442877727      |
| train_0/next_q            | -9.260982070652528    |
| train_0/q_grads           | 0.008250346151180565  |
| train_0/q_grads_std       | 0.1660912401974201    |
| train_0/q_loss            | 0.9721848020596443    |
| train_0/reward            | -0.7072088438930223   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0308349609375       |
| train_0/target_q          | -9.501098144552703    |
| train_1/avg_q             | -16.25900741619395    |
| train_1/current_q         | -5.659081756297417    |
| train_1/fw_bonus          | -0.9788503676652909   |
| train_1/fw_loss           | 0.17890550568699837   |
| train_1/mu_grads          | -0.006085848261136562 |
| train_1/mu_grads_std      | 0.14949978329241276   |
| train_1/mu_loss           | 3.734029194087627     |
| train_1/n_subgoals        | 2649.0                |
| train_1/next_q            | -3.721802424088382    |
| train_1/q_grads           | 0.007570214394945651  |
| train_1/q_grads_std       | 0.17423087321221828   |
| train_1/q_loss            | 1.8328899311548146    |
| train_1/reward            | -2.7081220160580415   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 4.8828125e-05         |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.028690071725179313  |
| train_1/target_q          | -5.618182929214733    |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.01
Training epoch 5
Time for epoch 5: 436.16. Rollout time: 230.97, Training time: 205.16
Evaluating epoch 5
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 5                     |
| policy/steps              | 537650.0              |
| test/episodes             | 150.0                 |
| test/success_rate         | 0.08                  |
| test_0/avg_q              | -8.745236772918004    |
| test_1/avg_q              | -3.591461445514401    |
| test_1/n_subgoals         | 1077.0                |
| test_1/subgoal_succ_rate  | 0.41875580315691735   |
| train/episodes            | 600.0                 |
| train/success_rate        | 0.04                  |
| train_0/avg_q             | -23.721821963729028   |
| train_0/current_q         | -9.621538337980123    |
| train_0/fw_bonus          | -0.9972217857837677   |
| train_0/fw_loss           | 0.020115903858095408  |
| train_0/mu_grads          | -0.03592567220330238  |
| train_0/mu_grads_std      | 0.34483600705862044   |
| train_0/mu_loss           | 9.459570203954565     |
| train_0/next_q            | -9.442746526764413    |
| train_0/q_grads           | 0.007288553239777684  |
| train_0/q_grads_std       | 0.17818230018019676   |
| train_0/q_loss            | 1.1555241950766315    |
| train_0/reward            | -0.7371899672911241   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.025048828125        |
| train_0/target_q          | -9.714766102321764    |
| train_1/avg_q             | -15.815336919876415   |
| train_1/current_q         | -5.84076255621999     |
| train_1/fw_bonus          | -0.9762432217597962   |
| train_1/fw_loss           | 0.1987998392432928    |
| train_1/mu_grads          | -0.008044035290367901 |
| train_1/mu_grads_std      | 0.16164168789982797   |
| train_1/mu_loss           | 3.9588813332019512    |
| train_1/n_subgoals        | 2668.0                |
| train_1/next_q            | -3.9408732001512234   |
| train_1/q_grads           | 0.0035104928188957273 |
| train_1/q_grads_std       | 0.18878621459007264   |
| train_1/q_loss            | 2.2208165447873207    |
| train_1/reward            | -2.683483413146314    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0                   |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.02623688155922039   |
| train_1/target_q          | -5.801419537555726    |
-----------------------------------------------------
New best value for test/success_rate: 0.08. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.03
Training epoch 6
Time for epoch 6: 446.43. Rollout time: 233.26, Training time: 213.13
Evaluating epoch 6
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 6                     |
| policy/steps              | 622038.0              |
| test/episodes             | 175.0                 |
| test/success_rate         | 0.28                  |
| test_0/avg_q              | -6.70377008797739     |
| test_1/avg_q              | -7.730870305873946    |
| test_1/n_subgoals         | 1212.0                |
| test_1/subgoal_succ_rate  | 0.5668316831683168    |
| train/episodes            | 700.0                 |
| train/success_rate        | 0.08                  |
| train_0/avg_q             | -23.01221252210417    |
| train_0/current_q         | -9.92748828451478     |
| train_0/fw_bonus          | -0.9968808606266976   |
| train_0/fw_loss           | 0.02246633330360055   |
| train_0/mu_grads          | -0.04157842192798853  |
| train_0/mu_grads_std      | 0.36739256605505943   |
| train_0/mu_loss           | 9.737288755228501     |
| train_0/next_q            | -9.698840530307626    |
| train_0/q_grads           | 0.007396571442950517  |
| train_0/q_grads_std       | 0.1886281494051218    |
| train_0/q_loss            | 1.3549790061788396    |
| train_0/reward            | -0.7786330550388811   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0173828125          |
| train_0/target_q          | -10.009464352846157   |
| train_1/avg_q             | -15.99830266366708    |
| train_1/current_q         | -5.576190828533468    |
| train_1/fw_bonus          | -0.9738790974020958   |
| train_1/fw_loss           | 0.21683975718915463   |
| train_1/mu_grads          | -0.010456568421795964 |
| train_1/mu_grads_std      | 0.1697475254535675    |
| train_1/mu_loss           | 3.598967122726278     |
| train_1/n_subgoals        | 2623.0                |
| train_1/next_q            | -3.5730281617122204   |
| train_1/q_grads           | 0.0003496972614811966 |
| train_1/q_grads_std       | 0.2014867603778839    |
| train_1/q_loss            | 2.742361037135789     |
| train_1/reward            | -2.678680956079188    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0                   |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.036218070911170415  |
| train_1/target_q          | -5.553538317411551    |
-----------------------------------------------------
New best value for test/success_rate: 0.28. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.1
Training epoch 7
Time for epoch 7: 444.10. Rollout time: 234.03, Training time: 210.04
Evaluating epoch 7
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
------------------------------------------------------
| epoch                     | 7                      |
| policy/steps              | 701198.0               |
| test/episodes             | 200.0                  |
| test/success_rate         | 0.6                    |
| test_0/avg_q              | -8.161935500440302     |
| test_1/avg_q              | -1.8638907677145764    |
| test_1/n_subgoals         | 1542.0                 |
| test_1/subgoal_succ_rate  | 0.8093385214007782     |
| train/episodes            | 800.0                  |
| train/success_rate        | 0.08                   |
| train_0/avg_q             | -22.61587245287911     |
| train_0/current_q         | -9.93669939490852      |
| train_0/fw_bonus          | -0.996611824631691     |
| train_0/fw_loss           | 0.024321040650829674   |
| train_0/mu_grads          | -0.046004431787878275  |
| train_0/mu_grads_std      | 0.38217876106500626    |
| train_0/mu_loss           | 9.696700121305877      |
| train_0/next_q            | -9.649305287681258     |
| train_0/q_grads           | 0.005035408667754382   |
| train_0/q_grads_std       | 0.19855488501489163    |
| train_0/q_loss            | 1.2022721908021772     |
| train_0/reward            | -0.8077528101479402    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.0225830078125        |
| train_0/target_q          | -10.00716165303794     |
| train_1/avg_q             | -15.4970897401397      |
| train_1/current_q         | -5.534284010119679     |
| train_1/fw_bonus          | -0.971740011870861     |
| train_1/fw_loss           | 0.23316247761249542    |
| train_1/mu_grads          | -0.012762163300067186  |
| train_1/mu_grads_std      | 0.17965820245444775    |
| train_1/mu_loss           | 3.5142902547894144     |
| train_1/n_subgoals        | 2637.0                 |
| train_1/next_q            | -3.475840205196062     |
| train_1/q_grads           | -0.0010232529413769953 |
| train_1/q_grads_std       | 0.21246423125267028    |
| train_1/q_loss            | 3.246033977919109      |
| train_1/reward            | -2.6850309495919644    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0                    |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.049677664012135005   |
| train_1/target_q          | -5.504893252269608     |
------------------------------------------------------
New best value for test/success_rate: 0.6. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.24
Training epoch 8
Time for epoch 8: 454.86. Rollout time: 235.43, Training time: 219.39
Evaluating epoch 8
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
------------------------------------------------------
| epoch                     | 8                      |
| policy/steps              | 777053.0               |
| test/episodes             | 225.0                  |
| test/success_rate         | 0.72                   |
| test_0/avg_q              | -8.65553442494095      |
| test_1/avg_q              | -7.234813325645478     |
| test_1/n_subgoals         | 1027.0                 |
| test_1/subgoal_succ_rate  | 0.7643622200584226     |
| train/episodes            | 900.0                  |
| train/success_rate        | 0.11                   |
| train_0/avg_q             | -22.0781418639095      |
| train_0/current_q         | -9.809067430073446     |
| train_0/fw_bonus          | -0.9963946744799614    |
| train_0/fw_loss           | 0.025818151934072376   |
| train_0/mu_grads          | -0.05003179842606187   |
| train_0/mu_grads_std      | 0.39675990045070647    |
| train_0/mu_loss           | 9.559560698701329      |
| train_0/next_q            | -9.495126820804355     |
| train_0/q_grads           | 0.0035747296293266117  |
| train_0/q_grads_std       | 0.20869951657950878    |
| train_0/q_loss            | 1.1550884922164317     |
| train_0/reward            | -0.8205342144188762    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.0186767578125        |
| train_0/target_q          | -9.878462819852993     |
| train_1/avg_q             | -14.708660127941458    |
| train_1/current_q         | -5.333935638971341     |
| train_1/fw_bonus          | -0.9705725893378258    |
| train_1/fw_loss           | 0.24207072667777538    |
| train_1/mu_grads          | -0.014284289116039873  |
| train_1/mu_grads_std      | 0.18843295201659202    |
| train_1/mu_loss           | 3.3128308376936473     |
| train_1/n_subgoals        | 2590.0                 |
| train_1/next_q            | -3.2813633954345844    |
| train_1/q_grads           | -0.0035770392743870614 |
| train_1/q_grads_std       | 0.22274182587862015    |
| train_1/q_loss            | 3.6593238265542767     |
| train_1/reward            | -2.591922930493456     |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 2.44140625e-05         |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.07027027027027027    |
| train_1/target_q          | -5.29843136772728      |
------------------------------------------------------
New best value for test/success_rate: 0.72. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.42
Training epoch 9
Time for epoch 9: 441.01. Rollout time: 218.40, Training time: 222.57
Evaluating epoch 9
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 9                     |
| policy/steps              | 847242.0              |
| test/episodes             | 250.0                 |
| test/success_rate         | 0.68                  |
| test_0/avg_q              | -8.56356275493618     |
| test_1/avg_q              | -1.9698532621490041   |
| test_1/n_subgoals         | 755.0                 |
| test_1/subgoal_succ_rate  | 0.6410596026490066    |
| train/episodes            | 1000.0                |
| train/success_rate        | 0.32                  |
| train_0/avg_q             | -21.115878463807903   |
| train_0/current_q         | -9.742031211328479    |
| train_0/fw_bonus          | -0.9961855009198188   |
| train_0/fw_loss           | 0.027260149596258998  |
| train_0/mu_grads          | -0.05306680593639612  |
| train_0/mu_grads_std      | 0.41050623580813406   |
| train_0/mu_loss           | 9.513713607833662     |
| train_0/next_q            | -9.426695850362126    |
| train_0/q_grads           | 0.003393311914987862  |
| train_0/q_grads_std       | 0.2187659002840519    |
| train_0/q_loss            | 1.240654965885478     |
| train_0/reward            | -0.8206467189142131   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.015087890625        |
| train_0/target_q          | -9.818042891971263    |
| train_1/avg_q             | -14.924997178314772   |
| train_1/current_q         | -5.067137380065748    |
| train_1/fw_bonus          | -0.9696302980184555   |
| train_1/fw_loss           | 0.24926097504794598   |
| train_1/mu_grads          | -0.016285832971334457 |
| train_1/mu_grads_std      | 0.19554688669741155   |
| train_1/mu_loss           | 2.957900091535772     |
| train_1/n_subgoals        | 2394.0                |
| train_1/next_q            | -2.9219740048031007   |
| train_1/q_grads           | -0.005821923341136426 |
| train_1/q_grads_std       | 0.2346048943698406    |
| train_1/q_loss            | 3.3653332781435368    |
| train_1/reward            | -2.6191531818440126   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0                   |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0960735171261487    |
| train_1/target_q          | -5.028426693893754    |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.5700000000000001
Training epoch 10
Time for epoch 10: 429.43. Rollout time: 210.81, Training time: 218.58
Evaluating epoch 10
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 10                    |
| policy/steps              | 911977.0              |
| test/episodes             | 275.0                 |
| test/success_rate         | 0.76                  |
| test_0/avg_q              | -12.590205556248081   |
| test_1/avg_q              | -2.7677727758570723   |
| test_1/n_subgoals         | 294.0                 |
| test_1/subgoal_succ_rate  | 0.25510204081632654   |
| train/episodes            | 1100.0                |
| train/success_rate        | 0.35                  |
| train_0/avg_q             | -20.761359657024      |
| train_0/current_q         | -9.69711905266534     |
| train_0/fw_bonus          | -0.9959948182106018   |
| train_0/fw_loss           | 0.028574750991538167  |
| train_0/mu_grads          | -0.05514187905937433  |
| train_0/mu_grads_std      | 0.4233394719660282    |
| train_0/mu_loss           | 9.438534825910676     |
| train_0/next_q            | -9.354667236565245    |
| train_0/q_grads           | 0.0032221471425145864 |
| train_0/q_grads_std       | 0.22827278189361094   |
| train_0/q_loss            | 1.0839632660628102    |
| train_0/reward            | -0.8229794190880056   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0130859375          |
| train_0/target_q          | -9.76238658520785     |
| train_1/avg_q             | -13.777916115909877   |
| train_1/current_q         | -4.74009602682937     |
| train_1/fw_bonus          | -0.9688359811902046   |
| train_1/fw_loss           | 0.2553222082555294    |
| train_1/mu_grads          | -0.017895355867221952 |
| train_1/mu_grads_std      | 0.20103998705744744   |
| train_1/mu_loss           | 2.64922259949582      |
| train_1/n_subgoals        | 2283.0                |
| train_1/next_q            | -2.6197584157118934   |
| train_1/q_grads           | -0.007756918692030013 |
| train_1/q_grads_std       | 0.2462532188743353    |
| train_1/q_loss            | 3.9244477525203463    |
| train_1/reward            | -2.5183747661496456   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0                   |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.13009198423127463   |
| train_1/target_q          | -4.717567610855239    |
-----------------------------------------------------
Saving periodic policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101/policy_10.pkl ...
New best value for test/success_rate: 0.76. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.69
Training epoch 11
Time for epoch 11: 370.97. Rollout time: 170.10, Training time: 200.84
Evaluating epoch 11
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 11                    |
| policy/steps              | 971830.0              |
| test/episodes             | 300.0                 |
| test/success_rate         | 0.76                  |
| test_0/avg_q              | -8.508291252292354    |
| test_1/avg_q              | -2.379571957665572    |
| test_1/n_subgoals         | 312.0                 |
| test_1/subgoal_succ_rate  | 0.34294871794871795   |
| train/episodes            | 1200.0                |
| train/success_rate        | 0.51                  |
| train_0/avg_q             | -20.177272780831178   |
| train_0/current_q         | -9.586368043834245    |
| train_0/fw_bonus          | -0.9959006637334824   |
| train_0/fw_loss           | 0.02922391747124493   |
| train_0/mu_grads          | -0.057951224781572816 |
| train_0/mu_grads_std      | 0.4360727332532406    |
| train_0/mu_loss           | 9.31219280823924      |
| train_0/next_q            | -9.215676902941109    |
| train_0/q_grads           | 0.0022907352773472666 |
| train_0/q_grads_std       | 0.23800031654536724   |
| train_0/q_loss            | 1.0690376258502305    |
| train_0/reward            | -0.8306829294368072   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0161376953125       |
| train_0/target_q          | -9.646988855140227    |
| train_1/avg_q             | -12.936018514776752   |
| train_1/current_q         | -4.76845149082228     |
| train_1/fw_bonus          | -0.9688091099262237   |
| train_1/fw_loss           | 0.25552726425230504   |
| train_1/mu_grads          | -0.019281926285475492 |
| train_1/mu_grads_std      | 0.20734724029898643   |
| train_1/mu_loss           | 2.636329745001286     |
| train_1/n_subgoals        | 2096.0                |
| train_1/next_q            | -2.5973646895957097   |
| train_1/q_grads           | -0.010666471836157144 |
| train_1/q_grads_std       | 0.2579068996012211    |
| train_1/q_loss            | 3.8289731623390084    |
| train_1/reward            | -2.5304530082339625   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0                   |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.12213740458015267   |
| train_1/target_q          | -4.71824671485097     |
-----------------------------------------------------
New best value for test/success_rate: 0.76. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.73
Training epoch 12
Time for epoch 12: 351.00. Rollout time: 150.51, Training time: 200.46
Evaluating epoch 12
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 12                    |
| policy/steps              | 1023514.0             |
| test/episodes             | 325.0                 |
| test/success_rate         | 0.84                  |
| test_0/avg_q              | -7.532235168763118    |
| test_1/avg_q              | -2.002499255037096    |
| test_1/n_subgoals         | 319.0                 |
| test_1/subgoal_succ_rate  | 0.5047021943573667    |
| train/episodes            | 1300.0                |
| train/success_rate        | 0.67                  |
| train_0/avg_q             | -19.514530006147805   |
| train_0/current_q         | -9.628733797945966    |
| train_0/fw_bonus          | -0.9958137050271034   |
| train_0/fw_loss           | 0.02982343346811831   |
| train_0/mu_grads          | -0.06025913199409842  |
| train_0/mu_grads_std      | 0.44636116698384287   |
| train_0/mu_loss           | 9.357985056175869     |
| train_0/next_q            | -9.259695190935188    |
| train_0/q_grads           | 0.002851770998677239  |
| train_0/q_grads_std       | 0.2466495044529438    |
| train_0/q_loss            | 0.9889993882090679    |
| train_0/reward            | -0.8341768145382957   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0124755859375       |
| train_0/target_q          | -9.695899628463838    |
| train_1/avg_q             | -12.935164657028889   |
| train_1/current_q         | -4.527030203726672    |
| train_1/fw_bonus          | -0.9693826556205749   |
| train_1/fw_loss           | 0.251150643825531     |
| train_1/mu_grads          | -0.021295574912801384 |
| train_1/mu_grads_std      | 0.21368979029357432   |
| train_1/mu_loss           | 2.415226883061058     |
| train_1/n_subgoals        | 1850.0                |
| train_1/next_q            | -2.362730284444649    |
| train_1/q_grads           | -0.012785854772664607 |
| train_1/q_grads_std       | 0.2692406065762043    |
| train_1/q_loss            | 3.869531181378039     |
| train_1/reward            | -2.4699701763980557   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0                   |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.14540540540540542   |
| train_1/target_q          | -4.468729720798095    |
-----------------------------------------------------
New best value for test/success_rate: 0.84. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.76
Training epoch 13
Time for epoch 13: 376.54. Rollout time: 156.04, Training time: 220.46
Evaluating epoch 13
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 13                    |
| policy/steps              | 1071418.0             |
| test/episodes             | 350.0                 |
| test/success_rate         | 0.92                  |
| test_0/avg_q              | -10.477462944875601   |
| test_1/avg_q              | -2.9046345877074193   |
| test_1/n_subgoals         | 198.0                 |
| test_1/subgoal_succ_rate  | 0.43434343434343436   |
| train/episodes            | 1400.0                |
| train/success_rate        | 0.66                  |
| train_0/avg_q             | -19.00485835708566    |
| train_0/current_q         | -9.706704585825785    |
| train_0/fw_bonus          | -0.9956780731678009   |
| train_0/fw_loss           | 0.030758470203727482  |
| train_0/mu_grads          | -0.06165239699184895  |
| train_0/mu_grads_std      | 0.4563841328024864    |
| train_0/mu_loss           | 9.452926257560978     |
| train_0/next_q            | -9.335021306452205    |
| train_0/q_grads           | 0.002821912302169949  |
| train_0/q_grads_std       | 0.25555970817804335   |
| train_0/q_loss            | 1.088014344895374     |
| train_0/reward            | -0.8410276731134217   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.012939453125        |
| train_0/target_q          | -9.77719148669404     |
| train_1/avg_q             | -12.163164858206349   |
| train_1/current_q         | -4.345333808375985    |
| train_1/fw_bonus          | -0.9682750970125198   |
| train_1/fw_loss           | 0.25960214287042616   |
| train_1/mu_grads          | -0.022459124540910126 |
| train_1/mu_grads_std      | 0.21929280869662762   |
| train_1/mu_loss           | 2.2380804669563146    |
| train_1/n_subgoals        | 1755.0                |
| train_1/next_q            | -2.1904207112841334   |
| train_1/q_grads           | -0.014756955462507904 |
| train_1/q_grads_std       | 0.28026043996214867   |
| train_1/q_loss            | 3.7082574542103437    |
| train_1/reward            | -2.4391879134203918   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0                   |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.15042735042735042   |
| train_1/target_q          | -4.303080681166394    |
-----------------------------------------------------
New best value for test/success_rate: 0.92. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.82
Training epoch 14
Time for epoch 14: 372.69. Rollout time: 161.14, Training time: 211.50
Evaluating epoch 14
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 14                    |
| policy/steps              | 1120784.0             |
| test/episodes             | 375.0                 |
| test/success_rate         | 0.8                   |
| test_0/avg_q              | -7.038357080369106    |
| test_1/avg_q              | -1.0626074380137478   |
| test_1/n_subgoals         | 2528.0                |
| test_1/subgoal_succ_rate  | 0.9727056962025317    |
| train/episodes            | 1500.0                |
| train/success_rate        | 0.71                  |
| train_0/avg_q             | -19.482628688129225   |
| train_0/current_q         | -9.939530772767986    |
| train_0/fw_bonus          | -0.9954925239086151   |
| train_0/fw_loss           | 0.03203770485706627   |
| train_0/mu_grads          | -0.06399112362414598  |
| train_0/mu_grads_std      | 0.46625459343194964   |
| train_0/mu_loss           | 9.683464295846974     |
| train_0/next_q            | -9.551207490480609    |
| train_0/q_grads           | 0.0021752807631855832 |
| train_0/q_grads_std       | 0.2627036176621914    |
| train_0/q_loss            | 1.0135255422434177    |
| train_0/reward            | -0.8488914410165307   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0072998046875       |
| train_0/target_q          | -9.998669305028077    |
| train_1/avg_q             | -12.08875077404537    |
| train_1/current_q         | -4.165491478980465    |
| train_1/fw_bonus          | -0.9683426409959793   |
| train_1/fw_loss           | 0.25908666625618937   |
| train_1/mu_grads          | -0.023473250772804023 |
| train_1/mu_grads_std      | 0.22399497851729394   |
| train_1/mu_loss           | 2.091104883417444     |
| train_1/n_subgoals        | 1768.0                |
| train_1/next_q            | -2.048010837945105    |
| train_1/q_grads           | -0.01751292929984629  |
| train_1/q_grads_std       | 0.29122633412480353   |
| train_1/q_loss            | 3.877876717679102     |
| train_1/reward            | -2.358144010588876    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0                   |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.13631221719457012   |
| train_1/target_q          | -4.116985764078832    |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.8300000000000001
Training epoch 15
Time for epoch 15: 384.14. Rollout time: 159.63, Training time: 224.47
Evaluating epoch 15
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 15                    |
| policy/steps              | 1171173.0             |
| test/episodes             | 400.0                 |
| test/success_rate         | 0.8                   |
| test_0/avg_q              | -10.638162311950115   |
| test_1/avg_q              | -1.5070789954290942   |
| test_1/n_subgoals         | 838.0                 |
| test_1/subgoal_succ_rate  | 0.8281622911694511    |
| train/episodes            | 1600.0                |
| train/success_rate        | 0.72                  |
| train_0/avg_q             | -19.52022799745481    |
| train_0/current_q         | -9.981399128589377    |
| train_0/fw_bonus          | -0.9953719973564148   |
| train_0/fw_loss           | 0.03286857609637082   |
| train_0/mu_grads          | -0.06549192927777767  |
| train_0/mu_grads_std      | 0.47498711571097374   |
| train_0/mu_loss           | 9.700381036068137     |
| train_0/next_q            | -9.561335549742427    |
| train_0/q_grads           | 0.0011738595028873533 |
| train_0/q_grads_std       | 0.2694100372493267    |
| train_0/q_loss            | 1.0351825279375082    |
| train_0/reward            | -0.8619350420562114   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.005615234375        |
| train_0/target_q          | -10.039391687379691   |
| train_1/avg_q             | -12.131946685465799   |
| train_1/current_q         | -3.9387474382516054   |
| train_1/fw_bonus          | -0.9659301832318306   |
| train_1/fw_loss           | 0.27749537527561186   |
| train_1/mu_grads          | -0.024910559644922614 |
| train_1/mu_grads_std      | 0.22992248497903348   |
| train_1/mu_loss           | 1.8340814017148268    |
| train_1/n_subgoals        | 1802.0                |
| train_1/next_q            | -1.7869566655895561   |
| train_1/q_grads           | -0.019578874018043278 |
| train_1/q_grads_std       | 0.30169247314333913   |
| train_1/q_loss            | 4.0641863876426       |
| train_1/reward            | -2.376821902517986    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0                   |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.15593784683684794   |
| train_1/target_q          | -3.905913898221557    |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.8400000000000001
Training epoch 16
Time for epoch 16: 405.07. Rollout time: 160.47, Training time: 244.55
Evaluating epoch 16
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
------------------------------------------------------
| epoch                     | 16                     |
| policy/steps              | 1214427.0              |
| test/episodes             | 425.0                  |
| test/success_rate         | 0.84                   |
| test_0/avg_q              | -9.50887202680287      |
| test_1/avg_q              | -1.7194533171656814    |
| test_1/n_subgoals         | 220.0                  |
| test_1/subgoal_succ_rate  | 0.38181818181818183    |
| train/episodes            | 1700.0                 |
| train/success_rate        | 0.77                   |
| train_0/avg_q             | -19.047460145539514    |
| train_0/current_q         | -10.076601222081726    |
| train_0/fw_bonus          | -0.9954073980450631    |
| train_0/fw_loss           | 0.032624520687386395   |
| train_0/mu_grads          | -0.06799729820340872   |
| train_0/mu_grads_std      | 0.48269455060362815    |
| train_0/mu_loss           | 9.794697742581798      |
| train_0/next_q            | -9.670332337180072     |
| train_0/q_grads           | -0.0001599789673491614 |
| train_0/q_grads_std       | 0.27492189183831217    |
| train_0/q_loss            | 1.0723521000576037     |
| train_0/reward            | -0.8658623492319748    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.00478515625          |
| train_0/target_q          | -10.13344302901688     |
| train_1/avg_q             | -12.03977765669253     |
| train_1/current_q         | -3.738437850789699     |
| train_1/fw_bonus          | -0.9665357545018196    |
| train_1/fw_loss           | 0.27287444695830343    |
| train_1/mu_grads          | -0.026229468081146477  |
| train_1/mu_grads_std      | 0.23634163439273834    |
| train_1/mu_loss           | 1.617888161614792      |
| train_1/n_subgoals        | 1553.0                 |
| train_1/next_q            | -1.5848180100686422    |
| train_1/q_grads           | -0.02331654876470566   |
| train_1/q_grads_std       | 0.3139200329780579     |
| train_1/q_loss            | 3.8850777924287825     |
| train_1/reward            | -2.3409930696056107    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0                    |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.1519639407598197     |
| train_1/target_q          | -3.700419646325095     |
------------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.8400000000000001
Training epoch 17
Time for epoch 17: 341.59. Rollout time: 146.15, Training time: 195.40
Evaluating epoch 17
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
------------------------------------------------------
| epoch                     | 17                     |
| policy/steps              | 1260087.0              |
| test/episodes             | 450.0                  |
| test/success_rate         | 0.92                   |
| test_0/avg_q              | -9.739535822559265     |
| test_1/avg_q              | -1.6781584531868878    |
| test_1/n_subgoals         | 182.0                  |
| test_1/subgoal_succ_rate  | 0.36813186813186816    |
| train/episodes            | 1800.0                 |
| train/success_rate        | 0.74                   |
| train_0/avg_q             | -19.007159427610677    |
| train_0/current_q         | -9.932579100357088     |
| train_0/fw_bonus          | -0.9953898966312409    |
| train_0/fw_loss           | 0.03274523010477424    |
| train_0/mu_grads          | -0.07048830147832633   |
| train_0/mu_grads_std      | 0.48975093588232993    |
| train_0/mu_loss           | 9.665957495027884      |
| train_0/next_q            | -9.501159291917483     |
| train_0/q_grads           | -0.0006865576287964359 |
| train_0/q_grads_std       | 0.28078492879867556    |
| train_0/q_loss            | 0.9894708500789605     |
| train_0/reward            | -0.8704539191712684    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.004541015625         |
| train_0/target_q          | -9.99871038124515      |
| train_1/avg_q             | -11.811493057121359    |
| train_1/current_q         | -3.5902739317815096    |
| train_1/fw_bonus          | -0.9666676461696625    |
| train_1/fw_loss           | 0.27186796739697455    |
| train_1/mu_grads          | -0.02821421273984015   |
| train_1/mu_grads_std      | 0.24305022470653057    |
| train_1/mu_loss           | 1.4648144009175035     |
| train_1/n_subgoals        | 1695.0                 |
| train_1/next_q            | -1.428183274308561     |
| train_1/q_grads           | -0.02667160644195974   |
| train_1/q_grads_std       | 0.3259223081171513     |
| train_1/q_loss            | 3.5456148749529506     |
| train_1/reward            | -2.339176680674427     |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0                    |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.1705014749262537     |
| train_1/target_q          | -3.5658681088482047    |
------------------------------------------------------
New best value for test/success_rate: 0.92. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.84
Training epoch 18
Time for epoch 18: 368.34. Rollout time: 149.19, Training time: 219.11
Evaluating epoch 18
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
------------------------------------------------------
| epoch                     | 18                     |
| policy/steps              | 1305074.0              |
| test/episodes             | 475.0                  |
| test/success_rate         | 0.96                   |
| test_0/avg_q              | -12.0886149723342      |
| test_1/avg_q              | -1.586012368407244     |
| test_1/n_subgoals         | 141.0                  |
| test_1/subgoal_succ_rate  | 0.48226950354609927    |
| train/episodes            | 1900.0                 |
| train/success_rate        | 0.74                   |
| train_0/avg_q             | -19.4662136845105      |
| train_0/current_q         | -10.043566257016582    |
| train_0/fw_bonus          | -0.9953382432460784    |
| train_0/fw_loss           | 0.03310134126804769    |
| train_0/mu_grads          | -0.07180273681879043   |
| train_0/mu_grads_std      | 0.49633816480636594    |
| train_0/mu_loss           | 9.757412848707409      |
| train_0/next_q            | -9.607697277208306     |
| train_0/q_grads           | -0.0019398522097617387 |
| train_0/q_grads_std       | 0.28598819971084594    |
| train_0/q_loss            | 0.9380727478391184     |
| train_0/reward            | -0.8729312400781055    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.0048828125           |
| train_0/target_q          | -10.10840910851822     |
| train_1/avg_q             | -12.362794293292225    |
| train_1/current_q         | -3.4704920445714316    |
| train_1/fw_bonus          | -0.9653741553425789    |
| train_1/fw_loss           | 0.2817382074892521     |
| train_1/mu_grads          | -0.030684879701584577  |
| train_1/mu_grads_std      | 0.2500052943825722     |
| train_1/mu_loss           | 1.3135903828444444     |
| train_1/n_subgoals        | 1672.0                 |
| train_1/next_q            | -1.3017469191944575    |
| train_1/q_grads           | -0.0290981309954077    |
| train_1/q_grads_std       | 0.3380584746599197     |
| train_1/q_loss            | 3.427593698739505      |
| train_1/reward            | -2.3452436869887605    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0                    |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.1303827751196172     |
| train_1/target_q          | -3.4514487334786255    |
------------------------------------------------------
New best value for test/success_rate: 0.96. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.88
Training epoch 19
Time for epoch 19: 360.22. Rollout time: 146.42, Training time: 213.73
Evaluating epoch 19
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
------------------------------------------------------
| epoch                     | 19                     |
| policy/steps              | 1348607.0              |
| test/episodes             | 500.0                  |
| test/success_rate         | 0.92                   |
| test_0/avg_q              | -11.442351074709125    |
| test_1/avg_q              | -3.067512561172046     |
| test_1/n_subgoals         | 158.0                  |
| test_1/subgoal_succ_rate  | 0.310126582278481      |
| train/episodes            | 2000.0                 |
| train/success_rate        | 0.8                    |
| train_0/avg_q             | -19.47068643709587     |
| train_0/current_q         | -10.00986850534284     |
| train_0/fw_bonus          | -0.9953023701906204    |
| train_0/fw_loss           | 0.03334866631776094    |
| train_0/mu_grads          | -0.07273779585957527   |
| train_0/mu_grads_std      | 0.5028244525194168     |
| train_0/mu_loss           | 9.727223637134736      |
| train_0/next_q            | -9.572382596262207     |
| train_0/q_grads           | -0.0036305657995399088 |
| train_0/q_grads_std       | 0.2911604464054108     |
| train_0/q_loss            | 0.9052614194737609     |
| train_0/reward            | -0.8744237363222055    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.0032958984375        |
| train_0/target_q          | -10.074127272725836    |
| train_1/avg_q             | -11.904723252331962    |
| train_1/current_q         | -3.361896668640843     |
| train_1/fw_bonus          | -0.96543188393116      |
| train_1/fw_loss           | 0.2812978200614452     |
| train_1/mu_grads          | -0.031604702956974505  |
| train_1/mu_grads_std      | 0.2566127337515354     |
| train_1/mu_loss           | 1.1977352483898742     |
| train_1/n_subgoals        | 1614.0                 |
| train_1/next_q            | -1.2111053584564626    |
| train_1/q_grads           | -0.032011127565056086  |
| train_1/q_grads_std       | 0.349968446791172      |
| train_1/q_loss            | 3.2224227867107245     |
| train_1/reward            | -2.325317930169331     |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0                    |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.14745972738537794    |
| train_1/target_q          | -3.346313945962875     |
------------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.9099999999999999
Training epoch 20
Time for epoch 20: 320.45. Rollout time: 119.44, Training time: 200.98
Evaluating epoch 20
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 20                    |
| policy/steps              | 1389398.0             |
| test/episodes             | 525.0                 |
| test/success_rate         | 0.92                  |
| test_0/avg_q              | -8.548465493144068    |
| test_1/avg_q              | -1.5028874360718587   |
| test_1/n_subgoals         | 185.0                 |
| test_1/subgoal_succ_rate  | 0.41081081081081083   |
| train/episodes            | 2100.0                |
| train/success_rate        | 0.82                  |
| train_0/avg_q             | -18.535284549165254   |
| train_0/current_q         | -9.925626867736355    |
| train_0/fw_bonus          | -0.995300655066967    |
| train_0/fw_loss           | 0.03336051246151328   |
| train_0/mu_grads          | -0.07517120670527219  |
| train_0/mu_grads_std      | 0.5096023172140122    |
| train_0/mu_loss           | 9.65315906812592      |
| train_0/next_q            | -9.48438941075393     |
| train_0/q_grads           | -0.005022880982141942 |
| train_0/q_grads_std       | 0.29578370302915574   |
| train_0/q_loss            | 0.9317060568517714    |
| train_0/reward            | -0.8765839512798266   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0026611328125       |
| train_0/target_q          | -9.998151135025214    |
| train_1/avg_q             | -11.458313877437158   |
| train_1/current_q         | -3.3364093317323986   |
| train_1/fw_bonus          | -0.9660815820097923   |
| train_1/fw_loss           | 0.276340115070343     |
| train_1/mu_grads          | -0.03221940193325281  |
| train_1/mu_grads_std      | 0.26253301873803137   |
| train_1/mu_loss           | 1.2040224085955284    |
| train_1/n_subgoals        | 1489.0                |
| train_1/next_q            | -1.2071915693135087   |
| train_1/q_grads           | -0.03443005681037903  |
| train_1/q_grads_std       | 0.36151200234889985   |
| train_1/q_loss            | 3.3190935346246726    |
| train_1/reward            | -2.3089642433340485   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0                   |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.15580926796507724   |
| train_1/target_q          | -3.3296296614581875   |
-----------------------------------------------------
Saving periodic policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101/policy_20.pkl ...
Mean of test/success_rate of last 4 epochs: 0.9299999999999999
Training epoch 21
Time for epoch 21: 328.63. Rollout time: 134.91, Training time: 193.69
Evaluating epoch 21
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 21                    |
| policy/steps              | 1435351.0             |
| test/episodes             | 550.0                 |
| test/success_rate         | 0.96                  |
| test_0/avg_q              | -12.105421036278074   |
| test_1/avg_q              | -1.297293981965955    |
| test_1/n_subgoals         | 131.0                 |
| test_1/subgoal_succ_rate  | 0.3511450381679389    |
| train/episodes            | 2200.0                |
| train/success_rate        | 0.73                  |
| train_0/avg_q             | -19.81629195962117    |
| train_0/current_q         | -9.903208661878262    |
| train_0/fw_bonus          | -0.995272408425808    |
| train_0/fw_loss           | 0.03355521759949624   |
| train_0/mu_grads          | -0.07557051852345467  |
| train_0/mu_grads_std      | 0.5158594593405723    |
| train_0/mu_loss           | 9.637342634197369     |
| train_0/next_q            | -9.462525778660908    |
| train_0/q_grads           | -0.006386356567963958 |
| train_0/q_grads_std       | 0.30083879008889197   |
| train_0/q_loss            | 0.8963218350138268    |
| train_0/reward            | -0.8728232286339335   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.003076171875        |
| train_0/target_q          | -9.987442412502812    |
| train_1/avg_q             | -12.36202538875806    |
| train_1/current_q         | -3.2463771975381177   |
| train_1/fw_bonus          | -0.9639186039566994   |
| train_1/fw_loss           | 0.2928451284766197    |
| train_1/mu_grads          | -0.0334452573210001   |
| train_1/mu_grads_std      | 0.26815572902560236   |
| train_1/mu_loss           | 1.1011362355925       |
| train_1/n_subgoals        | 1716.0                |
| train_1/next_q            | -1.125431783419756    |
| train_1/q_grads           | -0.03650171076878905  |
| train_1/q_grads_std       | 0.3704990081489086    |
| train_1/q_loss            | 3.1754965395071992    |
| train_1/reward            | -2.299945714891874    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0                   |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.12296037296037296   |
| train_1/target_q          | -3.2484059444978124   |
-----------------------------------------------------
New best value for test/success_rate: 0.96. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.94
Training epoch 22
Time for epoch 22: 296.50. Rollout time: 116.85, Training time: 179.61
Evaluating epoch 22
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 22                    |
| policy/steps              | 1477724.0             |
| test/episodes             | 575.0                 |
| test/success_rate         | 0.96                  |
| test_0/avg_q              | -7.749180118616868    |
| test_1/avg_q              | -1.0183179138572638   |
| test_1/n_subgoals         | 127.0                 |
| test_1/subgoal_succ_rate  | 0.4251968503937008    |
| train/episodes            | 2300.0                |
| train/success_rate        | 0.76                  |
| train_0/avg_q             | -19.52234567086581    |
| train_0/current_q         | -10.410885407976675   |
| train_0/fw_bonus          | -0.995279823243618    |
| train_0/fw_loss           | 0.033504072297364476  |
| train_0/mu_grads          | -0.07686750460416078  |
| train_0/mu_grads_std      | 0.5214499592781067    |
| train_0/mu_loss           | 10.136341183933926    |
| train_0/next_q            | -9.972240519085373    |
| train_0/q_grads           | -0.007742326287552714 |
| train_0/q_grads_std       | 0.3058307833969593    |
| train_0/q_loss            | 0.9230678476919596    |
| train_0/reward            | -0.8763223977068264   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0029296875          |
| train_0/target_q          | -10.471917869606283   |
| train_1/avg_q             | -12.080678226419602   |
| train_1/current_q         | -3.221573882198766    |
| train_1/fw_bonus          | -0.962886793911457    |
| train_1/fw_loss           | 0.3007186256349087    |
| train_1/mu_grads          | -0.03489590622484684  |
| train_1/mu_grads_std      | 0.2731457225978374    |
| train_1/mu_loss           | 1.0763851064984618    |
| train_1/n_subgoals        | 1584.0                |
| train_1/next_q            | -1.099489634042261    |
| train_1/q_grads           | -0.038906009215861556 |
| train_1/q_grads_std       | 0.3797725006937981    |
| train_1/q_loss            | 3.4031773512488277    |
| train_1/reward            | -2.3126882484310043   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0                   |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.12563131313131312   |
| train_1/target_q          | -3.2323267226773793   |
-----------------------------------------------------
New best value for test/success_rate: 0.96. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.94
Training epoch 23
Time for epoch 23: 289.35. Rollout time: 106.79, Training time: 182.53
Evaluating epoch 23
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 23                    |
| policy/steps              | 1517902.0             |
| test/episodes             | 600.0                 |
| test/success_rate         | 0.96                  |
| test_0/avg_q              | -12.820387835070216   |
| test_1/avg_q              | -1.6213391062095226   |
| test_1/n_subgoals         | 160.0                 |
| test_1/subgoal_succ_rate  | 0.425                 |
| train/episodes            | 2400.0                |
| train/success_rate        | 0.79                  |
| train_0/avg_q             | -19.823808152092134   |
| train_0/current_q         | -10.32870513801078    |
| train_0/fw_bonus          | -0.9952206924557686   |
| train_0/fw_loss           | 0.03391175945289433   |
| train_0/mu_grads          | -0.07690069805830717  |
| train_0/mu_grads_std      | 0.5266056478023529    |
| train_0/mu_loss           | 10.068834606575994    |
| train_0/next_q            | -9.903462543615474    |
| train_0/q_grads           | -0.009075257368385792 |
| train_0/q_grads_std       | 0.3107461430132389    |
| train_0/q_loss            | 0.9521185106534172    |
| train_0/reward            | -0.8730483365543478   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0032470703125       |
| train_0/target_q          | -10.397653068905885   |
| train_1/avg_q             | -11.970172674388794   |
| train_1/current_q         | -3.1865104224702216   |
| train_1/fw_bonus          | -0.9641402170062066   |
| train_1/fw_loss           | 0.2911540687084198    |
| train_1/mu_grads          | -0.03745458973571658  |
| train_1/mu_grads_std      | 0.27947214618325233   |
| train_1/mu_loss           | 1.029151975197976     |
| train_1/n_subgoals        | 1477.0                |
| train_1/next_q            | -1.0662329729251847   |
| train_1/q_grads           | -0.04134643841534853  |
| train_1/q_grads_std       | 0.3887217968702316    |
| train_1/q_loss            | 3.3617703736107294    |
| train_1/reward            | -2.3013560978939496   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0                   |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.13473256601218686   |
| train_1/target_q          | -3.1946522273461175   |
-----------------------------------------------------
New best value for test/success_rate: 0.96. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.95
Training epoch 24
Time for epoch 24: 303.86. Rollout time: 117.74, Training time: 186.09
Evaluating epoch 24
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 24                    |
| policy/steps              | 1562405.0             |
| test/episodes             | 625.0                 |
| test/success_rate         | 0.92                  |
| test_0/avg_q              | -11.146286573236969   |
| test_1/avg_q              | -2.0779611946111807   |
| test_1/n_subgoals         | 166.0                 |
| test_1/subgoal_succ_rate  | 0.3493975903614458    |
| train/episodes            | 2500.0                |
| train/success_rate        | 0.79                  |
| train_0/avg_q             | -19.882075499839825   |
| train_0/current_q         | -10.226726421807015   |
| train_0/fw_bonus          | -0.995211911201477    |
| train_0/fw_loss           | 0.033972359634935854  |
| train_0/mu_grads          | -0.07791526280343533  |
| train_0/mu_grads_std      | 0.5315145760774612    |
| train_0/mu_loss           | 9.949738414087202     |
| train_0/next_q            | -9.790730656500916    |
| train_0/q_grads           | -0.009839731501415372 |
| train_0/q_grads_std       | 0.3157288581132889    |
| train_0/q_loss            | 0.9080888877129061    |
| train_0/reward            | -0.8712150843246491   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0027099609375       |
| train_0/target_q          | -10.279070744641896   |
| train_1/avg_q             | -11.877363542134512   |
| train_1/current_q         | -3.1013772267931143   |
| train_1/fw_bonus          | -0.9642926990985871   |
| train_1/fw_loss           | 0.2899904936552048    |
| train_1/mu_grads          | -0.039142456464469434 |
| train_1/mu_grads_std      | 0.28469125330448153   |
| train_1/mu_loss           | 1.0214776995485093    |
| train_1/n_subgoals        | 1644.0                |
| train_1/next_q            | -1.0479401244422966   |
| train_1/q_grads           | -0.0436552282422781   |
| train_1/q_grads_std       | 0.3968322493135929    |
| train_1/q_loss            | 3.4100063782580357    |
| train_1/reward            | -2.243972345934162    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0                   |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.14659367396593673   |
| train_1/target_q          | -3.120698691870918    |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.95
Training epoch 25
Time for epoch 25: 300.51. Rollout time: 122.98, Training time: 177.50
Evaluating epoch 25
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 25                    |
| policy/steps              | 1609202.0             |
| test/episodes             | 650.0                 |
| test/success_rate         | 0.88                  |
| test_0/avg_q              | -10.42147267307945    |
| test_1/avg_q              | -1.222083733394042    |
| test_1/n_subgoals         | 200.0                 |
| test_1/subgoal_succ_rate  | 0.34                  |
| train/episodes            | 2600.0                |
| train/success_rate        | 0.73                  |
| train_0/avg_q             | -19.588140640768213   |
| train_0/current_q         | -10.272261310596566   |
| train_0/fw_bonus          | -0.9953152298927307   |
| train_0/fw_loss           | 0.03326003719121218   |
| train_0/mu_grads          | -0.07798820715397596  |
| train_0/mu_grads_std      | 0.5365073204040527    |
| train_0/mu_loss           | 9.986187394284991     |
| train_0/next_q            | -9.838474095934341    |
| train_0/q_grads           | -0.010620938939973712 |
| train_0/q_grads_std       | 0.31975781321525576   |
| train_0/q_loss            | 0.8839145863993852    |
| train_0/reward            | -0.871991647469622    |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0035888671875       |
| train_0/target_q          | -10.344914093038192   |
| train_1/avg_q             | -12.024073196500677   |
| train_1/current_q         | -3.0941311178332542   |
| train_1/fw_bonus          | -0.9632504478096962   |
| train_1/fw_loss           | 0.29794362336397173   |
| train_1/mu_grads          | -0.04019790105521679  |
| train_1/mu_grads_std      | 0.2896303549408913    |
| train_1/mu_loss           | 0.9532459083424067    |
| train_1/n_subgoals        | 1672.0                |
| train_1/next_q            | -0.9602632112457549   |
| train_1/q_grads           | -0.046111464779824016 |
| train_1/q_grads_std       | 0.404630745947361     |
| train_1/q_loss            | 3.345197633275255     |
| train_1/reward            | -2.339530873750482    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0                   |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.10825358851674641   |
| train_1/target_q          | -3.1376599042440034   |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.9299999999999999
Training epoch 26
Time for epoch 26: 289.99. Rollout time: 107.49, Training time: 182.47
Evaluating epoch 26
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 26                    |
| policy/steps              | 1648827.0             |
| test/episodes             | 675.0                 |
| test/success_rate         | 0.96                  |
| test_0/avg_q              | -7.246055025016424    |
| test_1/avg_q              | -1.0617511567607423   |
| test_1/n_subgoals         | 131.0                 |
| test_1/subgoal_succ_rate  | 0.37404580152671757   |
| train/episodes            | 2700.0                |
| train/success_rate        | 0.84                  |
| train_0/avg_q             | -19.277677243429505   |
| train_0/current_q         | -10.29009316569603    |
| train_0/fw_bonus          | -0.9951920181512832   |
| train_0/fw_loss           | 0.03410934861749411   |
| train_0/mu_grads          | -0.0781453676521778   |
| train_0/mu_grads_std      | 0.5409426912665367    |
| train_0/mu_loss           | 10.014244954159537    |
| train_0/next_q            | -9.862299719335573    |
| train_0/q_grads           | -0.012213618285022676 |
| train_0/q_grads_std       | 0.3243397749960423    |
| train_0/q_loss            | 0.9104958425761904    |
| train_0/reward            | -0.8676029390888289   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.002099609375        |
| train_0/target_q          | -10.34168617712719    |
| train_1/avg_q             | -11.529584578250002   |
| train_1/current_q         | -2.929966641479909    |
| train_1/fw_bonus          | -0.9631603106856346   |
| train_1/fw_loss           | 0.2986314117908478    |
| train_1/mu_grads          | -0.04113514050841331  |
| train_1/mu_grads_std      | 0.29421885684132576   |
| train_1/mu_loss           | 0.8351914414932015    |
| train_1/n_subgoals        | 1469.0                |
| train_1/next_q            | -0.8592377941677445   |
| train_1/q_grads           | -0.0485190711915493   |
| train_1/q_grads_std       | 0.4130575552582741    |
| train_1/q_loss            | 2.9648197830944927    |
| train_1/reward            | -2.270733926449611    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0                   |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.12865895166780122   |
| train_1/target_q          | -2.9830433019659925   |
-----------------------------------------------------
New best value for test/success_rate: 0.96. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.9299999999999999
Training epoch 27
Time for epoch 27: 291.58. Rollout time: 114.31, Training time: 177.24
Evaluating epoch 27
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 27                    |
| policy/steps              | 1691679.0             |
| test/episodes             | 700.0                 |
| test/success_rate         | 0.92                  |
| test_0/avg_q              | -12.100980509718847   |
| test_1/avg_q              | -1.116127507679913    |
| test_1/n_subgoals         | 156.0                 |
| test_1/subgoal_succ_rate  | 0.33974358974358976   |
| train/episodes            | 2800.0                |
| train/success_rate        | 0.78                  |
| train_0/avg_q             | -19.412590690935833   |
| train_0/current_q         | -10.411373164281736   |
| train_0/fw_bonus          | -0.9952003836631775   |
| train_0/fw_loss           | 0.03405172294005752   |
| train_0/mu_grads          | -0.07903665024787188  |
| train_0/mu_grads_std      | 0.5456194743514061    |
| train_0/mu_loss           | 10.148062979029714    |
| train_0/next_q            | -9.979429591312808    |
| train_0/q_grads           | -0.012981535540893674 |
| train_0/q_grads_std       | 0.32821568101644516   |
| train_0/q_loss            | 0.9250525302910871    |
| train_0/reward            | -0.8728052241229307   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.00302734375         |
| train_0/target_q          | -10.476272431787695   |
| train_1/avg_q             | -11.85337610500016    |
| train_1/current_q         | -2.8917433436388427   |
| train_1/fw_bonus          | -0.964000004529953    |
| train_1/fw_loss           | 0.2922239013016224    |
| train_1/mu_grads          | -0.04198407176882028  |
| train_1/mu_grads_std      | 0.29873166680336      |
| train_1/mu_loss           | 0.7994027579978088    |
| train_1/n_subgoals        | 1570.0                |
| train_1/next_q            | -0.8171928018309738   |
| train_1/q_grads           | -0.05100328316912055  |
| train_1/q_grads_std       | 0.4217583782970905    |
| train_1/q_loss            | 3.268182853637613     |
| train_1/reward            | -2.3059902387845796   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0                   |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.11337579617834395   |
| train_1/target_q          | -2.9863232581312715   |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.9199999999999999
Training epoch 28
Time for epoch 28: 298.43. Rollout time: 114.47, Training time: 183.93
Evaluating epoch 28
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 28                    |
| policy/steps              | 1734352.0             |
| test/episodes             | 725.0                 |
| test/success_rate         | 0.88                  |
| test_0/avg_q              | -13.52998097405297    |
| test_1/avg_q              | -1.1928263565287456   |
| test_1/n_subgoals         | 170.0                 |
| test_1/subgoal_succ_rate  | 0.27058823529411763   |
| train/episodes            | 2900.0                |
| train/success_rate        | 0.79                  |
| train_0/avg_q             | -19.35621390031361    |
| train_0/current_q         | -10.228989725875017   |
| train_0/fw_bonus          | -0.9952597841620445   |
| train_0/fw_loss           | 0.033642212161794305  |
| train_0/mu_grads          | -0.08033461403101683  |
| train_0/mu_grads_std      | 0.5503889814019203    |
| train_0/mu_loss           | 9.956709767684227     |
| train_0/next_q            | -9.798482340841229    |
| train_0/q_grads           | -0.01291026605758816  |
| train_0/q_grads_std       | 0.33236285895109174   |
| train_0/q_loss            | 0.9428512214331818    |
| train_0/reward            | -0.8728383843361371   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.00263671875         |
| train_0/target_q          | -10.315265228940481   |
| train_1/avg_q             | -11.936551369318154   |
| train_1/current_q         | -2.8596626131518277   |
| train_1/fw_bonus          | -0.9634752333164215   |
| train_1/fw_loss           | 0.2962283715605736    |
| train_1/mu_grads          | -0.04286534758284688  |
| train_1/mu_grads_std      | 0.30364964380860326   |
| train_1/mu_loss           | 0.7271619728111289    |
| train_1/n_subgoals        | 1546.0                |
| train_1/next_q            | -0.756978680279592    |
| train_1/q_grads           | -0.053369282931089404 |
| train_1/q_grads_std       | 0.4300271198153496    |
| train_1/q_loss            | 2.96319970188861      |
| train_1/reward            | -2.3161495695039775   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0                   |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.12419146183699871   |
| train_1/target_q          | -2.948716296254372    |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.9099999999999999
Training epoch 29
Time for epoch 29: 294.43. Rollout time: 114.82, Training time: 179.58
Evaluating epoch 29
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 29                    |
| policy/steps              | 1777522.0             |
| test/episodes             | 750.0                 |
| test/success_rate         | 0.96                  |
| test_0/avg_q              | -13.782070223304524   |
| test_1/avg_q              | -1.9199258201755198   |
| test_1/n_subgoals         | 126.0                 |
| test_1/subgoal_succ_rate  | 0.3333333333333333    |
| train/episodes            | 3000.0                |
| train/success_rate        | 0.75                  |
| train_0/avg_q             | -19.794006050579736   |
| train_0/current_q         | -10.520680481288915   |
| train_0/fw_bonus          | -0.9953298613429069   |
| train_0/fw_loss           | 0.033159151952713725  |
| train_0/mu_grads          | -0.07960381209850312  |
| train_0/mu_grads_std      | 0.5542375236749649    |
| train_0/mu_loss           | 10.247058286857023    |
| train_0/next_q            | -10.094331066449751   |
| train_0/q_grads           | -0.013925609085708857 |
| train_0/q_grads_std       | 0.3370264761149883    |
| train_0/q_loss            | 0.9353384967633029    |
| train_0/reward            | -0.8706769465003162   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0034423828125       |
| train_0/target_q          | -10.593352624908547   |
| train_1/avg_q             | -11.65351742597243    |
| train_1/current_q         | -2.802136632629259    |
| train_1/fw_bonus          | -0.9629117339849472   |
| train_1/fw_loss           | 0.3005282260477543    |
| train_1/mu_grads          | -0.04434428447857499  |
| train_1/mu_grads_std      | 0.30783103629946706   |
| train_1/mu_loss           | 0.7378623462908134    |
| train_1/n_subgoals        | 1612.0                |
| train_1/next_q            | -0.7592021087878823   |
| train_1/q_grads           | -0.05539540098980069  |
| train_1/q_grads_std       | 0.4380564443767071    |
| train_1/q_loss            | 2.9418682780795367    |
| train_1/reward            | -2.290630276864249    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0                   |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.11910669975186104   |
| train_1/target_q          | -2.9166114732468578   |
-----------------------------------------------------
New best value for test/success_rate: 0.96. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.9299999999999999
Training epoch 30
Time for epoch 30: 291.81. Rollout time: 107.74, Training time: 184.04
Evaluating epoch 30
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 30                    |
| policy/steps              | 1818577.0             |
| test/episodes             | 775.0                 |
| test/success_rate         | 0.92                  |
| test_0/avg_q              | -12.500061841917493   |
| test_1/avg_q              | -1.164349388968105    |
| test_1/n_subgoals         | 160.0                 |
| test_1/subgoal_succ_rate  | 0.30625               |
| train/episodes            | 3100.0                |
| train/success_rate        | 0.77                  |
| train_0/avg_q             | -20.024388533877588   |
| train_0/current_q         | -10.416192321750428   |
| train_0/fw_bonus          | -0.9953041508793831   |
| train_0/fw_loss           | 0.033336311532184484  |
| train_0/mu_grads          | -0.08018463607877493  |
| train_0/mu_grads_std      | 0.5581862613558769    |
| train_0/mu_loss           | 10.146101026000242    |
| train_0/next_q            | -9.993827146718633    |
| train_0/q_grads           | -0.014811032987199723 |
| train_0/q_grads_std       | 0.34128620773553847   |
| train_0/q_loss            | 0.9376257036729454    |
| train_0/reward            | -0.8654886372471082   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0032470703125       |
| train_0/target_q          | -10.493480232274607   |
| train_1/avg_q             | -11.78617546971946    |
| train_1/current_q         | -2.780229217003432    |
| train_1/fw_bonus          | -0.9621451139450073   |
| train_1/fw_loss           | 0.30637798011302947   |
| train_1/mu_grads          | -0.04494654322043061  |
| train_1/mu_grads_std      | 0.31220284923911096   |
| train_1/mu_loss           | 0.7081675949256756    |
| train_1/n_subgoals        | 1488.0                |
| train_1/next_q            | -0.7405439365226285   |
| train_1/q_grads           | -0.057178919203579426 |
| train_1/q_grads_std       | 0.4449467837810516    |
| train_1/q_loss            | 3.1016772200341682    |
| train_1/reward            | -2.295712063128667    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0                   |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.10551075268817205   |
| train_1/target_q          | -2.905898276486638    |
-----------------------------------------------------
Saving periodic policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101/policy_30.pkl ...
Mean of test/success_rate of last 4 epochs: 0.9199999999999999
Training epoch 31
Time for epoch 31: 293.94. Rollout time: 111.12, Training time: 182.79
Evaluating epoch 31
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 31                    |
| policy/steps              | 1860543.0             |
| test/episodes             | 800.0                 |
| test/success_rate         | 0.96                  |
| test_0/avg_q              | -7.0257915862862985   |
| test_1/avg_q              | -0.2638866034644078   |
| test_1/n_subgoals         | 739.0                 |
| test_1/subgoal_succ_rate  | 0.9336941813261164    |
| train/episodes            | 3200.0                |
| train/success_rate        | 0.74                  |
| train_0/avg_q             | -20.01680799898423    |
| train_0/current_q         | -10.509680962797123   |
| train_0/fw_bonus          | -0.9953267976641655   |
| train_0/fw_loss           | 0.033180288039147854  |
| train_0/mu_grads          | -0.08004569076001644  |
| train_0/mu_grads_std      | 0.5627844989299774    |
| train_0/mu_loss           | 10.233688271079497    |
| train_0/next_q            | -10.079521320497152   |
| train_0/q_grads           | -0.015262556774541736 |
| train_0/q_grads_std       | 0.34600295275449755   |
| train_0/q_loss            | 0.9230667055898488    |
| train_0/reward            | -0.8660069904362899   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0031005859375       |
| train_0/target_q          | -10.557295205462541   |
| train_1/avg_q             | -11.586604916179096   |
| train_1/current_q         | -2.7229784396055847   |
| train_1/fw_bonus          | -0.9622766703367234   |
| train_1/fw_loss           | 0.30537419840693475   |
| train_1/mu_grads          | -0.04575440688058734  |
| train_1/mu_grads_std      | 0.3162061467766762    |
| train_1/mu_loss           | 0.6574644450915748    |
| train_1/n_subgoals        | 1555.0                |
| train_1/next_q            | -0.6931203212778575   |
| train_1/q_grads           | -0.059138450771570206 |
| train_1/q_grads_std       | 0.4523284398019314    |
| train_1/q_loss            | 2.760112018044137     |
| train_1/reward            | -2.3048269806407915   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0                   |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.11382636655948553   |
| train_1/target_q          | -2.8722684570316623   |
-----------------------------------------------------
New best value for test/success_rate: 0.96. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.9299999999999999
Training epoch 32
Time for epoch 32: 299.75. Rollout time: 116.74, Training time: 182.98
Evaluating epoch 32
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 32                    |
| policy/steps              | 1905371.0             |
| test/episodes             | 825.0                 |
| test/success_rate         | 0.88                  |
| test_0/avg_q              | -8.518754224568777    |
| test_1/avg_q              | -0.6727533794508521   |
| test_1/n_subgoals         | 218.0                 |
| test_1/subgoal_succ_rate  | 0.41743119266055045   |
| train/episodes            | 3300.0                |
| train/success_rate        | 0.71                  |
| train_0/avg_q             | -20.34316813478296    |
| train_0/current_q         | -10.37022762982745    |
| train_0/fw_bonus          | -0.9953383564949035   |
| train_0/fw_loss           | 0.03310053418390453   |
| train_0/mu_grads          | -0.08095502573996782  |
| train_0/mu_grads_std      | 0.5673765867948533    |
| train_0/mu_loss           | 10.101988928806446    |
| train_0/next_q            | -9.944966012999682    |
| train_0/q_grads           | -0.015676638530567288 |
| train_0/q_grads_std       | 0.34992595911026003   |
| train_0/q_loss            | 0.8986757265296639    |
| train_0/reward            | -0.8631900583874085   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0031982421875       |
| train_0/target_q          | -10.44022704233376    |
| train_1/avg_q             | -11.64163465734503    |
| train_1/current_q         | -2.7367803820526064   |
| train_1/fw_bonus          | -0.9613129302859307   |
| train_1/fw_loss           | 0.31272814199328425   |
| train_1/mu_grads          | -0.047273652907460925 |
| train_1/mu_grads_std      | 0.32065359950065614   |
| train_1/mu_loss           | 0.7191861852436826    |
| train_1/n_subgoals        | 1600.0                |
| train_1/next_q            | -0.7552699230813823   |
| train_1/q_grads           | -0.061546461284160615 |
| train_1/q_grads_std       | 0.45975273549556733   |
| train_1/q_loss            | 3.050509681478542     |
| train_1/reward            | -2.2766258408162683   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0                   |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.111875              |
| train_1/target_q          | -2.8961416101980233   |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.9299999999999999
Training epoch 33
Time for epoch 33: 303.85. Rollout time: 115.89, Training time: 187.93
Evaluating epoch 33
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 33                    |
| policy/steps              | 1949251.0             |
| test/episodes             | 850.0                 |
| test/success_rate         | 0.88                  |
| test_0/avg_q              | -10.390259579880805   |
| test_1/avg_q              | -1.159238377849971    |
| test_1/n_subgoals         | 178.0                 |
| test_1/subgoal_succ_rate  | 0.3258426966292135    |
| train/episodes            | 3400.0                |
| train/success_rate        | 0.74                  |
| train_0/avg_q             | -20.006448261906744   |
| train_0/current_q         | -10.42922708448923    |
| train_0/fw_bonus          | -0.9953141301870346   |
| train_0/fw_loss           | 0.03326751226559281   |
| train_0/mu_grads          | -0.08246462158858776  |
| train_0/mu_grads_std      | 0.5719035863876343    |
| train_0/mu_loss           | 10.151812645911223    |
| train_0/next_q            | -10.00626903226316    |
| train_0/q_grads           | -0.015427606599405407 |
| train_0/q_grads_std       | 0.35397443249821664   |
| train_0/q_loss            | 0.9239943443949123    |
| train_0/reward            | -0.8639347358828673   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.001904296875        |
| train_0/target_q          | -10.50507109790993    |
| train_1/avg_q             | -11.349470278529681   |
| train_1/current_q         | -2.726310167618151    |
| train_1/fw_bonus          | -0.9600571930408478   |
| train_1/fw_loss           | 0.3223103806376457    |
| train_1/mu_grads          | -0.04835637053474784  |
| train_1/mu_grads_std      | 0.3251813255250454    |
| train_1/mu_loss           | 0.6824465192238058    |
| train_1/n_subgoals        | 1579.0                |
| train_1/next_q            | -0.7269021374794302   |
| train_1/q_grads           | -0.06389659531414509  |
| train_1/q_grads_std       | 0.46630355194211004   |
| train_1/q_loss            | 2.784876018710505     |
| train_1/reward            | -2.2723568268538656   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0                   |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.10956301456618113   |
| train_1/target_q          | -2.868551197448947    |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.9099999999999999
Training epoch 34
Time for epoch 34: 287.66. Rollout time: 106.62, Training time: 181.01
Evaluating epoch 34
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 34                    |
| policy/steps              | 1991273.0             |
| test/episodes             | 875.0                 |
| test/success_rate         | 0.84                  |
| test_0/avg_q              | -14.680548128981023   |
| test_1/avg_q              | -0.7842443599861568   |
| test_1/n_subgoals         | 220.0                 |
| test_1/subgoal_succ_rate  | 0.2636363636363636    |
| train/episodes            | 3500.0                |
| train/success_rate        | 0.73                  |
| train_0/avg_q             | -20.238668350350345   |
| train_0/current_q         | -10.485283240371022   |
| train_0/fw_bonus          | -0.9953649446368218   |
| train_0/fw_loss           | 0.032917205384001134  |
| train_0/mu_grads          | -0.08418814614415168  |
| train_0/mu_grads_std      | 0.5763803422451019    |
| train_0/mu_loss           | 10.234673131215175    |
| train_0/next_q            | -10.079849391374836   |
| train_0/q_grads           | -0.015830902196466923 |
| train_0/q_grads_std       | 0.35861963480710984   |
| train_0/q_loss            | 0.9345652360528437    |
| train_0/reward            | -0.8588204856059747   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0018310546875       |
| train_0/target_q          | -10.56361970665715    |
| train_1/avg_q             | -11.302833853825287   |
| train_1/current_q         | -2.745403441308757    |
| train_1/fw_bonus          | -0.9607717633247376   |
| train_1/fw_loss           | 0.3168576993048191    |
| train_1/mu_grads          | -0.04948602262884379  |
| train_1/mu_grads_std      | 0.3285858049988747    |
| train_1/mu_loss           | 0.6773516558868262    |
| train_1/n_subgoals        | 1473.0                |
| train_1/next_q            | -0.7253929619246877   |
| train_1/q_grads           | -0.06625376548618078  |
| train_1/q_grads_std       | 0.47258752435445783   |
| train_1/q_loss            | 2.9352910465849615    |
| train_1/reward            | -2.296449043261964    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0                   |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.11948404616429056   |
| train_1/target_q          | -2.882156248966789    |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.8899999999999999
Training epoch 35
Time for epoch 35: 297.86. Rollout time: 117.12, Training time: 180.71
Evaluating epoch 35
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 35                    |
| policy/steps              | 2034926.0             |
| test/episodes             | 900.0                 |
| test/success_rate         | 0.96                  |
| test_0/avg_q              | -11.73831378853939    |
| test_1/avg_q              | -1.0617661969307317   |
| test_1/n_subgoals         | 135.0                 |
| test_1/subgoal_succ_rate  | 0.45925925925925926   |
| train/episodes            | 3600.0                |
| train/success_rate        | 0.72                  |
| train_0/avg_q             | -20.819065498895576   |
| train_0/current_q         | -10.521228065190252   |
| train_0/fw_bonus          | -0.995348097383976    |
| train_0/fw_loss           | 0.03303339113481343   |
| train_0/mu_grads          | -0.08437019363045692  |
| train_0/mu_grads_std      | 0.5816575214266777    |
| train_0/mu_loss           | 10.262394908573617    |
| train_0/next_q            | -10.12286254539582    |
| train_0/q_grads           | -0.016823298018425702 |
| train_0/q_grads_std       | 0.3626618131995201    |
| train_0/q_loss            | 0.9864544800057317    |
| train_0/reward            | -0.8605191534192272   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0011962890625       |
| train_0/target_q          | -10.588694642155883   |
| train_1/avg_q             | -11.789881687133649   |
| train_1/current_q         | -2.677324847765843    |
| train_1/fw_bonus          | -0.9598525896668434   |
| train_1/fw_loss           | 0.3238715291023254    |
| train_1/mu_grads          | -0.05084099266678095  |
| train_1/mu_grads_std      | 0.3331760369241238    |
| train_1/mu_loss           | 0.6623822649632821    |
| train_1/n_subgoals        | 1605.0                |
| train_1/next_q            | -0.698037873241928    |
| train_1/q_grads           | -0.06844450961798429  |
| train_1/q_grads_std       | 0.477895500510931     |
| train_1/q_loss            | 2.921305065392678     |
| train_1/reward            | -2.2523471829968913   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0                   |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.09719626168224299   |
| train_1/target_q          | -2.824657326010307    |
-----------------------------------------------------
New best value for test/success_rate: 0.96. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.89
Training epoch 36
Time for epoch 36: 301.14. Rollout time: 112.85, Training time: 188.26
Evaluating epoch 36
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 36                    |
| policy/steps              | 2077940.0             |
| test/episodes             | 925.0                 |
| test/success_rate         | 0.88                  |
| test_0/avg_q              | -14.691503673163629   |
| test_1/avg_q              | -2.0757993767329115   |
| test_1/n_subgoals         | 340.0                 |
| test_1/subgoal_succ_rate  | 0.5941176470588235    |
| train/episodes            | 3700.0                |
| train/success_rate        | 0.73                  |
| train_0/avg_q             | -20.898794061958036   |
| train_0/current_q         | -10.607245403597329   |
| train_0/fw_bonus          | -0.9953812435269356   |
| train_0/fw_loss           | 0.03280491381883621   |
| train_0/mu_grads          | -0.0850774247199297   |
| train_0/mu_grads_std      | 0.5863104000687599    |
| train_0/mu_loss           | 10.352465533002825    |
| train_0/next_q            | -10.203426234014781   |
| train_0/q_grads           | -0.017589492816478015 |
| train_0/q_grads_std       | 0.36704046428203585   |
| train_0/q_loss            | 0.984191298414401     |
| train_0/reward            | -0.8583484835988202   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.001318359375        |
| train_0/target_q          | -10.675882383697864   |
| train_1/avg_q             | -11.612911347532494   |
| train_1/current_q         | -2.708460036593844    |
| train_1/fw_bonus          | -0.958467610180378    |
| train_1/fw_loss           | 0.33443993926048277   |
| train_1/mu_grads          | -0.05225164080038667  |
| train_1/mu_grads_std      | 0.33819957375526427   |
| train_1/mu_loss           | 0.655825236121305     |
| train_1/n_subgoals        | 1523.0                |
| train_1/next_q            | -0.684621983609346    |
| train_1/q_grads           | -0.07052522767335176  |
| train_1/q_grads_std       | 0.4836095318198204    |
| train_1/q_loss            | 3.0856143196432035    |
| train_1/reward            | -2.2892262785622735   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0                   |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.10177281680892974   |
| train_1/target_q          | -2.8516878057073938   |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.8899999999999999
Training epoch 37
Time for epoch 37: 281.79. Rollout time: 102.33, Training time: 179.43
Evaluating epoch 37
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 37                    |
| policy/steps              | 2115579.0             |
| test/episodes             | 950.0                 |
| test/success_rate         | 1.0                   |
| test_0/avg_q              | -9.526113262049366    |
| test_1/avg_q              | -0.872060590747466    |
| test_1/n_subgoals         | 98.0                  |
| test_1/subgoal_succ_rate  | 0.46938775510204084   |
| train/episodes            | 3800.0                |
| train/success_rate        | 0.76                  |
| train_0/avg_q             | -20.92653302229635    |
| train_0/current_q         | -10.696406259477735   |
| train_0/fw_bonus          | -0.995512068271637    |
| train_0/fw_loss           | 0.03190291626378894   |
| train_0/mu_grads          | -0.08648715130984783  |
| train_0/mu_grads_std      | 0.5909681141376495    |
| train_0/mu_loss           | 10.463285772467636    |
| train_0/next_q            | -10.31056224861412    |
| train_0/q_grads           | -0.017838569777086376 |
| train_0/q_grads_std       | 0.3711824007332325    |
| train_0/q_loss            | 0.9184627350390435    |
| train_0/reward            | -0.852192227543128    |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.001220703125        |
| train_0/target_q          | -10.768074321564447   |
| train_1/avg_q             | -11.069965333197416   |
| train_1/current_q         | -2.598675510812491    |
| train_1/fw_bonus          | -0.9584398835897445   |
| train_1/fw_loss           | 0.33465149328112603   |
| train_1/mu_grads          | -0.05446369089186191  |
| train_1/mu_grads_std      | 0.3440163865685463    |
| train_1/mu_loss           | 0.6141890982915206    |
| train_1/n_subgoals        | 1412.0                |
| train_1/next_q            | -0.6473792778595673   |
| train_1/q_grads           | -0.07316074576228856  |
| train_1/q_grads_std       | 0.48839383721351626   |
| train_1/q_loss            | 2.85376117813624      |
| train_1/reward            | -2.2290508897134713   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0                   |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.09915014164305949   |
| train_1/target_q          | -2.7540036837813      |
-----------------------------------------------------
New best value for test/success_rate: 1.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.9199999999999999
Training epoch 38
Time for epoch 38: 294.01. Rollout time: 111.01, Training time: 182.97
Evaluating epoch 38
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 38                    |
| policy/steps              | 2156695.0             |
| test/episodes             | 975.0                 |
| test/success_rate         | 0.92                  |
| test_0/avg_q              | -12.469444608951163   |
| test_1/avg_q              | -0.7467148046167568   |
| test_1/n_subgoals         | 138.0                 |
| test_1/subgoal_succ_rate  | 0.2753623188405797    |
| train/episodes            | 3900.0                |
| train/success_rate        | 0.79                  |
| train_0/avg_q             | -19.962558724351346   |
| train_0/current_q         | -10.860592083957396   |
| train_0/fw_bonus          | -0.9955080509185791   |
| train_0/fw_loss           | 0.03193061579950154   |
| train_0/mu_grads          | -0.08854490425437689  |
| train_0/mu_grads_std      | 0.5948624923825264    |
| train_0/mu_loss           | 10.59709594001439     |
| train_0/next_q            | -10.46288310316068    |
| train_0/q_grads           | -0.018154537351801992 |
| train_0/q_grads_std       | 0.3754065401852131    |
| train_0/q_loss            | 0.9526431198129519    |
| train_0/reward            | -0.858343566829717    |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.000732421875        |
| train_0/target_q          | -10.929104134392412   |
| train_1/avg_q             | -11.017016252481664   |
| train_1/current_q         | -2.602648531451739    |
| train_1/fw_bonus          | -0.9590296238660813   |
| train_1/fw_loss           | 0.33015142008662224   |
| train_1/mu_grads          | -0.05585909839719534  |
| train_1/mu_grads_std      | 0.3479793518781662    |
| train_1/mu_loss           | 0.6084103489846647    |
| train_1/n_subgoals        | 1500.0                |
| train_1/next_q            | -0.6413519382439652   |
| train_1/q_grads           | -0.07552811522036791  |
| train_1/q_grads_std       | 0.49442279189825056   |
| train_1/q_loss            | 3.0155828963532336    |
| train_1/reward            | -2.2578360182375037   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0                   |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.112                 |
| train_1/target_q          | -2.7749237061443006   |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.94
Training epoch 39
Time for epoch 39: 296.76. Rollout time: 112.47, Training time: 184.26
Evaluating epoch 39
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
----------------------------------------------------
| epoch                     | 39                   |
| policy/steps              | 2200794.0            |
| test/episodes             | 1000.0               |
| test/success_rate         | 0.84                 |
| test_0/avg_q              | -15.49471545502867   |
| test_1/avg_q              | -1.4913440886892402  |
| test_1/n_subgoals         | 240.0                |
| test_1/subgoal_succ_rate  | 0.4125               |
| train/episodes            | 4000.0               |
| train/success_rate        | 0.73                 |
| train_0/avg_q             | -20.57836524879845   |
| train_0/current_q         | -10.668830502413218  |
| train_0/fw_bonus          | -0.9954812288284302  |
| train_0/fw_loss           | 0.032115563610568644 |
| train_0/mu_grads          | -0.08967092726379633 |
| train_0/mu_grads_std      | 0.5991016179323196   |
| train_0/mu_loss           | 10.434630595555419   |
| train_0/next_q            | -10.279951891003218  |
| train_0/q_grads           | -0.01909673400223255 |
| train_0/q_grads_std       | 0.3794974714517593   |
| train_0/q_loss            | 1.0105769731298664   |
| train_0/reward            | -0.8575311959491956  |
| train_0/reward_-0.0_frac  | 0.0                  |
| train_0/reward_-1.0_frac  | 0.000634765625       |
| train_0/target_q          | -10.736335792797044  |
| train_1/avg_q             | -11.151001845704196  |
| train_1/current_q         | -2.59773049677987    |
| train_1/fw_bonus          | -0.9572129249572754  |
| train_1/fw_loss           | 0.34401402845978735  |
| train_1/mu_grads          | -0.05776974204927683 |
| train_1/mu_grads_std      | 0.3529368132352829   |
| train_1/mu_loss           | 0.5428891401748064   |
| train_1/n_subgoals        | 1552.0               |
| train_1/next_q            | -0.588694469664248   |
| train_1/q_grads           | -0.07770657911896706 |
| train_1/q_grads_std       | 0.5003803431987762   |
| train_1/q_loss            | 2.7917787916274097   |
| train_1/reward            | -2.2927944435617973  |
| train_1/reward_-0.0_frac  | 0.0                  |
| train_1/reward_-1.0_frac  | 0.0                  |
| train_1/reward_-27.0_frac | 0.0                  |
| train_1/subgoal_succ_rate | 0.09342783505154639  |
| train_1/target_q          | -2.7660583904527885  |
----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.9099999999999999
Training epoch 40
Time for epoch 40: 281.70. Rollout time: 107.20, Training time: 174.48
Evaluating epoch 40
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 40                    |
| policy/steps              | 2244841.0             |
| test/episodes             | 1025.0                |
| test/success_rate         | 0.72                  |
| test_0/avg_q              | -18.912695517052832   |
| test_1/avg_q              | -2.3822552315154932   |
| test_1/n_subgoals         | 285.0                 |
| test_1/subgoal_succ_rate  | 0.19649122807017544   |
| train/episodes            | 4100.0                |
| train/success_rate        | 0.79                  |
| train_0/avg_q             | -19.98412183958588    |
| train_0/current_q         | -10.908610287200164   |
| train_0/fw_bonus          | -0.9955767154693603   |
| train_0/fw_loss           | 0.031457204231992365  |
| train_0/mu_grads          | -0.09071799516677856  |
| train_0/mu_grads_std      | 0.6030819028615951    |
| train_0/mu_loss           | 10.650878942450017    |
| train_0/next_q            | -10.519981638421912   |
| train_0/q_grads           | -0.019147082511335612 |
| train_0/q_grads_std       | 0.38420932441949845   |
| train_0/q_loss            | 0.9968590868469779    |
| train_0/reward            | -0.8616932532182545   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.000439453125        |
| train_0/target_q          | -10.999121768590427   |
| train_1/avg_q             | -10.895840536606258   |
| train_1/current_q         | -2.56709840979322     |
| train_1/fw_bonus          | -0.9574832588434219   |
| train_1/fw_loss           | 0.34195125550031663   |
| train_1/mu_grads          | -0.059133962448686364 |
| train_1/mu_grads_std      | 0.35740589872002604   |
| train_1/mu_loss           | 0.5256643470079913    |
| train_1/n_subgoals        | 1490.0                |
| train_1/next_q            | -0.5537518733991507   |
| train_1/q_grads           | -0.07877784371376037  |
| train_1/q_grads_std       | 0.5062566414475441    |
| train_1/q_loss            | 2.9778828853850023    |
| train_1/reward            | -2.294949427244501    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0                   |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.12751677852348994   |
| train_1/target_q          | -2.7394678546095106   |
-----------------------------------------------------
Saving periodic policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101/policy_40.pkl ...
Mean of test/success_rate of last 4 epochs: 0.8699999999999999
Training epoch 41
Time for epoch 41: 281.90. Rollout time: 103.35, Training time: 178.52
Evaluating epoch 41
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 41                    |
| policy/steps              | 2282985.0             |
| test/episodes             | 1050.0                |
| test/success_rate         | 0.96                  |
| test_0/avg_q              | -8.756412194738019    |
| test_1/avg_q              | -0.4275858590708298   |
| test_1/n_subgoals         | 128.0                 |
| test_1/subgoal_succ_rate  | 0.4296875             |
| train/episodes            | 4200.0                |
| train/success_rate        | 0.77                  |
| train_0/avg_q             | -20.434430287318317   |
| train_0/current_q         | -10.940044846116658   |
| train_0/fw_bonus          | -0.9956195786595344   |
| train_0/fw_loss           | 0.03116170596331358   |
| train_0/mu_grads          | -0.09061405900865793  |
| train_0/mu_grads_std      | 0.6071218892931938    |
| train_0/mu_loss           | 10.681793612094069    |
| train_0/next_q            | -10.557235879027655   |
| train_0/q_grads           | -0.019242425821721552 |
| train_0/q_grads_std       | 0.3887618325650692    |
| train_0/q_loss            | 0.9438345353437054    |
| train_0/reward            | -0.8618138322603045   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.000732421875        |
| train_0/target_q          | -11.033650848602472   |
| train_1/avg_q             | -10.71857740300312    |
| train_1/current_q         | -2.520283393594941    |
| train_1/fw_bonus          | -0.958917373418808    |
| train_1/fw_loss           | 0.33100789338350295   |
| train_1/mu_grads          | -0.06064874678850174  |
| train_1/mu_grads_std      | 0.36269863322377205   |
| train_1/mu_loss           | 0.4990342289292089    |
| train_1/n_subgoals        | 1400.0                |
| train_1/next_q            | -0.5291023294807459   |
| train_1/q_grads           | -0.08069789726287127  |
| train_1/q_grads_std       | 0.51301029920578      |
| train_1/q_loss            | 2.777629072196274     |
| train_1/reward            | -2.2519302720778795   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0                   |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.10357142857142858   |
| train_1/target_q          | -2.677554185189424    |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.86
Training epoch 42
Time for epoch 42: 270.75. Rollout time: 91.34, Training time: 179.38
Evaluating epoch 42
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
----------------------------------------------------
| epoch                     | 42                   |
| policy/steps              | 2318699.0            |
| test/episodes             | 1075.0               |
| test/success_rate         | 0.88                 |
| test_0/avg_q              | -12.512229485511503  |
| test_1/avg_q              | -1.721965250483133   |
| test_1/n_subgoals         | 936.0                |
| test_1/subgoal_succ_rate  | 0.8985042735042735   |
| train/episodes            | 4300.0               |
| train/success_rate        | 0.86                 |
| train_0/avg_q             | -20.36152184801751   |
| train_0/current_q         | -11.03611262860377   |
| train_0/fw_bonus          | -0.9956049606204033  |
| train_0/fw_loss           | 0.031262538861483335 |
| train_0/mu_grads          | -0.09192135278135538 |
| train_0/mu_grads_std      | 0.6110470667481422   |
| train_0/mu_loss           | 10.788431476872388   |
| train_0/next_q            | -10.64175157135105   |
| train_0/q_grads           | -0.01981172701343894 |
| train_0/q_grads_std       | 0.39301917180418966  |
| train_0/q_loss            | 1.0179806087111478   |
| train_0/reward            | -0.8682554882347177  |
| train_0/reward_-0.0_frac  | 0.0                  |
| train_0/reward_-1.0_frac  | 0.000634765625       |
| train_0/target_q          | -11.122906634342371  |
| train_1/avg_q             | -11.131422108633274  |
| train_1/current_q         | -2.4883779666067936  |
| train_1/fw_bonus          | -0.9592184528708458  |
| train_1/fw_loss           | 0.32871051505208015  |
| train_1/mu_grads          | -0.0628404576331377  |
| train_1/mu_grads_std      | 0.36744272112846377  |
| train_1/mu_loss           | 0.4481897916219898   |
| train_1/n_subgoals        | 1265.0               |
| train_1/next_q            | -0.48222428121909094 |
| train_1/q_grads           | -0.08300457708537579 |
| train_1/q_grads_std       | 0.5203228667378426   |
| train_1/q_loss            | 2.3191981253511598   |
| train_1/reward            | -2.2493871333328572  |
| train_1/reward_-0.0_frac  | 0.0                  |
| train_1/reward_-1.0_frac  | 0.0                  |
| train_1/reward_-27.0_frac | 0.0                  |
| train_1/subgoal_succ_rate | 0.10118577075098814  |
| train_1/target_q          | -2.638269366876981   |
----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.85
Training epoch 43
Time for epoch 43: 273.78. Rollout time: 97.91, Training time: 175.85
Evaluating epoch 43
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 43                    |
| policy/steps              | 2355046.0             |
| test/episodes             | 1100.0                |
| test/success_rate         | 0.96                  |
| test_0/avg_q              | -9.885371396505874    |
| test_1/avg_q              | -0.7149596882288521   |
| test_1/n_subgoals         | 106.0                 |
| test_1/subgoal_succ_rate  | 0.3113207547169811    |
| train/episodes            | 4400.0                |
| train/success_rate        | 0.79                  |
| train_0/avg_q             | -20.838146416382006   |
| train_0/current_q         | -10.975223994484079   |
| train_0/fw_bonus          | -0.995620085299015    |
| train_0/fw_loss           | 0.03115821606479585   |
| train_0/mu_grads          | -0.09265088047832251  |
| train_0/mu_grads_std      | 0.61526640355587      |
| train_0/mu_loss           | 10.722215202693473    |
| train_0/next_q            | -10.587378702627252   |
| train_0/q_grads           | -0.019845501193776725 |
| train_0/q_grads_std       | 0.3974441416561604    |
| train_0/q_loss            | 1.0765746440939803    |
| train_0/reward            | -0.8642675719824183   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0007080078125       |
| train_0/target_q          | -11.040155478776736   |
| train_1/avg_q             | -10.856805868118585   |
| train_1/current_q         | -2.5360782906324992   |
| train_1/fw_bonus          | -0.9584266513586044   |
| train_1/fw_loss           | 0.3347525462508202    |
| train_1/mu_grads          | -0.06409306842833758  |
| train_1/mu_grads_std      | 0.3711931236088276    |
| train_1/mu_loss           | 0.4756469699620557    |
| train_1/n_subgoals        | 1360.0                |
| train_1/next_q            | -0.5069956750766268   |
| train_1/q_grads           | -0.08470989782363177  |
| train_1/q_grads_std       | 0.5266767874360084    |
| train_1/q_loss            | 2.5215037508143494    |
| train_1/reward            | -2.27590426950992     |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0                   |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.11544117647058824   |
| train_1/target_q          | -2.684218319581259    |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.88
Training epoch 44
Time for epoch 44: 271.53. Rollout time: 93.95, Training time: 177.55
Evaluating epoch 44
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
----------------------------------------------------
| epoch                     | 44                   |
| policy/steps              | 2390495.0            |
| test/episodes             | 1125.0               |
| test/success_rate         | 0.92                 |
| test_0/avg_q              | -16.30440358163643   |
| test_1/avg_q              | -1.6206194170229131  |
| test_1/n_subgoals         | 138.0                |
| test_1/subgoal_succ_rate  | 0.2753623188405797   |
| train/episodes            | 4500.0               |
| train/success_rate        | 0.81                 |
| train_0/avg_q             | -20.111360030766377  |
| train_0/current_q         | -10.669513018331738  |
| train_0/fw_bonus          | -0.9957406014204025  |
| train_0/fw_loss           | 0.030327383801341055 |
| train_0/mu_grads          | -0.09475101009011269 |
| train_0/mu_grads_std      | 0.6198281675577164   |
| train_0/mu_loss           | 10.419340170850052   |
| train_0/next_q            | -10.271763374379852  |
| train_0/q_grads           | -0.02037437870167196 |
| train_0/q_grads_std       | 0.4018544368445873   |
| train_0/q_loss            | 0.9729718578260664   |
| train_0/reward            | -0.8638948680320027  |
| train_0/reward_-0.0_frac  | 0.0                  |
| train_0/reward_-1.0_frac  | 0.000830078125       |
| train_0/target_q          | -10.744566308479369  |
| train_1/avg_q             | -10.684546892529127  |
| train_1/current_q         | -2.499642332044364   |
| train_1/fw_bonus          | -0.9579112648963928  |
| train_1/fw_loss           | 0.338685168325901    |
| train_1/mu_grads          | -0.06584724169224501 |
| train_1/mu_grads_std      | 0.3743045412003994   |
| train_1/mu_loss           | 0.4435753889999708   |
| train_1/n_subgoals        | 1289.0               |
| train_1/next_q            | -0.47686299830502155 |
| train_1/q_grads           | -0.08627333529293538 |
| train_1/q_grads_std       | 0.5328370198607445   |
| train_1/q_loss            | 2.517804073588607    |
| train_1/reward            | -2.263891881134259   |
| train_1/reward_-0.0_frac  | 0.0                  |
| train_1/reward_-1.0_frac  | 0.0                  |
| train_1/reward_-27.0_frac | 0.0                  |
| train_1/subgoal_succ_rate | 0.10861132660977502  |
| train_1/target_q          | -2.644568624015048   |
----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.9299999999999999
Training epoch 45
Time for epoch 45: 287.41. Rollout time: 100.66, Training time: 186.72
Evaluating epoch 45
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 45                    |
| policy/steps              | 2427824.0             |
| test/episodes             | 1150.0                |
| test/success_rate         | 0.96                  |
| test_0/avg_q              | -13.471803757418845   |
| test_1/avg_q              | -1.3755973067507117   |
| test_1/n_subgoals         | 129.0                 |
| test_1/subgoal_succ_rate  | 0.3953488372093023    |
| train/episodes            | 4600.0                |
| train/success_rate        | 0.82                  |
| train_0/avg_q             | -20.571570525872204   |
| train_0/current_q         | -10.795559013992838   |
| train_0/fw_bonus          | -0.995727650821209    |
| train_0/fw_loss           | 0.03041670904494822   |
| train_0/mu_grads          | -0.09425114318728448  |
| train_0/mu_grads_std      | 0.6242389932274819    |
| train_0/mu_loss           | 10.544164682326052    |
| train_0/next_q            | -10.40010926381718    |
| train_0/q_grads           | -0.020744645688682794 |
| train_0/q_grads_std       | 0.4060912184417248    |
| train_0/q_loss            | 0.9238971031489728    |
| train_0/reward            | -0.8639648711152403   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.00078125            |
| train_0/target_q          | -10.872358180759978   |
| train_1/avg_q             | -10.981248495020852   |
| train_1/current_q         | -2.487087275722896    |
| train_1/fw_bonus          | -0.9582378044724464   |
| train_1/fw_loss           | 0.3361934654414654    |
| train_1/mu_grads          | -0.06719063874334097  |
| train_1/mu_grads_std      | 0.3779414974153042    |
| train_1/mu_loss           | 0.44123839059952896   |
| train_1/n_subgoals        | 1376.0                |
| train_1/next_q            | -0.48792621333795194  |
| train_1/q_grads           | -0.08745579496026039  |
| train_1/q_grads_std       | 0.5377618834376335    |
| train_1/q_loss            | 2.9240093999028467    |
| train_1/reward            | -2.2588831847740947   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0                   |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.10319767441860465   |
| train_1/target_q          | -2.6480572582649864   |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.9299999999999999
Training epoch 46
Time for epoch 46: 294.45. Rollout time: 109.02, Training time: 185.40
Evaluating epoch 46
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 46                    |
| policy/steps              | 2470706.0             |
| test/episodes             | 1175.0                |
| test/success_rate         | 0.84                  |
| test_0/avg_q              | -18.488447859303477   |
| test_1/avg_q              | -2.100847794170044    |
| test_1/n_subgoals         | 184.0                 |
| test_1/subgoal_succ_rate  | 0.20108695652173914   |
| train/episodes            | 4700.0                |
| train/success_rate        | 0.72                  |
| train_0/avg_q             | -20.67771891121003    |
| train_0/current_q         | -10.81037403066712    |
| train_0/fw_bonus          | -0.9958287194371224   |
| train_0/fw_loss           | 0.029719940666109324  |
| train_0/mu_grads          | -0.09636553097516298  |
| train_0/mu_grads_std      | 0.6280593410134315    |
| train_0/mu_loss           | 10.541151997313094    |
| train_0/next_q            | -10.408297086661335   |
| train_0/q_grads           | -0.021493340935558082 |
| train_0/q_grads_std       | 0.41024725064635276   |
| train_0/q_loss            | 0.9363994409478751    |
| train_0/reward            | -0.8649239756621683   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.001171875           |
| train_0/target_q          | -10.88294227073047    |
| train_1/avg_q             | -11.257246372848181   |
| train_1/current_q         | -2.5155551297722676   |
| train_1/fw_bonus          | -0.9574870496988297   |
| train_1/fw_loss           | 0.3419221565127373    |
| train_1/mu_grads          | -0.068859594874084    |
| train_1/mu_grads_std      | 0.3820596314966679    |
| train_1/mu_loss           | 0.47871623730534535   |
| train_1/n_subgoals        | 1514.0                |
| train_1/next_q            | -0.5367039598062282   |
| train_1/q_grads           | -0.08899412248283625  |
| train_1/q_grads_std       | 0.5433467850089073    |
| train_1/q_loss            | 2.8819500369428024    |
| train_1/reward            | -2.2641520393510293   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0                   |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.10634081902245707   |
| train_1/target_q          | -2.690611069554058    |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.9199999999999999
Training epoch 47
Time for epoch 47: 296.95. Rollout time: 110.80, Training time: 186.12
Evaluating epoch 47
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 47                    |
| policy/steps              | 2513096.0             |
| test/episodes             | 1200.0                |
| test/success_rate         | 0.88                  |
| test_0/avg_q              | -10.213988464353859   |
| test_1/avg_q              | -2.937481116204546    |
| test_1/n_subgoals         | 732.0                 |
| test_1/subgoal_succ_rate  | 0.8565573770491803    |
| train/episodes            | 4800.0                |
| train/success_rate        | 0.76                  |
| train_0/avg_q             | -20.397111853902825   |
| train_0/current_q         | -10.794057362582581   |
| train_0/fw_bonus          | -0.9958401501178742   |
| train_0/fw_loss           | 0.02964110393077135   |
| train_0/mu_grads          | -0.09732933100312949  |
| train_0/mu_grads_std      | 0.6314991697669029    |
| train_0/mu_loss           | 10.544637978243909    |
| train_0/next_q            | -10.390653400047139   |
| train_0/q_grads           | -0.021468400582671167 |
| train_0/q_grads_std       | 0.414552703499794     |
| train_0/q_loss            | 0.9098975632076194    |
| train_0/reward            | -0.8655127719321172   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.001416015625        |
| train_0/target_q          | -10.86708545924903    |
| train_1/avg_q             | -11.124261811625285   |
| train_1/current_q         | -2.471976893261214    |
| train_1/fw_bonus          | -0.9576821535825729   |
| train_1/fw_loss           | 0.3404335580766201    |
| train_1/mu_grads          | -0.07053322847932578  |
| train_1/mu_grads_std      | 0.3860515668988228    |
| train_1/mu_loss           | 0.4457221434679277    |
| train_1/n_subgoals        | 1523.0                |
| train_1/next_q            | -0.48656165701209586  |
| train_1/q_grads           | -0.09141032360494136  |
| train_1/q_grads_std       | 0.5497442215681076    |
| train_1/q_loss            | 2.8840797940498915    |
| train_1/reward            | -2.250889043213101    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0                   |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.10439921208141825   |
| train_1/target_q          | -2.6448979589441253   |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.8999999999999999
Training epoch 48
Time for epoch 48: 268.82. Rollout time: 84.79, Training time: 184.00
Evaluating epoch 48
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 48                    |
| policy/steps              | 2544216.0             |
| test/episodes             | 1225.0                |
| test/success_rate         | 1.0                   |
| test_0/avg_q              | -11.104852214076105   |
| test_1/avg_q              | -0.3233660033536878   |
| test_1/n_subgoals         | 89.0                  |
| test_1/subgoal_succ_rate  | 0.39325842696629215   |
| train/episodes            | 4900.0                |
| train/success_rate        | 0.84                  |
| train_0/avg_q             | -20.196844803530624   |
| train_0/current_q         | -11.029344247437226   |
| train_0/fw_bonus          | -0.9958376705646514   |
| train_0/fw_loss           | 0.029658137634396554  |
| train_0/mu_grads          | -0.09718999061733484  |
| train_0/mu_grads_std      | 0.6355931133031845    |
| train_0/mu_loss           | 10.787158444251881    |
| train_0/next_q            | -10.638395241329897   |
| train_0/q_grads           | -0.020804658718407153 |
| train_0/q_grads_std       | 0.41862956881523133   |
| train_0/q_loss            | 0.9540496867258778    |
| train_0/reward            | -0.8665558555796451   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0009521484375       |
| train_0/target_q          | -11.113191761962756   |
| train_1/avg_q             | -10.662162073247467   |
| train_1/current_q         | -2.5359231206536217   |
| train_1/fw_bonus          | -0.9584547594189644   |
| train_1/fw_loss           | 0.33453803732991216   |
| train_1/mu_grads          | -0.07143620178103446  |
| train_1/mu_grads_std      | 0.39097175374627113   |
| train_1/mu_loss           | 0.4593931134452359    |
| train_1/n_subgoals        | 1167.0                |
| train_1/next_q            | -0.508388340490784    |
| train_1/q_grads           | -0.09422364663332701  |
| train_1/q_grads_std       | 0.5567486315965653    |
| train_1/q_loss            | 2.8581101420930195    |
| train_1/reward            | -2.305171548464932    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0                   |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.10539845758354756   |
| train_1/target_q          | -2.7108015364775      |
-----------------------------------------------------
New best value for test/success_rate: 1.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.9199999999999999
Training epoch 49
Time for epoch 49: 285.55. Rollout time: 105.45, Training time: 180.07
Evaluating epoch 49
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 49                    |
| policy/steps              | 2583630.0             |
| test/episodes             | 1250.0                |
| test/success_rate         | 0.96                  |
| test_0/avg_q              | -9.162740612576895    |
| test_1/avg_q              | -1.2676653061218648   |
| test_1/n_subgoals         | 115.0                 |
| test_1/subgoal_succ_rate  | 0.28695652173913044   |
| train/episodes            | 5000.0                |
| train/success_rate        | 0.79                  |
| train_0/avg_q             | -20.333757241493746   |
| train_0/current_q         | -10.7332181682973     |
| train_0/fw_bonus          | -0.995899786055088    |
| train_0/fw_loss           | 0.029230058658868074  |
| train_0/mu_grads          | -0.09811108130961657  |
| train_0/mu_grads_std      | 0.6394039005041122    |
| train_0/mu_loss           | 10.481782082338086    |
| train_0/next_q            | -10.345191259444967   |
| train_0/q_grads           | -0.021215926529839634 |
| train_0/q_grads_std       | 0.4227103516459465    |
| train_0/q_loss            | 0.9169392403444119    |
| train_0/reward            | -0.8633487223425618   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.00126953125         |
| train_0/target_q          | -10.828058004328284   |
| train_1/avg_q             | -10.597191906818876   |
| train_1/current_q         | -2.553744294827829    |
| train_1/fw_bonus          | -0.9578458651900291   |
| train_1/fw_loss           | 0.3391841843724251    |
| train_1/mu_grads          | -0.07291428185999393  |
| train_1/mu_grads_std      | 0.395454790443182     |
| train_1/mu_loss           | 0.47056765716726856   |
| train_1/n_subgoals        | 1462.0                |
| train_1/next_q            | -0.5174939198052158   |
| train_1/q_grads           | -0.09725256022065878  |
| train_1/q_grads_std       | 0.5631147190928459    |
| train_1/q_loss            | 2.858679152304608     |
| train_1/reward            | -2.317818404853824    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0                   |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.11491108071135431   |
| train_1/target_q          | -2.7248005031653766   |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.9199999999999999
Training epoch 50
Time for epoch 50: 280.36. Rollout time: 93.23, Training time: 187.11
Evaluating epoch 50
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 50                    |
| policy/steps              | 2619857.0             |
| test/episodes             | 1275.0                |
| test/success_rate         | 0.88                  |
| test_0/avg_q              | -15.006072632585761   |
| test_1/avg_q              | -2.1089750753535625   |
| test_1/n_subgoals         | 159.0                 |
| test_1/subgoal_succ_rate  | 0.1761006289308176    |
| train/episodes            | 5100.0                |
| train/success_rate        | 0.82                  |
| train_0/avg_q             | -20.086661256261173   |
| train_0/current_q         | -10.90635320518952    |
| train_0/fw_bonus          | -0.995935583114624    |
| train_0/fw_loss           | 0.028983248956501485  |
| train_0/mu_grads          | -0.09736371878534555  |
| train_0/mu_grads_std      | 0.6431697070598602    |
| train_0/mu_loss           | 10.65268180753001     |
| train_0/next_q            | -10.518990753924134   |
| train_0/q_grads           | -0.021713278954848647 |
| train_0/q_grads_std       | 0.4269081182777882    |
| train_0/q_loss            | 0.9447448798635207    |
| train_0/reward            | -0.8633576723870646   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.001708984375        |
| train_0/target_q          | -10.981066380138197   |
| train_1/avg_q             | -11.0152369995738     |
| train_1/current_q         | -2.5961516781724665   |
| train_1/fw_bonus          | -0.958768892288208    |
| train_1/fw_loss           | 0.3321410357952118    |
| train_1/mu_grads          | -0.07452273964881898  |
| train_1/mu_grads_std      | 0.3994540758430958    |
| train_1/mu_loss           | 0.5059235780054092    |
| train_1/n_subgoals        | 1287.0                |
| train_1/next_q            | -0.5607062145234979   |
| train_1/q_grads           | -0.09957397729158401  |
| train_1/q_grads_std       | 0.5677200153470039    |
| train_1/q_loss            | 2.9365855650960184    |
| train_1/reward            | -2.3192210283355963   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0                   |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.11344211344211344   |
| train_1/target_q          | -2.765964124431626    |
-----------------------------------------------------
Saving periodic policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101/policy_50.pkl ...
Mean of test/success_rate of last 4 epochs: 0.9299999999999999
Training epoch 51
Time for epoch 51: 276.66. Rollout time: 95.80, Training time: 180.83
Evaluating epoch 51
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 51                    |
| policy/steps              | 2656821.0             |
| test/episodes             | 1300.0                |
| test/success_rate         | 0.92                  |
| test_0/avg_q              | -17.270196824491233   |
| test_1/avg_q              | -1.1029446171762183   |
| test_1/n_subgoals         | 131.0                 |
| test_1/subgoal_succ_rate  | 0.21374045801526717   |
| train/episodes            | 5200.0                |
| train/success_rate        | 0.79                  |
| train_0/avg_q             | -20.314966560649765   |
| train_0/current_q         | -10.858745713566089   |
| train_0/fw_bonus          | -0.995964626967907    |
| train_0/fw_loss           | 0.028782929852604867  |
| train_0/mu_grads          | -0.09865249954164028  |
| train_0/mu_grads_std      | 0.6469818085432053    |
| train_0/mu_loss           | 10.613712509674922    |
| train_0/next_q            | -10.47153052117589    |
| train_0/q_grads           | -0.021959803067147732 |
| train_0/q_grads_std       | 0.4305835247039795    |
| train_0/q_loss            | 0.9144139457806439    |
| train_0/reward            | -0.863017035980738    |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0015625             |
| train_0/target_q          | -10.937938535185559   |
| train_1/avg_q             | -10.777910563553938   |
| train_1/current_q         | -2.530381541381675    |
| train_1/fw_bonus          | -0.9590503692626953   |
| train_1/fw_loss           | 0.32999307960271834   |
| train_1/mu_grads          | -0.07631584238260984  |
| train_1/mu_grads_std      | 0.4029986418783665    |
| train_1/mu_loss           | 0.5221029476275156    |
| train_1/n_subgoals        | 1357.0                |
| train_1/next_q            | -0.586225098650106    |
| train_1/q_grads           | -0.10188895761966706  |
| train_1/q_grads_std       | 0.5727326035499573    |
| train_1/q_loss            | 3.0294720309387775    |
| train_1/reward            | -2.240318013560318    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0                   |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.11864406779661017   |
| train_1/target_q          | -2.706433165985703    |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.94
Training epoch 52
Time for epoch 52: 293.26. Rollout time: 101.78, Training time: 191.44
Evaluating epoch 52
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
----------------------------------------------------
| epoch                     | 52                   |
| policy/steps              | 2694990.0            |
| test/episodes             | 1325.0               |
| test/success_rate         | 0.92                 |
| test_0/avg_q              | -12.095370893244993  |
| test_1/avg_q              | -1.5634033463699415  |
| test_1/n_subgoals         | 152.0                |
| test_1/subgoal_succ_rate  | 0.2565789473684211   |
| train/episodes            | 5300.0               |
| train/success_rate        | 0.77                 |
| train_0/avg_q             | -21.2071460279991    |
| train_0/current_q         | -11.05317721961871   |
| train_0/fw_bonus          | -0.9960257768630981  |
| train_0/fw_loss           | 0.02836135681718588  |
| train_0/mu_grads          | -0.09873519781976939 |
| train_0/mu_grads_std      | 0.6507083445787429   |
| train_0/mu_loss           | 10.79528481929467    |
| train_0/next_q            | -10.675678055998622  |
| train_0/q_grads           | -0.02234764429740608 |
| train_0/q_grads_std       | 0.4345783174037933   |
| train_0/q_loss            | 0.9727297588636208   |
| train_0/reward            | -0.8591138664007303  |
| train_0/reward_-0.0_frac  | 0.0                  |
| train_0/reward_-1.0_frac  | 0.0015869140625      |
| train_0/target_q          | -11.126349399605791  |
| train_1/avg_q             | -11.828886520642016  |
| train_1/current_q         | -2.594935183608455   |
| train_1/fw_bonus          | -0.957510131597519   |
| train_1/fw_loss           | 0.3417462266981602   |
| train_1/mu_grads          | -0.07863772884011269 |
| train_1/mu_grads_std      | 0.40755384787917137  |
| train_1/mu_loss           | 0.545760702653292    |
| train_1/n_subgoals        | 1377.0               |
| train_1/next_q            | -0.5808397090755263  |
| train_1/q_grads           | -0.10381757263094187 |
| train_1/q_grads_std       | 0.5766364634037018   |
| train_1/q_loss            | 2.9439788190603777   |
| train_1/reward            | -2.2947913490097562  |
| train_1/reward_-0.0_frac  | 0.0                  |
| train_1/reward_-1.0_frac  | 0.0                  |
| train_1/reward_-27.0_frac | 0.0                  |
| train_1/subgoal_succ_rate | 0.09513435003631082  |
| train_1/target_q          | -2.757044633423683   |
----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.9199999999999999
Training epoch 53
Time for epoch 53: 281.26. Rollout time: 101.46, Training time: 179.77
Evaluating epoch 53
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
----------------------------------------------------
| epoch                     | 53                   |
| policy/steps              | 2734648.0            |
| test/episodes             | 1350.0               |
| test/success_rate         | 0.88                 |
| test_0/avg_q              | -14.59864589627852   |
| test_1/avg_q              | -2.6184122479605754  |
| test_1/n_subgoals         | 412.0                |
| test_1/subgoal_succ_rate  | 0.7233009708737864   |
| train/episodes            | 5400.0               |
| train/success_rate        | 0.76                 |
| train_0/avg_q             | -20.97181871939522   |
| train_0/current_q         | -10.778245718413508  |
| train_0/fw_bonus          | -0.9960298091173172  |
| train_0/fw_loss           | 0.02833346384577453  |
| train_0/mu_grads          | -0.09873337130993605 |
| train_0/mu_grads_std      | 0.6541872531175613   |
| train_0/mu_loss           | 10.536445572888761   |
| train_0/next_q            | -10.39151731688536   |
| train_0/q_grads           | -0.0228052640799433  |
| train_0/q_grads_std       | 0.4379947856068611   |
| train_0/q_loss            | 0.9009401972749057   |
| train_0/reward            | -0.8617882176476996  |
| train_0/reward_-0.0_frac  | 0.0                  |
| train_0/reward_-1.0_frac  | 0.0013916015625      |
| train_0/target_q          | -10.848807516881049  |
| train_1/avg_q             | -11.182901483640146  |
| train_1/current_q         | -2.567273230286546   |
| train_1/fw_bonus          | -0.9571449533104897  |
| train_1/fw_loss           | 0.3445327259600163   |
| train_1/mu_grads          | -0.08024898394942284 |
| train_1/mu_grads_std      | 0.4113119341433048   |
| train_1/mu_loss           | 0.528697110462369    |
| train_1/n_subgoals        | 1420.0               |
| train_1/next_q            | -0.5766892923489071  |
| train_1/q_grads           | -0.10524098724126815 |
| train_1/q_grads_std       | 0.5810632318258285   |
| train_1/q_loss            | 2.8990506129145004   |
| train_1/reward            | -2.274155622852413   |
| train_1/reward_-0.0_frac  | 0.0                  |
| train_1/reward_-1.0_frac  | 0.0                  |
| train_1/reward_-27.0_frac | 0.0                  |
| train_1/subgoal_succ_rate | 0.09929577464788733  |
| train_1/target_q          | -2.73142592529256    |
----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.9
Training epoch 54
Time for epoch 54: 291.68. Rollout time: 102.51, Training time: 189.14
Evaluating epoch 54
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
----------------------------------------------------
| epoch                     | 54                   |
| policy/steps              | 2772404.0            |
| test/episodes             | 1375.0               |
| test/success_rate         | 0.96                 |
| test_0/avg_q              | -14.284287173553539  |
| test_1/avg_q              | -2.680201179497146   |
| test_1/n_subgoals         | 109.0                |
| test_1/subgoal_succ_rate  | 0.3302752293577982   |
| train/episodes            | 5500.0               |
| train/success_rate        | 0.8                  |
| train_0/avg_q             | -20.494077133257548  |
| train_0/current_q         | -11.127499494252493  |
| train_0/fw_bonus          | -0.9960198536515236  |
| train_0/fw_loss           | 0.02840223181992769  |
| train_0/mu_grads          | -0.09882837813347578 |
| train_0/mu_grads_std      | 0.6577776297926903   |
| train_0/mu_loss           | 10.887710412571241   |
| train_0/next_q            | -10.751764326081101  |
| train_0/q_grads           | -0.02298627463169396 |
| train_0/q_grads_std       | 0.44146115332841873  |
| train_0/q_loss            | 0.9588508521822316   |
| train_0/reward            | -0.8618406556925038  |
| train_0/reward_-0.0_frac  | 0.0                  |
| train_0/reward_-1.0_frac  | 0.0012451171875      |
| train_0/target_q          | -11.220768123077576  |
| train_1/avg_q             | -10.779340422242095  |
| train_1/current_q         | -2.5341764972667846  |
| train_1/fw_bonus          | -0.9574385076761246  |
| train_1/fw_loss           | 0.3422927476465702   |
| train_1/mu_grads          | -0.08150206468999385 |
| train_1/mu_grads_std      | 0.4150025315582752   |
| train_1/mu_loss           | 0.4995690069787477   |
| train_1/n_subgoals        | 1409.0               |
| train_1/next_q            | -0.5300069631683785  |
| train_1/q_grads           | -0.10693879462778569 |
| train_1/q_grads_std       | 0.5855667501688003   |
| train_1/q_loss            | 2.740361827484713    |
| train_1/reward            | -2.2918071357227747  |
| train_1/reward_-0.0_frac  | 0.0                  |
| train_1/reward_-1.0_frac  | 0.0                  |
| train_1/reward_-27.0_frac | 0.0                  |
| train_1/subgoal_succ_rate | 0.11071682044002838  |
| train_1/target_q          | -2.7118655832555567  |
----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.92
Training epoch 55
Time for epoch 55: 282.18. Rollout time: 95.14, Training time: 187.01
Evaluating epoch 55
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
----------------------------------------------------
| epoch                     | 55                   |
| policy/steps              | 2807688.0            |
| test/episodes             | 1400.0               |
| test/success_rate         | 1.0                  |
| test_0/avg_q              | -10.315607737120038  |
| test_1/avg_q              | -0.9508353882054235  |
| test_1/n_subgoals         | 86.0                 |
| test_1/subgoal_succ_rate  | 0.43023255813953487  |
| train/episodes            | 5600.0               |
| train/success_rate        | 0.81                 |
| train_0/avg_q             | -21.2829839472521    |
| train_0/current_q         | -10.913711301124298  |
| train_0/fw_bonus          | -0.9960183784365654  |
| train_0/fw_loss           | 0.028412341699004175 |
| train_0/mu_grads          | -0.09873541332781315 |
| train_0/mu_grads_std      | 0.6608440905809403   |
| train_0/mu_loss           | 10.660238929943855   |
| train_0/next_q            | -10.525257625799355  |
| train_0/q_grads           | -0.02305228589102626 |
| train_0/q_grads_std       | 0.4446563184261322   |
| train_0/q_loss            | 0.9118002333188068   |
| train_0/reward            | -0.8619867734993022  |
| train_0/reward_-0.0_frac  | 0.0                  |
| train_0/reward_-1.0_frac  | 0.0011962890625      |
| train_0/target_q          | -10.998473149440695  |
| train_1/avg_q             | -11.415298606663816  |
| train_1/current_q         | -2.5476713705665337  |
| train_1/fw_bonus          | -0.9576284766197205  |
| train_1/fw_loss           | 0.34084310159087183  |
| train_1/mu_grads          | -0.0833065751940012  |
| train_1/mu_grads_std      | 0.41834994703531264  |
| train_1/mu_loss           | 0.5411669561913437   |
| train_1/n_subgoals        | 1326.0               |
| train_1/next_q            | -0.5924556948490247  |
| train_1/q_grads           | -0.10886884406208992 |
| train_1/q_grads_std       | 0.5899089813232422   |
| train_1/q_loss            | 3.147818176140592    |
| train_1/reward            | -2.2463712308202957  |
| train_1/reward_-0.0_frac  | 0.0                  |
| train_1/reward_-1.0_frac  | 0.0                  |
| train_1/reward_-27.0_frac | 0.0                  |
| train_1/subgoal_succ_rate | 0.08974358974358974  |
| train_1/target_q          | -2.7204660679102792  |
----------------------------------------------------
New best value for test/success_rate: 1.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.94
Training epoch 56
Time for epoch 56: 290.94. Rollout time: 100.16, Training time: 190.76
Evaluating epoch 56
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
----------------------------------------------------
| epoch                     | 56                   |
| policy/steps              | 2846606.0            |
| test/episodes             | 1425.0               |
| test/success_rate         | 0.84                 |
| test_0/avg_q              | -20.87965729039243   |
| test_1/avg_q              | -4.209400382444772   |
| test_1/n_subgoals         | 177.0                |
| test_1/subgoal_succ_rate  | 0.13559322033898305  |
| train/episodes            | 5700.0               |
| train/success_rate        | 0.79                 |
| train_0/avg_q             | -20.70223124993349   |
| train_0/current_q         | -10.660483546925088  |
| train_0/fw_bonus          | -0.9959881722927093  |
| train_0/fw_loss           | 0.028620632085949182 |
| train_0/mu_grads          | -0.0986826304346323  |
| train_0/mu_grads_std      | 0.6642253264784813   |
| train_0/mu_loss           | 10.411633783645634   |
| train_0/next_q            | -10.26429900947349   |
| train_0/q_grads           | -0.02321024634875357 |
| train_0/q_grads_std       | 0.4481152296066284   |
| train_0/q_loss            | 0.8760231050479597   |
| train_0/reward            | -0.8585917689706548  |
| train_0/reward_-0.0_frac  | 0.0                  |
| train_0/reward_-1.0_frac  | 0.0012939453125      |
| train_0/target_q          | -10.746882779744222  |
| train_1/avg_q             | -11.25248170724071   |
| train_1/current_q         | -2.595293615569143   |
| train_1/fw_bonus          | -0.9566907286643982  |
| train_1/fw_loss           | 0.34799875915050504  |
| train_1/mu_grads          | -0.08439405895769596 |
| train_1/mu_grads_std      | 0.42140295580029485  |
| train_1/mu_loss           | 0.5667805118503404   |
| train_1/n_subgoals        | 1359.0               |
| train_1/next_q            | -0.6074670698297658  |
| train_1/q_grads           | -0.11120403259992599 |
| train_1/q_grads_std       | 0.5953305259346962   |
| train_1/q_loss            | 3.086801260994874    |
| train_1/reward            | -2.2836184826541284  |
| train_1/reward_-0.0_frac  | 0.0                  |
| train_1/reward_-1.0_frac  | 0.0                  |
| train_1/reward_-27.0_frac | 0.0                  |
| train_1/subgoal_succ_rate | 0.09197939661515821  |
| train_1/target_q          | -2.771555484127422   |
----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.9199999999999999
Training epoch 57
Time for epoch 57: 290.16. Rollout time: 106.58, Training time: 183.55
Evaluating epoch 57
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 57                    |
| policy/steps              | 2890782.0             |
| test/episodes             | 1450.0                |
| test/success_rate         | 0.72                  |
| test_0/avg_q              | -11.420253007922831   |
| test_1/avg_q              | -2.2692112491090883   |
| test_1/n_subgoals         | 282.0                 |
| test_1/subgoal_succ_rate  | 0.21631205673758866   |
| train/episodes            | 5800.0                |
| train/success_rate        | 0.79                  |
| train_0/avg_q             | -20.323907693340182   |
| train_0/current_q         | -10.94181303231915    |
| train_0/fw_bonus          | -0.9960411176085472   |
| train_0/fw_loss           | 0.02825562171638012   |
| train_0/mu_grads          | -0.09974507540464402  |
| train_0/mu_grads_std      | 0.6674587294459343    |
| train_0/mu_loss           | 10.681381134808396    |
| train_0/next_q            | -10.553893920998235   |
| train_0/q_grads           | -0.022690302738919855 |
| train_0/q_grads_std       | 0.451338279992342     |
| train_0/q_loss            | 0.968616521912417     |
| train_0/reward            | -0.8591543217389699   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0012451171875       |
| train_0/target_q          | -11.00146267250052    |
| train_1/avg_q             | -10.757376457685938   |
| train_1/current_q         | -2.580033878488892    |
| train_1/fw_bonus          | -0.956204055249691    |
| train_1/fw_loss           | 0.35171241164207456   |
| train_1/mu_grads          | -0.0853840958327055   |
| train_1/mu_grads_std      | 0.42379122227430344   |
| train_1/mu_loss           | 0.565160207882764     |
| train_1/n_subgoals        | 1490.0                |
| train_1/next_q            | -0.6143770101469797   |
| train_1/q_grads           | -0.11350587327033282  |
| train_1/q_grads_std       | 0.6001572772860527    |
| train_1/q_loss            | 2.9617759061349274    |
| train_1/reward            | -2.2643989158426847   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0                   |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.10268456375838926   |
| train_1/target_q          | -2.7540431548612374   |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.8799999999999999
Training epoch 58
Time for epoch 58: 291.74. Rollout time: 106.06, Training time: 185.65
Evaluating epoch 58
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
----------------------------------------------------
| epoch                     | 58                   |
| policy/steps              | 2927924.0            |
| test/episodes             | 1475.0               |
| test/success_rate         | 1.0                  |
| test_0/avg_q              | -10.95560624988376   |
| test_1/avg_q              | -1.097962450191748   |
| test_1/n_subgoals         | 82.0                 |
| test_1/subgoal_succ_rate  | 0.36585365853658536  |
| train/episodes            | 5900.0               |
| train/success_rate        | 0.78                 |
| train_0/avg_q             | -19.280849814458982  |
| train_0/current_q         | -10.97202734476279   |
| train_0/fw_bonus          | -0.9960510343313217  |
| train_0/fw_loss           | 0.02818716671317816  |
| train_0/mu_grads          | -0.09965272080153227 |
| train_0/mu_grads_std      | 0.6710648268461228   |
| train_0/mu_loss           | 10.746514024616237   |
| train_0/next_q            | -10.591150345162417  |
| train_0/q_grads           | -0.02484339871443808 |
| train_0/q_grads_std       | 0.4533024147152901   |
| train_0/q_loss            | 1.0889938067714262   |
| train_0/reward            | -0.8623611496521335  |
| train_0/reward_-0.0_frac  | 0.0                  |
| train_0/reward_-1.0_frac  | 0.0013916015625      |
| train_0/target_q          | -11.033502865157356  |
| train_1/avg_q             | -10.985971406884149  |
| train_1/current_q         | -2.6696863217081273  |
| train_1/fw_bonus          | -0.9563339471817016  |
| train_1/fw_loss           | 0.3507212348282337   |
| train_1/mu_grads          | -0.08694556374102831 |
| train_1/mu_grads_std      | 0.4273861192166805   |
| train_1/mu_loss           | 0.6250334787868475   |
| train_1/n_subgoals        | 1393.0               |
| train_1/next_q            | -0.661828508973439   |
| train_1/q_grads           | -0.11531291976571083 |
| train_1/q_grads_std       | 0.6048967242240906   |
| train_1/q_loss            | 3.2971258748135996   |
| train_1/reward            | -2.3069484770621784  |
| train_1/reward_-0.0_frac  | 0.0                  |
| train_1/reward_-1.0_frac  | 0.0                  |
| train_1/reward_-27.0_frac | 0.0                  |
| train_1/subgoal_succ_rate | 0.07753050969131371  |
| train_1/target_q          | -2.836761807317854   |
----------------------------------------------------
New best value for test/success_rate: 1.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.8899999999999999
Training epoch 59
Time for epoch 59: 275.39. Rollout time: 100.76, Training time: 174.60
Evaluating epoch 59
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 59                    |
| policy/steps              | 2965897.0             |
| test/episodes             | 1500.0                |
| test/success_rate         | 0.96                  |
| test_0/avg_q              | -12.728606344564593   |
| test_1/avg_q              | -0.7719870701308875   |
| test_1/n_subgoals         | 107.0                 |
| test_1/subgoal_succ_rate  | 0.27102803738317754   |
| train/episodes            | 6000.0                |
| train/success_rate        | 0.78                  |
| train_0/avg_q             | -20.63930935738294    |
| train_0/current_q         | -10.88629189512885    |
| train_0/fw_bonus          | -0.9961110934615135   |
| train_0/fw_loss           | 0.027773142280057072  |
| train_0/mu_grads          | -0.09957555048167706  |
| train_0/mu_grads_std      | 0.6738305509090423    |
| train_0/mu_loss           | 10.661859493248988    |
| train_0/next_q            | -10.514204400211275   |
| train_0/q_grads           | -0.023025461519137026 |
| train_0/q_grads_std       | 0.4544406428933144    |
| train_0/q_loss            | 0.913792342343607     |
| train_0/reward            | -0.8579816321020189   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0021728515625       |
| train_0/target_q          | -10.967381799633475   |
| train_1/avg_q             | -10.765711087144073   |
| train_1/current_q         | -2.7156017689052225   |
| train_1/fw_bonus          | -0.9559412598609924   |
| train_1/fw_loss           | 0.3537177570164204    |
| train_1/mu_grads          | -0.0884589534252882   |
| train_1/mu_grads_std      | 0.4314209409058094    |
| train_1/mu_loss           | 0.6483558408713842    |
| train_1/n_subgoals        | 1402.0                |
| train_1/next_q            | -0.7048892868004966   |
| train_1/q_grads           | -0.11695787459611892  |
| train_1/q_grads_std       | 0.6091087877750396    |
| train_1/q_loss            | 3.2438875233069226    |
| train_1/reward            | -2.2965231556561774   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0                   |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0920114122681883    |
| train_1/target_q          | -2.8638542670154132   |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.88
Training epoch 60
Time for epoch 60: 277.92. Rollout time: 103.70, Training time: 174.18
Evaluating epoch 60
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 60                    |
| policy/steps              | 3006704.0             |
| test/episodes             | 1525.0                |
| test/success_rate         | 0.8                   |
| test_0/avg_q              | -16.14348969749077    |
| test_1/avg_q              | -3.8033723174204592   |
| test_1/n_subgoals         | 210.0                 |
| test_1/subgoal_succ_rate  | 0.11428571428571428   |
| train/episodes            | 6100.0                |
| train/success_rate        | 0.76                  |
| train_0/avg_q             | -21.4354469641534     |
| train_0/current_q         | -11.007019797442215   |
| train_0/fw_bonus          | -0.996102599799633    |
| train_0/fw_loss           | 0.027831763681024312  |
| train_0/mu_grads          | -0.10067669209092855  |
| train_0/mu_grads_std      | 0.676962035894394     |
| train_0/mu_loss           | 10.773026567078457    |
| train_0/next_q            | -10.629592549028093   |
| train_0/q_grads           | -0.022431948967278004 |
| train_0/q_grads_std       | 0.4570122443139553    |
| train_0/q_loss            | 1.0081315808747555    |
| train_0/reward            | -0.8539145648581326   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0012939453125       |
| train_0/target_q          | -11.081670284884108   |
| train_1/avg_q             | -11.809188298630644   |
| train_1/current_q         | -2.7585504445363154   |
| train_1/fw_bonus          | -0.954840412735939    |
| train_1/fw_loss           | 0.3621179111301899    |
| train_1/mu_grads          | -0.09072024319320918  |
| train_1/mu_grads_std      | 0.43607954159379003   |
| train_1/mu_loss           | 0.693864984264049     |
| train_1/n_subgoals        | 1398.0                |
| train_1/next_q            | -0.7453304375783798   |
| train_1/q_grads           | -0.11894408464431763  |
| train_1/q_grads_std       | 0.6124993801116944    |
| train_1/q_loss            | 3.4551767481474385    |
| train_1/reward            | -2.3108231413418254   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0                   |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.07296137339055794   |
| train_1/target_q          | -2.9089385509005172   |
-----------------------------------------------------
Saving periodic policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101/policy_60.pkl ...
Mean of test/success_rate of last 4 epochs: 0.8699999999999999
Training epoch 61
Time for epoch 61: 271.04. Rollout time: 94.43, Training time: 176.58
Evaluating epoch 61
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 61                    |
| policy/steps              | 3041428.0             |
| test/episodes             | 1550.0                |
| test/success_rate         | 0.96                  |
| test_0/avg_q              | -14.443282897619921   |
| test_1/avg_q              | -2.565916567975465    |
| test_1/n_subgoals         | 106.0                 |
| test_1/subgoal_succ_rate  | 0.20754716981132076   |
| train/episodes            | 6200.0                |
| train/success_rate        | 0.85                  |
| train_0/avg_q             | -20.37281165485177    |
| train_0/current_q         | -10.90246340968992    |
| train_0/fw_bonus          | -0.9962218046188355   |
| train_0/fw_loss           | 0.027009879238903522  |
| train_0/mu_grads          | -0.1011417742818594   |
| train_0/mu_grads_std      | 0.6799598500132561    |
| train_0/mu_loss           | 10.657119122363449    |
| train_0/next_q            | -10.530763280691849   |
| train_0/q_grads           | -0.021673616487532854 |
| train_0/q_grads_std       | 0.46006748974323275   |
| train_0/q_loss            | 0.9032637953750402    |
| train_0/reward            | -0.8520217611247063   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.002001953125        |
| train_0/target_q          | -10.99843025672628    |
| train_1/avg_q             | -10.935526552302187   |
| train_1/current_q         | -2.768009725382425    |
| train_1/fw_bonus          | -0.9556102156639099   |
| train_1/fw_loss           | 0.3562437638640404    |
| train_1/mu_grads          | -0.09131634403020143  |
| train_1/mu_grads_std      | 0.4398380026221275    |
| train_1/mu_loss           | 0.6865838342921877    |
| train_1/n_subgoals        | 1277.0                |
| train_1/next_q            | -0.7421762693807195   |
| train_1/q_grads           | -0.12047835141420364  |
| train_1/q_grads_std       | 0.6156800866127015    |
| train_1/q_loss            | 3.444190019784885     |
| train_1/reward            | -2.3151354647758127   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0                   |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.09005481597494126   |
| train_1/target_q          | -2.9096831072560616   |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.9299999999999999
Training epoch 62
Time for epoch 62: 277.06. Rollout time: 103.09, Training time: 173.93
Evaluating epoch 62
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 62                    |
| policy/steps              | 3081871.0             |
| test/episodes             | 1575.0                |
| test/success_rate         | 0.84                  |
| test_0/avg_q              | -12.154054516837213   |
| test_1/avg_q              | -3.717638445342589    |
| test_1/n_subgoals         | 193.0                 |
| test_1/subgoal_succ_rate  | 0.17616580310880828   |
| train/episodes            | 6300.0                |
| train/success_rate        | 0.81                  |
| train_0/avg_q             | -20.609451152945002   |
| train_0/current_q         | -10.947661189915408   |
| train_0/fw_bonus          | -0.9961401939392089   |
| train_0/fw_loss           | 0.02757251407019794   |
| train_0/mu_grads          | -0.10180813893675804  |
| train_0/mu_grads_std      | 0.6838821187615395    |
| train_0/mu_loss           | 10.694970184097803    |
| train_0/next_q            | -10.565027226021414   |
| train_0/q_grads           | -0.021104843681678177 |
| train_0/q_grads_std       | 0.4633507288992405    |
| train_0/q_loss            | 1.0555920498161087    |
| train_0/reward            | -0.8544549715243193   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.002392578125        |
| train_0/target_q          | -11.024824202463439   |
| train_1/avg_q             | -11.180846790748896   |
| train_1/current_q         | -2.708646601303321    |
| train_1/fw_bonus          | -0.9554055154323577   |
| train_1/fw_loss           | 0.357805834710598     |
| train_1/mu_grads          | -0.09200060367584229  |
| train_1/mu_grads_std      | 0.44345489591360093   |
| train_1/mu_loss           | 0.6656633072408173    |
| train_1/n_subgoals        | 1418.0                |
| train_1/next_q            | -0.7020580816982674   |
| train_1/q_grads           | -0.12237189244478941  |
| train_1/q_grads_std       | 0.6198444366455078    |
| train_1/q_loss            | 3.3721414263539815    |
| train_1/reward            | -2.2778223258817887   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0                   |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0930888575458392    |
| train_1/target_q          | -2.846239661147708    |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.8899999999999999
Training epoch 63
Time for epoch 63: 272.34. Rollout time: 100.03, Training time: 172.27
Evaluating epoch 63
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 63                    |
| policy/steps              | 3119384.0             |
| test/episodes             | 1600.0                |
| test/success_rate         | 0.92                  |
| test_0/avg_q              | -14.646071722751454   |
| test_1/avg_q              | -1.5884791536581857   |
| test_1/n_subgoals         | 140.0                 |
| test_1/subgoal_succ_rate  | 0.2714285714285714    |
| train/episodes            | 6400.0                |
| train/success_rate        | 0.8                   |
| train_0/avg_q             | -21.181971878190545   |
| train_0/current_q         | -10.982666936277928   |
| train_0/fw_bonus          | -0.996185103058815    |
| train_0/fw_loss           | 0.027262913575395942  |
| train_0/mu_grads          | -0.10422141533344983  |
| train_0/mu_grads_std      | 0.6875038206577301    |
| train_0/mu_loss           | 10.753869198616595    |
| train_0/next_q            | -10.60802533104637    |
| train_0/q_grads           | -0.021074443636462093 |
| train_0/q_grads_std       | 0.466150239109993     |
| train_0/q_loss            | 0.9933184045630735    |
| train_0/reward            | -0.857615612706286    |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.002392578125        |
| train_0/target_q          | -11.06111333655806    |
| train_1/avg_q             | -12.050418502076118   |
| train_1/current_q         | -2.7187854693739633   |
| train_1/fw_bonus          | -0.9559519767761231   |
| train_1/fw_loss           | 0.3536359779536724    |
| train_1/mu_grads          | -0.09289950970560312  |
| train_1/mu_grads_std      | 0.44641597643494607   |
| train_1/mu_loss           | 0.6584300393812572    |
| train_1/n_subgoals        | 1341.0                |
| train_1/next_q            | -0.7031786327353563   |
| train_1/q_grads           | -0.12369629349559545  |
| train_1/q_grads_std       | 0.6237069517374039    |
| train_1/q_loss            | 3.3980093690212185    |
| train_1/reward            | -2.2761278865891654   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0                   |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.07606263982102908   |
| train_1/target_q          | -2.846068110268514    |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.88
Training epoch 64
Time for epoch 64: 272.27. Rollout time: 98.64, Training time: 173.61
Evaluating epoch 64
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
----------------------------------------------------
| epoch                     | 64                   |
| policy/steps              | 3157876.0            |
| test/episodes             | 1625.0               |
| test/success_rate         | 0.88                 |
| test_0/avg_q              | -14.704899914641091  |
| test_1/avg_q              | -1.8999435485857974  |
| test_1/n_subgoals         | 163.0                |
| test_1/subgoal_succ_rate  | 0.20245398773006135  |
| train/episodes            | 6500.0               |
| train/success_rate        | 0.8                  |
| train_0/avg_q             | -21.24303243035532   |
| train_0/current_q         | -11.026295126356597  |
| train_0/fw_bonus          | -0.9962083995342255  |
| train_0/fw_loss           | 0.02710234965197742  |
| train_0/mu_grads          | -0.10520731434226036 |
| train_0/mu_grads_std      | 0.6907253623008728   |
| train_0/mu_loss           | 10.761624377573156   |
| train_0/next_q            | -10.642340859874613  |
| train_0/q_grads           | -0.02080616788007319 |
| train_0/q_grads_std       | 0.46926228776574136  |
| train_0/q_loss            | 0.9843040165173088   |
| train_0/reward            | -0.8549664059410134  |
| train_0/reward_-0.0_frac  | 0.0                  |
| train_0/reward_-1.0_frac  | 0.002685546875       |
| train_0/target_q          | -11.107987799214245  |
| train_1/avg_q             | -11.72378955825664   |
| train_1/current_q         | -2.7391754988351424  |
| train_1/fw_bonus          | -0.9558437466621399  |
| train_1/fw_loss           | 0.3544618278741837   |
| train_1/mu_grads          | -0.09485547747462988 |
| train_1/mu_grads_std      | 0.44995363503694535  |
| train_1/mu_loss           | 0.6407908168029337   |
| train_1/n_subgoals        | 1374.0               |
| train_1/next_q            | -0.6924802690591062  |
| train_1/q_grads           | -0.12570917867124082 |
| train_1/q_grads_std       | 0.6281859681010247   |
| train_1/q_loss            | 3.258345070894331    |
| train_1/reward            | -2.3097189945907304  |
| train_1/reward_-0.0_frac  | 0.0                  |
| train_1/reward_-1.0_frac  | 0.0                  |
| train_1/reward_-27.0_frac | 0.0                  |
| train_1/subgoal_succ_rate | 0.10334788937409024  |
| train_1/target_q          | -2.866453002469393   |
----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.8999999999999999
Training epoch 65
Time for epoch 65: 264.57. Rollout time: 98.20, Training time: 166.35
Evaluating epoch 65
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 65                    |
| policy/steps              | 3195289.0             |
| test/episodes             | 1650.0                |
| test/success_rate         | 0.92                  |
| test_0/avg_q              | -14.676762027369262   |
| test_1/avg_q              | -2.7123899519458603   |
| test_1/n_subgoals         | 160.0                 |
| test_1/subgoal_succ_rate  | 0.35625               |
| train/episodes            | 6600.0                |
| train/success_rate        | 0.77                  |
| train_0/avg_q             | -21.404963667861615   |
| train_0/current_q         | -10.937281975214674   |
| train_0/fw_bonus          | -0.9962009951472283   |
| train_0/fw_loss           | 0.027153344312682747  |
| train_0/mu_grads          | -0.10570308938622475  |
| train_0/mu_grads_std      | 0.6941913291811943    |
| train_0/mu_loss           | 10.704252032381088    |
| train_0/next_q            | -10.57084990261483    |
| train_0/q_grads           | -0.021246648393571377 |
| train_0/q_grads_std       | 0.47267057150602343   |
| train_0/q_loss            | 0.9304179857109073    |
| train_0/reward            | -0.8532799864391564   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0020263671875       |
| train_0/target_q          | -11.02138973897244    |
| train_1/avg_q             | -11.901920060110323   |
| train_1/current_q         | -2.765657851643281    |
| train_1/fw_bonus          | -0.9559001594781875   |
| train_1/fw_loss           | 0.354031378030777     |
| train_1/mu_grads          | -0.09643486980348825  |
| train_1/mu_grads_std      | 0.4536326602101326    |
| train_1/mu_loss           | 0.6591471055126523    |
| train_1/n_subgoals        | 1338.0                |
| train_1/next_q            | -0.7127247490162075   |
| train_1/q_grads           | -0.12798576056957245  |
| train_1/q_grads_std       | 0.632158662378788     |
| train_1/q_loss            | 3.2378681097747046    |
| train_1/reward            | -2.310080995181488    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0                   |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.07025411061285501   |
| train_1/target_q          | -2.8853439779748538   |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.89
Training epoch 66
Time for epoch 66: 272.22. Rollout time: 94.28, Training time: 177.91
Evaluating epoch 66
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 66                    |
| policy/steps              | 3230537.0             |
| test/episodes             | 1675.0                |
| test/success_rate         | 0.92                  |
| test_0/avg_q              | -16.190441650401773   |
| test_1/avg_q              | -3.0841886483409886   |
| test_1/n_subgoals         | 136.0                 |
| test_1/subgoal_succ_rate  | 0.27941176470588236   |
| train/episodes            | 6700.0                |
| train/success_rate        | 0.81                  |
| train_0/avg_q             | -20.177144257001693   |
| train_0/current_q         | -10.959884846263819   |
| train_0/fw_bonus          | -0.9961724326014518   |
| train_0/fw_loss           | 0.02735043000429869   |
| train_0/mu_grads          | -0.10661667902022601  |
| train_0/mu_grads_std      | 0.6976999461650848    |
| train_0/mu_loss           | 10.726841350880523    |
| train_0/next_q            | -10.575178080889513   |
| train_0/q_grads           | -0.021264523826539518 |
| train_0/q_grads_std       | 0.4759929522871971    |
| train_0/q_loss            | 0.9215076766814772    |
| train_0/reward            | -0.8569166383709671   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0021728515625       |
| train_0/target_q          | -11.045729345226496   |
| train_1/avg_q             | -11.360734403977705   |
| train_1/current_q         | -2.7301017617764214   |
| train_1/fw_bonus          | -0.9559094354510307   |
| train_1/fw_loss           | 0.3539606176316738    |
| train_1/mu_grads          | -0.09769773930311203  |
| train_1/mu_grads_std      | 0.457059882581234     |
| train_1/mu_loss           | 0.6451145218635878    |
| train_1/n_subgoals        | 1279.0                |
| train_1/next_q            | -0.6907698905018845   |
| train_1/q_grads           | -0.13000257983803748  |
| train_1/q_grads_std       | 0.6359556451439857    |
| train_1/q_loss            | 2.9661079798264813    |
| train_1/reward            | -2.3015322887502405   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0                   |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.09929632525410477   |
| train_1/target_q          | -2.861028027831701    |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.91
Training epoch 67
Time for epoch 67: 269.00. Rollout time: 97.36, Training time: 171.60
Evaluating epoch 67
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 67                    |
| policy/steps              | 3266438.0             |
| test/episodes             | 1700.0                |
| test/success_rate         | 1.0                   |
| test_0/avg_q              | -10.06623463367576    |
| test_1/avg_q              | -0.5493259831347386   |
| test_1/n_subgoals         | 86.0                  |
| test_1/subgoal_succ_rate  | 0.4069767441860465    |
| train/episodes            | 6800.0                |
| train/success_rate        | 0.81                  |
| train_0/avg_q             | -20.268682384913856   |
| train_0/current_q         | -10.799432974427491   |
| train_0/fw_bonus          | -0.9961695417761802   |
| train_0/fw_loss           | 0.027370214043185116  |
| train_0/mu_grads          | -0.10716041326522827  |
| train_0/mu_grads_std      | 0.7007802188396454    |
| train_0/mu_loss           | 10.563738059670126    |
| train_0/next_q            | -10.415923693266683   |
| train_0/q_grads           | -0.020975993340834977 |
| train_0/q_grads_std       | 0.47903793677687645   |
| train_0/q_loss            | 1.000483097047581     |
| train_0/reward            | -0.8581976590525301   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.00224609375         |
| train_0/target_q          | -10.896220594536757   |
| train_1/avg_q             | -10.846875766836302   |
| train_1/current_q         | -2.6905036927862755   |
| train_1/fw_bonus          | -0.9536105021834373   |
| train_1/fw_loss           | 0.3715030297636986    |
| train_1/mu_grads          | -0.0991045942530036   |
| train_1/mu_grads_std      | 0.4608212739229202    |
| train_1/mu_loss           | 0.6152009492787143    |
| train_1/n_subgoals        | 1357.0                |
| train_1/next_q            | -0.6781230258670945   |
| train_1/q_grads           | -0.1320767853409052   |
| train_1/q_grads_std       | 0.6404528707265854    |
| train_1/q_loss            | 2.9829300323944636    |
| train_1/reward            | -2.2713860227613623   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0                   |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.1105379513633014    |
| train_1/target_q          | -2.8247679176139586   |
-----------------------------------------------------
New best value for test/success_rate: 1.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.93
Training epoch 68
Time for epoch 68: 277.91. Rollout time: 100.07, Training time: 177.81
Evaluating epoch 68
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
----------------------------------------------------
| epoch                     | 68                   |
| policy/steps              | 3305003.0            |
| test/episodes             | 1725.0               |
| test/success_rate         | 0.88                 |
| test_0/avg_q              | -16.867605527349923  |
| test_1/avg_q              | -2.815093418264432   |
| test_1/n_subgoals         | 160.0                |
| test_1/subgoal_succ_rate  | 0.24375              |
| train/episodes            | 6900.0               |
| train/success_rate        | 0.8                  |
| train_0/avg_q             | -21.01899363509661   |
| train_0/current_q         | -11.099797277371374  |
| train_0/fw_bonus          | -0.9961261227726936  |
| train_0/fw_loss           | 0.027669508010149002 |
| train_0/mu_grads          | -0.10725481193512679 |
| train_0/mu_grads_std      | 0.7037050440907479   |
| train_0/mu_loss           | 10.853192545598436   |
| train_0/next_q            | -10.717428127664004  |
| train_0/q_grads           | -0.02093026153743267 |
| train_0/q_grads_std       | 0.48262240961194036  |
| train_0/q_loss            | 0.9239209010311278   |
| train_0/reward            | -0.8565022358350689  |
| train_0/reward_-0.0_frac  | 0.0                  |
| train_0/reward_-1.0_frac  | 0.00185546875        |
| train_0/target_q          | -11.180329632473235  |
| train_1/avg_q             | -11.283150517857313  |
| train_1/current_q         | -2.702768674885579   |
| train_1/fw_bonus          | -0.9540501445531845  |
| train_1/fw_loss           | 0.36814822554588317  |
| train_1/mu_grads          | -0.10098234731703996 |
| train_1/mu_grads_std      | 0.4639912433922291   |
| train_1/mu_loss           | 0.6145534176741717   |
| train_1/n_subgoals        | 1378.0               |
| train_1/next_q            | -0.6736672259189043  |
| train_1/q_grads           | -0.13432173244655132 |
| train_1/q_grads_std       | 0.6438117325305939   |
| train_1/q_loss            | 2.7659686725583894   |
| train_1/reward            | -2.2887444700805646  |
| train_1/reward_-0.0_frac  | 0.0                  |
| train_1/reward_-1.0_frac  | 0.0                  |
| train_1/reward_-27.0_frac | 0.0                  |
| train_1/subgoal_succ_rate | 0.09724238026124818  |
| train_1/target_q          | -2.8296943791811104  |
----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.9299999999999999
Training epoch 69
Time for epoch 69: 267.90. Rollout time: 92.98, Training time: 174.89
Evaluating epoch 69
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 69                    |
| policy/steps              | 3342492.0             |
| test/episodes             | 1750.0                |
| test/success_rate         | 0.84                  |
| test_0/avg_q              | -17.392373179384915   |
| test_1/avg_q              | -3.953634406645414    |
| test_1/n_subgoals         | 287.0                 |
| test_1/subgoal_succ_rate  | 0.4843205574912892    |
| train/episodes            | 7000.0                |
| train/success_rate        | 0.79                  |
| train_0/avg_q             | -20.87519511974891    |
| train_0/current_q         | -10.936563499403945   |
| train_0/fw_bonus          | -0.9961709454655647   |
| train_0/fw_loss           | 0.027360573783516882  |
| train_0/mu_grads          | -0.10720103532075882  |
| train_0/mu_grads_std      | 0.7065591469407082    |
| train_0/mu_loss           | 10.685755363265395    |
| train_0/next_q            | -10.551929218112345   |
| train_0/q_grads           | -0.021186336036771537 |
| train_0/q_grads_std       | 0.4862224437296391    |
| train_0/q_loss            | 0.9070692532683953    |
| train_0/reward            | -0.8554880480493011   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0017333984375       |
| train_0/target_q          | -11.016325005416405   |
| train_1/avg_q             | -11.655711902508642   |
| train_1/current_q         | -2.7171779113937378   |
| train_1/fw_bonus          | -0.9544686481356621   |
| train_1/fw_loss           | 0.3649547971785069    |
| train_1/mu_grads          | -0.10173842627555132  |
| train_1/mu_grads_std      | 0.46668618097901343   |
| train_1/mu_loss           | 0.6858288557841931    |
| train_1/n_subgoals        | 1302.0                |
| train_1/next_q            | -0.7327908822170184   |
| train_1/q_grads           | -0.13677624501287938  |
| train_1/q_grads_std       | 0.6476971074938774    |
| train_1/q_loss            | 3.0093489851730655    |
| train_1/reward            | -2.251092746800714    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0                   |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.08986175115207373   |
| train_1/target_q          | -2.8405628743907236   |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.9099999999999999
Training epoch 70
Time for epoch 70: 275.82. Rollout time: 98.64, Training time: 177.15
Evaluating epoch 70
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 70                    |
| policy/steps              | 3380085.0             |
| test/episodes             | 1775.0                |
| test/success_rate         | 0.92                  |
| test_0/avg_q              | -14.524608022383402   |
| test_1/avg_q              | -2.859099636933705    |
| test_1/n_subgoals         | 151.0                 |
| test_1/subgoal_succ_rate  | 0.33774834437086093   |
| train/episodes            | 7100.0                |
| train/success_rate        | 0.75                  |
| train_0/avg_q             | -21.404947909584394   |
| train_0/current_q         | -11.073258965634093   |
| train_0/fw_bonus          | -0.9961899489164352   |
| train_0/fw_loss           | 0.02722951122559607   |
| train_0/mu_grads          | -0.10848702006042003  |
| train_0/mu_grads_std      | 0.7098042607307434    |
| train_0/mu_loss           | 10.846031856233594    |
| train_0/next_q            | -10.693800946537477   |
| train_0/q_grads           | -0.021825351379811765 |
| train_0/q_grads_std       | 0.4889684647321701    |
| train_0/q_loss            | 0.9932532870359235    |
| train_0/reward            | -0.8589901150775404   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0017333984375       |
| train_0/target_q          | -11.146885005874164   |
| train_1/avg_q             | -12.08389526056842    |
| train_1/current_q         | -2.75482772144412     |
| train_1/fw_bonus          | -0.9543389767408371   |
| train_1/fw_loss           | 0.36594429761171343   |
| train_1/mu_grads          | -0.1032985521480441   |
| train_1/mu_grads_std      | 0.46924187168478965   |
| train_1/mu_loss           | 0.7062581572993911    |
| train_1/n_subgoals        | 1369.0                |
| train_1/next_q            | -0.7644819770969018   |
| train_1/q_grads           | -0.13939485214650632  |
| train_1/q_grads_std       | 0.6520974904298782    |
| train_1/q_loss            | 3.330160957946744     |
| train_1/reward            | -2.273893581653101    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0                   |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.09057706355003653   |
| train_1/target_q          | -2.883600022542715    |
-----------------------------------------------------
Saving periodic policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101/policy_70.pkl ...
Mean of test/success_rate of last 4 epochs: 0.9099999999999999
Training epoch 71
Time for epoch 71: 278.49. Rollout time: 101.86, Training time: 176.60
Evaluating epoch 71
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
----------------------------------------------------
| epoch                     | 71                   |
| policy/steps              | 3419406.0            |
| test/episodes             | 1800.0               |
| test/success_rate         | 0.84                 |
| test_0/avg_q              | -15.401696680802473  |
| test_1/avg_q              | -3.0907659794937876  |
| test_1/n_subgoals         | 195.0                |
| test_1/subgoal_succ_rate  | 0.23076923076923078  |
| train/episodes            | 7200.0               |
| train/success_rate        | 0.82                 |
| train_0/avg_q             | -20.92772394758747   |
| train_0/current_q         | -10.966863784566817  |
| train_0/fw_bonus          | -0.9961844325065613  |
| train_0/fw_loss           | 0.027267616288736463 |
| train_0/mu_grads          | -0.1087933037430048  |
| train_0/mu_grads_std      | 0.7126771196722984   |
| train_0/mu_loss           | 10.71734227218261    |
| train_0/next_q            | -10.589412707594594  |
| train_0/q_grads           | -0.0220862437505275  |
| train_0/q_grads_std       | 0.4919261746108532   |
| train_0/q_loss            | 0.9810189154549676   |
| train_0/reward            | -0.8610212038442114  |
| train_0/reward_-0.0_frac  | 0.0                  |
| train_0/reward_-1.0_frac  | 0.0015380859375      |
| train_0/target_q          | -11.040736958598975  |
| train_1/avg_q             | -11.416209251958465  |
| train_1/current_q         | -2.8194215315125852  |
| train_1/fw_bonus          | -0.9533391922712326  |
| train_1/fw_loss           | 0.37357326671481134  |
| train_1/mu_grads          | -0.10507294330745935 |
| train_1/mu_grads_std      | 0.47166570723056794  |
| train_1/mu_loss           | 0.7671839512433807   |
| train_1/n_subgoals        | 1383.0               |
| train_1/next_q            | -0.8226661168616065  |
| train_1/q_grads           | -0.14279698431491852 |
| train_1/q_grads_std       | 0.6567193284630776   |
| train_1/q_loss            | 3.184641995158085    |
| train_1/reward            | -2.2902879227724044  |
| train_1/reward_-0.0_frac  | 0.0                  |
| train_1/reward_-1.0_frac  | 0.0                  |
| train_1/reward_-27.0_frac | 0.0                  |
| train_1/subgoal_succ_rate | 0.08821402747650037  |
| train_1/target_q          | -2.944361467474559   |
----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.87
Training epoch 72
Time for epoch 72: 271.53. Rollout time: 98.11, Training time: 173.40
Evaluating epoch 72
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 72                    |
| policy/steps              | 3456913.0             |
| test/episodes             | 1825.0                |
| test/success_rate         | 0.92                  |
| test_0/avg_q              | -16.429107671076455   |
| test_1/avg_q              | -3.703585555436576    |
| test_1/n_subgoals         | 140.0                 |
| test_1/subgoal_succ_rate  | 0.24285714285714285   |
| train/episodes            | 7300.0                |
| train/success_rate        | 0.78                  |
| train_0/avg_q             | -21.080107178588925   |
| train_0/current_q         | -11.270407044519867   |
| train_0/fw_bonus          | -0.9961480975151062   |
| train_0/fw_loss           | 0.0275180630851537    |
| train_0/mu_grads          | -0.10993804801255465  |
| train_0/mu_grads_std      | 0.7153368011116982    |
| train_0/mu_loss           | 11.04697873667109     |
| train_0/next_q            | -10.895490637102665   |
| train_0/q_grads           | -0.021707935072481633 |
| train_0/q_grads_std       | 0.4949204444885254    |
| train_0/q_loss            | 0.9594074454440455    |
| train_0/reward            | -0.8615980596598092   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0014892578125       |
| train_0/target_q          | -11.360910476953567   |
| train_1/avg_q             | -11.518731108469186   |
| train_1/current_q         | -2.840073053062779    |
| train_1/fw_bonus          | -0.9532962784171104   |
| train_1/fw_loss           | 0.3739007204771042    |
| train_1/mu_grads          | -0.10717871263623238  |
| train_1/mu_grads_std      | 0.47598044499754905   |
| train_1/mu_loss           | 0.7677273782859223    |
| train_1/n_subgoals        | 1358.0                |
| train_1/next_q            | -0.8235562081474426   |
| train_1/q_grads           | -0.1453160174190998   |
| train_1/q_grads_std       | 0.6611708462238312    |
| train_1/q_loss            | 3.332967782806402     |
| train_1/reward            | -2.3095948228532506   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0                   |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.09204712812960236   |
| train_1/target_q          | -2.9596555773665      |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.88
Training epoch 73
Time for epoch 73: 272.65. Rollout time: 98.98, Training time: 173.63
Evaluating epoch 73
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 73                    |
| policy/steps              | 3494544.0             |
| test/episodes             | 1850.0                |
| test/success_rate         | 0.92                  |
| test_0/avg_q              | -17.800409999705565   |
| test_1/avg_q              | -2.912029557224888    |
| test_1/n_subgoals         | 128.0                 |
| test_1/subgoal_succ_rate  | 0.2265625             |
| train/episodes            | 7400.0                |
| train/success_rate        | 0.85                  |
| train_0/avg_q             | -20.677630531781585   |
| train_0/current_q         | -11.041285730587523   |
| train_0/fw_bonus          | -0.9961748272180557   |
| train_0/fw_loss           | 0.027333835884928704  |
| train_0/mu_grads          | -0.10924122240394354  |
| train_0/mu_grads_std      | 0.7179950550198555    |
| train_0/mu_loss           | 10.811834188840269    |
| train_0/next_q            | -10.655128552256539   |
| train_0/q_grads           | -0.022015211917459965 |
| train_0/q_grads_std       | 0.49813205301761626   |
| train_0/q_loss            | 0.8832106633764102    |
| train_0/reward            | -0.8596821329621889   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0012939453125       |
| train_0/target_q          | -11.122759248109622   |
| train_1/avg_q             | -11.805227064322711   |
| train_1/current_q         | -2.879614883009274    |
| train_1/fw_bonus          | -0.9535143703222275   |
| train_1/fw_loss           | 0.372236567735672     |
| train_1/mu_grads          | -0.10911514908075333  |
| train_1/mu_grads_std      | 0.4806382104754448    |
| train_1/mu_loss           | 0.7832123368092244    |
| train_1/n_subgoals        | 1378.0                |
| train_1/next_q            | -0.8452577185579695   |
| train_1/q_grads           | -0.14783614538609982  |
| train_1/q_grads_std       | 0.6662426516413689    |
| train_1/q_loss            | 2.9469071962040347    |
| train_1/reward            | -2.3162045195687826   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0                   |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.09288824383164006   |
| train_1/target_q          | -2.9761556822440647   |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.9
Training epoch 74
Time for epoch 74: 278.23. Rollout time: 101.54, Training time: 176.66
Evaluating epoch 74
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
----------------------------------------------------
| epoch                     | 74                   |
| policy/steps              | 3533515.0            |
| test/episodes             | 1875.0               |
| test/success_rate         | 0.88                 |
| test_0/avg_q              | -12.939061627276745  |
| test_1/avg_q              | -4.787216714674851   |
| test_1/n_subgoals         | 411.0                |
| test_1/subgoal_succ_rate  | 0.7274939172749392   |
| train/episodes            | 7500.0               |
| train/success_rate        | 0.81                 |
| train_0/avg_q             | -21.54417869978881   |
| train_0/current_q         | -11.18803314879525   |
| train_0/fw_bonus          | -0.9960952132940293  |
| train_0/fw_loss           | 0.027882701996713877 |
| train_0/mu_grads          | -0.1105340862646699  |
| train_0/mu_grads_std      | 0.7208697542548179   |
| train_0/mu_loss           | 10.935045145658874   |
| train_0/next_q            | -10.798991958104967  |
| train_0/q_grads           | -0.02209674883633852 |
| train_0/q_grads_std       | 0.5012333214282989   |
| train_0/q_loss            | 0.9816542652213471   |
| train_0/reward            | -0.8638736551034526  |
| train_0/reward_-0.0_frac  | 0.0                  |
| train_0/reward_-1.0_frac  | 0.0013916015625      |
| train_0/target_q          | -11.26689963268059   |
| train_1/avg_q             | -12.443699295251236  |
| train_1/current_q         | -2.847106178235299   |
| train_1/fw_bonus          | -0.9521302968263626  |
| train_1/fw_loss           | 0.38279807269573213  |
| train_1/mu_grads          | -0.11089856009930373 |
| train_1/mu_grads_std      | 0.4845361053943634   |
| train_1/mu_loss           | 0.8138239330872798   |
| train_1/n_subgoals        | 1383.0               |
| train_1/next_q            | -0.87713078636553    |
| train_1/q_grads           | -0.14939948692917823 |
| train_1/q_grads_std       | 0.6704004898667335   |
| train_1/q_loss            | 3.1407306354068414   |
| train_1/reward            | -2.2518631380175065  |
| train_1/reward_-0.0_frac  | 0.0                  |
| train_1/reward_-1.0_frac  | 0.0                  |
| train_1/reward_-27.0_frac | 0.0                  |
| train_1/subgoal_succ_rate | 0.07013738250180766  |
| train_1/target_q          | -2.9519879267839415  |
----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.89
Training epoch 75
Time for epoch 75: 273.13. Rollout time: 99.05, Training time: 174.04
Evaluating epoch 75
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 75                    |
| policy/steps              | 3572182.0             |
| test/episodes             | 1900.0                |
| test/success_rate         | 0.84                  |
| test_0/avg_q              | -18.841738940696978   |
| test_1/avg_q              | -3.315035170219811    |
| test_1/n_subgoals         | 180.0                 |
| test_1/subgoal_succ_rate  | 0.15555555555555556   |
| train/episodes            | 7600.0                |
| train/success_rate        | 0.8                   |
| train_0/avg_q             | -20.844872236632384   |
| train_0/current_q         | -11.237466161572403   |
| train_0/fw_bonus          | -0.9961674198508262   |
| train_0/fw_loss           | 0.027384882047772407  |
| train_0/mu_grads          | -0.11032290309667588  |
| train_0/mu_grads_std      | 0.7233539938926696    |
| train_0/mu_loss           | 11.002060288720347    |
| train_0/next_q            | -10.84912632155039    |
| train_0/q_grads           | -0.022305444255471228 |
| train_0/q_grads_std       | 0.5040050476789475    |
| train_0/q_loss            | 0.876849799495524     |
| train_0/reward            | -0.8658326253174892   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0011474609375       |
| train_0/target_q          | -11.325681067280481   |
| train_1/avg_q             | -11.813811904385362   |
| train_1/current_q         | -2.8895936667771687   |
| train_1/fw_bonus          | -0.9532404839992523   |
| train_1/fw_loss           | 0.3743264742195606    |
| train_1/mu_grads          | -0.11097616013139486  |
| train_1/mu_grads_std      | 0.48722038194537165   |
| train_1/mu_loss           | 0.8507951117687448    |
| train_1/n_subgoals        | 1351.0                |
| train_1/next_q            | -0.8847593168661196   |
| train_1/q_grads           | -0.15231836549937725  |
| train_1/q_grads_std       | 0.6752152815461159    |
| train_1/q_loss            | 3.5211683351074283    |
| train_1/reward            | -2.2976240873278586   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0                   |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.09104367135455219   |
| train_1/target_q          | -3.0028208723167817   |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.89
Training epoch 76
Time for epoch 76: 277.39. Rollout time: 104.99, Training time: 172.37
Evaluating epoch 76
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 76                    |
| policy/steps              | 3612177.0             |
| test/episodes             | 1925.0                |
| test/success_rate         | 0.92                  |
| test_0/avg_q              | -8.959790567415517    |
| test_1/avg_q              | -2.4466407779224797   |
| test_1/n_subgoals         | 136.0                 |
| test_1/subgoal_succ_rate  | 0.22794117647058823   |
| train/episodes            | 7700.0                |
| train/success_rate        | 0.77                  |
| train_0/avg_q             | -21.475631772905555   |
| train_0/current_q         | -11.042379093084033   |
| train_0/fw_bonus          | -0.9961883574724197   |
| train_0/fw_loss           | 0.027240488911047578  |
| train_0/mu_grads          | -0.11121248677372933  |
| train_0/mu_grads_std      | 0.7260213419795036    |
| train_0/mu_loss           | 10.795371119563594    |
| train_0/next_q            | -10.660324552527516   |
| train_0/q_grads           | -0.022073978185653688 |
| train_0/q_grads_std       | 0.5069581508636475    |
| train_0/q_loss            | 0.962274293526366     |
| train_0/reward            | -0.8635490432505322   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.000927734375        |
| train_0/target_q          | -11.136251935660999   |
| train_1/avg_q             | -12.372986095193683   |
| train_1/current_q         | -2.914998615072996    |
| train_1/fw_bonus          | -0.9521632000803948   |
| train_1/fw_loss           | 0.38254689499735833   |
| train_1/mu_grads          | -0.11250075120478868  |
| train_1/mu_grads_std      | 0.491537594050169     |
| train_1/mu_loss           | 0.8241299104784272    |
| train_1/n_subgoals        | 1448.0                |
| train_1/next_q            | -0.8891586834895222   |
| train_1/q_grads           | -0.1546097680926323   |
| train_1/q_grads_std       | 0.6795922562479972    |
| train_1/q_loss            | 3.0852610175196644    |
| train_1/reward            | -2.3225134558746503   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0                   |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.09116022099447514   |
| train_1/target_q          | -3.0140233732213995   |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.89
Training epoch 77
Time for epoch 77: 267.98. Rollout time: 91.23, Training time: 176.72
Evaluating epoch 77
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 77                    |
| policy/steps              | 3647614.0             |
| test/episodes             | 1950.0                |
| test/success_rate         | 0.88                  |
| test_0/avg_q              | -14.184959881196818   |
| test_1/avg_q              | -2.484275277950791    |
| test_1/n_subgoals         | 174.0                 |
| test_1/subgoal_succ_rate  | 0.25287356321839083   |
| train/episodes            | 7800.0                |
| train/success_rate        | 0.81                  |
| train_0/avg_q             | -21.121592008406463   |
| train_0/current_q         | -11.040164571057014   |
| train_0/fw_bonus          | -0.9961898729205132   |
| train_0/fw_loss           | 0.027230000076815485  |
| train_0/mu_grads          | -0.11168456263840199  |
| train_0/mu_grads_std      | 0.7285635456442833    |
| train_0/mu_loss           | 10.789477010604507    |
| train_0/next_q            | -10.655695362055535   |
| train_0/q_grads           | -0.022658495558425785 |
| train_0/q_grads_std       | 0.5098086908459664    |
| train_0/q_loss            | 0.8802513447825584    |
| train_0/reward            | -0.8597042087705631   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.000830078125        |
| train_0/target_q          | -11.119732225989901   |
| train_1/avg_q             | -12.58555960106629    |
| train_1/current_q         | -2.9497391334881216   |
| train_1/fw_bonus          | -0.9528064548969268   |
| train_1/fw_loss           | 0.3776383772492409    |
| train_1/mu_grads          | -0.11404768377542496  |
| train_1/mu_grads_std      | 0.49495147839188575   |
| train_1/mu_loss           | 0.8671427226334675    |
| train_1/n_subgoals        | 1247.0                |
| train_1/next_q            | -0.9355828589852141   |
| train_1/q_grads           | -0.15672325007617474  |
| train_1/q_grads_std       | 0.6828596413135528    |
| train_1/q_loss            | 3.3618373454737216    |
| train_1/reward            | -2.3171811084357614   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0                   |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.08340016038492382   |
| train_1/target_q          | -3.046381287283302    |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.88
Training epoch 78
Time for epoch 78: 261.16. Rollout time: 94.94, Training time: 166.19
Evaluating epoch 78
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 78                    |
| policy/steps              | 3683448.0             |
| test/episodes             | 1975.0                |
| test/success_rate         | 0.96                  |
| test_0/avg_q              | -8.718426177222431    |
| test_1/avg_q              | -0.8140160011559366   |
| test_1/n_subgoals         | 110.0                 |
| test_1/subgoal_succ_rate  | 0.3                   |
| train/episodes            | 7900.0                |
| train/success_rate        | 0.83                  |
| train_0/avg_q             | -20.534334860808478   |
| train_0/current_q         | -11.212007222084942   |
| train_0/fw_bonus          | -0.9961547702550888   |
| train_0/fw_loss           | 0.02747212992981076   |
| train_0/mu_grads          | -0.11226006504148245  |
| train_0/mu_grads_std      | 0.7309200748801231    |
| train_0/mu_loss           | 10.951951529764106    |
| train_0/next_q            | -10.821615210564127   |
| train_0/q_grads           | -0.022257411293685435 |
| train_0/q_grads_std       | 0.5131299749016762    |
| train_0/q_loss            | 0.864391306595863     |
| train_0/reward            | -0.8614898619951419   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0011474609375       |
| train_0/target_q          | -11.279373913425049   |
| train_1/avg_q             | -11.633846337162476   |
| train_1/current_q         | -2.923804222054416    |
| train_1/fw_bonus          | -0.9522445350885391   |
| train_1/fw_loss           | 0.38192630261182786   |
| train_1/mu_grads          | -0.11529815196990967  |
| train_1/mu_grads_std      | 0.4981617569923401    |
| train_1/mu_loss           | 0.8593327255937726    |
| train_1/n_subgoals        | 1316.0                |
| train_1/next_q            | -0.9158205656343503   |
| train_1/q_grads           | -0.15968779735267163  |
| train_1/q_grads_std       | 0.6871154233813286    |
| train_1/q_loss            | 2.989473847128012     |
| train_1/reward            | -2.306988134666608    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0                   |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.09574468085106383   |
| train_1/target_q          | -3.0197075688329442   |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.9
Training epoch 79
Time for epoch 79: 273.73. Rollout time: 96.51, Training time: 177.19
Evaluating epoch 79
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 79                    |
| policy/steps              | 3718167.0             |
| test/episodes             | 2000.0                |
| test/success_rate         | 0.96                  |
| test_0/avg_q              | -8.62323388044812     |
| test_1/avg_q              | -3.86036912074239     |
| test_1/n_subgoals         | 647.0                 |
| test_1/subgoal_succ_rate  | 0.9165378670788253    |
| train/episodes            | 8000.0                |
| train/success_rate        | 0.81                  |
| train_0/avg_q             | -20.62440410045009    |
| train_0/current_q         | -10.996322395327034   |
| train_0/fw_bonus          | -0.9962158516049385   |
| train_0/fw_loss           | 0.027050928259268404  |
| train_0/mu_grads          | -0.11428722739219666  |
| train_0/mu_grads_std      | 0.7339342519640922    |
| train_0/mu_loss           | 10.753452568429285    |
| train_0/next_q            | -10.609295406494187   |
| train_0/q_grads           | -0.022057387977838516 |
| train_0/q_grads_std       | 0.5161118149757385    |
| train_0/q_loss            | 0.9387721959537139    |
| train_0/reward            | -0.8629882901048405   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0009521484375       |
| train_0/target_q          | -11.071270006985353   |
| train_1/avg_q             | -11.912010306297146   |
| train_1/current_q         | -2.9191111389220668   |
| train_1/fw_bonus          | -0.9508390069007874   |
| train_1/fw_loss           | 0.39265149682760236   |
| train_1/mu_grads          | -0.11587915234267712  |
| train_1/mu_grads_std      | 0.5010189041495323    |
| train_1/mu_loss           | 0.8788423953080937    |
| train_1/n_subgoals        | 1285.0                |
| train_1/next_q            | -0.9394586910729925   |
| train_1/q_grads           | -0.16211284473538398  |
| train_1/q_grads_std       | 0.6914891466498375    |
| train_1/q_loss            | 3.4228264799711114    |
| train_1/reward            | -2.2758750324519497   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0                   |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.09182879377431907   |
| train_1/target_q          | -3.0157937515544275   |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.9299999999999999
Training epoch 80
Time for epoch 80: 262.50. Rollout time: 94.48, Training time: 168.00
Evaluating epoch 80
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
----------------------------------------------------
| epoch                     | 80                   |
| policy/steps              | 3753803.0            |
| test/episodes             | 2025.0               |
| test/success_rate         | 0.96                 |
| test_0/avg_q              | -13.757075171612433  |
| test_1/avg_q              | -2.091941065729343   |
| test_1/n_subgoals         | 100.0                |
| test_1/subgoal_succ_rate  | 0.26                 |
| train/episodes            | 8100.0               |
| train/success_rate        | 0.84                 |
| train_0/avg_q             | -19.96144096464467   |
| train_0/current_q         | -11.275350631402388  |
| train_0/fw_bonus          | -0.9961574882268905  |
| train_0/fw_loss           | 0.027453310368582607 |
| train_0/mu_grads          | -0.11502085439860821 |
| train_0/mu_grads_std      | 0.737202575802803    |
| train_0/mu_loss           | 11.021509445132818   |
| train_0/next_q            | -10.878799229574152  |
| train_0/q_grads           | -0.02173906988464296 |
| train_0/q_grads_std       | 0.5196208760142327   |
| train_0/q_loss            | 0.9517534760182264   |
| train_0/reward            | -0.8661048022244359  |
| train_0/reward_-0.0_frac  | 0.0                  |
| train_0/reward_-1.0_frac  | 0.0009765625         |
| train_0/target_q          | -11.348619983198256  |
| train_1/avg_q             | -11.515476444283966  |
| train_1/current_q         | -2.895989844927211   |
| train_1/fw_bonus          | -0.9514920949935913  |
| train_1/fw_loss           | 0.38766791075468066  |
| train_1/mu_grads          | -0.11667818166315555 |
| train_1/mu_grads_std      | 0.5040421321988106   |
| train_1/mu_loss           | 0.8721198996101045   |
| train_1/n_subgoals        | 1327.0               |
| train_1/next_q            | -0.9189642173096061  |
| train_1/q_grads           | -0.163871556147933   |
| train_1/q_grads_std       | 0.6957789093255997   |
| train_1/q_loss            | 3.187012007749721    |
| train_1/reward            | -2.2547980725965315  |
| train_1/reward_-0.0_frac  | 0.0                  |
| train_1/reward_-1.0_frac  | 0.0                  |
| train_1/reward_-27.0_frac | 0.0                  |
| train_1/subgoal_succ_rate | 0.10022607385079126  |
| train_1/target_q          | -2.980831097755403   |
----------------------------------------------------
Saving periodic policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101/policy_80.pkl ...
Mean of test/success_rate of last 4 epochs: 0.94
Training epoch 81
Time for epoch 81: 276.35. Rollout time: 100.46, Training time: 175.85
Evaluating epoch 81
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
----------------------------------------------------
| epoch                     | 81                   |
| policy/steps              | 3790186.0            |
| test/episodes             | 2050.0               |
| test/success_rate         | 0.96                 |
| test_0/avg_q              | -9.241454073675833   |
| test_1/avg_q              | -0.533342268113683   |
| test_1/n_subgoals         | 96.0                 |
| test_1/subgoal_succ_rate  | 0.3645833333333333   |
| train/episodes            | 8200.0               |
| train/success_rate        | 0.8                  |
| train_0/avg_q             | -20.64485319181266   |
| train_0/current_q         | -11.271258422457706  |
| train_0/fw_bonus          | -0.9961923360824585  |
| train_0/fw_loss           | 0.027213093545287847 |
| train_0/mu_grads          | -0.11587812714278697 |
| train_0/mu_grads_std      | 0.740394939482212    |
| train_0/mu_loss           | 11.02482796440733    |
| train_0/next_q            | -10.884268854981164  |
| train_0/q_grads           | -0.02211970123462379 |
| train_0/q_grads_std       | 0.522305716574192    |
| train_0/q_loss            | 0.903063374680066    |
| train_0/reward            | -0.865985339134204   |
| train_0/reward_-0.0_frac  | 0.0                  |
| train_0/reward_-1.0_frac  | 0.0013916015625      |
| train_0/target_q          | -11.339304046014947  |
| train_1/avg_q             | -11.743101875127772  |
| train_1/current_q         | -2.8304121544190446  |
| train_1/fw_bonus          | -0.9519723072648049  |
| train_1/fw_loss           | 0.3840035505592823   |
| train_1/mu_grads          | -0.11851688325405121 |
| train_1/mu_grads_std      | 0.5071625903248786   |
| train_1/mu_loss           | 0.8073767319715334   |
| train_1/n_subgoals        | 1357.0               |
| train_1/next_q            | -0.85837187910351    |
| train_1/q_grads           | -0.1667054131627083  |
| train_1/q_grads_std       | 0.7001948401331901   |
| train_1/q_loss            | 3.2731116333247847   |
| train_1/reward            | -2.2548082635395987  |
| train_1/reward_-0.0_frac  | 0.0                  |
| train_1/reward_-1.0_frac  | 0.0                  |
| train_1/reward_-27.0_frac | 0.0                  |
| train_1/subgoal_succ_rate | 0.10611643330876934  |
| train_1/target_q          | -2.934819523019418   |
----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.96
Training epoch 82
Time for epoch 82: 262.20. Rollout time: 92.56, Training time: 169.61
Evaluating epoch 82
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
----------------------------------------------------
| epoch                     | 82                   |
| policy/steps              | 3825310.0            |
| test/episodes             | 2075.0               |
| test/success_rate         | 0.96                 |
| test_0/avg_q              | -8.852664173361804   |
| test_1/avg_q              | -2.601982893260662   |
| test_1/n_subgoals         | 133.0                |
| test_1/subgoal_succ_rate  | 0.47368421052631576  |
| train/episodes            | 8300.0               |
| train/success_rate        | 0.83                 |
| train_0/avg_q             | -20.588832952727003  |
| train_0/current_q         | -11.161350608323497  |
| train_0/fw_bonus          | -0.9961753875017166  |
| train_0/fw_loss           | 0.027329942397773265 |
| train_0/mu_grads          | -0.11652324367314577 |
| train_0/mu_grads_std      | 0.7435321718454361   |
| train_0/mu_loss           | 10.902006357322557   |
| train_0/next_q            | -10.7607809597794    |
| train_0/q_grads           | -0.02239845655858517 |
| train_0/q_grads_std       | 0.5254915982484818   |
| train_0/q_loss            | 0.8723331465746504   |
| train_0/reward            | -0.8662008286897617  |
| train_0/reward_-0.0_frac  | 0.0                  |
| train_0/reward_-1.0_frac  | 0.001220703125       |
| train_0/target_q          | -11.232052997538437  |
| train_1/avg_q             | -11.729592358721442  |
| train_1/current_q         | -2.954134366414506   |
| train_1/fw_bonus          | -0.9512170627713203  |
| train_1/fw_loss           | 0.389766538888216    |
| train_1/mu_grads          | -0.11883337758481502 |
| train_1/mu_grads_std      | 0.5098192140460014   |
| train_1/mu_loss           | 0.8944843466228957   |
| train_1/n_subgoals        | 1301.0               |
| train_1/next_q            | -0.9510606622626436  |
| train_1/q_grads           | -0.1687859557569027  |
| train_1/q_grads_std       | 0.7048446908593178   |
| train_1/q_loss            | 3.23571871103036     |
| train_1/reward            | -2.276280831174154   |
| train_1/reward_-0.0_frac  | 0.0                  |
| train_1/reward_-1.0_frac  | 0.0                  |
| train_1/reward_-27.0_frac | 0.0                  |
| train_1/subgoal_succ_rate | 0.09069946195234435  |
| train_1/target_q          | -3.030865267099083   |
----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.96
Training epoch 83
Time for epoch 83: 274.84. Rollout time: 104.83, Training time: 169.98
Evaluating epoch 83
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
----------------------------------------------------
| epoch                     | 83                   |
| policy/steps              | 3865235.0            |
| test/episodes             | 2100.0               |
| test/success_rate         | 0.92                 |
| test_0/avg_q              | -18.250170736369494  |
| test_1/avg_q              | -2.639087452922284   |
| test_1/n_subgoals         | 129.0                |
| test_1/subgoal_succ_rate  | 0.14728682170542637  |
| train/episodes            | 8400.0               |
| train/success_rate        | 0.76                 |
| train_0/avg_q             | -20.96850336654899   |
| train_0/current_q         | -11.25103984663998   |
| train_0/fw_bonus          | -0.996209979057312   |
| train_0/fw_loss           | 0.027091475622728466 |
| train_0/mu_grads          | -0.1179418746381998  |
| train_0/mu_grads_std      | 0.7463619157671928   |
| train_0/mu_loss           | 11.007879739052578   |
| train_0/next_q            | -10.865489402582401  |
| train_0/q_grads           | -0.02266971687786281 |
| train_0/q_grads_std       | 0.5281641691923141   |
| train_0/q_loss            | 0.9378294783774216   |
| train_0/reward            | -0.8664841341880674  |
| train_0/reward_-0.0_frac  | 0.0                  |
| train_0/reward_-1.0_frac  | 0.0013427734375      |
| train_0/target_q          | -11.329655072576669  |
| train_1/avg_q             | -11.938169449161567  |
| train_1/current_q         | -2.9507901255529077  |
| train_1/fw_bonus          | -0.9505273267626763  |
| train_1/fw_loss           | 0.39502980262041093  |
| train_1/mu_grads          | -0.11999674085527659 |
| train_1/mu_grads_std      | 0.5126026675105095   |
| train_1/mu_loss           | 0.8592506861715254   |
| train_1/n_subgoals        | 1460.0               |
| train_1/next_q            | -0.9282417615555886  |
| train_1/q_grads           | -0.1714952304959297  |
| train_1/q_grads_std       | 0.7090993225574493   |
| train_1/q_loss            | 3.2458958509675755   |
| train_1/reward            | -2.3211157168887437  |
| train_1/reward_-0.0_frac  | 0.0                  |
| train_1/reward_-1.0_frac  | 0.0                  |
| train_1/reward_-27.0_frac | 0.0                  |
| train_1/subgoal_succ_rate | 0.11301369863013698  |
| train_1/target_q          | -3.0458618874776944  |
----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.95
Training epoch 84
Time for epoch 84: 276.92. Rollout time: 103.36, Training time: 173.53
Evaluating epoch 84
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
----------------------------------------------------
| epoch                     | 84                   |
| policy/steps              | 3902999.0            |
| test/episodes             | 2125.0               |
| test/success_rate         | 0.96                 |
| test_0/avg_q              | -15.467998067058254  |
| test_1/avg_q              | -2.054507264958819   |
| test_1/n_subgoals         | 107.0                |
| test_1/subgoal_succ_rate  | 0.2616822429906542   |
| train/episodes            | 8500.0               |
| train/success_rate        | 0.79                 |
| train_0/avg_q             | -21.251011036383723  |
| train_0/current_q         | -11.112915870396526  |
| train_0/fw_bonus          | -0.9962499797344208  |
| train_0/fw_loss           | 0.026815612334758043 |
| train_0/mu_grads          | -0.11947753485292197 |
| train_0/mu_grads_std      | 0.7487473100423813   |
| train_0/mu_loss           | 10.870458115053015   |
| train_0/next_q            | -10.720813447758932  |
| train_0/q_grads           | -0.02248150440864265 |
| train_0/q_grads_std       | 0.5312752857804298   |
| train_0/q_loss            | 0.9504606396703856   |
| train_0/reward            | -0.8665307588969882  |
| train_0/reward_-0.0_frac  | 0.0                  |
| train_0/reward_-1.0_frac  | 0.0013427734375      |
| train_0/target_q          | -11.184462567197334  |
| train_1/avg_q             | -12.291355447296633  |
| train_1/current_q         | -2.9808634465125055  |
| train_1/fw_bonus          | -0.9513709992170334  |
| train_1/fw_loss           | 0.38859198316931726  |
| train_1/mu_grads          | -0.12079930901527405 |
| train_1/mu_grads_std      | 0.5154225111007691   |
| train_1/mu_loss           | 0.8756778762872894   |
| train_1/n_subgoals        | 1400.0               |
| train_1/next_q            | -0.9458188957691887  |
| train_1/q_grads           | -0.17394158020615577 |
| train_1/q_grads_std       | 0.7130738452076912   |
| train_1/q_loss            | 2.8465446128481213   |
| train_1/reward            | -2.325245092276236   |
| train_1/reward_-0.0_frac  | 0.0                  |
| train_1/reward_-1.0_frac  | 0.0                  |
| train_1/reward_-27.0_frac | 0.0                  |
| train_1/subgoal_succ_rate | 0.09571428571428571  |
| train_1/target_q          | -3.069867996482599   |
----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.95
Training epoch 85
Time for epoch 85: 259.99. Rollout time: 92.23, Training time: 167.74
Evaluating epoch 85
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 85                    |
| policy/steps              | 3938015.0             |
| test/episodes             | 2150.0                |
| test/success_rate         | 0.96                  |
| test_0/avg_q              | -13.795069236194966   |
| test_1/avg_q              | -1.052058578886751    |
| test_1/n_subgoals         | 110.0                 |
| test_1/subgoal_succ_rate  | 0.2909090909090909    |
| train/episodes            | 8600.0                |
| train/success_rate        | 0.84                  |
| train_0/avg_q             | -20.49959145293905    |
| train_0/current_q         | -11.011981344483408   |
| train_0/fw_bonus          | -0.9962076753377914   |
| train_0/fw_loss           | 0.02710747914388776   |
| train_0/mu_grads          | -0.12030399516224861  |
| train_0/mu_grads_std      | 0.7514233320951462    |
| train_0/mu_loss           | 10.755423620975657    |
| train_0/next_q            | -10.619252878066279   |
| train_0/q_grads           | -0.022294215485453607 |
| train_0/q_grads_std       | 0.5338759273290634    |
| train_0/q_loss            | 0.9248947207341212    |
| train_0/reward            | -0.8678202238868835   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.001318359375        |
| train_0/target_q          | -11.09881178883982    |
| train_1/avg_q             | -11.919707896461968   |
| train_1/current_q         | -2.925755710857076    |
| train_1/fw_bonus          | -0.9511015176773071   |
| train_1/fw_loss           | 0.3906482562422752    |
| train_1/mu_grads          | -0.12193327136337757  |
| train_1/mu_grads_std      | 0.5186949849128724    |
| train_1/mu_loss           | 0.8804512728109234    |
| train_1/n_subgoals        | 1296.0                |
| train_1/next_q            | -0.9264467609986319   |
| train_1/q_grads           | -0.17606891952455045  |
| train_1/q_grads_std       | 0.7171673864126206    |
| train_1/q_loss            | 3.0378888480603377    |
| train_1/reward            | -2.281232410793018    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0                   |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.1111111111111111    |
| train_1/target_q          | -3.0164179228930887   |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.95
Training epoch 86
Time for epoch 86: 301.80. Rollout time: 92.84, Training time: 208.92
Evaluating epoch 86
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
----------------------------------------------------
| epoch                     | 86                   |
| policy/steps              | 3968033.0            |
| test/episodes             | 2175.0               |
| test/success_rate         | 1.0                  |
| test_0/avg_q              | -9.591772166953401   |
| test_1/avg_q              | -1.3346635828538025  |
| test_1/n_subgoals         | 102.0                |
| test_1/subgoal_succ_rate  | 0.4117647058823529   |
| train/episodes            | 8700.0               |
| train/success_rate        | 0.92                 |
| train_0/avg_q             | -19.69102219148475   |
| train_0/current_q         | -11.0373187609752    |
| train_0/fw_bonus          | -0.9962112843990326  |
| train_0/fw_loss           | 0.027082458464428784 |
| train_0/mu_grads          | -0.12132079675793647 |
| train_0/mu_grads_std      | 0.7541233316063881   |
| train_0/mu_loss           | 10.774401090845505   |
| train_0/next_q            | -10.636286952108136  |
| train_0/q_grads           | -0.02294376287609339 |
| train_0/q_grads_std       | 0.5366563260555267   |
| train_0/q_loss            | 0.9200335556247137   |
| train_0/reward            | -0.8703113654311891  |
| train_0/reward_-0.0_frac  | 0.0                  |
| train_0/reward_-1.0_frac  | 0.00166015625        |
| train_0/target_q          | -11.109516155225213  |
| train_1/avg_q             | -10.952083775145534  |
| train_1/current_q         | -2.9177365608678896  |
| train_1/fw_bonus          | -0.9524017408490181  |
| train_1/fw_loss           | 0.3807267025113106   |
| train_1/mu_grads          | -0.12373583354055881 |
| train_1/mu_grads_std      | 0.5216295659542084   |
| train_1/mu_loss           | 0.8570188549923244   |
| train_1/n_subgoals        | 1127.0               |
| train_1/next_q            | -0.906785088356594   |
| train_1/q_grads           | -0.1780619814991951  |
| train_1/q_grads_std       | 0.7215557768940926   |
| train_1/q_loss            | 3.1437480838653133   |
| train_1/reward            | -2.276027680159314   |
| train_1/reward_-0.0_frac  | 0.0                  |
| train_1/reward_-1.0_frac  | 0.0                  |
| train_1/reward_-27.0_frac | 0.0                  |
| train_1/subgoal_succ_rate | 0.11978704525288376  |
| train_1/target_q          | -2.9994721696625075  |
----------------------------------------------------
New best value for test/success_rate: 1.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.96
Training epoch 87
Time for epoch 87: 281.63. Rollout time: 109.44, Training time: 172.15
Evaluating epoch 87
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 87                    |
| policy/steps              | 4005642.0             |
| test/episodes             | 2200.0                |
| test/success_rate         | 0.96                  |
| test_0/avg_q              | -8.78554640754066     |
| test_1/avg_q              | -0.9237498024965446   |
| test_1/n_subgoals         | 105.0                 |
| test_1/subgoal_succ_rate  | 0.29523809523809524   |
| train/episodes            | 8800.0                |
| train/success_rate        | 0.77                  |
| train_0/avg_q             | -20.794705240508527   |
| train_0/current_q         | -11.154532053092009   |
| train_0/fw_bonus          | -0.9962306380271911   |
| train_0/fw_loss           | 0.026948985923081637  |
| train_0/mu_grads          | -0.12094480041414499  |
| train_0/mu_grads_std      | 0.7569190099835396    |
| train_0/mu_loss           | 10.887975733864964    |
| train_0/next_q            | -10.75113442553275    |
| train_0/q_grads           | -0.023665539221838117 |
| train_0/q_grads_std       | 0.5392368793487549    |
| train_0/q_loss            | 0.9297875655786612    |
| train_0/reward            | -0.8718726021768817   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0019287109375       |
| train_0/target_q          | -11.237038156745564   |
| train_1/avg_q             | -12.141019086183439   |
| train_1/current_q         | -2.9068234864959672   |
| train_1/fw_bonus          | -0.9491442710161209   |
| train_1/fw_loss           | 0.40558332204818726   |
| train_1/mu_grads          | -0.1254734542220831   |
| train_1/mu_grads_std      | 0.5254694446921349    |
| train_1/mu_loss           | 0.7940371879071078    |
| train_1/n_subgoals        | 1401.0                |
| train_1/next_q            | -0.84117102216705     |
| train_1/q_grads           | -0.17964433431625365  |
| train_1/q_grads_std       | 0.7261160001158714    |
| train_1/q_loss            | 2.9519353987407886    |
| train_1/reward            | -2.322161558517837    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0                   |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.10135617416131334   |
| train_1/target_q          | -2.98659769437241     |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.97
Training epoch 88
Time for epoch 88: 260.71. Rollout time: 88.78, Training time: 171.90
Evaluating epoch 88
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 88                    |
| policy/steps              | 4038770.0             |
| test/episodes             | 2225.0                |
| test/success_rate         | 0.96                  |
| test_0/avg_q              | -13.083084549870026   |
| test_1/avg_q              | -2.3807854032954365   |
| test_1/n_subgoals         | 120.0                 |
| test_1/subgoal_succ_rate  | 0.2916666666666667    |
| train/episodes            | 8900.0                |
| train/success_rate        | 0.87                  |
| train_0/avg_q             | -20.434311321343277   |
| train_0/current_q         | -11.070251313468255   |
| train_0/fw_bonus          | -0.9962701931595802   |
| train_0/fw_loss           | 0.0266762999817729    |
| train_0/mu_grads          | -0.12128643337637186  |
| train_0/mu_grads_std      | 0.7597054541110992    |
| train_0/mu_loss           | 10.78937829281228     |
| train_0/next_q            | -10.660091896398738   |
| train_0/q_grads           | -0.023884329944849014 |
| train_0/q_grads_std       | 0.5420750871300697    |
| train_0/q_loss            | 0.9169465946881787    |
| train_0/reward            | -0.8733473342505021   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.00126953125         |
| train_0/target_q          | -11.151371646868913   |
| train_1/avg_q             | -11.73488809555729    |
| train_1/current_q         | -2.9272702354541513   |
| train_1/fw_bonus          | -0.9502240404486656   |
| train_1/fw_loss           | 0.3973440207540989    |
| train_1/mu_grads          | -0.12777892053127288  |
| train_1/mu_grads_std      | 0.5292474731802941    |
| train_1/mu_loss           | 0.8852919625043153    |
| train_1/n_subgoals        | 1220.0                |
| train_1/next_q            | -0.9555784760246793   |
| train_1/q_grads           | -0.18163506239652633  |
| train_1/q_grads_std       | 0.730962447822094     |
| train_1/q_loss            | 3.1719005930515403    |
| train_1/reward            | -2.25486558051889     |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0                   |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.11147540983606558   |
| train_1/target_q          | -3.0081287304419986   |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.97
Training epoch 89
Time for epoch 89: 265.11. Rollout time: 87.65, Training time: 177.43
Evaluating epoch 89
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 89                    |
| policy/steps              | 4070895.0             |
| test/episodes             | 2250.0                |
| test/success_rate         | 1.0                   |
| test_0/avg_q              | -11.544706081626199   |
| test_1/avg_q              | -1.5397218935842687   |
| test_1/n_subgoals         | 76.0                  |
| test_1/subgoal_succ_rate  | 0.3684210526315789    |
| train/episodes            | 9000.0                |
| train/success_rate        | 0.86                  |
| train_0/avg_q             | -20.179095717094775   |
| train_0/current_q         | -10.948170856823856   |
| train_0/fw_bonus          | -0.9962866649031639   |
| train_0/fw_loss           | 0.026562826335430147  |
| train_0/mu_grads          | -0.12341081537306309  |
| train_0/mu_grads_std      | 0.7621968880295753    |
| train_0/mu_loss           | 10.698041499917442    |
| train_0/next_q            | -10.543372733640412   |
| train_0/q_grads           | -0.022894513327628374 |
| train_0/q_grads_std       | 0.5453925386071206    |
| train_0/q_loss            | 0.9204982462182884    |
| train_0/reward            | -0.8710992517411796   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0017822265625       |
| train_0/target_q          | -11.019431106931625   |
| train_1/avg_q             | -11.465188982705607   |
| train_1/current_q         | -2.853977957139018    |
| train_1/fw_bonus          | -0.9522365957498551   |
| train_1/fw_loss           | 0.3819868862628937    |
| train_1/mu_grads          | -0.12954993918538094  |
| train_1/mu_grads_std      | 0.5325972124934196    |
| train_1/mu_loss           | 0.8077411093847754    |
| train_1/n_subgoals        | 1219.0                |
| train_1/next_q            | -0.8667974917483136   |
| train_1/q_grads           | -0.183567887917161    |
| train_1/q_grads_std       | 0.7345683068037033    |
| train_1/q_loss            | 2.6209065816164427    |
| train_1/reward            | -2.259525212684457    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0                   |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.10828547990155865   |
| train_1/target_q          | -2.9443580664823257   |
-----------------------------------------------------
New best value for test/success_rate: 1.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.98
Training epoch 90
Time for epoch 90: 271.11. Rollout time: 99.34, Training time: 171.74
Evaluating epoch 90
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
----------------------------------------------------
| epoch                     | 90                   |
| policy/steps              | 4106800.0            |
| test/episodes             | 2275.0               |
| test/success_rate         | 1.0                  |
| test_0/avg_q              | -10.800380877525926  |
| test_1/avg_q              | -0.572890134715189   |
| test_1/n_subgoals         | 72.0                 |
| test_1/subgoal_succ_rate  | 0.3055555555555556   |
| train/episodes            | 9100.0               |
| train/success_rate        | 0.79                 |
| train_0/avg_q             | -20.782381594546667  |
| train_0/current_q         | -11.053338064796725  |
| train_0/fw_bonus          | -0.9962651565670967  |
| train_0/fw_loss           | 0.026711101550608872 |
| train_0/mu_grads          | -0.12189569473266601 |
| train_0/mu_grads_std      | 0.7650642216205596   |
| train_0/mu_loss           | 10.803661439590957   |
| train_0/next_q            | -10.66432022572242   |
| train_0/q_grads           | -0.02303794356994331 |
| train_0/q_grads_std       | 0.5478017374873161   |
| train_0/q_loss            | 0.9156664492070318   |
| train_0/reward            | -0.868292361226122   |
| train_0/reward_-0.0_frac  | 0.0                  |
| train_0/reward_-1.0_frac  | 0.0014404296875      |
| train_0/target_q          | -11.130950552464224  |
| train_1/avg_q             | -11.837547688811384  |
| train_1/current_q         | -2.9274251270263525  |
| train_1/fw_bonus          | -0.9493582755327225  |
| train_1/fw_loss           | 0.40395045801997187  |
| train_1/mu_grads          | -0.13052705191075803 |
| train_1/mu_grads_std      | 0.5349760338664055   |
| train_1/mu_loss           | 0.9002600782373109   |
| train_1/n_subgoals        | 1359.0               |
| train_1/next_q            | -0.9403074403012912  |
| train_1/q_grads           | -0.18529509231448174 |
| train_1/q_grads_std       | 0.7378602147102356   |
| train_1/q_loss            | 2.787772375130843    |
| train_1/reward            | -2.2596112959749006  |
| train_1/reward_-0.0_frac  | 0.0                  |
| train_1/reward_-1.0_frac  | 0.0                  |
| train_1/reward_-27.0_frac | 0.0                  |
| train_1/subgoal_succ_rate | 0.10963944076526858  |
| train_1/target_q          | -3.0006984804987624  |
----------------------------------------------------
Saving periodic policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101/policy_90.pkl ...
New best value for test/success_rate: 1.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.98
Training epoch 91
Time for epoch 91: 265.50. Rollout time: 92.74, Training time: 172.73
Evaluating epoch 91
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
----------------------------------------------------
| epoch                     | 91                   |
| policy/steps              | 4141011.0            |
| test/episodes             | 2300.0               |
| test/success_rate         | 0.96                 |
| test_0/avg_q              | -10.256268859106948  |
| test_1/avg_q              | -0.9339257982174662  |
| test_1/n_subgoals         | 106.0                |
| test_1/subgoal_succ_rate  | 0.3018867924528302   |
| train/episodes            | 9200.0               |
| train/success_rate        | 0.84                 |
| train_0/avg_q             | -20.30691137231044   |
| train_0/current_q         | -11.081922100147452  |
| train_0/fw_bonus          | -0.9962824776768684  |
| train_0/fw_loss           | 0.026591637544333933 |
| train_0/mu_grads          | -0.12174014616757631 |
| train_0/mu_grads_std      | 0.7678084462881088   |
| train_0/mu_loss           | 10.835751699998607   |
| train_0/next_q            | -10.689407088265735  |
| train_0/q_grads           | -0.02329971403814852 |
| train_0/q_grads_std       | 0.5499143213033676   |
| train_0/q_loss            | 0.931699717548625    |
| train_0/reward            | -0.8701460584212327  |
| train_0/reward_-0.0_frac  | 0.0                  |
| train_0/reward_-1.0_frac  | 0.0012451171875      |
| train_0/target_q          | -11.162926718848777  |
| train_1/avg_q             | -11.354302864323337  |
| train_1/current_q         | -2.9174387127062196  |
| train_1/fw_bonus          | -0.9504658877849579  |
| train_1/fw_loss           | 0.395498614013195    |
| train_1/mu_grads          | -0.13166288658976555 |
| train_1/mu_grads_std      | 0.538139283657074    |
| train_1/mu_loss           | 0.9109055107496344   |
| train_1/n_subgoals        | 1270.0               |
| train_1/next_q            | -0.9637957056757372  |
| train_1/q_grads           | -0.18765029609203338 |
| train_1/q_grads_std       | 0.7409535437822342   |
| train_1/q_loss            | 3.1340063594298657   |
| train_1/reward            | -2.2380254064828478  |
| train_1/reward_-0.0_frac  | 0.0                  |
| train_1/reward_-1.0_frac  | 0.0                  |
| train_1/reward_-27.0_frac | 0.0                  |
| train_1/subgoal_succ_rate | 0.1015748031496063   |
| train_1/target_q          | -2.9992022204003943  |
----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.98
Training epoch 92
Time for epoch 92: 271.01. Rollout time: 98.77, Training time: 172.20
Evaluating epoch 92
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 92                    |
| policy/steps              | 4177389.0             |
| test/episodes             | 2325.0                |
| test/success_rate         | 1.0                   |
| test_0/avg_q              | -10.906484842750496   |
| test_1/avg_q              | -0.9750461942058382   |
| test_1/n_subgoals         | 68.0                  |
| test_1/subgoal_succ_rate  | 0.22058823529411764   |
| train/episodes            | 9300.0                |
| train/success_rate        | 0.82                  |
| train_0/avg_q             | -20.792432451693696   |
| train_0/current_q         | -11.145427408760185   |
| train_0/fw_bonus          | -0.9962915107607841   |
| train_0/fw_loss           | 0.0265293603297323    |
| train_0/mu_grads          | -0.12170249577611685  |
| train_0/mu_grads_std      | 0.7703220188617707    |
| train_0/mu_loss           | 10.88401921612755     |
| train_0/next_q            | -10.758219899385292   |
| train_0/q_grads           | -0.023220560932531952 |
| train_0/q_grads_std       | 0.552233612537384     |
| train_0/q_loss            | 0.9363338284925602    |
| train_0/reward            | -0.8723638929961453   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0013671875          |
| train_0/target_q          | -11.226889220775147   |
| train_1/avg_q             | -11.872064967279847   |
| train_1/current_q         | -3.014185661912949    |
| train_1/fw_bonus          | -0.9496293872594833   |
| train_1/fw_loss           | 0.40188167840242384   |
| train_1/mu_grads          | -0.1333086747676134   |
| train_1/mu_grads_std      | 0.5420209109783173    |
| train_1/mu_loss           | 1.0034458878821106    |
| train_1/n_subgoals        | 1384.0                |
| train_1/next_q            | -1.0626656384399915   |
| train_1/q_grads           | -0.18961309269070625  |
| train_1/q_grads_std       | 0.7444981202483177    |
| train_1/q_loss            | 3.229878960198346     |
| train_1/reward            | -2.256108516750828    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0                   |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.09971098265895954   |
| train_1/target_q          | -3.101368818620614    |
-----------------------------------------------------
New best value for test/success_rate: 1.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.99
Training epoch 93
Time for epoch 93: 271.45. Rollout time: 92.05, Training time: 179.37
Evaluating epoch 93
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
----------------------------------------------------
| epoch                     | 93                   |
| policy/steps              | 4210575.0            |
| test/episodes             | 2350.0               |
| test/success_rate         | 1.0                  |
| test_0/avg_q              | -10.56380699724593   |
| test_1/avg_q              | -1.61027174808831    |
| test_1/n_subgoals         | 80.0                 |
| test_1/subgoal_succ_rate  | 0.275                |
| train/episodes            | 9400.0               |
| train/success_rate        | 0.82                 |
| train_0/avg_q             | -20.032514061708966  |
| train_0/current_q         | -11.051995702554231  |
| train_0/fw_bonus          | -0.9962732136249542  |
| train_0/fw_loss           | 0.026655477890744805 |
| train_0/mu_grads          | -0.12054300028830767 |
| train_0/mu_grads_std      | 0.7731253042817116   |
| train_0/mu_loss           | 10.786737910613962   |
| train_0/next_q            | -10.66088041413025   |
| train_0/q_grads           | -0.02260044850409031 |
| train_0/q_grads_std       | 0.5544901624321937   |
| train_0/q_loss            | 0.9166330670330687   |
| train_0/reward            | -0.8703085384524456  |
| train_0/reward_-0.0_frac  | 0.0                  |
| train_0/reward_-1.0_frac  | 0.0018310546875      |
| train_0/target_q          | -11.131369062621914  |
| train_1/avg_q             | -11.005177830373771  |
| train_1/current_q         | -3.0569040558146883  |
| train_1/fw_bonus          | -0.9498679399490356  |
| train_1/fw_loss           | 0.4000613264739513   |
| train_1/mu_grads          | -0.1345893569290638  |
| train_1/mu_grads_std      | 0.5443497911095619   |
| train_1/mu_loss           | 0.9903919779164264   |
| train_1/n_subgoals        | 1263.0               |
| train_1/next_q            | -1.0508644916638292  |
| train_1/q_grads           | -0.19184955395758152 |
| train_1/q_grads_std       | 0.7479146793484688   |
| train_1/q_loss            | 3.348945528448509    |
| train_1/reward            | -2.308353215086754   |
| train_1/reward_-0.0_frac  | 0.0                  |
| train_1/reward_-1.0_frac  | 0.0                  |
| train_1/reward_-27.0_frac | 0.0                  |
| train_1/subgoal_succ_rate | 0.11559778305621536  |
| train_1/target_q          | -3.1339339675241833  |
----------------------------------------------------
New best value for test/success_rate: 1.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.99
Training epoch 94
Time for epoch 94: 273.67. Rollout time: 92.04, Training time: 181.61
Evaluating epoch 94
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 94                    |
| policy/steps              | 4244716.0             |
| test/episodes             | 2375.0                |
| test/success_rate         | 0.96                  |
| test_0/avg_q              | -14.401353977593148   |
| test_1/avg_q              | -1.440214827222041    |
| test_1/n_subgoals         | 110.0                 |
| test_1/subgoal_succ_rate  | 0.3090909090909091    |
| train/episodes            | 9500.0                |
| train/success_rate        | 0.83                  |
| train_0/avg_q             | -20.090588149142842   |
| train_0/current_q         | -11.163379251132152   |
| train_0/fw_bonus          | -0.9962749689817428   |
| train_0/fw_loss           | 0.026643295818939806  |
| train_0/mu_grads          | -0.12141308318823577  |
| train_0/mu_grads_std      | 0.7755047023296356    |
| train_0/mu_loss           | 10.889715834011913    |
| train_0/next_q            | -10.766744274929568   |
| train_0/q_grads           | -0.022323643043637275 |
| train_0/q_grads_std       | 0.5569554537534713    |
| train_0/q_loss            | 0.8806908581326042    |
| train_0/reward            | -0.8725245562160125   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0014892578125       |
| train_0/target_q          | -11.238595381959094   |
| train_1/avg_q             | -11.265670794449592   |
| train_1/current_q         | -2.9583117693762992   |
| train_1/fw_bonus          | -0.9487512186169624   |
| train_1/fw_loss           | 0.4085826352238655    |
| train_1/mu_grads          | -0.13568241223692895  |
| train_1/mu_grads_std      | 0.5456716671586037    |
| train_1/mu_loss           | 0.9200981258208832    |
| train_1/n_subgoals        | 1260.0                |
| train_1/next_q            | -0.978500354133988    |
| train_1/q_grads           | -0.19460496716201306  |
| train_1/q_grads_std       | 0.7517664700746536    |
| train_1/q_loss            | 3.52495886623133      |
| train_1/reward            | -2.2621878063262555   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0                   |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.10317460317460317   |
| train_1/target_q          | -3.0439472280632307   |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.98
Training epoch 95
Time for epoch 95: 261.93. Rollout time: 86.69, Training time: 175.21
Evaluating epoch 95
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
----------------------------------------------------
| epoch                     | 95                   |
| policy/steps              | 4276070.0            |
| test/episodes             | 2400.0               |
| test/success_rate         | 1.0                  |
| test_0/avg_q              | -10.921306418429072  |
| test_1/avg_q              | -1.1550757680449606  |
| test_1/n_subgoals         | 78.0                 |
| test_1/subgoal_succ_rate  | 0.2692307692307692   |
| train/episodes            | 9600.0               |
| train/success_rate        | 0.86                 |
| train_0/avg_q             | -20.082023457233838  |
| train_0/current_q         | -11.003708768767723  |
| train_0/fw_bonus          | -0.9962538912892341  |
| train_0/fw_loss           | 0.026788693899288774 |
| train_0/mu_grads          | -0.12305915281176567 |
| train_0/mu_grads_std      | 0.7783193573355675   |
| train_0/mu_loss           | 10.743716607268462   |
| train_0/next_q            | -10.609467969732396  |
| train_0/q_grads           | -0.02228122460655868 |
| train_0/q_grads_std       | 0.5592894539237022   |
| train_0/q_loss            | 0.9166281578493137   |
| train_0/reward            | -0.8731967741379776  |
| train_0/reward_-0.0_frac  | 0.0                  |
| train_0/reward_-1.0_frac  | 0.0013427734375      |
| train_0/target_q          | -11.091626275433555  |
| train_1/avg_q             | -11.263383718688676  |
| train_1/current_q         | -2.9511086640278017  |
| train_1/fw_bonus          | -0.9509987279772758  |
| train_1/fw_loss           | 0.3914326265454292   |
| train_1/mu_grads          | -0.1362342841923237  |
| train_1/mu_grads_std      | 0.5473850309848786   |
| train_1/mu_loss           | 0.9387788523363211   |
| train_1/n_subgoals        | 1178.0               |
| train_1/next_q            | -0.9925486684579905  |
| train_1/q_grads           | -0.19690565168857574 |
| train_1/q_grads_std       | 0.7549922674894333   |
| train_1/q_loss            | 3.354102006583022    |
| train_1/reward            | -2.239440357188505   |
| train_1/reward_-0.0_frac  | 0.0                  |
| train_1/reward_-1.0_frac  | 0.0                  |
| train_1/reward_-27.0_frac | 0.0                  |
| train_1/subgoal_succ_rate | 0.11205432937181664  |
| train_1/target_q          | -3.0274038595064994  |
----------------------------------------------------
New best value for test/success_rate: 1.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.99
Training epoch 96
Time for epoch 96: 277.97. Rollout time: 93.36, Training time: 184.58
Evaluating epoch 96
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 96                    |
| policy/steps              | 4313147.0             |
| test/episodes             | 2425.0                |
| test/success_rate         | 0.8                   |
| test_0/avg_q              | -18.775691568531567   |
| test_1/avg_q              | -3.5287567803448505   |
| test_1/n_subgoals         | 212.0                 |
| test_1/subgoal_succ_rate  | 0.20754716981132076   |
| train/episodes            | 9700.0                |
| train/success_rate        | 0.8                   |
| train_0/avg_q             | -20.657562900744168   |
| train_0/current_q         | -10.757562887390948   |
| train_0/fw_bonus          | -0.9963003933429718   |
| train_0/fw_loss           | 0.026468101190403104  |
| train_0/mu_grads          | -0.12322229500859976  |
| train_0/mu_grads_std      | 0.7813152953982353    |
| train_0/mu_loss           | 10.49510433897768     |
| train_0/next_q            | -10.358333312970764   |
| train_0/q_grads           | -0.022676646569743754 |
| train_0/q_grads_std       | 0.5615165442228317    |
| train_0/q_loss            | 0.8567388558888173    |
| train_0/reward            | -0.8689237317063089   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.001611328125        |
| train_0/target_q          | -10.833552420888434   |
| train_1/avg_q             | -11.841682279103507   |
| train_1/current_q         | -2.9775094737450845   |
| train_1/fw_bonus          | -0.950591504573822    |
| train_1/fw_loss           | 0.3945400632917881    |
| train_1/mu_grads          | -0.13746059723198414  |
| train_1/mu_grads_std      | 0.5488086894154549    |
| train_1/mu_loss           | 0.9570734012718296    |
| train_1/n_subgoals        | 1277.0                |
| train_1/next_q            | -1.007797958612382    |
| train_1/q_grads           | -0.1987967699766159   |
| train_1/q_grads_std       | 0.758463303744793     |
| train_1/q_loss            | 3.4021406973620927    |
| train_1/reward            | -2.260084398300387    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0                   |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.10258418167580266   |
| train_1/target_q          | -3.0602306244789994   |
-----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.94
Training epoch 97
Time for epoch 97: 264.49. Rollout time: 91.15, Training time: 173.31
Evaluating epoch 97
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 97                    |
| policy/steps              | 4346480.0             |
| test/episodes             | 2450.0                |
| test/success_rate         | 1.0                   |
| test_0/avg_q              | -10.5457388549172     |
| test_1/avg_q              | -1.4316079472942314   |
| test_1/n_subgoals         | 80.0                  |
| test_1/subgoal_succ_rate  | 0.3625                |
| train/episodes            | 9800.0                |
| train/success_rate        | 0.79                  |
| train_0/avg_q             | -20.690791681923322   |
| train_0/current_q         | -11.189374297025362   |
| train_0/fw_bonus          | -0.9963486596941948   |
| train_0/fw_loss           | 0.02613540068268776   |
| train_0/mu_grads          | -0.12491224892437458  |
| train_0/mu_grads_std      | 0.784250196814537     |
| train_0/mu_loss           | 10.937280314040635    |
| train_0/next_q            | -10.794883316284437   |
| train_0/q_grads           | -0.022843424696475267 |
| train_0/q_grads_std       | 0.5637629389762878    |
| train_0/q_loss            | 0.8641085025714441    |
| train_0/reward            | -0.8666835989177344   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0013916015625       |
| train_0/target_q          | -11.262019674593372   |
| train_1/avg_q             | -11.834640482239278   |
| train_1/current_q         | -2.9109343089537525   |
| train_1/fw_bonus          | -0.9501796543598175   |
| train_1/fw_loss           | 0.39768277630209925   |
| train_1/mu_grads          | -0.13940141089260577  |
| train_1/mu_grads_std      | 0.5506733447313309    |
| train_1/mu_loss           | 0.98117820733939      |
| train_1/n_subgoals        | 1260.0                |
| train_1/next_q            | -1.0382613757260792   |
| train_1/q_grads           | -0.20113025456666947  |
| train_1/q_grads_std       | 0.7620205104351043    |
| train_1/q_loss            | 3.526702712493851     |
| train_1/reward            | -2.1803305564397304   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0                   |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.10873015873015873   |
| train_1/target_q          | -3.0068087238962584   |
-----------------------------------------------------
New best value for test/success_rate: 1.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.94
Training epoch 98
Time for epoch 98: 283.24. Rollout time: 104.34, Training time: 178.87
Evaluating epoch 98
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
----------------------------------------------------
| epoch                     | 98                   |
| policy/steps              | 4386281.0            |
| test/episodes             | 2475.0               |
| test/success_rate         | 0.88                 |
| test_0/avg_q              | -18.291527817106548  |
| test_1/avg_q              | -4.123139847564725   |
| test_1/n_subgoals         | 149.0                |
| test_1/subgoal_succ_rate  | 0.1476510067114094   |
| train/episodes            | 9900.0               |
| train/success_rate        | 0.77                 |
| train_0/avg_q             | -20.993576423190675  |
| train_0/current_q         | -10.961313050785293  |
| train_0/fw_bonus          | -0.9963618203997612  |
| train_0/fw_loss           | 0.026044650794938208 |
| train_0/mu_grads          | -0.12472618333995342 |
| train_0/mu_grads_std      | 0.7869030997157097   |
| train_0/mu_loss           | 10.705257111572632   |
| train_0/next_q            | -10.578079029621405  |
| train_0/q_grads           | -0.02273383070714772 |
| train_0/q_grads_std       | 0.5663997501134872   |
| train_0/q_loss            | 0.8806834712051916   |
| train_0/reward            | -0.8631895753725984  |
| train_0/reward_-0.0_frac  | 0.0                  |
| train_0/reward_-1.0_frac  | 0.0015625            |
| train_0/target_q          | -11.033107158903599  |
| train_1/avg_q             | -11.893686153071103  |
| train_1/current_q         | -3.041655125795389   |
| train_1/fw_bonus          | -0.9485105857253074  |
| train_1/fw_loss           | 0.4104189172387123   |
| train_1/mu_grads          | -0.14083273969590665 |
| train_1/mu_grads_std      | 0.5523383468389511   |
| train_1/mu_loss           | 1.0326599134101098   |
| train_1/n_subgoals        | 1433.0               |
| train_1/next_q            | -1.0891177364751445  |
| train_1/q_grads           | -0.20316135995090007 |
| train_1/q_grads_std       | 0.765945540368557    |
| train_1/q_loss            | 3.665019104221392    |
| train_1/reward            | -2.2693786217230807  |
| train_1/reward_-0.0_frac  | 0.0                  |
| train_1/reward_-1.0_frac  | 0.0                  |
| train_1/reward_-27.0_frac | 0.0                  |
| train_1/subgoal_succ_rate | 0.11025819958129798  |
| train_1/target_q          | -3.1371277981582453  |
----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.9199999999999999
Training epoch 99
Time for epoch 99: 273.96. Rollout time: 92.52, Training time: 181.41
Evaluating epoch 99
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|101
----------------------------------------------------
| epoch                     | 99                   |
| policy/steps              | 4420332.0            |
| test/episodes             | 2500.0               |
| test/success_rate         | 0.96                 |
| test_0/avg_q              | -12.076860389766397  |
| test_1/avg_q              | -2.0427658547776697  |
| test_1/n_subgoals         | 101.0                |
| test_1/subgoal_succ_rate  | 0.2079207920792079   |
| train/episodes            | 10000.0              |
| train/success_rate        | 0.85                 |
| train_0/avg_q             | -19.567996980932207  |
| train_0/current_q         | -11.12165578096131   |
| train_0/fw_bonus          | -0.9963316932320595  |
| train_0/fw_loss           | 0.02625230518169701  |
| train_0/mu_grads          | -0.1252179078757763  |
| train_0/mu_grads_std      | 0.7893274754285813   |
| train_0/mu_loss           | 10.885029032209815   |
| train_0/next_q            | -10.735066953524399  |
| train_0/q_grads           | -0.02334609949029982 |
| train_0/q_grads_std       | 0.5685720890760422   |
| train_0/q_loss            | 0.9412697207241155   |
| train_0/reward            | -0.8658553849494638  |
| train_0/reward_-0.0_frac  | 0.0                  |
| train_0/reward_-1.0_frac  | 0.0015869140625      |
| train_0/target_q          | -11.18517395634396   |
| train_1/avg_q             | -10.549028552913434  |
| train_1/current_q         | -2.9502605691395547  |
| train_1/fw_bonus          | -0.9487279459834099  |
| train_1/fw_loss           | 0.40876022800803186  |
| train_1/mu_grads          | -0.14194899126887323 |
| train_1/mu_grads_std      | 0.5552171096205711   |
| train_1/mu_loss           | 0.953944562930127    |
| train_1/n_subgoals        | 1272.0               |
| train_1/next_q            | -1.001287722482505   |
| train_1/q_grads           | -0.20428625121712685 |
| train_1/q_grads_std       | 0.7695859208703041   |
| train_1/q_loss            | 3.5658012398090575   |
| train_1/reward            | -2.240703716560165   |
| train_1/reward_-0.0_frac  | 0.0                  |
| train_1/reward_-1.0_frac  | 0.0                  |
| train_1/reward_-27.0_frac | 0.0                  |
| train_1/subgoal_succ_rate | 0.12106918238993711  |
| train_1/target_q          | -3.0383720898542377  |
----------------------------------------------------
Mean of test/success_rate of last 4 epochs: 0.91
All epochs are finished. Stopping the training now.
