Starting process id: 83141
T: 700
alg: chac
algorithm: src.chac
atomic_noise: 0.2
base_logdir: data
batch_size: 1024
bind_core: 0
buffer_size: 500
chac_params: {}
early_stop_data_column: test/success_rate
early_stop_threshold: 100.0
env_name: AntReacherEnv-v0
eta: 0.75
fw: 1
fw_hidden_size: 256,256,256
fw_lr: 0.001
gamma: 0.9985714285714286
graph: 1
info: 
make_env: <function prepare_params.<locals>.make_env at 0x7fa459dc90e0>
max_try_idx: 199
mu_hidden_size: 64
mu_lr: 0.001
n_episodes: 100
n_levels: 2
n_pre_episodes: 30
n_test_rollouts: 25
n_train_batches: 40
n_train_rollouts: 100
num_threads: 1
q_hidden_size: 64
q_lr: 0.001
random_action_perc: 0.3
regularization: False
render: 0
rollout_batch_size: 1
subgoal_noise: 0.2
subgoal_test_perc: 0.3
time_scales: 27,27
try_start_idx: 100
use_mpi: False
verbose: False

*** Warning ***
You are running src.chac with just a single MPI worker. This will work, but the HER experiments that we report in Plappert et al. (2018, https://arxiv.org/abs/1802.09464) were obtained with --num_cpu 19. This makes a significant difference and if you are looking to reproduce those results, be aware of this. Please also refer to https://github.com/openai/baselines/issues/314 for further details.
****************

dims: action = 8, subgoal = 5, end_goal = 3
subgoal_bounds: symmetric [11.75 11.75  0.5   3.    3.  ], offset [0.  0.  0.5 0.  0. ]
Running on CPU ...
Creating a CHAC agent

Hierarchy Level 0 with time scale 27
Actor(
  (fc1): Linear(in_features=34, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
  (fc4): Linear(in_features=64, out_features=8, bias=True)
)
Critic(
  (fc1): Linear(in_features=42, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
  (fc4): Linear(in_features=64, out_features=1, bias=True)
  (mse_loss): MSELoss()
)
ForwardModel(
  (mlp): Sequential(
    (0): Linear(in_features=37, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=256, bias=True)
    (5): ReLU()
    (6): Linear(in_features=256, out_features=29, bias=True)
    (7): Identity()
  )
  (mse_loss): MSELoss()
)

Hierarchy Level 1 with time scale 27
Actor(
  (fc1): Linear(in_features=32, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
  (fc4): Linear(in_features=64, out_features=5, bias=True)
)
Critic(
  (fc1): Linear(in_features=37, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
  (fc4): Linear(in_features=64, out_features=1, bias=True)
  (mse_loss): MSELoss()
)
ForwardModel(
  (mlp): Sequential(
    (0): Linear(in_features=34, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=256, bias=True)
    (5): ReLU()
    (6): Linear(in_features=256, out_features=29, bias=True)
    (7): Identity()
  )
  (mse_loss): MSELoss()
)
Training epoch 0
Time for epoch 0: 1204.07. Rollout time: 673.97, Training time: 529.95
Evaluating epoch 0
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 0                      |
| policy/steps              | 90964.0                |
| test/episodes             | 25.0                   |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -2.0279394350864086    |
| test_1/avg_q              | -6.539730991383965     |
| test_1/n_subgoals         | 3430.0                 |
| test_1/subgoal_succ_rate  | 0.8341107871720117     |
| train/episodes            | 100.0                  |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -8.383457658848643     |
| train_0/current_q         | -8.203445708067266     |
| train_0/fw_bonus          | -0.9956570535898208    |
| train_0/fw_loss           | 0.03147137695923448    |
| train_0/mu_grads          | 0.010245612962171436   |
| train_0/mu_grads_std      | 0.11877429094165563    |
| train_0/mu_loss           | 8.204493450812787      |
| train_0/next_q            | -8.216320119716311     |
| train_0/q_grads           | 0.010405544959940016   |
| train_0/q_grads_std       | 0.1306012634187937     |
| train_0/q_loss            | 1.3255258172069628     |
| train_0/reward            | -0.6070502475038666    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.0129638671875        |
| train_0/target_q          | -8.313181494996162     |
| train_1/avg_q             | -10.466564697590758    |
| train_1/current_q         | -5.434007366335619     |
| train_1/fw_bonus          | -0.9932864502072334    |
| train_1/fw_loss           | 0.07526302486658096    |
| train_1/mu_grads          | -0.0007253026589751244 |
| train_1/mu_grads_std      | 0.11868198160082102    |
| train_1/mu_loss           | 3.623478711514147      |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -3.617249177125146     |
| train_1/q_grads           | 0.022855620877817274   |
| train_1/q_grads_std       | 0.12338018510490656    |
| train_1/q_loss            | 1.1260651804388346     |
| train_1/reward            | -2.6611573041522205    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0110107421875        |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0025925925925925925  |
| train_1/target_q          | -5.394810197366906     |
------------------------------------------------------
Saving periodic policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_0.pkl ...
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Training epoch 1
Time for epoch 1: 1114.00. Rollout time: 691.23, Training time: 422.56
Evaluating epoch 1
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
-----------------------------------------------------
| epoch                     | 1                     |
| policy/steps              | 181364.0              |
| test/episodes             | 50.0                  |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -1.879655487949961    |
| test_1/avg_q              | -7.58578328181736     |
| test_1/n_subgoals         | 16040.0               |
| test_1/subgoal_succ_rate  | 0.9947630922693267    |
| train/episodes            | 200.0                 |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -21.308297755541165   |
| train_0/current_q         | -8.532988548688724    |
| train_0/fw_bonus          | -0.9979006692767143   |
| train_0/fw_loss           | 0.015652289567515253  |
| train_0/mu_grads          | 0.010403197281993926  |
| train_0/mu_grads_std      | 0.11883577052503824   |
| train_0/mu_loss           | 8.621028315569413     |
| train_0/next_q            | -8.614293788385455    |
| train_0/q_grads           | 0.004067843605298549  |
| train_0/q_grads_std       | 0.1477672252804041    |
| train_0/q_loss            | 1.9139128495546611    |
| train_0/reward            | -0.5894837105639453   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0547607421875       |
| train_0/target_q          | -8.65209001513573     |
| train_1/avg_q             | -18.02859336482127    |
| train_1/current_q         | -5.429429539610289    |
| train_1/fw_bonus          | -0.9913293153047562   |
| train_1/fw_loss           | 0.09062948990613222   |
| train_1/mu_grads          | -0.003644296247512102 |
| train_1/mu_grads_std      | 0.1344438198953867    |
| train_1/mu_loss           | 3.6441520713060513    |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -3.6438027582962405   |
| train_1/q_grads           | 0.0201997896656394    |
| train_1/q_grads_std       | 0.13608481772243977   |
| train_1/q_loss            | 1.035538415003694     |
| train_1/reward            | -2.6715627980142016   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.009033203125        |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.013333333333333334  |
| train_1/target_q          | -5.389876146791633    |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Training epoch 2
Time for epoch 2: 1027.81. Rollout time: 690.34, Training time: 337.31
Evaluating epoch 2
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
----------------------------------------------------
| epoch                     | 2                    |
| policy/steps              | 271742.0             |
| test/episodes             | 75.0                 |
| test/success_rate         | 0.0                  |
| test_0/avg_q              | -1.693962753221262   |
| test_1/avg_q              | -11.731873417702715  |
| test_1/n_subgoals         | 12428.0              |
| test_1/subgoal_succ_rate  | 0.9820566462825877   |
| train/episodes            | 300.0                |
| train/success_rate        | 0.0                  |
| train_0/avg_q             | -23.739971798830524  |
| train_0/current_q         | -8.553488058101374   |
| train_0/fw_bonus          | -0.9985772997140885  |
| train_0/fw_loss           | 0.010881540551781654 |
| train_0/mu_grads          | 0.010397935751825571 |
| train_0/mu_grads_std      | 0.1188345642760396   |
| train_0/mu_loss           | 8.451819216205644    |
| train_0/next_q            | -8.498649224414587   |
| train_0/q_grads           | 0.004576285439543426 |
| train_0/q_grads_std       | 0.16239472553133966  |
| train_0/q_loss            | 1.1002356209811883   |
| train_0/reward            | -0.5866865605868952  |
| train_0/reward_-0.0_frac  | 0.0                  |
| train_0/reward_-1.0_frac  | 0.096484375          |
| train_0/target_q          | -8.692441837412897   |
| train_1/avg_q             | -18.85180020684925   |
| train_1/current_q         | -6.379808263272983   |
| train_1/fw_bonus          | -0.9903541818261147  |
| train_1/fw_loss           | 0.09828581567853689  |
| train_1/mu_grads          | -0.00426555824233219 |
| train_1/mu_grads_std      | 0.14585348814725876  |
| train_1/mu_loss           | 4.974910624438408    |
| train_1/n_subgoals        | 2700.0               |
| train_1/next_q            | -4.975493967967033   |
| train_1/q_grads           | 0.01680344850756228  |
| train_1/q_grads_std       | 0.1444017607718706   |
| train_1/q_loss            | 0.7400148761669721   |
| train_1/reward            | -2.6108942071001366  |
| train_1/reward_-0.0_frac  | 0.0                  |
| train_1/reward_-1.0_frac  | 0.0072998046875      |
| train_1/reward_-27.0_frac | 0.0                  |
| train_1/subgoal_succ_rate | 0.013333333333333334 |
| train_1/target_q          | -6.340873994277773   |
----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Training epoch 3
Time for epoch 3: 947.24. Rollout time: 616.24, Training time: 330.81
Evaluating epoch 3
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
-----------------------------------------------------
| epoch                     | 3                     |
| policy/steps              | 362405.0              |
| test/episodes             | 100.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -12.043653217582632   |
| test_1/avg_q              | -12.731185802840539   |
| test_1/n_subgoals         | 8293.0                |
| test_1/subgoal_succ_rate  | 0.9539370553478838    |
| train/episodes            | 400.0                 |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -24.440662533481557   |
| train_0/current_q         | -9.432634081472258    |
| train_0/fw_bonus          | -0.9988713607192039   |
| train_0/fw_loss           | 0.008808195171877743  |
| train_0/mu_grads          | 0.010393862635828555  |
| train_0/mu_grads_std      | 0.11883167866617442   |
| train_0/mu_loss           | 9.491265818641343     |
| train_0/next_q            | -9.479966427568565    |
| train_0/q_grads           | 0.006683764746412635  |
| train_0/q_grads_std       | 0.17461831644177436   |
| train_0/q_loss            | 1.3865477665583923    |
| train_0/reward            | -0.5881699885347189   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.069287109375        |
| train_0/target_q          | -9.586106783713067    |
| train_1/avg_q             | -19.869130399944115   |
| train_1/current_q         | -7.012941042548202    |
| train_1/fw_bonus          | -0.9893417477607727   |
| train_1/fw_loss           | 0.10623492021113634   |
| train_1/mu_grads          | -0.003431435296079144 |
| train_1/mu_grads_std      | 0.1545665442943573    |
| train_1/mu_loss           | 5.79273199219614      |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -5.801350768782399    |
| train_1/q_grads           | 0.013988957065157593  |
| train_1/q_grads_std       | 0.1554555032402277    |
| train_1/q_loss            | 0.48109955923021663   |
| train_1/reward            | -2.611739733134891    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0069580078125       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.01                  |
| train_1/target_q          | -6.975500315350496    |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 4
Time for epoch 4: 2950.09. Rollout time: 2568.82, Training time: 381.09
Evaluating epoch 4
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
-----------------------------------------------------
| epoch                     | 4                     |
| policy/steps              | 452957.0              |
| test/episodes             | 125.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -14.482833949530539   |
| test_1/avg_q              | -11.583542331899748   |
| test_1/n_subgoals         | 1377.0                |
| test_1/subgoal_succ_rate  | 0.5294117647058824    |
| train/episodes            | 500.0                 |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.018437075559426   |
| train_0/current_q         | -9.658778638061921    |
| train_0/fw_bonus          | -0.9989625498652458   |
| train_0/fw_loss           | 0.00816528252325952   |
| train_0/mu_grads          | 0.010294183110818266  |
| train_0/mu_grads_std      | 0.12417569886893035   |
| train_0/mu_loss           | 9.68931643698559      |
| train_0/next_q            | -9.688042959749776    |
| train_0/q_grads           | 0.006511447043158114  |
| train_0/q_grads_std       | 0.1813075676560402    |
| train_0/q_loss            | 1.3668652463418236    |
| train_0/reward            | -0.5863617296301527   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0583251953125       |
| train_0/target_q          | -9.804715049898254    |
| train_1/avg_q             | -19.844548886932234   |
| train_1/current_q         | -6.903095330703152    |
| train_1/fw_bonus          | -0.9885017216205597   |
| train_1/fw_loss           | 0.11283038817346096   |
| train_1/mu_grads          | -0.004738586768507957 |
| train_1/mu_grads_std      | 0.16039393171668054   |
| train_1/mu_loss           | 5.663053047232011     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -5.659373284336293    |
| train_1/q_grads           | 0.01042863680049777   |
| train_1/q_grads_std       | 0.1623582925647497    |
| train_1/q_loss            | 0.4448003751164058    |
| train_1/reward            | -2.6063295321462645   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0068115234375       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.01                  |
| train_1/target_q          | -6.876830150869756    |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 5
Time for epoch 5: 1039.23. Rollout time: 652.43, Training time: 386.62
Evaluating epoch 5
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 5                      |
| policy/steps              | 543603.0               |
| test/episodes             | 150.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -0.0009126502034017131 |
| test_1/avg_q              | -12.595252374027583    |
| test_1/n_subgoals         | 3433.0                 |
| test_1/subgoal_succ_rate  | 0.8342557529857267     |
| train/episodes            | 600.0                  |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -25.09724220175554     |
| train_0/current_q         | -0.0038889437921651544 |
| train_0/fw_bonus          | -0.9989982977509498    |
| train_0/fw_loss           | 0.007913122675381601   |
| train_0/mu_grads          | 0.00996356273535639    |
| train_0/mu_grads_std      | 0.1320035308599472     |
| train_0/mu_loss           | 0.0032363659578158134  |
| train_0/next_q            | -0.0032474678763163185 |
| train_0/q_grads           | 0.004077747650444508   |
| train_0/q_grads_std       | 0.18655702844262123    |
| train_0/q_loss            | 0.4832718595865947     |
| train_0/reward            | -0.5880918620718149    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.0802001953125        |
| train_0/target_q          | -0.5884227265240721    |
| train_1/avg_q             | -19.662764529237045    |
| train_1/current_q         | -6.993990152238437     |
| train_1/fw_bonus          | -0.9881124004721642    |
| train_1/fw_loss           | 0.11588716767728328    |
| train_1/mu_grads          | -0.005243911501020193  |
| train_1/mu_grads_std      | 0.16871238313615322    |
| train_1/mu_loss           | 5.766316033104929      |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -5.7572111237707935    |
| train_1/q_grads           | 0.006640687794424593   |
| train_1/q_grads_std       | 0.1712905254215002     |
| train_1/q_loss            | 0.40580265722167247    |
| train_1/reward            | -2.612311303538809     |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0065673828125        |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.01037037037037037    |
| train_1/target_q          | -6.969360533974623     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 6
Time for epoch 6: 900.38. Rollout time: 582.10, Training time: 318.15
Evaluating epoch 6
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
-----------------------------------------------------
| epoch                     | 6                     |
| policy/steps              | 634089.0              |
| test/episodes             | 175.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -7.3916699792535825   |
| test_1/avg_q              | -12.296519399202413   |
| test_1/n_subgoals         | 3457.0                |
| test_1/subgoal_succ_rate  | 0.8356956899045415    |
| train/episodes            | 700.0                 |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -16.462336394255495   |
| train_0/current_q         | -9.601562014566614    |
| train_0/fw_bonus          | -0.9990218788385391   |
| train_0/fw_loss           | 0.007746930432040245  |
| train_0/mu_grads          | 0.009281955729238688  |
| train_0/mu_grads_std      | 0.13551203422248365   |
| train_0/mu_loss           | 9.614470312054507     |
| train_0/next_q            | -9.611758527522       |
| train_0/q_grads           | 0.0023421351215802133 |
| train_0/q_grads_std       | 0.20027967542409897   |
| train_0/q_loss            | 0.9116592643954309    |
| train_0/reward            | -0.5952022469351504   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0556884765625       |
| train_0/target_q          | -9.750029593645285    |
| train_1/avg_q             | -19.865065020704503   |
| train_1/current_q         | -6.967648374163811    |
| train_1/fw_bonus          | -0.9888604536652565   |
| train_1/fw_loss           | 0.11001381408423186   |
| train_1/mu_grads          | -0.006601882737595588 |
| train_1/mu_grads_std      | 0.178238832205534     |
| train_1/mu_loss           | 5.701912434194329     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -5.666719504660866    |
| train_1/q_grads           | 0.002917571528814733  |
| train_1/q_grads_std       | 0.18210065327584743   |
| train_1/q_loss            | 0.4738597562895208    |
| train_1/reward            | -2.6444135979982093   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0066650390625       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.012222222222222223  |
| train_1/target_q          | -6.943248583977635    |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 7
Time for epoch 7: 970.13. Rollout time: 640.69, Training time: 329.29
Evaluating epoch 7
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 7                      |
| policy/steps              | 724409.0               |
| test/episodes             | 200.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -2.653338397043917     |
| test_1/avg_q              | -11.248927004427244    |
| test_1/n_subgoals         | 2780.0                 |
| test_1/subgoal_succ_rate  | 0.7863309352517985     |
| train/episodes            | 800.0                  |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -26.003661495960895    |
| train_0/current_q         | -9.401875164113452     |
| train_0/fw_bonus          | -0.9989617362618446    |
| train_0/fw_loss           | 0.008170925790909677   |
| train_0/mu_grads          | 0.01051205766852945    |
| train_0/mu_grads_std      | 0.15057279020547867    |
| train_0/mu_loss           | 9.394144760676166      |
| train_0/next_q            | -9.394555929181964     |
| train_0/q_grads           | 0.00047104517580009997 |
| train_0/q_grads_std       | 0.20253562442958356    |
| train_0/q_loss            | 0.8492984613758177     |
| train_0/reward            | -0.6025663635231467    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.070751953125         |
| train_0/target_q          | -9.613349243054634     |
| train_1/avg_q             | -20.229695477870393    |
| train_1/current_q         | -6.559172416974825     |
| train_1/fw_bonus          | -0.9892834484577179    |
| train_1/fw_loss           | 0.10669266190379859    |
| train_1/mu_grads          | -0.008175842999480664  |
| train_1/mu_grads_std      | 0.18180664852261544    |
| train_1/mu_loss           | 5.133290343270591      |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -5.129726888316185     |
| train_1/q_grads           | -0.0006351802498102188 |
| train_1/q_grads_std       | 0.19431878440082073    |
| train_1/q_loss            | 0.3886201071609235     |
| train_1/reward            | -2.640838928029916     |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0068603515625        |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.014444444444444444   |
| train_1/target_q          | -6.53067379354429      |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 8
Time for epoch 8: 1075.14. Rollout time: 704.80, Training time: 370.12
Evaluating epoch 8
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 8                      |
| policy/steps              | 814689.0               |
| test/episodes             | 225.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -2.879434706957925     |
| test_1/avg_q              | -12.449223540572888    |
| test_1/n_subgoals         | 3406.0                 |
| test_1/subgoal_succ_rate  | 0.8326482677627716     |
| train/episodes            | 900.0                  |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -24.547178109777114    |
| train_0/current_q         | -4.3551337503962255    |
| train_0/fw_bonus          | -0.9990014776587486    |
| train_0/fw_loss           | 0.007890796510037035   |
| train_0/mu_grads          | 0.010031137708574533   |
| train_0/mu_grads_std      | 0.15589391738176345    |
| train_0/mu_loss           | 4.425825962209072      |
| train_0/next_q            | -4.39121485699132      |
| train_0/q_grads           | -0.005317396891769022  |
| train_0/q_grads_std       | 0.21193338222801686    |
| train_0/q_loss            | 1.0791387829699162     |
| train_0/reward            | -0.611017458376591     |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.0537841796875        |
| train_0/target_q          | -4.7138014341873475    |
| train_1/avg_q             | -19.91220445878506     |
| train_1/current_q         | -6.3648412925541065    |
| train_1/fw_bonus          | -0.990191350877285     |
| train_1/fw_loss           | 0.09956417102366685    |
| train_1/mu_grads          | -0.009126934804953635  |
| train_1/mu_grads_std      | 0.18544908091425896    |
| train_1/mu_loss           | 4.843970009249595      |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -4.833145485809311     |
| train_1/q_grads           | -0.0032807274314109237 |
| train_1/q_grads_std       | 0.20744118168950082    |
| train_1/q_loss            | 0.4042795195914438     |
| train_1/reward            | -2.668568347884866     |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.00693359375          |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.016296296296296295   |
| train_1/target_q          | -6.350411679010768     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 9
Time for epoch 9: 6067.76. Rollout time: 5730.82, Training time: 336.72
Evaluating epoch 9
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 9                      |
| policy/steps              | 905178.0               |
| test/episodes             | 250.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -8.836509765998513e-05 |
| test_1/avg_q              | -11.752043072997198    |
| test_1/n_subgoals         | 4158.0                 |
| test_1/subgoal_succ_rate  | 0.8698893698893699     |
| train/episodes            | 1000.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -19.510200529362557    |
| train_0/current_q         | -0.1800771176410061    |
| train_0/fw_bonus          | -0.9989728480577469    |
| train_0/fw_loss           | 0.008092545077670366   |
| train_0/mu_grads          | 0.012287184596061707   |
| train_0/mu_grads_std      | 0.16794417500495912    |
| train_0/mu_loss           | 0.1728108729254491     |
| train_0/next_q            | -0.17442510887669951   |
| train_0/q_grads           | -0.009352629189379513  |
| train_0/q_grads_std       | 0.21777518689632416    |
| train_0/q_loss            | 0.6457368513941759     |
| train_0/reward            | -0.6192683615350688    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.0687255859375        |
| train_0/target_q          | -0.7898414357354105    |
| train_1/avg_q             | -20.124767157734325    |
| train_1/current_q         | -6.3743911854356305    |
| train_1/fw_bonus          | -0.9906605005264282    |
| train_1/fw_loss           | 0.09588076341897249    |
| train_1/mu_grads          | -0.010193220665678382  |
| train_1/mu_grads_std      | 0.18924339450895786    |
| train_1/mu_loss           | 4.866299899079451      |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -4.856316545080088     |
| train_1/q_grads           | -0.00707690273411572   |
| train_1/q_grads_std       | 0.22030785195529462    |
| train_1/q_loss            | 0.3303724461404929     |
| train_1/reward            | -2.6698575054033427    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.005419921875         |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.012222222222222223   |
| train_1/target_q          | -6.361019397796589     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 10
Time for epoch 10: 1056.65. Rollout time: 677.69, Training time: 378.79
Evaluating epoch 10
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
-----------------------------------------------------
| epoch                     | 10                    |
| policy/steps              | 995804.0              |
| test/episodes             | 275.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -18.26986822696272    |
| test_1/avg_q              | -10.765491571016176   |
| test_1/n_subgoals         | 1351.0                |
| test_1/subgoal_succ_rate  | 0.5196150999259808    |
| train/episodes            | 1100.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -15.736082841029788   |
| train_0/current_q         | -9.6224980326238      |
| train_0/fw_bonus          | -0.9990613371133804   |
| train_0/fw_loss           | 0.007468744134530425  |
| train_0/mu_grads          | 0.012345767137594521  |
| train_0/mu_grads_std      | 0.16792306303977966   |
| train_0/mu_loss           | 9.687839662097968     |
| train_0/next_q            | -9.662245510618558    |
| train_0/q_grads           | -0.008333984948694705 |
| train_0/q_grads_std       | 0.22114304043352603   |
| train_0/q_loss            | 1.3410695622888875    |
| train_0/reward            | -0.6124606460416544   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0553955078125       |
| train_0/target_q          | -9.81518936158408     |
| train_1/avg_q             | -19.82843610203179    |
| train_1/current_q         | -6.70920475325263     |
| train_1/fw_bonus          | -0.9913394972681999   |
| train_1/fw_loss           | 0.09054963029921055   |
| train_1/mu_grads          | -0.012175894412212073 |
| train_1/mu_grads_std      | 0.19272330589592457   |
| train_1/mu_loss           | 5.327212139221423     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -5.323356615025331    |
| train_1/q_grads           | -0.010837701288983226 |
| train_1/q_grads_std       | 0.22901297099888324   |
| train_1/q_loss            | 0.32475589115643694   |
| train_1/reward            | -2.664607332037849    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.00537109375         |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.00962962962962963   |
| train_1/target_q          | -6.698652389298319    |
-----------------------------------------------------
Saving periodic policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_10.pkl ...
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 11
Time for epoch 11: 1103.11. Rollout time: 700.67, Training time: 402.25
Evaluating epoch 11
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
-----------------------------------------------------
| epoch                     | 11                    |
| policy/steps              | 1086200.0             |
| test/episodes             | 300.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -12.877772427385285   |
| test_1/avg_q              | -13.967564015524179   |
| test_1/n_subgoals         | 2703.0                |
| test_1/subgoal_succ_rate  | 0.779134295227525     |
| train/episodes            | 1200.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.543679422100936   |
| train_0/current_q         | -9.877349422060519    |
| train_0/fw_bonus          | -0.9991026416420936   |
| train_0/fw_loss           | 0.007177433359902352  |
| train_0/mu_grads          | 0.012345273629762232  |
| train_0/mu_grads_std      | 0.16792291402816772   |
| train_0/mu_loss           | 9.931435139161955     |
| train_0/next_q            | -9.912079317020424    |
| train_0/q_grads           | -0.00836104869376868  |
| train_0/q_grads_std       | 0.22463805750012397   |
| train_0/q_loss            | 1.328589716726278     |
| train_0/reward            | -0.6133132601455145   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0461181640625       |
| train_0/target_q          | -10.056256308653579   |
| train_1/avg_q             | -20.20745032966157    |
| train_1/current_q         | -6.659841211549737    |
| train_1/fw_bonus          | -0.9918713331222534   |
| train_1/fw_loss           | 0.08637376986443997   |
| train_1/mu_grads          | -0.014685114612802863 |
| train_1/mu_grads_std      | 0.1965074509382248    |
| train_1/mu_loss           | 5.315978732020308     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -5.302705912305314    |
| train_1/q_grads           | -0.015079984441399574 |
| train_1/q_grads_std       | 0.23849144876003264   |
| train_1/q_loss            | 0.34099710835713576   |
| train_1/reward            | -2.619110413098679    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.006396484375        |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.014074074074074074  |
| train_1/target_q          | -6.655473899702602    |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 12
Time for epoch 12: 1000.57. Rollout time: 622.53, Training time: 377.74
Evaluating epoch 12
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
-----------------------------------------------------
| epoch                     | 12                    |
| policy/steps              | 1176613.0             |
| test/episodes             | 325.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -20.63230010882714    |
| test_1/avg_q              | -13.163819984064762   |
| test_1/n_subgoals         | 1351.0                |
| test_1/subgoal_succ_rate  | 0.5196150999259808    |
| train/episodes            | 1300.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.5456039978765     |
| train_0/current_q         | -9.968324320874922    |
| train_0/fw_bonus          | -0.9992068752646446   |
| train_0/fw_loss           | 0.006442617543507367  |
| train_0/mu_grads          | 0.012341406452469528  |
| train_0/mu_grads_std      | 0.16792126446962358   |
| train_0/mu_loss           | 10.038775015493039    |
| train_0/next_q            | -10.02580086035098    |
| train_0/q_grads           | -0.008098663110285998 |
| train_0/q_grads_std       | 0.22965845204889773   |
| train_0/q_loss            | 1.7788472783525626    |
| train_0/reward            | -0.6111054282460827   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0438232421875       |
| train_0/target_q          | -10.120994255594661   |
| train_1/avg_q             | -19.886684534202832   |
| train_1/current_q         | -6.363617374119935    |
| train_1/fw_bonus          | -0.9922643199563026   |
| train_1/fw_loss           | 0.08328820243477822   |
| train_1/mu_grads          | -0.015625663893297316 |
| train_1/mu_grads_std      | 0.1999890383332968    |
| train_1/mu_loss           | 4.855060181308289     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -4.843520589464224    |
| train_1/q_grads           | -0.019095339719206093 |
| train_1/q_grads_std       | 0.24797577001154422   |
| train_1/q_loss            | 0.2591327717888953    |
| train_1/reward            | -2.6929635448035696   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0067626953125       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.014074074074074074  |
| train_1/target_q          | -6.350111088577352    |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 13
Time for epoch 13: 1120.65. Rollout time: 722.11, Training time: 398.27
Evaluating epoch 13
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
-----------------------------------------------------
| epoch                     | 13                    |
| policy/steps              | 1267085.0             |
| test/episodes             | 350.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -18.0422060635099     |
| test_1/avg_q              | -12.126973154332054   |
| test_1/n_subgoals         | 676.0                 |
| test_1/subgoal_succ_rate  | 0.0014792899408284023 |
| train/episodes            | 1400.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.66767427736985    |
| train_0/current_q         | -9.89724065064        |
| train_0/fw_bonus          | -0.9992732644081116   |
| train_0/fw_loss           | 0.005974508041981607  |
| train_0/mu_grads          | 0.0068094493704847995 |
| train_0/mu_grads_std      | 0.16964901685714723   |
| train_0/mu_loss           | 9.974539436359137     |
| train_0/next_q            | -9.966030447928683    |
| train_0/q_grads           | -0.00881152122747153  |
| train_0/q_grads_std       | 0.23753097504377366   |
| train_0/q_loss            | 1.5261382067799976    |
| train_0/reward            | -0.6125502489874635   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0291259765625       |
| train_0/target_q          | -10.063783514696407   |
| train_1/avg_q             | -20.0912699241822     |
| train_1/current_q         | -6.445929551372993    |
| train_1/fw_bonus          | -0.992532467842102    |
| train_1/fw_loss           | 0.08118292652070522   |
| train_1/mu_grads          | -0.018319196067750454 |
| train_1/mu_grads_std      | 0.20493430458009243   |
| train_1/mu_loss           | 5.023355805154695     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -5.0203731273337295   |
| train_1/q_grads           | -0.024400544445961715 |
| train_1/q_grads_std       | 0.25876816362142563   |
| train_1/q_loss            | 0.25739265169029063   |
| train_1/reward            | -2.6544868762073746   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.005908203125        |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.012592592592592593  |
| train_1/target_q          | -6.4376644716362845   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 14
Time for epoch 14: 1064.81. Rollout time: 678.31, Training time: 386.29
Evaluating epoch 14
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
-------------------------------------------------------
| epoch                     | 14                      |
| policy/steps              | 1357609.0               |
| test/episodes             | 375.0                   |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -3.09413658420493e-06   |
| test_1/avg_q              | -9.877682327285688      |
| test_1/n_subgoals         | 676.0                   |
| test_1/subgoal_succ_rate  | 0.0014792899408284023   |
| train/episodes            | 1500.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -21.1794304712118       |
| train_0/current_q         | -1.3787272799371786e-06 |
| train_0/fw_bonus          | -0.9992317572236061     |
| train_0/fw_loss           | 0.006267152319196612    |
| train_0/mu_grads          | 0.007363457232713699    |
| train_0/mu_grads_std      | 0.18669158220291138     |
| train_0/mu_loss           | 1.8478157516253457e-07  |
| train_0/next_q            | -1.9212830929645306e-07 |
| train_0/q_grads           | -0.007381144887767732   |
| train_0/q_grads_std       | 0.24515585601329803     |
| train_0/q_loss            | 0.5147290638038177      |
| train_0/reward            | -0.6118257610509318     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.0296142578125         |
| train_0/target_q          | -0.6118258944196824     |
| train_1/avg_q             | -19.832288947687616     |
| train_1/current_q         | -5.902872651166478      |
| train_1/fw_bonus          | -0.9930896669626236     |
| train_1/fw_loss           | 0.07680798210203647     |
| train_1/mu_grads          | -0.020696301432326435   |
| train_1/mu_grads_std      | 0.20994022376835347     |
| train_1/mu_loss           | 4.315220231447023       |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -4.3111882838094875     |
| train_1/q_grads           | -0.027361139841377734   |
| train_1/q_grads_std       | 0.27321132495999334     |
| train_1/q_loss            | 0.3436501682533627      |
| train_1/reward            | -2.601598834141987      |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.005615234375          |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.01037037037037037     |
| train_1/target_q          | -5.891704693629281      |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 15
Time for epoch 15: 892.25. Rollout time: 589.26, Training time: 302.81
Evaluating epoch 15
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
-------------------------------------------------------
| epoch                     | 15                      |
| policy/steps              | 1448277.0               |
| test/episodes             | 400.0                   |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -7.2215004213103935e-06 |
| test_1/avg_q              | -9.379931035998663      |
| test_1/n_subgoals         | 675.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 1600.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -4.763467013717331e-06  |
| train_0/current_q         | -2.9399448046267513e-06 |
| train_0/fw_bonus          | -0.9991665750741958     |
| train_0/fw_loss           | 0.006726752547547221    |
| train_0/mu_grads          | 0.007363460026681423    |
| train_0/mu_grads_std      | 0.18669158220291138     |
| train_0/mu_loss           | 6.207001511606854e-07   |
| train_0/next_q            | -6.738075506964769e-07  |
| train_0/q_grads           | -0.0073859102907590565  |
| train_0/q_grads_std       | 0.2451540060341358      |
| train_0/q_loss            | 0.529552498997174       |
| train_0/reward            | -0.6236930099112215     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.043505859375          |
| train_0/target_q          | -0.6236934727803554     |
| train_1/avg_q             | -19.130008466393924     |
| train_1/current_q         | -6.105666220235628      |
| train_1/fw_bonus          | -0.9934744849801064     |
| train_1/fw_loss           | 0.07378662377595901     |
| train_1/mu_grads          | -0.02282754248008132    |
| train_1/mu_grads_std      | 0.21377163790166379     |
| train_1/mu_loss           | 4.467316851155272       |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -4.469186656405675      |
| train_1/q_grads           | -0.0290743475779891     |
| train_1/q_grads_std       | 0.29155518114566803     |
| train_1/q_loss            | 0.38413900214461855     |
| train_1/reward            | -2.666367889155299      |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.0057373046875         |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.011111111111111112    |
| train_1/target_q          | -6.103390870741192      |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 16
Time for epoch 16: 1906.39. Rollout time: 1578.97, Training time: 327.24
Evaluating epoch 16
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
-------------------------------------------------------
| epoch                     | 16                      |
| policy/steps              | 1538425.0               |
| test/episodes             | 425.0                   |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -2.5579335858788414e-06 |
| test_1/avg_q              | -8.344172977181147      |
| test_1/n_subgoals         | 1352.0                  |
| test_1/subgoal_succ_rate  | 0.5199704142011834      |
| train/episodes            | 1700.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -9.066112888033601e-06  |
| train_0/current_q         | -9.081704320565622e-06  |
| train_0/fw_bonus          | -0.9991559594869613     |
| train_0/fw_loss           | 0.006801596225704998    |
| train_0/mu_grads          | 0.007363461423665285    |
| train_0/mu_grads_std      | 0.18669158220291138     |
| train_0/mu_loss           | 2.4767782783774247e-06  |
| train_0/next_q            | -2.4730269501751673e-06 |
| train_0/q_grads           | -0.0074136878829449415  |
| train_0/q_grads_std       | 0.2451416954398155      |
| train_0/q_loss            | 0.5448262393209207      |
| train_0/reward            | -0.6359218013629289     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.0332763671875         |
| train_0/target_q          | -0.635923460190307      |
| train_1/avg_q             | -19.48098066562119      |
| train_1/current_q         | -6.196554658610073      |
| train_1/fw_bonus          | -0.9934933289885521     |
| train_1/fw_loss           | 0.07363871578127146     |
| train_1/mu_grads          | -0.0251827082131058     |
| train_1/mu_grads_std      | 0.2232626758515835      |
| train_1/mu_loss           | 4.5638774905934465      |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -4.575857517211491      |
| train_1/q_grads           | -0.032567739859223364   |
| train_1/q_grads_std       | 0.31001356095075605     |
| train_1/q_loss            | 0.38896737987359764     |
| train_1/reward            | -2.6094451089549695     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.004736328125          |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.021111111111111112    |
| train_1/target_q          | -6.189305061378109      |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 17
Time for epoch 17: 959.04. Rollout time: 620.85, Training time: 337.91
Evaluating epoch 17
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
-----------------------------------------------------
| epoch                     | 17                    |
| policy/steps              | 1628867.0             |
| test/episodes             | 450.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -0.5391514807704424   |
| test_1/avg_q              | -8.716410334379468    |
| test_1/n_subgoals         | 1376.0                |
| test_1/subgoal_succ_rate  | 0.5290697674418605    |
| train/episodes            | 1800.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -3.3487422015033363   |
| train_0/current_q         | -0.9169522798862824   |
| train_0/fw_bonus          | -0.9991441115736961   |
| train_0/fw_loss           | 0.006885129096917808  |
| train_0/mu_grads          | 0.012136395089328289  |
| train_0/mu_grads_std      | 0.21001031138002874   |
| train_0/mu_loss           | 0.8535721129054       |
| train_0/next_q            | -0.842997124028415    |
| train_0/q_grads           | -0.018103325134143235 |
| train_0/q_grads_std       | 0.24984322898089886   |
| train_0/q_loss            | 0.5940295679913519    |
| train_0/reward            | -0.6377132729139703   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0440185546875       |
| train_0/target_q          | -1.36403340920825     |
| train_1/avg_q             | -19.70851371462377    |
| train_1/current_q         | -6.070933375831378    |
| train_1/fw_bonus          | -0.9934598714113235   |
| train_1/fw_loss           | 0.0739013945683837    |
| train_1/mu_grads          | -0.02743296199478209  |
| train_1/mu_grads_std      | 0.23054780699312688   |
| train_1/mu_loss           | 4.38115896015062      |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -4.374847409482444    |
| train_1/q_grads           | -0.03748052753508091  |
| train_1/q_grads_std       | 0.32521083652973176   |
| train_1/q_loss            | 0.34802668115633517   |
| train_1/reward            | -2.594202369458435    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0052734375          |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.011851851851851851  |
| train_1/target_q          | -6.06591175934941     |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 18
Time for epoch 18: 1221.94. Rollout time: 751.25, Training time: 470.45
Evaluating epoch 18
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
-------------------------------------------------------
| epoch                     | 18                      |
| policy/steps              | 1719228.0               |
| test/episodes             | 475.0                   |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -4.742141955957804e-11  |
| test_1/avg_q              | -10.380350078814025     |
| test_1/n_subgoals         | 675.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 1900.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -5.960004209359294      |
| train_0/current_q         | -2.119617426966782e-07  |
| train_0/fw_bonus          | -0.9990807563066483     |
| train_0/fw_loss           | 0.007331833802163601    |
| train_0/mu_grads          | 0.012134750373661518    |
| train_0/mu_grads_std      | 0.21001194417476654     |
| train_0/mu_loss           | 4.5069765253648245e-09  |
| train_0/next_q            | -2.1666720682163958e-08 |
| train_0/q_grads           | -0.01846493878401816    |
| train_0/q_grads_std       | 0.25236234068870544     |
| train_0/q_loss            | 0.5576452454682729      |
| train_0/reward            | -0.646175217219934      |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.03291015625           |
| train_0/target_q          | -0.6461752190392117     |
| train_1/avg_q             | -19.43408811671121      |
| train_1/current_q         | -6.582155714219335      |
| train_1/fw_bonus          | -0.9932632088661194     |
| train_1/fw_loss           | 0.07544546872377396     |
| train_1/mu_grads          | -0.027711681090295315   |
| train_1/mu_grads_std      | 0.2335758950561285      |
| train_1/mu_loss           | 4.98348546240393        |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -4.976201383434412      |
| train_1/q_grads           | -0.040672172978520396   |
| train_1/q_grads_std       | 0.3358279250562191      |
| train_1/q_loss            | 0.4156461801328725      |
| train_1/reward            | -2.666022102865827      |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.0044189453125         |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.015925925925925927    |
| train_1/target_q          | -6.580619972500654      |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 19
Time for epoch 19: 1193.40. Rollout time: 762.75, Training time: 430.24
Evaluating epoch 19
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
-------------------------------------------------------
| epoch                     | 19                      |
| policy/steps              | 1809675.0               |
| test/episodes             | 500.0                   |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -3.315015955430203e-11  |
| test_1/avg_q              | -10.610649387440336     |
| test_1/n_subgoals         | 675.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 2000.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -7.760849651930408e-09  |
| train_0/current_q         | -1.7755881980749465e-07 |
| train_0/fw_bonus          | -0.9990963891148568     |
| train_0/fw_loss           | 0.007221598655451089    |
| train_0/mu_grads          | 0.012134750373661518    |
| train_0/mu_grads_std      | 0.21001194417476654     |
| train_0/mu_loss           | 2.5644357895741712e-08  |
| train_0/next_q            | -3.485986121302899e-09  |
| train_0/q_grads           | -0.018462543468922378   |
| train_0/q_grads_std       | 0.25236257910728455     |
| train_0/q_loss            | 0.563736954430162       |
| train_0/reward            | -0.651049731288731      |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.0498779296875         |
| train_0/target_q          | -0.6510497326262985     |
| train_1/avg_q             | -19.428649544766717     |
| train_1/current_q         | -6.433598210370585      |
| train_1/fw_bonus          | -0.9931381240487098     |
| train_1/fw_loss           | 0.07642758190631867     |
| train_1/mu_grads          | -0.030350280040875078   |
| train_1/mu_grads_std      | 0.2372139524668455      |
| train_1/mu_loss           | 4.79001491552327        |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -4.779687022092791      |
| train_1/q_grads           | -0.04417024897411466    |
| train_1/q_grads_std       | 0.35040473863482474     |
| train_1/q_loss            | 0.4915676923049267      |
| train_1/reward            | -2.6375545500068256     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.0034423828125         |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.013703703703703704    |
| train_1/target_q          | -6.4294717326006126     |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 20
Time for epoch 20: 1700.84. Rollout time: 958.83, Training time: 741.32
Evaluating epoch 20
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
-------------------------------------------------------
| epoch                     | 20                      |
| policy/steps              | 1900189.0               |
| test/episodes             | 525.0                   |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -3.403759653664241e-11  |
| test_1/avg_q              | -10.593521306415177     |
| test_1/n_subgoals         | 4124.0                  |
| test_1/subgoal_succ_rate  | 0.8685741998060136      |
| train/episodes            | 2100.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -1.9617887250464295e-08 |
| train_0/current_q         | -8.376377875545465e-08  |
| train_0/fw_bonus          | -0.9991044923663139     |
| train_0/fw_loss           | 0.007164415961597115    |
| train_0/mu_grads          | 0.012134750373661518    |
| train_0/mu_grads_std      | 0.21001194417476654     |
| train_0/mu_loss           | 3.4156259460011555e-09  |
| train_0/next_q            | -3.8277416939124106e-09 |
| train_0/q_grads           | -0.018459229031577706   |
| train_0/q_grads_std       | 0.2523637168109417      |
| train_0/q_loss            | 0.5523653161389037      |
| train_0/reward            | -0.6419514171044284     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.0309814453125         |
| train_0/target_q          | -0.6419514190711214     |
| train_1/avg_q             | -19.452269890674366     |
| train_1/current_q         | -6.562754695840074      |
| train_1/fw_bonus          | -0.9927147522568702     |
| train_1/fw_loss           | 0.07975162230432034     |
| train_1/mu_grads          | -0.0326972896233201     |
| train_1/mu_grads_std      | 0.2395136531442404      |
| train_1/mu_loss           | 4.930987118754294       |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -4.91243444438133       |
| train_1/q_grads           | -0.04684334993362427    |
| train_1/q_grads_std       | 0.36270410642027856     |
| train_1/q_loss            | 0.573947024074428       |
| train_1/reward            | -2.6838842575827586     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.0038818359375         |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.013333333333333334    |
| train_1/target_q          | -6.560065833429055      |
-------------------------------------------------------
Saving periodic policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_20.pkl ...
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 21
Time for epoch 21: 1658.01. Rollout time: 975.70, Training time: 681.69
Evaluating epoch 21
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
-------------------------------------------------------
| epoch                     | 21                      |
| policy/steps              | 1990708.0               |
| test/episodes             | 550.0                   |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -6.86509727951829e-11   |
| test_1/avg_q              | -9.013108232068825      |
| test_1/n_subgoals         | 3453.0                  |
| test_1/subgoal_succ_rate  | 0.8355053576600058      |
| train/episodes            | 2200.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -4.5691617140937564e-08 |
| train_0/current_q         | -2.4321786160417727e-07 |
| train_0/fw_bonus          | -0.9991374462842941     |
| train_0/fw_loss           | 0.006932056555524468    |
| train_0/mu_grads          | 0.012134750373661518    |
| train_0/mu_grads_std      | 0.21001194417476654     |
| train_0/mu_loss           | 1.717206854347857e-08   |
| train_0/next_q            | -1.1366673722740226e-08 |
| train_0/q_grads           | -0.018392864614725113   |
| train_0/q_grads_std       | 0.2524376600980759      |
| train_0/q_loss            | 0.5535487598871306      |
| train_0/reward            | -0.6428934937186568     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.0480712890625         |
| train_0/target_q          | -0.6428934971061662     |
| train_1/avg_q             | -19.645300317215117     |
| train_1/current_q         | -6.085148747445446      |
| train_1/fw_bonus          | -0.9921013653278351     |
| train_1/fw_loss           | 0.08456779941916466     |
| train_1/mu_grads          | -0.03487537270411849    |
| train_1/mu_grads_std      | 0.24198003970086573     |
| train_1/mu_loss           | 4.399787451937275       |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -4.370104001340476      |
| train_1/q_grads           | -0.050334601942449805   |
| train_1/q_grads_std       | 0.37593685984611513     |
| train_1/q_loss            | 0.6733696467207176      |
| train_1/reward            | -2.641488473861318      |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.0035888671875         |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.015185185185185185    |
| train_1/target_q          | -6.082531113458947      |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 22
Time for epoch 22: 1287.44. Rollout time: 853.35, Training time: 433.49
Evaluating epoch 22
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
-----------------------------------------------------
| epoch                     | 22                    |
| policy/steps              | 2081147.0             |
| test/episodes             | 575.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -7.78281426782127     |
| test_1/avg_q              | -8.908636184969598    |
| test_1/n_subgoals         | 3430.0                |
| test_1/subgoal_succ_rate  | 0.8341107871720117    |
| train/episodes            | 2300.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -5.084024787023212    |
| train_0/current_q         | -8.596606172653983    |
| train_0/fw_bonus          | -0.9991979986429215   |
| train_0/fw_loss           | 0.006505158508662135  |
| train_0/mu_grads          | 0.006930210092104971  |
| train_0/mu_grads_std      | 0.21042119823396205   |
| train_0/mu_loss           | 8.576229883890168     |
| train_0/next_q            | -8.55183463170656     |
| train_0/q_grads           | -0.019359830021858215 |
| train_0/q_grads_std       | 0.2569029115140438    |
| train_0/q_loss            | 0.6544837793215562    |
| train_0/reward            | -0.6395211794646457   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.030517578125        |
| train_0/target_q          | -8.778170658006587    |
| train_1/avg_q             | -19.679805169400453   |
| train_1/current_q         | -6.128451037030002    |
| train_1/fw_bonus          | -0.9918084233999253   |
| train_1/fw_loss           | 0.08686769027262926   |
| train_1/mu_grads          | -0.036409045569598675 |
| train_1/mu_grads_std      | 0.2452982746064663    |
| train_1/mu_loss           | 4.389107341855539     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -4.352835291765954    |
| train_1/q_grads           | -0.05506688039749861  |
| train_1/q_grads_std       | 0.3899556793272495    |
| train_1/q_loss            | 0.4771921344680578    |
| train_1/reward            | -2.7429838469623062   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0028076171875       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.013333333333333334  |
| train_1/target_q          | -6.121906151562749    |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 23
Time for epoch 23: 1058.22. Rollout time: 665.69, Training time: 392.02
Evaluating epoch 23
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
-----------------------------------------------------
| epoch                     | 23                    |
| policy/steps              | 2171854.0             |
| test/episodes             | 600.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -18.38154836606174    |
| test_1/avg_q              | -10.008260354118525   |
| test_1/n_subgoals         | 2728.0                |
| test_1/subgoal_succ_rate  | 0.781524926686217     |
| train/episodes            | 2400.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -25.764779845072145   |
| train_0/current_q         | -10.107986274188033   |
| train_0/fw_bonus          | -0.999258004128933    |
| train_0/fw_loss           | 0.006082106591202318  |
| train_0/mu_grads          | 0.006932674045674503  |
| train_0/mu_grads_std      | 0.2104212362319231    |
| train_0/mu_loss           | 10.162669628029175    |
| train_0/next_q            | -10.131466519850425   |
| train_0/q_grads           | -0.01901006358675659  |
| train_0/q_grads_std       | 0.2584549553692341    |
| train_0/q_loss            | 1.4276495179237956    |
| train_0/reward            | -0.6286414043879631   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.02666015625         |
| train_0/target_q          | -10.239397406843276   |
| train_1/avg_q             | -19.756874035295663   |
| train_1/current_q         | -6.069977088699041    |
| train_1/fw_bonus          | -0.9919866546988487   |
| train_1/fw_loss           | 0.08546839449554682   |
| train_1/mu_grads          | -0.03898527882993221  |
| train_1/mu_grads_std      | 0.24916635900735856   |
| train_1/mu_loss           | 4.349816788489643     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -4.328047864208953    |
| train_1/q_grads           | -0.059185054153203964 |
| train_1/q_grads_std       | 0.40150598883628846   |
| train_1/q_loss            | 0.5026043269160057    |
| train_1/reward            | -2.7425340175293966   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0037109375          |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.00962962962962963   |
| train_1/target_q          | -6.071386224303831    |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 24
Time for epoch 24: 971.61. Rollout time: 620.94, Training time: 350.42
Evaluating epoch 24
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
-----------------------------------------------------
| epoch                     | 24                    |
| policy/steps              | 2262406.0             |
| test/episodes             | 625.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -13.88823958658871    |
| test_1/avg_q              | -10.649388937276905   |
| test_1/n_subgoals         | 677.0                 |
| test_1/subgoal_succ_rate  | 0.0029542097488921715 |
| train/episodes            | 2500.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.549416630235378   |
| train_0/current_q         | -9.30784911563208     |
| train_0/fw_bonus          | -0.9992891818284988   |
| train_0/fw_loss           | 0.0058622937533073126 |
| train_0/mu_grads          | 0.0069389222655445336 |
| train_0/mu_grads_std      | 0.21042160093784332   |
| train_0/mu_loss           | 9.263599669494406     |
| train_0/next_q            | -9.277203809985952    |
| train_0/q_grads           | -0.019469162356108426 |
| train_0/q_grads_std       | 0.26102995350956915   |
| train_0/q_loss            | 0.8363155828049601    |
| train_0/reward            | -0.6133415811087616   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0259033203125       |
| train_0/target_q          | -9.53648655331313     |
| train_1/avg_q             | -19.93660367800839    |
| train_1/current_q         | -5.6918962367743635   |
| train_1/fw_bonus          | -0.9916017234325409   |
| train_1/fw_loss           | 0.08849056884646415   |
| train_1/mu_grads          | -0.040960962790995835 |
| train_1/mu_grads_std      | 0.25224817246198655   |
| train_1/mu_loss           | 3.8789283987110084    |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -3.851245701737401    |
| train_1/q_grads           | -0.06352510284632444  |
| train_1/q_grads_std       | 0.4118429683148861    |
| train_1/q_loss            | 0.33624556888549784   |
| train_1/reward            | -2.747399946213773    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0041748046875       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.011481481481481481  |
| train_1/target_q          | -5.685431287320879    |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 25
Time for epoch 25: 986.74. Rollout time: 640.44, Training time: 346.00
Evaluating epoch 25
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
----------------------------------------------------
| epoch                     | 25                   |
| policy/steps              | 2352906.0            |
| test/episodes             | 650.0                |
| test/success_rate         | 0.0                  |
| test_0/avg_q              | -16.913406594126972  |
| test_1/avg_q              | -9.331296804363351   |
| test_1/n_subgoals         | 1351.0               |
| test_1/subgoal_succ_rate  | 0.5196150999259808   |
| train/episodes            | 2600.0               |
| train/success_rate        | 0.0                  |
| train_0/avg_q             | -26.548026557992866  |
| train_0/current_q         | -9.928251773149352   |
| train_0/fw_bonus          | -0.9993544027209282  |
| train_0/fw_loss           | 0.005402378435246646 |
| train_0/mu_grads          | 0.006919225852470845 |
| train_0/mu_grads_std      | 0.21042731627821923  |
| train_0/mu_loss           | 9.959734142737494    |
| train_0/next_q            | -9.95150250972838    |
| train_0/q_grads           | -0.02001216714270413 |
| train_0/q_grads_std       | 0.2643350251019001   |
| train_0/q_loss            | 1.2756251467142565   |
| train_0/reward            | -0.606857644378033   |
| train_0/reward_-0.0_frac  | 0.0                  |
| train_0/reward_-1.0_frac  | 0.023681640625       |
| train_0/target_q          | -10.088504031263076  |
| train_1/avg_q             | -20.04014837056486   |
| train_1/current_q         | -5.758741026232944   |
| train_1/fw_bonus          | -0.9916945978999138  |
| train_1/fw_loss           | 0.08776150681078435  |
| train_1/mu_grads          | -0.0436398234218359  |
| train_1/mu_grads_std      | 0.25576822459697723  |
| train_1/mu_loss           | 4.061054028814719    |
| train_1/n_subgoals        | 2700.0               |
| train_1/next_q            | -4.038507547527466   |
| train_1/q_grads           | -0.06661913357675076 |
| train_1/q_grads_std       | 0.42037243098020555  |
| train_1/q_loss            | 0.2923644609820082   |
| train_1/reward            | -2.649992739882509   |
| train_1/reward_-0.0_frac  | 0.0                  |
| train_1/reward_-1.0_frac  | 0.003955078125       |
| train_1/reward_-27.0_frac | 0.0                  |
| train_1/subgoal_succ_rate | 0.011851851851851851 |
| train_1/target_q          | -5.747249200539386   |
----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 26
Time for epoch 26: 1021.76. Rollout time: 645.28, Training time: 376.25
Evaluating epoch 26
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
----------------------------------------------------
| epoch                     | 26                   |
| policy/steps              | 2443253.0            |
| test/episodes             | 675.0                |
| test/success_rate         | 0.0                  |
| test_0/avg_q              | -19.627260017451334  |
| test_1/avg_q              | -10.876512124393503  |
| test_1/n_subgoals         | 1351.0               |
| test_1/subgoal_succ_rate  | 0.5196150999259808   |
| train/episodes            | 2700.0               |
| train/success_rate        | 0.0                  |
| train_0/avg_q             | -26.62797247605503   |
| train_0/current_q         | -9.741634995304851   |
| train_0/fw_bonus          | -0.9994133353233338  |
| train_0/fw_loss           | 0.004986825829837472 |
| train_0/mu_grads          | 0.006855969130992889 |
| train_0/mu_grads_std      | 0.21043428964912891  |
| train_0/mu_loss           | 9.809649953992576    |
| train_0/next_q            | -9.793938131203761   |
| train_0/q_grads           | -0.02019789647310972 |
| train_0/q_grads_std       | 0.26736858636140826  |
| train_0/q_loss            | 1.4601006226255688   |
| train_0/reward            | -0.5933958164383512  |
| train_0/reward_-0.0_frac  | 0.0                  |
| train_0/reward_-1.0_frac  | 0.0244873046875      |
| train_0/target_q          | -9.896100107596812   |
| train_1/avg_q             | -20.022054830883313  |
| train_1/current_q         | -6.263876475187075   |
| train_1/fw_bonus          | -0.9912519574165344  |
| train_1/fw_loss           | 0.09123699925839901  |
| train_1/mu_grads          | -0.0459124444052577  |
| train_1/mu_grads_std      | 0.2591310299932957   |
| train_1/mu_loss           | 4.794977363737364    |
| train_1/n_subgoals        | 2700.0               |
| train_1/next_q            | -4.778057345613024   |
| train_1/q_grads           | -0.06980276498943568 |
| train_1/q_grads_std       | 0.42592613101005555  |
| train_1/q_loss            | 0.3057754520774303   |
| train_1/reward            | -2.618644284605762   |
| train_1/reward_-0.0_frac  | 0.0                  |
| train_1/reward_-1.0_frac  | 0.0035400390625      |
| train_1/reward_-27.0_frac | 0.0                  |
| train_1/subgoal_succ_rate | 0.013333333333333334 |
| train_1/target_q          | -6.25503997618147    |
----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 27
Time for epoch 27: 976.99. Rollout time: 628.21, Training time: 348.60
Evaluating epoch 27
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
-------------------------------------------------------
| epoch                     | 27                      |
| policy/steps              | 2533678.0               |
| test/episodes             | 700.0                   |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -1.0532280207335593e-10 |
| test_1/avg_q              | -10.982055286020321     |
| test_1/n_subgoals         | 675.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 2800.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -16.672550753554095     |
| train_0/current_q         | -2.6162483743606326e-07 |
| train_0/fw_bonus          | -0.9994541496038437     |
| train_0/fw_loss           | 0.004699083068408072    |
| train_0/mu_grads          | -0.0020088537130504847  |
| train_0/mu_grads_std      | 0.2127426266670227      |
| train_0/mu_loss           | 2.5018651816268894e-08  |
| train_0/next_q            | -2.847657290755006e-08  |
| train_0/q_grads           | -0.017921213805675507   |
| train_0/q_grads_std       | 0.27125197649002075     |
| train_0/q_loss            | 0.4820760981527066      |
| train_0/reward            | -0.5856789682417002     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.026416015625          |
| train_0/target_q          | -0.5856789959689007     |
| train_1/avg_q             | -19.990034063299532     |
| train_1/current_q         | -6.259066107872001      |
| train_1/fw_bonus          | -0.9913602381944656     |
| train_1/fw_loss           | 0.09038666635751724     |
| train_1/mu_grads          | -0.04764498146250844    |
| train_1/mu_grads_std      | 0.2607355073094368      |
| train_1/mu_loss           | 4.810496149859825       |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -4.807980511233811      |
| train_1/q_grads           | -0.07344132997095584    |
| train_1/q_grads_std       | 0.4294133186340332      |
| train_1/q_loss            | 0.1995042863399669      |
| train_1/reward            | -2.6174811740751465     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.0037841796875         |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.012222222222222223    |
| train_1/target_q          | -6.2485634706469355     |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 28
Time for epoch 28: 1020.48. Rollout time: 634.12, Training time: 386.06
Evaluating epoch 28
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
-------------------------------------------------------
| epoch                     | 28                      |
| policy/steps              | 2623869.0               |
| test/episodes             | 725.0                   |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -2.766517658469677e-11  |
| test_1/avg_q              | -10.658162145820215     |
| test_1/n_subgoals         | 2703.0                  |
| test_1/subgoal_succ_rate  | 0.779134295227525       |
| train/episodes            | 2900.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -7.62624289280674e-06   |
| train_0/current_q         | -1.0633029789902023e-06 |
| train_0/fw_bonus          | -0.9994258850812912     |
| train_0/fw_loss           | 0.004898400697857142    |
| train_0/mu_grads          | -0.0020087752724066376  |
| train_0/mu_grads_std      | 0.2127426415681839      |
| train_0/mu_loss           | 1.4010270871125875e-07  |
| train_0/next_q            | -2.66595299368881e-07   |
| train_0/q_grads           | -0.017923012422397733   |
| train_0/q_grads_std       | 0.27125148475170135     |
| train_0/q_loss            | 0.4861803518932012      |
| train_0/reward            | -0.5889677994666271     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.0227783203125         |
| train_0/target_q          | -0.5889680593041853     |
| train_1/avg_q             | -19.87992608828458      |
| train_1/current_q         | -6.436708756471566      |
| train_1/fw_bonus          | -0.9908147275447845     |
| train_1/fw_loss           | 0.09466985408216715     |
| train_1/mu_grads          | -0.05009180074557662    |
| train_1/mu_grads_std      | 0.26128321811556815     |
| train_1/mu_loss           | 4.995079411978668       |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -4.99264784281296       |
| train_1/q_grads           | -0.07737349886447191    |
| train_1/q_grads_std       | 0.4326266460120678      |
| train_1/q_loss            | 0.18889941547318068     |
| train_1/reward            | -2.666856511779406      |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.003564453125          |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.016666666666666666    |
| train_1/target_q          | -6.431181947416638      |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 29
Time for epoch 29: 1029.17. Rollout time: 644.09, Training time: 384.86
Evaluating epoch 29
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
-------------------------------------------------------
| epoch                     | 29                      |
| policy/steps              | 2714234.0               |
| test/episodes             | 750.0                   |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -7.063766405088574e-11  |
| test_1/avg_q              | -11.10837897972018      |
| test_1/n_subgoals         | 675.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 3000.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -5.200005098723024e-06  |
| train_0/current_q         | -2.1600565057070865e-06 |
| train_0/fw_bonus          | -0.9994226649403573     |
| train_0/fw_loss           | 0.004921121115330607    |
| train_0/mu_grads          | -0.0020085560507141055  |
| train_0/mu_grads_std      | 0.21274270117282867     |
| train_0/mu_loss           | 3.8285064360160565e-07  |
| train_0/next_q            | -2.46378764351635e-07   |
| train_0/q_grads           | -0.017941268580034374   |
| train_0/q_grads_std       | 0.2712500624358654      |
| train_0/q_loss            | 0.4859512890668552      |
| train_0/reward            | -0.5887838883685618     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.0273193359375         |
| train_0/target_q          | -0.5887841289131486     |
| train_1/avg_q             | -19.98715675275276      |
| train_1/current_q         | -6.361118384998722      |
| train_1/fw_bonus          | -0.9903901368379593     |
| train_1/fw_loss           | 0.0980034451931715      |
| train_1/mu_grads          | -0.05212506167590618    |
| train_1/mu_grads_std      | 0.26271782666444776     |
| train_1/mu_loss           | 4.9610828491509364      |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -4.949727416988273      |
| train_1/q_grads           | -0.08170958682894706    |
| train_1/q_grads_std       | 0.4371217764914036      |
| train_1/q_loss            | 0.23152730882994232     |
| train_1/reward            | -2.567287476517231      |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.0042724609375         |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.013333333333333334    |
| train_1/target_q          | -6.349901911267041      |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 30
Time for epoch 30: 1008.34. Rollout time: 637.30, Training time: 370.64
Evaluating epoch 30
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
-------------------------------------------------------
| epoch                     | 30                      |
| policy/steps              | 2804707.0               |
| test/episodes             | 775.0                   |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -6.765259844672623e-16  |
| test_1/avg_q              | -12.791680976723853     |
| test_1/n_subgoals         | 2728.0                  |
| test_1/subgoal_succ_rate  | 0.781524926686217       |
| train/episodes            | 3100.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -0.014694291670410425   |
| train_0/current_q         | -1.0768190875202486e-07 |
| train_0/fw_bonus          | -0.9994050726294518     |
| train_0/fw_loss           | 0.0050450873444788154   |
| train_0/mu_grads          | -0.0012459498830139637  |
| train_0/mu_grads_std      | 0.2130027562379837      |
| train_0/mu_loss           | 1.3718456091897537e-08  |
| train_0/next_q            | -7.749341137078026e-08  |
| train_0/q_grads           | -0.019883656688034535   |
| train_0/q_grads_std       | 0.27309752330183984     |
| train_0/q_loss            | 0.4885326412528495      |
| train_0/reward            | -0.5908485847390693     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.0296875               |
| train_0/target_q          | -0.5908486606826113     |
| train_1/avg_q             | -20.01051830322762      |
| train_1/current_q         | -6.277210519185703      |
| train_1/fw_bonus          | -0.990121528506279      |
| train_1/fw_loss           | 0.10011243578046561     |
| train_1/mu_grads          | -0.05287976944819093    |
| train_1/mu_grads_std      | 0.2652026802301407      |
| train_1/mu_loss           | 4.842468084242293       |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -4.81442476595199       |
| train_1/q_grads           | -0.08333638720214367    |
| train_1/q_grads_std       | 0.4431061655282974      |
| train_1/q_loss            | 0.20048969599358157     |
| train_1/reward            | -2.644673938201231      |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.00400390625           |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.012962962962962963    |
| train_1/target_q          | -6.270166615997576      |
-------------------------------------------------------
Saving periodic policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_30.pkl ...
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 31
Time for epoch 31: 1005.87. Rollout time: 640.64, Training time: 364.80
Evaluating epoch 31
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
-------------------------------------------------------
| epoch                     | 31                      |
| policy/steps              | 2895242.0               |
| test/episodes             | 800.0                   |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -1.734690096546571e-16  |
| test_1/avg_q              | -16.737519839203273     |
| test_1/n_subgoals         | 2027.0                  |
| test_1/subgoal_succ_rate  | 0.6926492353231376      |
| train/episodes            | 3200.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -7.581306962768436e-09  |
| train_0/current_q         | -4.3337600110142844e-10 |
| train_0/fw_bonus          | -0.9994254410266876     |
| train_0/fw_loss           | 0.004901513352524489    |
| train_0/mu_grads          | -0.0012459498830139637  |
| train_0/mu_grads_std      | 0.2130027562379837      |
| train_0/mu_loss           | 8.962796951454614e-12   |
| train_0/next_q            | -7.61721525528181e-12   |
| train_0/q_grads           | -0.01994982734322548    |
| train_0/q_grads_std       | 0.2731461822986603      |
| train_0/q_loss            | 0.48567857280981774     |
| train_0/reward            | -0.5885648570638295     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.0182373046875         |
| train_0/target_q          | -0.5885648570712931     |
| train_1/avg_q             | -20.136668768226414     |
| train_1/current_q         | -6.478998664468497      |
| train_1/fw_bonus          | -0.9904054746031761     |
| train_1/fw_loss           | 0.09788305964320898     |
| train_1/mu_grads          | -0.053913564700633285   |
| train_1/mu_grads_std      | 0.2678706765174866      |
| train_1/mu_loss           | 5.161335961420498       |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -5.132169231523325      |
| train_1/q_grads           | -0.0867389315739274     |
| train_1/q_grads_std       | 0.44906738549470904     |
| train_1/q_loss            | 0.24695538052474345     |
| train_1/reward            | -2.5794128919646027     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.0038330078125         |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.01074074074074074     |
| train_1/target_q          | -6.469712596927755      |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 32
Time for epoch 32: 1038.97. Rollout time: 663.68, Training time: 375.02
Evaluating epoch 32
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
-------------------------------------------------------
| epoch                     | 32                      |
| policy/steps              | 2985782.0               |
| test/episodes             | 825.0                   |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -9.512101453298163e-16  |
| test_1/avg_q              | -10.573479572610745     |
| test_1/n_subgoals         | 3455.0                  |
| test_1/subgoal_succ_rate  | 0.8356005788712012      |
| train/episodes            | 3300.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -2.1952274370500009e-10 |
| train_0/current_q         | -7.904041890394331e-11  |
| train_0/fw_bonus          | -0.9994699284434319     |
| train_0/fw_loss           | 0.004587831284152344    |
| train_0/mu_grads          | -0.0012459498830139637  |
| train_0/mu_grads_std      | 0.2130027562379837      |
| train_0/mu_loss           | 9.277974589672241e-12   |
| train_0/next_q            | -1.1077247693534565e-11 |
| train_0/q_grads           | -0.01994987390935421    |
| train_0/q_grads_std       | 0.2731461822986603      |
| train_0/q_loss            | 0.48644995793257345     |
| train_0/reward            | -0.5891802274523797     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.021142578125          |
| train_0/target_q          | -0.5891802274632347     |
| train_1/avg_q             | -20.309372029277068     |
| train_1/current_q         | -6.626208548519019      |
| train_1/fw_bonus          | -0.9903285801410675     |
| train_1/fw_loss           | 0.09848674368113279     |
| train_1/mu_grads          | -0.05627744924277067    |
| train_1/mu_grads_std      | 0.2711861848831177      |
| train_1/mu_loss           | 5.318902510833674       |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -5.309188314496258      |
| train_1/q_grads           | -0.09037062730640173    |
| train_1/q_grads_std       | 0.45438442304730414     |
| train_1/q_loss            | 0.1661294790890864      |
| train_1/reward            | -2.5924307803885314     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.0032470703125         |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.011111111111111112    |
| train_1/target_q          | -6.621594524746483      |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 33
Time for epoch 33: 1025.89. Rollout time: 648.26, Training time: 377.30
Evaluating epoch 33
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
-------------------------------------------------------
| epoch                     | 33                      |
| policy/steps              | 3076257.0               |
| test/episodes             | 850.0                   |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -4.777882682347857e-16  |
| test_1/avg_q              | -11.667263381379295     |
| test_1/n_subgoals         | 3431.0                  |
| test_1/subgoal_succ_rate  | 0.8341591372777616      |
| train/episodes            | 3400.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -1.172969223444931e-08  |
| train_0/current_q         | -1.4620919910561562e-10 |
| train_0/fw_bonus          | -0.999475160241127      |
| train_0/fw_loss           | 0.004551065166015178    |
| train_0/mu_grads          | -0.0012459498830139637  |
| train_0/mu_grads_std      | 0.2130027562379837      |
| train_0/mu_loss           | 1.3652139183202561e-11  |
| train_0/next_q            | -6.404635637318868e-12  |
| train_0/q_grads           | -0.019950264925137164   |
| train_0/q_grads_std       | 0.27314627170562744     |
| train_0/q_loss            | 0.4841626261755395      |
| train_0/reward            | -0.5873504764604149     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.0219970703125         |
| train_0/target_q          | -0.5873504764666908     |
| train_1/avg_q             | -20.290244445790634     |
| train_1/current_q         | -6.996175761629024      |
| train_1/fw_bonus          | -0.9909094423055649     |
| train_1/fw_loss           | 0.09392614252865314     |
| train_1/mu_grads          | -0.058417893294245      |
| train_1/mu_grads_std      | 0.27358427494764326     |
| train_1/mu_loss           | 5.777292322264705       |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -5.781166439510424      |
| train_1/q_grads           | -0.09358704276382923    |
| train_1/q_grads_std       | 0.45886728912591934     |
| train_1/q_loss            | 0.1968503535414336      |
| train_1/reward            | -2.6017312672051047     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.002978515625          |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.011851851851851851    |
| train_1/target_q          | -6.99629975616926       |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 34
Time for epoch 34: 1055.21. Rollout time: 658.62, Training time: 396.27
Evaluating epoch 34
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 34                     |
| policy/steps              | 3166557.0              |
| test/episodes             | 875.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -1.645898893745431e-18 |
| test_1/avg_q              | -13.960606845121665    |
| test_1/n_subgoals         | 1377.0                 |
| test_1/subgoal_succ_rate  | 0.5294117647058824     |
| train/episodes            | 3500.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -6.453386982472006e-07 |
| train_0/current_q         | -5.725377130673872e-11 |
| train_0/fw_bonus          | -0.9994780108332634    |
| train_0/fw_loss           | 0.00453090847004205    |
| train_0/mu_grads          | -0.0012459498830139637 |
| train_0/mu_grads_std      | 0.2130027562379837     |
| train_0/mu_loss           | 1.397740344365621e-12  |
| train_0/next_q            | -2.964999483362555e-12 |
| train_0/q_grads           | -0.020379839511588217  |
| train_0/q_grads_std       | 0.2736741006374359     |
| train_0/q_loss            | 0.4877105481873035     |
| train_0/reward            | -0.5901893259135249    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.020751953125         |
| train_0/target_q          | -0.5901893259164304    |
| train_1/avg_q             | -20.021981553825434    |
| train_1/current_q         | -6.645714777950498     |
| train_1/fw_bonus          | -0.9917224481701851    |
| train_1/fw_loss           | 0.08754277098923921    |
| train_1/mu_grads          | -0.06130307745188475   |
| train_1/mu_grads_std      | 0.2752350874245167     |
| train_1/mu_loss           | 5.320487255051762      |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -5.323834006444528     |
| train_1/q_grads           | -0.09727636091411114   |
| train_1/q_grads_std       | 0.46380959302186964    |
| train_1/q_loss            | 0.18772605723559135    |
| train_1/reward            | -2.6051022435494815    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0030517578125        |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.015555555555555555   |
| train_1/target_q          | -6.638125038610411     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 35
Time for epoch 35: 1072.70. Rollout time: 671.07, Training time: 401.48
Evaluating epoch 35
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 35                     |
| policy/steps              | 3256924.0              |
| test/episodes             | 900.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -2.361459994436296e-17 |
| test_1/avg_q              | -10.75326772209908     |
| test_1/n_subgoals         | 3457.0                 |
| test_1/subgoal_succ_rate  | 0.8356956899045415     |
| train/episodes            | 3600.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -5.449156078511779e-11 |
| train_0/current_q         | -6.673685681444312e-11 |
| train_0/fw_bonus          | -0.9995048150420189    |
| train_0/fw_loss           | 0.004341913072858005   |
| train_0/mu_grads          | -0.0012459498830139637 |
| train_0/mu_grads_std      | 0.2130027562379837     |
| train_0/mu_loss           | 1.1401232294801603e-12 |
| train_0/next_q            | -8.488604335163756e-12 |
| train_0/q_grads           | -0.020380052365362646  |
| train_0/q_grads_std       | 0.2736741304397583     |
| train_0/q_loss            | 0.4838516644029333     |
| train_0/reward            | -0.5871015445827652    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.023291015625         |
| train_0/target_q          | -0.5871015445910839    |
| train_1/avg_q             | -20.13928000316427     |
| train_1/current_q         | -6.109187394524639     |
| train_1/fw_bonus          | -0.9917039439082146    |
| train_1/fw_loss           | 0.08768809046596289    |
| train_1/mu_grads          | -0.06311203110963107   |
| train_1/mu_grads_std      | 0.2779039487242699     |
| train_1/mu_loss           | 4.551896944962192      |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -4.551764971495604     |
| train_1/q_grads           | -0.09977760557085276   |
| train_1/q_grads_std       | 0.4677279680967331     |
| train_1/q_loss            | 0.21386159715903147    |
| train_1/reward            | -2.6948925377579145    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0030029296875        |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.013333333333333334   |
| train_1/target_q          | -6.105101092812569     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 36
Time for epoch 36: 1036.88. Rollout time: 652.01, Training time: 384.39
Evaluating epoch 36
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
-------------------------------------------------------
| epoch                     | 36                      |
| policy/steps              | 3347423.0               |
| test/episodes             | 925.0                   |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -6.16263643995287e-18   |
| test_1/avg_q              | -7.840839298285431      |
| test_1/n_subgoals         | 2027.0                  |
| test_1/subgoal_succ_rate  | 0.6926492353231376      |
| train/episodes            | 3700.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -1.4406635673945528e-10 |
| train_0/current_q         | -3.04358785384021e-10   |
| train_0/fw_bonus          | -0.9995221942663193     |
| train_0/fw_loss           | 0.004219346464378759    |
| train_0/mu_grads          | -0.0012459498830139637  |
| train_0/mu_grads_std      | 0.2130027562379837      |
| train_0/mu_loss           | 2.145092072613403e-13   |
| train_0/next_q            | -4.4202352322655065e-13 |
| train_0/q_grads           | -0.020380963338539004   |
| train_0/q_grads_std       | 0.2736741602420807      |
| train_0/q_loss            | 0.4872911366055265      |
| train_0/reward            | -0.5898524726213509     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.0218505859375         |
| train_0/target_q          | -0.5898524726217838     |
| train_1/avg_q             | -20.083865646598106     |
| train_1/current_q         | -5.828775034990633      |
| train_1/fw_bonus          | -0.992220675945282      |
| train_1/fw_loss           | 0.08363088816404343     |
| train_1/mu_grads          | -0.06650143284350633    |
| train_1/mu_grads_std      | 0.281014221906662       |
| train_1/mu_loss           | 4.236756849725287       |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -4.232570399410106      |
| train_1/q_grads           | -0.09959501400589943    |
| train_1/q_grads_std       | 0.47426552027463914     |
| train_1/q_loss            | 0.24395854481680262     |
| train_1/reward            | -2.6254050236409965     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.0028564453125         |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.011481481481481481    |
| train_1/target_q          | -5.823236756800268      |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 37
Time for epoch 37: 1007.76. Rollout time: 641.43, Training time: 365.97
Evaluating epoch 37
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
-------------------------------------------------------
| epoch                     | 37                      |
| policy/steps              | 3437844.0               |
| test/episodes             | 950.0                   |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -1.4545244768139688e-17 |
| test_1/avg_q              | -14.16959173712626      |
| test_1/n_subgoals         | 675.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 3800.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -7.374868391405493e-11  |
| train_0/current_q         | -8.660822976022665e-11  |
| train_0/fw_bonus          | -0.9995242759585381     |
| train_0/fw_loss           | 0.004204643855337054    |
| train_0/mu_grads          | -0.0012459498830139637  |
| train_0/mu_grads_std      | 0.2130027562379837      |
| train_0/mu_loss           | 1.5801034987512372e-13  |
| train_0/next_q            | -3.8853287749248426e-13 |
| train_0/q_grads           | -0.020382470963522793   |
| train_0/q_grads_std       | 0.2736741602420807      |
| train_0/q_loss            | 0.4872220515062319      |
| train_0/reward            | -0.5897980623572948     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.02158203125           |
| train_0/target_q          | -0.5897980623576755     |
| train_1/avg_q             | -20.070310482423896     |
| train_1/current_q         | -6.833554855505293      |
| train_1/fw_bonus          | -0.9923278495669365     |
| train_1/fw_loss           | 0.08278951160609722     |
| train_1/mu_grads          | -0.06886710096150636    |
| train_1/mu_grads_std      | 0.28252786844968797     |
| train_1/mu_loss           | 5.525115899113638       |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -5.497262829840795      |
| train_1/q_grads           | -0.10227154884487391    |
| train_1/q_grads_std       | 0.47932257503271103     |
| train_1/q_loss            | 0.2890128343192925      |
| train_1/reward            | -2.6755477818889633     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.002490234375          |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.014074074074074074    |
| train_1/target_q          | -6.822555487951038      |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 38
Time for epoch 38: 976.08. Rollout time: 622.84, Training time: 352.99
Evaluating epoch 38
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
-------------------------------------------------------
| epoch                     | 38                      |
| policy/steps              | 3528197.0               |
| test/episodes             | 975.0                   |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -4.2931769078575165e-18 |
| test_1/avg_q              | -13.47082537183335      |
| test_1/n_subgoals         | 675.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 3900.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -1.5338241308517808e-10 |
| train_0/current_q         | -2.356029752062124e-10  |
| train_0/fw_bonus          | -0.9995313361287117     |
| train_0/fw_loss           | 0.004154926486080512    |
| train_0/mu_grads          | -0.0012459498830139637  |
| train_0/mu_grads_std      | 0.2130027562379837      |
| train_0/mu_loss           | 2.0081015505927646e-13  |
| train_0/next_q            | -2.730394426560539e-13  |
| train_0/q_grads           | -0.020391663582995535   |
| train_0/q_grads_std       | 0.2736744537949562      |
| train_0/q_loss            | 0.48669794688125007     |
| train_0/reward            | -0.5893786825192364     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.0228759765625         |
| train_0/target_q          | -0.5893786825195039     |
| train_1/avg_q             | -20.24640095945073      |
| train_1/current_q         | -6.863079676260819      |
| train_1/fw_bonus          | -0.9928746342658996     |
| train_1/fw_loss           | 0.07849637866020202     |
| train_1/mu_grads          | -0.06944076996296644    |
| train_1/mu_grads_std      | 0.28339107409119607     |
| train_1/mu_loss           | 5.578372183593023       |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -5.567554946761755      |
| train_1/q_grads           | -0.10467937793582678    |
| train_1/q_grads_std       | 0.48159106001257895     |
| train_1/q_loss            | 0.21079090976688963     |
| train_1/reward            | -2.6521554258000832     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.0024169921875         |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.015555555555555555    |
| train_1/target_q          | -6.8527573764481815     |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 39
Time for epoch 39: 862.31. Rollout time: 560.91, Training time: 301.23
Evaluating epoch 39
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 39                     |
| policy/steps              | 3618847.0              |
| test/episodes             | 1000.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -1.270249236746877e-16 |
| test_1/avg_q              | -16.068131247942677    |
| test_1/n_subgoals         | 1353.0                 |
| test_1/subgoal_succ_rate  | 0.5203252032520326     |
| train/episodes            | 4000.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -6.564412052448773e-11 |
| train_0/current_q         | -5.576266933660774e-11 |
| train_0/fw_bonus          | -0.9995368793606758    |
| train_0/fw_loss           | 0.004115834238473326   |
| train_0/mu_grads          | -0.0012459498830139637 |
| train_0/mu_grads_std      | 0.2130027562379837     |
| train_0/mu_loss           | 5.952589498649726e-13  |
| train_0/next_q            | -9.631684934062204e-13 |
| train_0/q_grads           | -0.02040837500244379   |
| train_0/q_grads_std       | 0.27367310374975207    |
| train_0/q_loss            | 0.4862432091432087     |
| train_0/reward            | -0.58901385778845      |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.0233642578125        |
| train_0/target_q          | -0.5890138577893935    |
| train_1/avg_q             | -20.21818753094908     |
| train_1/current_q         | -7.394809586379449     |
| train_1/fw_bonus          | -0.9929450124502182    |
| train_1/fw_loss           | 0.07794387526810169    |
| train_1/mu_grads          | -0.07102153673768044   |
| train_1/mu_grads_std      | 0.2830354042351246     |
| train_1/mu_loss           | 6.256435753018877      |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -6.2571746573645965    |
| train_1/q_grads           | -0.10808086302131414   |
| train_1/q_grads_std       | 0.48647875934839246    |
| train_1/q_loss            | 0.266329564026251      |
| train_1/reward            | -2.651403976934307     |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0024169921875        |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.009259259259259259   |
| train_1/target_q          | -7.374831427631138     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 40
Time for epoch 40: 725.30. Rollout time: 473.93, Training time: 251.22
Evaluating epoch 40
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
-------------------------------------------------------
| epoch                     | 40                      |
| policy/steps              | 3709127.0               |
| test/episodes             | 1025.0                  |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -1.0698755605669752e-16 |
| test_1/avg_q              | -14.015438469971729     |
| test_1/n_subgoals         | 675.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 4100.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -7.732301364602117e-11  |
| train_0/current_q         | -1.852853032791984e-10  |
| train_0/fw_bonus          | -0.9995368212461472     |
| train_0/fw_loss           | 0.004116288787918166    |
| train_0/mu_grads          | -0.0012459498830139637  |
| train_0/mu_grads_std      | 0.2130027562379837      |
| train_0/mu_loss           | 2.0842870150327524e-12  |
| train_0/next_q            | -7.679308896045619e-13  |
| train_0/q_grads           | -0.020454374607652426   |
| train_0/q_grads_std       | 0.27366126254200934     |
| train_0/q_loss            | 0.4897004680489269      |
| train_0/reward            | -0.5917787341448275     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.0224609375            |
| train_0/target_q          | -0.5917787341455797     |
| train_1/avg_q             | -20.076755476913704     |
| train_1/current_q         | -7.274383928388239      |
| train_1/fw_bonus          | -0.9929335206747055     |
| train_1/fw_loss           | 0.07803404778242111     |
| train_1/mu_grads          | -0.07073484212160111    |
| train_1/mu_grads_std      | 0.2838695041835308      |
| train_1/mu_loss           | 6.096115349939275       |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -6.091108036707679      |
| train_1/q_grads           | -0.10891600232571363    |
| train_1/q_grads_std       | 0.49390351101756097     |
| train_1/q_loss            | 0.1940863203433929      |
| train_1/reward            | -2.653250467824182      |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.0021240234375         |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.016296296296296295    |
| train_1/target_q          | -7.261126235861072      |
-------------------------------------------------------
Saving periodic policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_40.pkl ...
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 41
Time for epoch 41: 742.12. Rollout time: 483.87, Training time: 258.14
Evaluating epoch 41
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
-----------------------------------------------------
| epoch                     | 41                    |
| policy/steps              | 3799853.0             |
| test/episodes             | 1050.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -4.668243600478999    |
| test_1/avg_q              | -12.154941697951672   |
| test_1/n_subgoals         | 2053.0                |
| test_1/subgoal_succ_rate  | 0.6970287384315635    |
| train/episodes            | 4200.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -0.7747004370774464   |
| train_0/current_q         | -4.959804158962933    |
| train_0/fw_bonus          | -0.9995106995105744   |
| train_0/fw_loss           | 0.004300442000385374  |
| train_0/mu_grads          | 0.00812220472143963   |
| train_0/mu_grads_std      | 0.2362718664109707    |
| train_0/mu_loss           | 5.047872156980915     |
| train_0/next_q            | -4.969610533137406    |
| train_0/q_grads           | -0.027737015392631292 |
| train_0/q_grads_std       | 0.27842145338654517   |
| train_0/q_loss            | 1.5035681373923921    |
| train_0/reward            | -0.5905820190997474   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0247314453125       |
| train_0/target_q          | -4.864499508971053    |
| train_1/avg_q             | -20.23668864623579    |
| train_1/current_q         | -6.8718849755871005   |
| train_1/fw_bonus          | -0.9926295682787896   |
| train_1/fw_loss           | 0.08042051866650582   |
| train_1/mu_grads          | -0.07146678231656552  |
| train_1/mu_grads_std      | 0.2854940786957741    |
| train_1/mu_loss           | 5.566931610801386     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -5.558552095412805    |
| train_1/q_grads           | -0.11040054447948933  |
| train_1/q_grads_std       | 0.5018862307071685    |
| train_1/q_loss            | 0.2978256844701991    |
| train_1/reward            | -2.65242375081325     |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0023193359375       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.008148148148148147  |
| train_1/target_q          | -6.855540420145952    |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 42
Time for epoch 42: 745.64. Rollout time: 480.97, Training time: 264.55
Evaluating epoch 42
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
-----------------------------------------------------
| epoch                     | 42                    |
| policy/steps              | 3890291.0             |
| test/episodes             | 1075.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -12.796449999714973   |
| test_1/avg_q              | -11.951797589058957   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 4300.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -21.690800099260954   |
| train_0/current_q         | -9.349182549998705    |
| train_0/fw_bonus          | -0.9994759172201156   |
| train_0/fw_loss           | 0.00454568910645321   |
| train_0/mu_grads          | 0.007829812029376626  |
| train_0/mu_grads_std      | 0.24287580959498883   |
| train_0/mu_loss           | 9.344575759922972     |
| train_0/next_q            | -9.349731800144227    |
| train_0/q_grads           | -0.027884485293179752 |
| train_0/q_grads_std       | 0.27838641703128814   |
| train_0/q_loss            | 0.7076253784048198    |
| train_0/reward            | -0.5885957841121126   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0240478515625       |
| train_0/target_q          | -9.50769918144276     |
| train_1/avg_q             | -20.030997112176212   |
| train_1/current_q         | -6.300601204999963    |
| train_1/fw_bonus          | -0.9916751399636269   |
| train_1/fw_loss           | 0.08791432362049818   |
| train_1/mu_grads          | -0.07305970508605242  |
| train_1/mu_grads_std      | 0.28692217022180555   |
| train_1/mu_loss           | 4.859525349197652     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -4.841412082240662    |
| train_1/q_grads           | -0.11108799520879983  |
| train_1/q_grads_std       | 0.5076900750398636    |
| train_1/q_loss            | 0.37313773038014586   |
| train_1/reward            | -2.6277222496653847   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0033203125          |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.014074074074074074  |
| train_1/target_q          | -6.284382277642145    |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 43
Time for epoch 43: 740.51. Rollout time: 480.69, Training time: 259.72
Evaluating epoch 43
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
-----------------------------------------------------
| epoch                     | 43                    |
| policy/steps              | 3980606.0             |
| test/episodes             | 1100.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -18.719484926630397   |
| test_1/avg_q              | -13.619906300068982   |
| test_1/n_subgoals         | 1377.0                |
| test_1/subgoal_succ_rate  | 0.5294117647058824    |
| train/episodes            | 4400.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.25779177594627    |
| train_0/current_q         | -9.704799634886339    |
| train_0/fw_bonus          | -0.9994321808218956   |
| train_0/fw_loss           | 0.004853973037097603  |
| train_0/mu_grads          | 0.005070790182799101  |
| train_0/mu_grads_std      | 0.24545924440026284   |
| train_0/mu_loss           | 9.796015774141239     |
| train_0/next_q            | -9.77234066187741     |
| train_0/q_grads           | -0.028700515069067477 |
| train_0/q_grads_std       | 0.2788988836109638    |
| train_0/q_loss            | 1.265667212869408     |
| train_0/reward            | -0.5929976948336844   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.028857421875        |
| train_0/target_q          | -9.85645337866681     |
| train_1/avg_q             | -19.963997148334172   |
| train_1/current_q         | -6.310466194949025    |
| train_1/fw_bonus          | -0.9903930306434632   |
| train_1/fw_loss           | 0.09798083417117595   |
| train_1/mu_grads          | -0.07415706794708968  |
| train_1/mu_grads_std      | 0.2885202348232269    |
| train_1/mu_loss           | 4.885366537827243     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -4.884308660624072    |
| train_1/q_grads           | -0.11185988672077656  |
| train_1/q_grads_std       | 0.5117493569850922    |
| train_1/q_loss            | 0.2541933050797528    |
| train_1/reward            | -2.615536487224017    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.002880859375        |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.015925925925925927  |
| train_1/target_q          | -6.302787706652044    |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 44
Time for epoch 44: 742.51. Rollout time: 479.21, Training time: 263.21
Evaluating epoch 44
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
----------------------------------------------------
| epoch                     | 44                   |
| policy/steps              | 4071002.0            |
| test/episodes             | 1125.0               |
| test/success_rate         | 0.0                  |
| test_0/avg_q              | -12.514747315817576  |
| test_1/avg_q              | -12.828092783478038  |
| test_1/n_subgoals         | 675.0                |
| test_1/subgoal_succ_rate  | 0.0                  |
| train/episodes            | 4500.0               |
| train/success_rate        | 0.0                  |
| train_0/avg_q             | -26.51639928341062   |
| train_0/current_q         | -9.517849854214658   |
| train_0/fw_bonus          | -0.9994447514414787  |
| train_0/fw_loss           | 0.004765410197433084 |
| train_0/mu_grads          | 0.010303745977580547 |
| train_0/mu_grads_std      | 0.24799174070358276  |
| train_0/mu_loss           | 9.54614668450811     |
| train_0/next_q            | -9.545425699157523   |
| train_0/q_grads           | -0.0296553551685065  |
| train_0/q_grads_std       | 0.2796121135354042   |
| train_0/q_loss            | 1.0674511540699148   |
| train_0/reward            | -0.589704936954513   |
| train_0/reward_-0.0_frac  | 0.0                  |
| train_0/reward_-1.0_frac  | 0.021044921875       |
| train_0/target_q          | -9.667795186480706   |
| train_1/avg_q             | -20.18919390755034   |
| train_1/current_q         | -6.3043414395116     |
| train_1/fw_bonus          | -0.990067207813263   |
| train_1/fw_loss           | 0.10053888633847237  |
| train_1/mu_grads          | -0.07472870405763388 |
| train_1/mu_grads_std      | 0.29007756859064104  |
| train_1/mu_loss           | 4.8989730258362965   |
| train_1/n_subgoals        | 2700.0               |
| train_1/next_q            | -4.9023347980425065  |
| train_1/q_grads           | -0.11353902835398913 |
| train_1/q_grads_std       | 0.5171972587704659   |
| train_1/q_loss            | 0.24706436818877395  |
| train_1/reward            | -2.5853891175640458  |
| train_1/reward_-0.0_frac  | 0.0                  |
| train_1/reward_-1.0_frac  | 0.003759765625       |
| train_1/reward_-27.0_frac | 0.0                  |
| train_1/subgoal_succ_rate | 0.014814814814814815 |
| train_1/target_q          | -6.296952469483841   |
----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 45
Time for epoch 45: 748.56. Rollout time: 479.27, Training time: 269.14
Evaluating epoch 45
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
----------------------------------------------------
| epoch                     | 45                   |
| policy/steps              | 4161459.0            |
| test/episodes             | 1150.0               |
| test/success_rate         | 0.0                  |
| test_0/avg_q              | -14.386346359654286  |
| test_1/avg_q              | -11.254914940706486  |
| test_1/n_subgoals         | 675.0                |
| test_1/subgoal_succ_rate  | 0.0                  |
| train/episodes            | 4600.0               |
| train/success_rate        | 0.0                  |
| train_0/avg_q             | -26.53557198527518   |
| train_0/current_q         | -9.679854261746398   |
| train_0/fw_bonus          | -0.9994639351963996  |
| train_0/fw_loss           | 0.004630134662147611 |
| train_0/mu_grads          | 0.010303742252290249 |
| train_0/mu_grads_std      | 0.24799174070358276  |
| train_0/mu_loss           | 9.706496886699094    |
| train_0/next_q            | -9.709844094722019   |
| train_0/q_grads           | -0.03022254235111177 |
| train_0/q_grads_std       | 0.2808156557381153   |
| train_0/q_loss            | 1.0472265381833004   |
| train_0/reward            | -0.5909182293842605  |
| train_0/reward_-0.0_frac  | 0.0                  |
| train_0/reward_-1.0_frac  | 0.0248046875         |
| train_0/target_q          | -9.821299641511382   |
| train_1/avg_q             | -20.139777458408084  |
| train_1/current_q         | -6.103445175134075   |
| train_1/fw_bonus          | -0.9890638336539268  |
| train_1/fw_loss           | 0.108416891656816    |
| train_1/mu_grads          | -0.07578865177929402 |
| train_1/mu_grads_std      | 0.29222635328769686  |
| train_1/mu_loss           | 4.669269489467597    |
| train_1/n_subgoals        | 2700.0               |
| train_1/next_q            | -4.672531135505873   |
| train_1/q_grads           | -0.1147280476987362  |
| train_1/q_grads_std       | 0.5211639866232872   |
| train_1/q_loss            | 0.30802790379595246  |
| train_1/reward            | -2.521362640685402   |
| train_1/reward_-0.0_frac  | 0.0                  |
| train_1/reward_-1.0_frac  | 0.0028564453125      |
| train_1/reward_-27.0_frac | 0.0                  |
| train_1/subgoal_succ_rate | 0.012222222222222223 |
| train_1/target_q          | -6.087245518893357   |
----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 46
Time for epoch 46: 742.90. Rollout time: 476.28, Training time: 266.49
Evaluating epoch 46
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
-----------------------------------------------------
| epoch                     | 46                    |
| policy/steps              | 4252087.0             |
| test/episodes             | 1175.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -18.40526066110813    |
| test_1/avg_q              | -11.13515931207884    |
| test_1/n_subgoals         | 1351.0                |
| test_1/subgoal_succ_rate  | 0.5196150999259808    |
| train/episodes            | 4700.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.564261023991865   |
| train_0/current_q         | -9.399685358978514    |
| train_0/fw_bonus          | -0.9994803965091705   |
| train_0/fw_loss           | 0.004514094348996878  |
| train_0/mu_grads          | 0.010303651914000511  |
| train_0/mu_grads_std      | 0.24799171090126038   |
| train_0/mu_loss           | 9.415361744906319     |
| train_0/next_q            | -9.409540922813022    |
| train_0/q_grads           | -0.030032815132290124 |
| train_0/q_grads_std       | 0.28180027306079863   |
| train_0/q_loss            | 0.8609364918611689    |
| train_0/reward            | -0.5855573535758595   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.025634765625        |
| train_0/target_q          | -9.564763977735078    |
| train_1/avg_q             | -20.146634057351353   |
| train_1/current_q         | -6.862508867408136    |
| train_1/fw_bonus          | -0.989619193971157    |
| train_1/fw_loss           | 0.10405651405453682   |
| train_1/mu_grads          | -0.07673949655145407  |
| train_1/mu_grads_std      | 0.29323542192578317   |
| train_1/mu_loss           | 5.689306930857959     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -5.6959365433248115   |
| train_1/q_grads           | -0.1159326085820794   |
| train_1/q_grads_std       | 0.526498767733574     |
| train_1/q_loss            | 0.3867162227737823    |
| train_1/reward            | -2.560997327684163    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0021240234375       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.01037037037037037   |
| train_1/target_q          | -6.847885026137545    |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 47
Time for epoch 47: 781.50. Rollout time: 494.98, Training time: 286.38
Evaluating epoch 47
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
----------------------------------------------------
| epoch                     | 47                   |
| policy/steps              | 4342552.0            |
| test/episodes             | 1200.0               |
| test/success_rate         | 0.0                  |
| test_0/avg_q              | -21.28974210602037   |
| test_1/avg_q              | -11.781887147908913  |
| test_1/n_subgoals         | 675.0                |
| test_1/subgoal_succ_rate  | 0.0                  |
| train/episodes            | 4800.0               |
| train/success_rate        | 0.0                  |
| train_0/avg_q             | -26.721009474647595  |
| train_0/current_q         | -9.839947780053267   |
| train_0/fw_bonus          | -0.9995048567652702  |
| train_0/fw_loss           | 0.004341559554450214 |
| train_0/mu_grads          | 0.010303743183612823 |
| train_0/mu_grads_std      | 0.24799174070358276  |
| train_0/mu_loss           | 9.917863599466784    |
| train_0/next_q            | -9.905115010485067   |
| train_0/q_grads           | -0.03005401585251093 |
| train_0/q_grads_std       | 0.28387595638632773  |
| train_0/q_loss            | 1.3778272742548083   |
| train_0/reward            | -0.5895734810710564  |
| train_0/reward_-0.0_frac  | 0.0                  |
| train_0/reward_-1.0_frac  | 0.023486328125       |
| train_0/target_q          | -9.98680639169828    |
| train_1/avg_q             | -20.134059702944914  |
| train_1/current_q         | -6.218072994318646   |
| train_1/fw_bonus          | -0.9899034991860389  |
| train_1/fw_loss           | 0.10182436481118202  |
| train_1/mu_grads          | -0.07742063626646996 |
| train_1/mu_grads_std      | 0.29481716752052306  |
| train_1/mu_loss           | 4.7749812429482414   |
| train_1/n_subgoals        | 2700.0               |
| train_1/next_q            | -4.776778832268596   |
| train_1/q_grads           | -0.11686395201832056 |
| train_1/q_grads_std       | 0.5321942761540412   |
| train_1/q_loss            | 0.23187382472378681  |
| train_1/reward            | -2.616073284115191   |
| train_1/reward_-0.0_frac  | 0.0                  |
| train_1/reward_-1.0_frac  | 0.001806640625       |
| train_1/reward_-27.0_frac | 0.0                  |
| train_1/subgoal_succ_rate | 0.012222222222222223 |
| train_1/target_q          | -6.201774114519912   |
----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 48
Time for epoch 48: 765.99. Rollout time: 491.01, Training time: 274.88
Evaluating epoch 48
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
-----------------------------------------------------
| epoch                     | 48                    |
| policy/steps              | 4433193.0             |
| test/episodes             | 1225.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -20.728155641951137   |
| test_1/avg_q              | -12.719583961598879   |
| test_1/n_subgoals         | 1351.0                |
| test_1/subgoal_succ_rate  | 0.5196150999259808    |
| train/episodes            | 4900.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.730803388081206   |
| train_0/current_q         | -9.91441803234558     |
| train_0/fw_bonus          | -0.9995450422167778   |
| train_0/fw_loss           | 0.0040582625253591685 |
| train_0/mu_grads          | 0.010301155038177967  |
| train_0/mu_grads_std      | 0.247990682721138     |
| train_0/mu_loss           | 9.99156782999492      |
| train_0/next_q            | -9.978774871211987    |
| train_0/q_grads           | -0.02981372196227312  |
| train_0/q_grads_std       | 0.285704156011343     |
| train_0/q_loss            | 1.6157065770688235    |
| train_0/reward            | -0.5864707330969395   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.023974609375        |
| train_0/target_q          | -10.06437293344441    |
| train_1/avg_q             | -20.184306459862867   |
| train_1/current_q         | -6.602256950908133    |
| train_1/fw_bonus          | -0.9903518825769424   |
| train_1/fw_loss           | 0.09830385036766529   |
| train_1/mu_grads          | -0.07871413100510835  |
| train_1/mu_grads_std      | 0.29733057171106336   |
| train_1/mu_loss           | 5.297574720463465     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -5.310924857903226    |
| train_1/q_grads           | -0.1188651330769062   |
| train_1/q_grads_std       | 0.5351811096072197    |
| train_1/q_loss            | 0.2667304257079609    |
| train_1/reward            | -2.5747119932235365   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.00185546875         |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.011481481481481481  |
| train_1/target_q          | -6.588631236357308    |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 49
Time for epoch 49: 750.34. Rollout time: 485.10, Training time: 265.11
Evaluating epoch 49
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
-----------------------------------------------------
| epoch                     | 49                    |
| policy/steps              | 4523790.0             |
| test/episodes             | 1250.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -16.74595083174491    |
| test_1/avg_q              | -13.983551805811151   |
| test_1/n_subgoals         | 2053.0                |
| test_1/subgoal_succ_rate  | 0.6970287384315635    |
| train/episodes            | 5000.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.730276050353933   |
| train_0/current_q         | -9.800819500122515    |
| train_0/fw_bonus          | -0.9995444774627685   |
| train_0/fw_loss           | 0.00406223923782818   |
| train_0/mu_grads          | 0.010301155038177967  |
| train_0/mu_grads_std      | 0.247990682721138     |
| train_0/mu_loss           | 9.890865854254681     |
| train_0/next_q            | -9.87276707689288     |
| train_0/q_grads           | -0.029742359882220626 |
| train_0/q_grads_std       | 0.28824960738420485   |
| train_0/q_loss            | 1.4587473243397597    |
| train_0/reward            | -0.5853902699051104   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0244140625          |
| train_0/target_q          | -9.962141315984685    |
| train_1/avg_q             | -20.135899924236014   |
| train_1/current_q         | -6.683343394937561    |
| train_1/fw_bonus          | -0.9907421678304672   |
| train_1/fw_loss           | 0.09523947518318891   |
| train_1/mu_grads          | -0.08084010779857635  |
| train_1/mu_grads_std      | 0.2976909205317497    |
| train_1/mu_loss           | 5.443020610247407     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -5.443084411256639    |
| train_1/q_grads           | -0.12147925719618798  |
| train_1/q_grads_std       | 0.5400994837284088    |
| train_1/q_loss            | 0.20997993264059941   |
| train_1/reward            | -2.590117183804614    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0012939453125       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.01037037037037037   |
| train_1/target_q          | -6.670208802560015    |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 50
Time for epoch 50: 752.86. Rollout time: 488.04, Training time: 264.68
Evaluating epoch 50
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
-----------------------------------------------------
| epoch                     | 50                    |
| policy/steps              | 4614284.0             |
| test/episodes             | 1275.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -19.34206061318778    |
| test_1/avg_q              | -12.537938414373404   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 5100.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.73434082373051    |
| train_0/current_q         | -9.833980625507882    |
| train_0/fw_bonus          | -0.9995394304394722   |
| train_0/fw_loss           | 0.004097838333109394  |
| train_0/mu_grads          | 0.010301215574145317  |
| train_0/mu_grads_std      | 0.2479907125234604    |
| train_0/mu_loss           | 9.87943378211707      |
| train_0/next_q            | -9.879713239076086    |
| train_0/q_grads           | -0.030520049296319483 |
| train_0/q_grads_std       | 0.2900529034435749    |
| train_0/q_loss            | 1.4060055611037519    |
| train_0/reward            | -0.5889961854016292   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0230224609375       |
| train_0/target_q          | -9.994113119527466    |
| train_1/avg_q             | -20.195665647678997   |
| train_1/current_q         | -6.774059991815899    |
| train_1/fw_bonus          | -0.9906616628170013   |
| train_1/fw_loss           | 0.09587151315063239   |
| train_1/mu_grads          | -0.08167720623314381  |
| train_1/mu_grads_std      | 0.29807612374424935   |
| train_1/mu_loss           | 5.536296273916017     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -5.523432127257026    |
| train_1/q_grads           | -0.12305166944861412  |
| train_1/q_grads_std       | 0.5468608483672142    |
| train_1/q_loss            | 0.21824990994365684   |
| train_1/reward            | -2.6017242572484975   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.001318359375        |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.014074074074074074  |
| train_1/target_q          | -6.7559302535515915   |
-----------------------------------------------------
Saving periodic policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_50.pkl ...
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 51
Time for epoch 51: 1800.05. Rollout time: 1386.63, Training time: 413.25
Evaluating epoch 51
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
-----------------------------------------------------
| epoch                     | 51                    |
| policy/steps              | 4704638.0             |
| test/episodes             | 1300.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -18.108476398959787   |
| test_1/avg_q              | -8.554959555993152    |
| test_1/n_subgoals         | 2028.0                |
| test_1/subgoal_succ_rate  | 0.6928007889546351    |
| train/episodes            | 5200.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.762922441292343   |
| train_0/current_q         | -9.905580768644963    |
| train_0/fw_bonus          | -0.9995479971170426   |
| train_0/fw_loss           | 0.004037436243379489  |
| train_0/mu_grads          | 0.010301215574145317  |
| train_0/mu_grads_std      | 0.2479907125234604    |
| train_0/mu_loss           | 10.0034983037495      |
| train_0/next_q            | -9.983709550874499    |
| train_0/q_grads           | -0.030931660626083612 |
| train_0/q_grads_std       | 0.29307173043489454   |
| train_0/q_loss            | 1.9196475547155216    |
| train_0/reward            | -0.5833037469401461   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.023583984375        |
| train_0/target_q          | -10.044624591254466   |
| train_1/avg_q             | -20.121214257840993   |
| train_1/current_q         | -6.368542878097185    |
| train_1/fw_bonus          | -0.9903680175542832   |
| train_1/fw_loss           | 0.09817709792405367   |
| train_1/mu_grads          | -0.08261672183871269  |
| train_1/mu_grads_std      | 0.3004780150949955    |
| train_1/mu_loss           | 5.010965699223393     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -5.026765876685483    |
| train_1/q_grads           | -0.12414941098541021  |
| train_1/q_grads_std       | 0.5526794284582138    |
| train_1/q_loss            | 0.22203030199095694   |
| train_1/reward            | -2.5519010893272935   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0013427734375       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.015555555555555555  |
| train_1/target_q          | -6.360747918841468    |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 52
