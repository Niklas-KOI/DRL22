Starting process id: 2438
T: 700
alg: chac
algorithm: src.chac
atomic_noise: 0.2
base_logdir: data
batch_size: 1024
bind_core: 0
buffer_size: 500
chac_params: {}
early_stop_data_column: test/success_rate
early_stop_threshold: 100.0
env_name: AntReacherEnv-v0
eta: 0.75
fw: 1
fw_hidden_size: 256,256,256
fw_lr: 0.001
gamma: 0.9985714285714286
graph: 1
info: 
make_env: <function prepare_params.<locals>.make_env at 0x7f7a995fc3b0>
max_try_idx: 199
mu_hidden_size: 64
mu_lr: 0.001
n_episodes: 100
n_levels: 2
n_pre_episodes: 30
n_test_rollouts: 25
n_train_batches: 40
n_train_rollouts: 100
num_threads: 1
q_hidden_size: 64
q_lr: 0.001
random_action_perc: 0.3
regularization: True
render: 0
rollout_batch_size: 1
subgoal_noise: 0.2
subgoal_test_perc: 0.3
time_scales: 27,27
try_start_idx: 100
use_mpi: False
verbose: False

*** Warning ***
You are running src.chac with just a single MPI worker. This will work, but the HER experiments that we report in Plappert et al. (2018, https://arxiv.org/abs/1802.09464) were obtained with --num_cpu 19. This makes a significant difference and if you are looking to reproduce those results, be aware of this. Please also refer to https://github.com/openai/baselines/issues/314 for further details.
****************

dims: action = 8, subgoal = 5, end_goal = 3
subgoal_bounds: symmetric [11.75 11.75  0.5   3.    3.  ], offset [0.  0.  0.5 0.  0. ]
Running on CPU ...
Creating a CHAC agent

Hierarchy Level 0 with time scale 27
Actor(
  (fc1): Linear(in_features=34, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
  (fc4): Linear(in_features=64, out_features=8, bias=True)
)
Critic(
  (fc1): Linear(in_features=42, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
  (fc4): Linear(in_features=64, out_features=1, bias=True)
  (mse_loss): MSELoss()
)
ForwardModel(
  (mlp): Sequential(
    (0): Linear(in_features=37, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=256, bias=True)
    (5): ReLU()
    (6): Linear(in_features=256, out_features=29, bias=True)
    (7): Identity()
  )
  (mse_loss): MSELoss()
)

Hierarchy Level 1 with time scale 27
Actor(
  (fc1): Linear(in_features=32, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
  (fc4): Linear(in_features=64, out_features=5, bias=True)
)
Critic(
  (fc1): Linear(in_features=37, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
  (fc4): Linear(in_features=64, out_features=1, bias=True)
  (mse_loss): MSELoss()
)
ForwardModel(
  (mlp): Sequential(
    (0): Linear(in_features=34, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=256, bias=True)
    (5): ReLU()
    (6): Linear(in_features=256, out_features=29, bias=True)
    (7): Identity()
  )
  (mse_loss): MSELoss()
)
Training epoch 0
Time for epoch 0: 514.34. Rollout time: 239.44, Training time: 274.86
Evaluating epoch 0
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 0                     |
| policy/steps              | 90839.0               |
| test/episodes             | 25.0                  |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -21.466540424050194   |
| test_1/avg_q              | -20.061033888161965   |
| test_1/n_subgoals         | 1353.0                |
| test_1/subgoal_succ_rate  | 0.5203252032520326    |
| train/episodes            | 100.0                 |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -13.348622274445342   |
| train_0/current_q         | -7.757612793040539    |
| train_0/fw_bonus          | -0.9959300726652145   |
| train_0/fw_loss           | 0.03244976061396301   |
| train_0/mu_grads          | -0.003115333378082141 |
| train_0/mu_grads_std      | 0.13056274317204952   |
| train_0/mu_loss           | 7.605775849599013     |
| train_0/next_q            | -7.605780525301009    |
| train_0/q_grads           | 0.0067345866234973075 |
| train_0/q_grads_std       | 0.11684890519827604   |
| train_0/q_loss            | 0.32405904018760234   |
| train_0/reward            | -0.6262713357584289   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0005615234375       |
| train_0/target_q          | -7.863351144646421    |
| train_1/avg_q             | -13.040250274946496   |
| train_1/current_q         | -4.383666945367932    |
| train_1/fw_bonus          | -0.994734251499176    |
| train_1/fw_loss           | 0.0646008849143982    |
| train_1/mu_grads          | -0.011178472824394704 |
| train_1/mu_grads_std      | 0.25067340284585954   |
| train_1/mu_loss           | 3.255071010528397     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -2.2585414624423588   |
| train_1/q_grads           | 0.017771474132314325  |
| train_1/q_grads_std       | 0.1271185077726841    |
| train_1/q_loss            | 0.9013434068847583    |
| train_1/reward            | -2.6655658904357553   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0133544921875       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.004814814814814815  |
| train_1/target_q          | -4.38308884544834     |
-----------------------------------------------------
Saving periodic policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_0.pkl ...
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Training epoch 1
Time for epoch 1: 433.74. Rollout time: 236.70, Training time: 197.01
Evaluating epoch 1
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 1                     |
| policy/steps              | 180657.0              |
| test/episodes             | 50.0                  |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -16.71914773895712    |
| test_1/avg_q              | -18.420756698421872   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 200.0                 |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -21.045267597596403   |
| train_0/current_q         | -8.087292671068116    |
| train_0/fw_bonus          | -0.9978091612458229   |
| train_0/fw_loss           | 0.017660787887871267  |
| train_0/mu_grads          | -0.012627492635510862 |
| train_0/mu_grads_std      | 0.16669862084090709   |
| train_0/mu_loss           | 7.990397907072264     |
| train_0/next_q            | -7.980523405243465    |
| train_0/q_grads           | 0.00802669848781079   |
| train_0/q_grads_std       | 0.13454201444983482   |
| train_0/q_loss            | 0.3465702370186412    |
| train_0/reward            | -0.6121109830306523   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0087890625          |
| train_0/target_q          | -8.13942657448907     |
| train_1/avg_q             | -20.33203516641942    |
| train_1/current_q         | -10.222393803962134   |
| train_1/fw_bonus          | -0.9935546398162842   |
| train_1/fw_loss           | 0.07525314390659332   |
| train_1/mu_grads          | -0.020901661878451706 |
| train_1/mu_grads_std      | 0.24716336354613305   |
| train_1/mu_loss           | 10.842839433436305    |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -9.859114168547942    |
| train_1/q_grads           | 0.001984027397702448  |
| train_1/q_grads_std       | 0.14903330393135547   |
| train_1/q_loss            | 3.4818464604840784    |
| train_1/reward            | -2.6530578488895116   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0103271484375       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.020370370370370372  |
| train_1/target_q          | -10.083974174641273   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Training epoch 2
Time for epoch 2: 438.89. Rollout time: 235.67, Training time: 203.19
Evaluating epoch 2
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 2                     |
| policy/steps              | 271477.0              |
| test/episodes             | 75.0                  |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -25.106856812073254   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 300.0                 |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -22.264085241633214   |
| train_0/current_q         | -8.73294332356993     |
| train_0/fw_bonus          | -0.9983283445239067   |
| train_0/fw_loss           | 0.013574628555215896  |
| train_0/mu_grads          | -0.018124200636520982 |
| train_0/mu_grads_std      | 0.21958992667496205   |
| train_0/mu_loss           | 8.664047251927187     |
| train_0/next_q            | -8.661099179114206    |
| train_0/q_grads           | 0.004652959294617176  |
| train_0/q_grads_std       | 0.1509657919406891    |
| train_0/q_loss            | 0.34643760175404426   |
| train_0/reward            | -0.6168550381797104   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.01083984375         |
| train_0/target_q          | -8.846879899667645    |
| train_1/avg_q             | -20.285710981377242   |
| train_1/current_q         | -5.596776761228214    |
| train_1/fw_bonus          | -0.9932298555970192   |
| train_1/fw_loss           | 0.07818600293248892   |
| train_1/mu_grads          | -0.01744551407173276  |
| train_1/mu_grads_std      | 0.2528083622455597    |
| train_1/mu_loss           | 5.065605467443559     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -3.836054630943712    |
| train_1/q_grads           | -0.012543143541552126 |
| train_1/q_grads_std       | 0.16259391903877257   |
| train_1/q_loss            | 5.052206363655509     |
| train_1/reward            | -2.6413677655564243   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.008056640625        |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.005555555555555556  |
| train_1/target_q          | -5.533163500306469    |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Training epoch 3
Time for epoch 3: 432.21. Rollout time: 230.11, Training time: 202.07
Evaluating epoch 3
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
------------------------------------------------------
| epoch                     | 3                      |
| policy/steps              | 362578.0               |
| test/episodes             | 100.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -27.0                  |
| test_1/avg_q              | -21.029377825268202    |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 400.0                  |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -26.65214415085884     |
| train_0/current_q         | -8.831014223961414     |
| train_0/fw_bonus          | -0.9985879987478257    |
| train_0/fw_loss           | 0.011531352670863271   |
| train_0/mu_grads          | -0.017325551761314274  |
| train_0/mu_grads_std      | 0.24908596202731131    |
| train_0/mu_loss           | 8.748050574291295      |
| train_0/next_q            | -8.747464202458142     |
| train_0/q_grads           | 0.0047886878601275384  |
| train_0/q_grads_std       | 0.1557400304824114     |
| train_0/q_loss            | 0.32910426582636204    |
| train_0/reward            | -0.6187326873176062    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.0500244140625        |
| train_0/target_q          | -8.927266333923413     |
| train_1/avg_q             | -21.486874878574348    |
| train_1/current_q         | -4.3479475925127655    |
| train_1/fw_bonus          | -0.9925564616918564    |
| train_1/fw_loss           | 0.08426678534597158    |
| train_1/mu_grads          | -0.01296584545634687   |
| train_1/mu_grads_std      | 0.28789376765489577    |
| train_1/mu_loss           | 3.2477844798911817     |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -2.2388869957129005    |
| train_1/q_grads           | -0.02021090337075293   |
| train_1/q_grads_std       | 0.175550365075469      |
| train_1/q_loss            | 1.0525938948864888     |
| train_1/reward            | -2.653302688727126     |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.00712890625          |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.00037037037037037035 |
| train_1/target_q          | -4.3164905057451195    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 4
Time for epoch 4: 449.07. Rollout time: 242.90, Training time: 206.13
Evaluating epoch 4
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 4                     |
| policy/steps              | 453654.0              |
| test/episodes             | 125.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -26.999985827877396   |
| test_1/avg_q              | -20.336988418354135   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 500.0                 |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.658281955017234   |
| train_0/current_q         | -9.05876310893288     |
| train_0/fw_bonus          | -0.9986978396773338   |
| train_0/fw_loss           | 0.010666706110350787  |
| train_0/mu_grads          | -0.02046169210225344  |
| train_0/mu_grads_std      | 0.2712717466056347    |
| train_0/mu_loss           | 8.986967665097437     |
| train_0/next_q            | -8.985196292696564    |
| train_0/q_grads           | -0.002861175744328648 |
| train_0/q_grads_std       | 0.16576949059963225   |
| train_0/q_loss            | 0.3054272568337299    |
| train_0/reward            | -0.6150248013971578   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.06181640625         |
| train_0/target_q          | -9.188001295851958    |
| train_1/avg_q             | -20.978087583071535   |
| train_1/current_q         | -3.823658470277498    |
| train_1/fw_bonus          | -0.9914575174450875   |
| train_1/fw_loss           | 0.09419045168906451   |
| train_1/mu_grads          | -0.011257761530578137 |
| train_1/mu_grads_std      | 0.2718996234238148    |
| train_1/mu_loss           | 2.5089540211045804    |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -1.5173254417563502   |
| train_1/q_grads           | -0.027565789828076958 |
| train_1/q_grads_std       | 0.19461504146456718   |
| train_1/q_loss            | 1.6772937928759588    |
| train_1/reward            | -2.688349000510425    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0068359375          |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0007407407407407407 |
| train_1/target_q          | -3.824610339117703    |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 5
Time for epoch 5: 463.09. Rollout time: 245.74, Training time: 217.31
Evaluating epoch 5
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 5                     |
| policy/steps              | 544690.0              |
| test/episodes             | 150.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -26.99600967196546    |
| test_1/avg_q              | -19.877596861490915   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 600.0                 |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.717706879376465   |
| train_0/current_q         | -9.131573939441177    |
| train_0/fw_bonus          | -0.9987860560417176   |
| train_0/fw_loss           | 0.009972465550526976  |
| train_0/mu_grads          | -0.022000333527103066 |
| train_0/mu_grads_std      | 0.29065086618065833   |
| train_0/mu_loss           | 9.043499262877852     |
| train_0/next_q            | -9.040234392393081    |
| train_0/q_grads           | -0.002940616960404441 |
| train_0/q_grads_std       | 0.17047908194363118   |
| train_0/q_loss            | 0.26891409845853625   |
| train_0/reward            | -0.608546609953919    |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.040234375           |
| train_0/target_q          | -9.267619880531209    |
| train_1/avg_q             | -19.85999202695373    |
| train_1/current_q         | -3.312146575564552    |
| train_1/fw_bonus          | -0.9910176262259484   |
| train_1/fw_loss           | 0.09816287402063609   |
| train_1/mu_grads          | -0.015197468642145395 |
| train_1/mu_grads_std      | 0.32369915395975113   |
| train_1/mu_loss           | 1.8145584093715654    |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -0.8153760374292972   |
| train_1/q_grads           | -0.026044351141899826 |
| train_1/q_grads_std       | 0.21049676574766635   |
| train_1/q_loss            | 0.9266085307422223    |
| train_1/reward            | -2.693201179933021    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0062744140625       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0014814814814814814 |
| train_1/target_q          | -3.2994015550810523   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 6
Time for epoch 6: 473.99. Rollout time: 252.39, Training time: 221.55
Evaluating epoch 6
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 6                     |
| policy/steps              | 635815.0              |
| test/episodes             | 175.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -26.99999410846115    |
| test_1/avg_q              | -20.964612350916006   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 700.0                 |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.991701512825433   |
| train_0/current_q         | -9.261484268733408    |
| train_0/fw_bonus          | -0.9988669604063034   |
| train_0/fw_loss           | 0.009335743519477547  |
| train_0/mu_grads          | -0.02130293771624565  |
| train_0/mu_grads_std      | 0.3078257992863655    |
| train_0/mu_loss           | 9.18167788849254      |
| train_0/next_q            | -9.17720047356209     |
| train_0/q_grads           | -0.003180824493756518 |
| train_0/q_grads_std       | 0.17517822980880737   |
| train_0/q_loss            | 0.25850328734218825   |
| train_0/reward            | -0.6084752826624026   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.087744140625        |
| train_0/target_q          | -9.407828392748423    |
| train_1/avg_q             | -20.01445757876328    |
| train_1/current_q         | -3.4071888261836962   |
| train_1/fw_bonus          | -0.9912235826253891   |
| train_1/fw_loss           | 0.09630292356014251   |
| train_1/mu_grads          | -0.026261581247672437 |
| train_1/mu_grads_std      | 0.40053127259016036   |
| train_1/mu_loss           | 1.9773087451299083    |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -0.9768838444585866   |
| train_1/q_grads           | -0.02578782821074128  |
| train_1/q_grads_std       | 0.22232843190431595   |
| train_1/q_loss            | 0.5526183364986366    |
| train_1/reward            | -2.6647065296114305   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.006298828125        |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -3.3983414536525323   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 7
Time for epoch 7: 482.40. Rollout time: 251.65, Training time: 230.71
Evaluating epoch 7
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-------------------------------------------------------
| epoch                     | 7                       |
| policy/steps              | 726940.0                |
| test/episodes             | 200.0                   |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -27.0                   |
| test_1/avg_q              | -20.58407834728433      |
| test_1/n_subgoals         | 675.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 800.0                   |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -26.999349460871223     |
| train_0/current_q         | -9.395442523742549      |
| train_0/fw_bonus          | -0.9989107310771942     |
| train_0/fw_loss           | 0.00899120110552758     |
| train_0/mu_grads          | -0.021004304755479097   |
| train_0/mu_grads_std      | 0.3166284091770649      |
| train_0/mu_loss           | 9.312912735293821       |
| train_0/next_q            | -9.310012368341571      |
| train_0/q_grads           | -0.0033124835637863724  |
| train_0/q_grads_std       | 0.1797722525894642      |
| train_0/q_loss            | 0.2406433420273036      |
| train_0/reward            | -0.6058678188226623     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.10126953125           |
| train_0/target_q          | -9.551118112001332      |
| train_1/avg_q             | -20.21829129299807      |
| train_1/current_q         | -2.6878779830136716     |
| train_1/fw_bonus          | -0.9914611473679542     |
| train_1/fw_loss           | 0.09415766224265099     |
| train_1/mu_grads          | -0.025764578208327293   |
| train_1/mu_grads_std      | 0.42816840186715127     |
| train_1/mu_loss           | 1.0000457115187742      |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -4.8064174705135225e-05 |
| train_1/q_grads           | -0.027160291466861962   |
| train_1/q_grads_std       | 0.23423854485154152     |
| train_1/q_loss            | 0.17647062915835915     |
| train_1/reward            | -2.6739578166820137     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.0068603515625         |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -2.6739908042332763     |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 8
Time for epoch 8: 499.11. Rollout time: 257.79, Training time: 241.29
Evaluating epoch 8
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
------------------------------------------------------
| epoch                     | 8                      |
| policy/steps              | 818065.0               |
| test/episodes             | 225.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -27.0                  |
| test_1/avg_q              | -19.53836864437751     |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 900.0                  |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -27.0                  |
| train_0/current_q         | -9.38585836281718      |
| train_0/fw_bonus          | -0.9989947482943535    |
| train_0/fw_loss           | 0.008329999970737845   |
| train_0/mu_grads          | -0.016352910781279206  |
| train_0/mu_grads_std      | 0.3297679550945759     |
| train_0/mu_loss           | 9.300112795122935      |
| train_0/next_q            | -9.29406016138835      |
| train_0/q_grads           | -0.0038380462967325    |
| train_0/q_grads_std       | 0.18419832251966       |
| train_0/q_loss            | 0.22430012204554703    |
| train_0/reward            | -0.6043141151763848    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.141259765625         |
| train_0/target_q          | -9.541570680870413     |
| train_1/avg_q             | -20.114923033049504    |
| train_1/current_q         | -2.6887266810129176    |
| train_1/fw_bonus          | -0.9921883389353752    |
| train_1/fw_loss           | 0.08759107626974583    |
| train_1/mu_grads          | -0.02649200139567256   |
| train_1/mu_grads_std      | 0.42871880903840065    |
| train_1/mu_loss           | 1.0000177475490264     |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -1.982405663464118e-05 |
| train_1/q_grads           | -0.028878189530223607  |
| train_1/q_grads_std       | 0.24453637935221195    |
| train_1/q_loss            | 0.11085042597625616    |
| train_1/reward            | -2.67813131053299      |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.006689453125         |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -2.6781453672278075    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 9
Time for epoch 9: 521.01. Rollout time: 266.85, Training time: 254.13
Evaluating epoch 9
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
------------------------------------------------------
| epoch                     | 9                      |
| policy/steps              | 909190.0               |
| test/episodes             | 250.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -27.0                  |
| test_1/avg_q              | -20.504415735397327    |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 1000.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -27.0                  |
| train_0/current_q         | -9.410792704959931     |
| train_0/fw_bonus          | -0.9990208238363266    |
| train_0/fw_loss           | 0.008124753856100142   |
| train_0/mu_grads          | -0.014601844549179076  |
| train_0/mu_grads_std      | 0.34242496863007543    |
| train_0/mu_loss           | 9.31680208292139       |
| train_0/next_q            | -9.312069162221412     |
| train_0/q_grads           | -0.003925201727543026  |
| train_0/q_grads_std       | 0.19007186330854892    |
| train_0/q_loss            | 0.2182673032709947     |
| train_0/reward            | -0.6055840625242126    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.14677734375          |
| train_0/target_q          | -9.564637719378187     |
| train_1/avg_q             | -20.18566273661081     |
| train_1/current_q         | -2.6572691566025632    |
| train_1/fw_bonus          | -0.9924039021134377    |
| train_1/fw_loss           | 0.08564452175050974    |
| train_1/mu_grads          | -0.026265426119789482  |
| train_1/mu_grads_std      | 0.42946378588676454    |
| train_1/mu_loss           | 1.0000026579115129     |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -3.061878305023238e-06 |
| train_1/q_grads           | -0.03147388994693756   |
| train_1/q_grads_std       | 0.2552155047655106     |
| train_1/q_loss            | 0.11335564285697668    |
| train_1/reward            | -2.6494010219619666    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0070556640625        |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -2.6494031228573753    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 10
Time for epoch 10: 457.60. Rollout time: 233.98, Training time: 223.59
Evaluating epoch 10
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-------------------------------------------------------
| epoch                     | 10                      |
| policy/steps              | 1000315.0               |
| test/episodes             | 275.0                   |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -27.0                   |
| test_1/avg_q              | -20.411199233743716     |
| test_1/n_subgoals         | 675.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 1100.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -27.0                   |
| train_0/current_q         | -9.51468266058109       |
| train_0/fw_bonus          | -0.9990088537335395     |
| train_0/fw_loss           | 0.008218913001473992    |
| train_0/mu_grads          | -0.013262977940030396   |
| train_0/mu_grads_std      | 0.35299974456429484     |
| train_0/mu_loss           | 9.410987595110564       |
| train_0/next_q            | -9.405677065065918      |
| train_0/q_grads           | -0.004830156883690507   |
| train_0/q_grads_std       | 0.19523563385009765     |
| train_0/q_loss            | 0.20430376740604062     |
| train_0/reward            | -0.6100488449108525     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.15498046875           |
| train_0/target_q          | -9.672778798953516      |
| train_1/avg_q             | -20.24001437726435      |
| train_1/current_q         | -2.6669990815139664     |
| train_1/fw_bonus          | -0.9927330955862999     |
| train_1/fw_loss           | 0.08267178200185299     |
| train_1/mu_grads          | -0.024692238448187707   |
| train_1/mu_grads_std      | 0.4321429945528507      |
| train_1/mu_loss           | 1.0000024136218035      |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -3.2903829426590675e-06 |
| train_1/q_grads           | -0.03319719135761261    |
| train_1/q_grads_std       | 0.2641466177999973      |
| train_1/q_loss            | 0.0720915787695278      |
| train_1/reward            | -2.659132980856884      |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.007421875             |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -2.659135253899465      |
-------------------------------------------------------
Saving periodic policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_10.pkl ...
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 11
Time for epoch 11: 461.48. Rollout time: 241.86, Training time: 219.59
Evaluating epoch 11
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
------------------------------------------------------
| epoch                     | 11                     |
| policy/steps              | 1091440.0              |
| test/episodes             | 300.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -27.0                  |
| test_1/avg_q              | -20.368361864473464    |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 1200.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -27.0                  |
| train_0/current_q         | -9.460193653556303     |
| train_0/fw_bonus          | -0.9990023016929627    |
| train_0/fw_loss           | 0.00827054182300344    |
| train_0/mu_grads          | -0.01523200327064842   |
| train_0/mu_grads_std      | 0.36133487448096274    |
| train_0/mu_loss           | 9.348339721147646      |
| train_0/next_q            | -9.34155818468246      |
| train_0/q_grads           | -0.004610777145717293  |
| train_0/q_grads_std       | 0.20114602334797382    |
| train_0/q_loss            | 0.21707076390143212    |
| train_0/reward            | -0.6169685916560411    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.15322265625          |
| train_0/target_q          | -9.618762022427813     |
| train_1/avg_q             | -19.95528040034033     |
| train_1/current_q         | -2.640927335641409     |
| train_1/fw_bonus          | -0.9929568976163864    |
| train_1/fw_loss           | 0.08065066952258348    |
| train_1/mu_grads          | -0.023548056464642288  |
| train_1/mu_grads_std      | 0.4348102755844593     |
| train_1/mu_loss           | 1.0000000920311607     |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -8.441849210204973e-08 |
| train_1/q_grads           | -0.03569547738879919   |
| train_1/q_grads_std       | 0.2747615225613117     |
| train_1/q_loss            | 0.05723651046599837    |
| train_1/reward            | -2.6335907906155627    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0064453125           |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -2.6335908459544575    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 12
Time for epoch 12: 512.64. Rollout time: 273.12, Training time: 239.40
Evaluating epoch 12
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-------------------------------------------------------
| epoch                     | 12                      |
| policy/steps              | 1182565.0               |
| test/episodes             | 325.0                   |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -27.0                   |
| test_1/avg_q              | -20.06914592807738      |
| test_1/n_subgoals         | 675.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 1300.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -27.0                   |
| train_0/current_q         | -9.429309579633218      |
| train_0/fw_bonus          | -0.9989900469779969     |
| train_0/fw_loss           | 0.008366944768931716    |
| train_0/mu_grads          | -0.012906667636707425   |
| train_0/mu_grads_std      | 0.37020597159862517     |
| train_0/mu_loss           | 9.29998345184754        |
| train_0/next_q            | -9.292300553872598      |
| train_0/q_grads           | -0.005354282271582633   |
| train_0/q_grads_std       | 0.20726318173110486     |
| train_0/q_loss            | 0.22782785899872054     |
| train_0/reward            | -0.6276588885688398     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.143359375             |
| train_0/target_q          | -9.583935019052063      |
| train_1/avg_q             | -20.20799068083775      |
| train_1/current_q         | -2.688220287275371      |
| train_1/fw_bonus          | -0.993265850841999      |
| train_1/fw_loss           | 0.07786089014261961     |
| train_1/mu_grads          | -0.021673875488340856   |
| train_1/mu_grads_std      | 0.4393330566585064      |
| train_1/mu_loss           | 1.0000000332395942      |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -3.6498086788714485e-08 |
| train_1/q_grads           | -0.03741148607805371    |
| train_1/q_grads_std       | 0.2810298129916191      |
| train_1/q_loss            | 0.07127195843987964     |
| train_1/reward            | -2.6826487379843456     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.0071044921875         |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -2.6826487673428927     |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 13
Time for epoch 13: 526.85. Rollout time: 274.23, Training time: 252.56
Evaluating epoch 13
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-------------------------------------------------------
| epoch                     | 13                      |
| policy/steps              | 1273690.0               |
| test/episodes             | 350.0                   |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -27.0                   |
| test_1/avg_q              | -20.490006263425247     |
| test_1/n_subgoals         | 675.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 1400.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -27.0                   |
| train_0/current_q         | -9.577719116411561      |
| train_0/fw_bonus          | -0.9990049511194229     |
| train_0/fw_loss           | 0.008249655028339475    |
| train_0/mu_grads          | -0.010592459444887936   |
| train_0/mu_grads_std      | 0.37995249405503273     |
| train_0/mu_loss           | 9.442805198208523       |
| train_0/next_q            | -9.436363831780552      |
| train_0/q_grads           | -0.0067427730071358384  |
| train_0/q_grads_std       | 0.21437140516936778     |
| train_0/q_loss            | 0.22895718884008903     |
| train_0/reward            | -0.6354445968958317     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.153955078125          |
| train_0/target_q          | -9.733132065795868      |
| train_1/avg_q             | -20.03257846416904      |
| train_1/current_q         | -2.689913828935874      |
| train_1/fw_bonus          | -0.993425378203392      |
| train_1/fw_loss           | 0.07642029598355293     |
| train_1/mu_grads          | -0.01915997751057148    |
| train_1/mu_grads_std      | 0.45329528078436854     |
| train_1/mu_loss           | 1.0000000027094331      |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -3.4875146515873678e-09 |
| train_1/q_grads           | -0.04024164993315935    |
| train_1/q_grads_std       | 0.28889253437519075     |
| train_1/q_loss            | 0.09820025888595327     |
| train_1/reward            | -2.6865887674368425     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.0071533203125         |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -2.6865887698407542     |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 14
Time for epoch 14: 519.03. Rollout time: 286.32, Training time: 232.67
Evaluating epoch 14
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-------------------------------------------------------
| epoch                     | 14                      |
| policy/steps              | 1364815.0               |
| test/episodes             | 375.0                   |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -27.0                   |
| test_1/avg_q              | -20.035665781298587     |
| test_1/n_subgoals         | 675.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 1500.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -27.0                   |
| train_0/current_q         | -9.550284159546848      |
| train_0/fw_bonus          | -0.9990176230669021     |
| train_0/fw_loss           | 0.008149956294801086    |
| train_0/mu_grads          | -0.009554768074303865   |
| train_0/mu_grads_std      | 0.3897554248571396      |
| train_0/mu_loss           | 9.410942891125732       |
| train_0/next_q            | -9.402464174593337      |
| train_0/q_grads           | -0.007831280445680022   |
| train_0/q_grads_std       | 0.220788486674428       |
| train_0/q_loss            | 0.22936243816063798     |
| train_0/reward            | -0.6389084573038417     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.169921875             |
| train_0/target_q          | -9.704873675703954      |
| train_1/avg_q             | -20.213984580136028     |
| train_1/current_q         | -2.6434175398170288     |
| train_1/fw_bonus          | -0.9935604006052017     |
| train_1/fw_loss           | 0.07520106136798858     |
| train_1/mu_grads          | -0.0015153661428485067  |
| train_1/mu_grads_std      | 0.5389974057674408      |
| train_1/mu_loss           | 1.000235208078872       |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -0.00011902440610531823 |
| train_1/q_grads           | -0.041906677093356846   |
| train_1/q_grads_std       | 0.2965505883097649      |
| train_1/q_loss            | 0.11981526836353976     |
| train_1/reward            | -2.642365930027154      |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.0073486328125         |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -2.6424536171294513     |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 15
Time for epoch 15: 496.92. Rollout time: 260.59, Training time: 236.29
Evaluating epoch 15
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
------------------------------------------------------
| epoch                     | 15                     |
| policy/steps              | 1455940.0              |
| test/episodes             | 400.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -27.0                  |
| test_1/avg_q              | -26.999997126587168    |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 1600.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -27.0                  |
| train_0/current_q         | -9.579454789206233     |
| train_0/fw_bonus          | -0.9990042954683304    |
| train_0/fw_loss           | 0.008254751039203256   |
| train_0/mu_grads          | -0.009376435168087482  |
| train_0/mu_grads_std      | 0.3973499909043312     |
| train_0/mu_loss           | 9.432272076736044      |
| train_0/next_q            | -9.425378597038607     |
| train_0/q_grads           | -0.008238213183358312  |
| train_0/q_grads_std       | 0.2267045296728611     |
| train_0/q_loss            | 0.21863127615638298    |
| train_0/reward            | -0.6422193373986375    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.1349609375           |
| train_0/target_q          | -9.734740129113751     |
| train_1/avg_q             | -24.06007936010844     |
| train_1/current_q         | -23.864158884739634    |
| train_1/fw_bonus          | -0.9939220070838928    |
| train_1/fw_loss           | 0.07193564344197512    |
| train_1/mu_grads          | -0.0019247004529461265 |
| train_1/mu_grads_std      | 0.5423035025596619     |
| train_1/mu_loss           | 28.0                   |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -27.0                  |
| train_1/q_grads           | -0.04247446544468403   |
| train_1/q_grads_std       | 0.3120743803679943     |
| train_1/q_loss            | 9.48094381968436       |
| train_1/reward            | -2.6360659545669476    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0074462890625        |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -23.625771032691965    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 16
Time for epoch 16: 484.90. Rollout time: 265.08, Training time: 219.77
Evaluating epoch 16
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
------------------------------------------------------
| epoch                     | 16                     |
| policy/steps              | 1547065.0              |
| test/episodes             | 425.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -27.0                  |
| test_1/avg_q              | -26.988312609346345    |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 1700.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -27.0                  |
| train_0/current_q         | -9.485121365617355     |
| train_0/fw_bonus          | -0.9990819379687309    |
| train_0/fw_loss           | 0.00764374245190993    |
| train_0/mu_grads          | -0.010350359766744078  |
| train_0/mu_grads_std      | 0.40333556309342383    |
| train_0/mu_loss           | 9.336242800674594      |
| train_0/next_q            | -9.32838513858582      |
| train_0/q_grads           | -0.00871745334006846   |
| train_0/q_grads_std       | 0.2322749249637127     |
| train_0/q_loss            | 0.2189413588452025     |
| train_0/reward            | -0.63919919543805      |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.1863037109375        |
| train_0/target_q          | -9.639903623517853     |
| train_1/avg_q             | -26.742977631649723    |
| train_1/current_q         | -23.887710485990112    |
| train_1/fw_bonus          | -0.9941940158605576    |
| train_1/fw_loss           | 0.06947948057204485    |
| train_1/mu_grads          | -0.0019247004529461265 |
| train_1/mu_grads_std      | 0.5423035025596619     |
| train_1/mu_loss           | 28.0                   |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -27.0                  |
| train_1/q_grads           | -0.04195302547886968   |
| train_1/q_grads_std       | 0.318947310000658      |
| train_1/q_loss            | 6.192756042459843      |
| train_1/reward            | -2.6644603664793975    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.008349609375         |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -23.711659096948164    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 17
Time for epoch 17: 434.68. Rollout time: 234.14, Training time: 200.51
Evaluating epoch 17
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
------------------------------------------------------
| epoch                     | 17                     |
| policy/steps              | 1638190.0              |
| test/episodes             | 450.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -27.0                  |
| test_1/avg_q              | -24.99585914538797     |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 1800.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -27.0                  |
| train_0/current_q         | -9.49692032938319      |
| train_0/fw_bonus          | -0.9991277188062668    |
| train_0/fw_loss           | 0.007283402769826352   |
| train_0/mu_grads          | -0.01017522111069411   |
| train_0/mu_grads_std      | 0.41023681685328484    |
| train_0/mu_loss           | 9.350426607112835      |
| train_0/next_q            | -9.346842200110075     |
| train_0/q_grads           | -0.009852721984498203  |
| train_0/q_grads_std       | 0.23678703606128693    |
| train_0/q_loss            | 0.2172119476691527     |
| train_0/reward            | -0.6343217257290235    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.1615234375           |
| train_0/target_q          | -9.653944542109874     |
| train_1/avg_q             | -26.090572920320216    |
| train_1/current_q         | -23.50478148634986     |
| train_1/fw_bonus          | -0.9947399228811264    |
| train_1/fw_loss           | 0.06454975148662925    |
| train_1/mu_grads          | -0.0019247004529461265 |
| train_1/mu_grads_std      | 0.5423035025596619     |
| train_1/mu_loss           | 28.0                   |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -27.0                  |
| train_1/q_grads           | -0.04277343638241291   |
| train_1/q_grads_std       | 0.3270177379250526     |
| train_1/q_loss            | 5.08100435816291       |
| train_1/reward            | -2.6545992885810845    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.008203125            |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -23.41885172998735     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 18
Time for epoch 18: 414.09. Rollout time: 219.09, Training time: 194.96
Evaluating epoch 18
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
------------------------------------------------------
| epoch                     | 18                     |
| policy/steps              | 1729315.0              |
| test/episodes             | 475.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -27.0                  |
| test_1/avg_q              | -25.36364980431432     |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 1900.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -27.0                  |
| train_0/current_q         | -9.428257867511576     |
| train_0/fw_bonus          | -0.9991761520504951    |
| train_0/fw_loss           | 0.006902216305024922   |
| train_0/mu_grads          | -0.009738895972259343  |
| train_0/mu_grads_std      | 0.4156568057835102     |
| train_0/mu_loss           | 9.281572709738937      |
| train_0/next_q            | -9.276480714757966     |
| train_0/q_grads           | -0.010406692582182585  |
| train_0/q_grads_std       | 0.24100060537457466    |
| train_0/q_loss            | 0.19711911658580375    |
| train_0/reward            | -0.6316238729694306    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.1941650390625        |
| train_0/target_q          | -9.57837789142673      |
| train_1/avg_q             | -25.098256641903077    |
| train_1/current_q         | -23.331616919596293    |
| train_1/fw_bonus          | -0.9953292354941368    |
| train_1/fw_loss           | 0.05922817438840866    |
| train_1/mu_grads          | -0.0019247004529461265 |
| train_1/mu_grads_std      | 0.5423035025596619     |
| train_1/mu_loss           | 28.0                   |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -27.0                  |
| train_1/q_grads           | -0.04256971599534154   |
| train_1/q_grads_std       | 0.3345519877970219     |
| train_1/q_loss            | 5.739157213849255      |
| train_1/reward            | -2.645811069047704     |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0073486328125        |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -23.321562045610218    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 19
Time for epoch 19: 409.49. Rollout time: 214.38, Training time: 195.09
Evaluating epoch 19
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 19                    |
| policy/steps              | 1820440.0             |
| test/episodes             | 500.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -26.999999324938614   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 2000.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -27.0                 |
| train_0/current_q         | -9.54594742517119     |
| train_0/fw_bonus          | -0.9992195397615433   |
| train_0/fw_loss           | 0.0065608124248683454 |
| train_0/mu_grads          | -0.0100533795543015   |
| train_0/mu_grads_std      | 0.4223754771053791    |
| train_0/mu_loss           | 9.384942237151122     |
| train_0/next_q            | -9.381300935969445    |
| train_0/q_grads           | -0.011246808408759534 |
| train_0/q_grads_std       | 0.24666329622268676   |
| train_0/q_loss            | 0.1979354221063364    |
| train_0/reward            | -0.6411745127563335   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.2123291015625       |
| train_0/target_q          | -9.698223048430389    |
| train_1/avg_q             | -24.49201266227579    |
| train_1/current_q         | -23.621733805698888   |
| train_1/fw_bonus          | -0.9958192095160484   |
| train_1/fw_loss           | 0.054803519323468206  |
| train_1/mu_grads          | -0.01204595621675253  |
| train_1/mu_grads_std      | 0.5348801016807556    |
| train_1/mu_loss           | 28.0                  |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -27.0                 |
| train_1/q_grads           | -0.042704211082309484 |
| train_1/q_grads_std       | 0.3402103081345558    |
| train_1/q_loss            | 7.003525230325529     |
| train_1/reward            | -2.670561249689854    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.00595703125         |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -23.41284982390862    |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 20
Time for epoch 20: 407.19. Rollout time: 217.93, Training time: 189.23
Evaluating epoch 20
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 20                    |
| policy/steps              | 1911565.0             |
| test/episodes             | 525.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -26.99999985806387    |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 2100.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -27.0                 |
| train_0/current_q         | -9.562508922488487    |
| train_0/fw_bonus          | -0.9992281764745712   |
| train_0/fw_loss           | 0.006492781394626945  |
| train_0/mu_grads          | -0.01165545724797994  |
| train_0/mu_grads_std      | 0.4294023305177689    |
| train_0/mu_loss           | 9.403231206644298     |
| train_0/next_q            | -9.397401711928996    |
| train_0/q_grads           | -0.011378678074106575 |
| train_0/q_grads_std       | 0.25155859142541886   |
| train_0/q_loss            | 0.1926668992101262    |
| train_0/reward            | -0.6414513322673884   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.2042724609375       |
| train_0/target_q          | -9.714478753238438    |
| train_1/avg_q             | -26.98960512686082    |
| train_1/current_q         | -23.64120706910708    |
| train_1/fw_bonus          | -0.9958589509129524   |
| train_1/fw_loss           | 0.054444652330130336  |
| train_1/mu_grads          | -0.01204595621675253  |
| train_1/mu_grads_std      | 0.5348801016807556    |
| train_1/mu_loss           | 28.0                  |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -27.0                 |
| train_1/q_grads           | -0.04235292105004192  |
| train_1/q_grads_std       | 0.34247410073876383   |
| train_1/q_loss            | 5.750159997626731     |
| train_1/reward            | -2.645320940325837    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.005517578125        |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -23.438643205950854   |
-----------------------------------------------------
Saving periodic policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_20.pkl ...
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 21
Time for epoch 21: 398.76. Rollout time: 217.48, Training time: 181.25
Evaluating epoch 21
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 21                    |
| policy/steps              | 2002690.0             |
| test/episodes             | 550.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -26.99999972930731    |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 2200.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -27.0                 |
| train_0/current_q         | -9.567932103540295    |
| train_0/fw_bonus          | -0.9992181494832039   |
| train_0/fw_loss           | 0.006571785709820688  |
| train_0/mu_grads          | -0.012116601667366922 |
| train_0/mu_grads_std      | 0.4360396809875965    |
| train_0/mu_loss           | 9.411622581145071     |
| train_0/next_q            | -9.4054742879683      |
| train_0/q_grads           | -0.011867977771908044 |
| train_0/q_grads_std       | 0.2567678354680538    |
| train_0/q_loss            | 0.18850168064705558   |
| train_0/reward            | -0.6411375430423505   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.201171875           |
| train_0/target_q          | -9.724820489959097    |
| train_1/avg_q             | -26.971398695327      |
| train_1/current_q         | -23.736096062928183   |
| train_1/fw_bonus          | -0.9956425666809082   |
| train_1/fw_loss           | 0.05639883521944285   |
| train_1/mu_grads          | -0.01204595621675253  |
| train_1/mu_grads_std      | 0.5348801016807556    |
| train_1/mu_loss           | 28.0                  |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -27.0                 |
| train_1/q_grads           | -0.0425208454951644   |
| train_1/q_grads_std       | 0.346555782109499     |
| train_1/q_loss            | 6.0741489702427485    |
| train_1/reward            | -2.63907787378339     |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.005859375           |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -23.562891350345904   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 22
Time for epoch 22: 397.44. Rollout time: 214.73, Training time: 182.68
Evaluating epoch 22
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 22                    |
| policy/steps              | 2093815.0             |
| test/episodes             | 575.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -26.999992966489664   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 2300.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -27.0                 |
| train_0/current_q         | -9.497620976606987    |
| train_0/fw_bonus          | -0.9992006421089172   |
| train_0/fw_loss           | 0.006709552416577935  |
| train_0/mu_grads          | -0.012240871414542198 |
| train_0/mu_grads_std      | 0.4436462208628654    |
| train_0/mu_loss           | 9.333764549973974     |
| train_0/next_q            | -9.329582598974923    |
| train_0/q_grads           | -0.012601827224716543 |
| train_0/q_grads_std       | 0.26208404898643495   |
| train_0/q_loss            | 0.19374433128859803   |
| train_0/reward            | -0.6445023077070801   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.202392578125        |
| train_0/target_q          | -9.653285709766061    |
| train_1/avg_q             | -26.991083693969912   |
| train_1/current_q         | -23.844405353930433   |
| train_1/fw_bonus          | -0.9954466730356216   |
| train_1/fw_loss           | 0.05816768743097782   |
| train_1/mu_grads          | -0.01204595621675253  |
| train_1/mu_grads_std      | 0.5348801016807556    |
| train_1/mu_loss           | 28.0                  |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -27.0                 |
| train_1/q_grads           | -0.042996104992926124 |
| train_1/q_grads_std       | 0.3518051035702229    |
| train_1/q_loss            | 6.997052076579469     |
| train_1/reward            | -2.646847508571591    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0059814453125       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -23.654640477321603   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 23
Time for epoch 23: 399.30. Rollout time: 216.13, Training time: 183.14
Evaluating epoch 23
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 23                    |
| policy/steps              | 2184940.0             |
| test/episodes             | 600.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -25.068626577689386   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 2400.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -27.0                 |
| train_0/current_q         | -9.571362085667198    |
| train_0/fw_bonus          | -0.99919753074646     |
| train_0/fw_loss           | 0.006734055245760828  |
| train_0/mu_grads          | -0.012376315961591899 |
| train_0/mu_grads_std      | 0.4495192676782608    |
| train_0/mu_loss           | 9.412870847590058     |
| train_0/next_q            | -9.40764597783936     |
| train_0/q_grads           | -0.012966234795749187 |
| train_0/q_grads_std       | 0.2676206551492214    |
| train_0/q_loss            | 0.2040140244310285    |
| train_0/reward            | -0.6457100252831879   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.194921875           |
| train_0/target_q          | -9.723542377984703    |
| train_1/avg_q             | -26.753012332716423   |
| train_1/current_q         | -23.84764221107323    |
| train_1/fw_bonus          | -0.9947502762079239   |
| train_1/fw_loss           | 0.06445623822510242   |
| train_1/mu_grads          | -0.020654693245887756 |
| train_1/mu_grads_std      | 0.5368746519088745    |
| train_1/mu_loss           | 28.0                  |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -27.0                 |
| train_1/q_grads           | -0.04561519268900156  |
| train_1/q_grads_std       | 0.35689777955412866   |
| train_1/q_loss            | 4.5331224251092666    |
| train_1/reward            | -2.6900327849853056   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.00576171875         |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -23.744983468579072   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 24
Time for epoch 24: 410.69. Rollout time: 218.63, Training time: 192.03
Evaluating epoch 24
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
------------------------------------------------------
| epoch                     | 24                     |
| policy/steps              | 2276065.0              |
| test/episodes             | 625.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -27.0                  |
| test_1/avg_q              | -26.999999999996955    |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 2500.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -27.0                  |
| train_0/current_q         | -9.482057427102543     |
| train_0/fw_bonus          | -0.9991618618369102    |
| train_0/fw_loss           | 0.0070146357640624045  |
| train_0/mu_grads          | -0.0125000104540959    |
| train_0/mu_grads_std      | 0.4557022228837013     |
| train_0/mu_loss           | 9.315915993341626      |
| train_0/next_q            | -9.308953891261638     |
| train_0/q_grads           | -0.01380912885069847   |
| train_0/q_grads_std       | 0.27312454432249067    |
| train_0/q_loss            | 0.1992575953136817     |
| train_0/reward            | -0.6455180072411167    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.16142578125          |
| train_0/target_q          | -9.637466201403711     |
| train_1/avg_q             | -22.97532698004937     |
| train_1/current_q         | -2.1343319232194884    |
| train_1/fw_bonus          | -0.9946750149130821    |
| train_1/fw_loss           | 0.06513594286516308    |
| train_1/mu_grads          | -0.04661758244037628   |
| train_1/mu_grads_std      | 0.5800696015357971     |
| train_1/mu_loss           | 1.0                    |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -3.651417937143917e-24 |
| train_1/q_grads           | -0.04507812736555934   |
| train_1/q_grads_std       | 0.3651453398168087     |
| train_1/q_loss            | 7.335294113247369      |
| train_1/reward            | -2.6004021510605524    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.006689453125         |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -2.6004021510605524    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 25
Time for epoch 25: 405.68. Rollout time: 214.98, Training time: 190.67
Evaluating epoch 25
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 25                    |
| policy/steps              | 2367190.0             |
| test/episodes             | 650.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -27.0                 |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 2600.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -27.0                 |
| train_0/current_q         | -9.573168765099917    |
| train_0/fw_bonus          | -0.9991647616028786   |
| train_0/fw_loss           | 0.006991841504350305  |
| train_0/mu_grads          | -0.013691551564261318 |
| train_0/mu_grads_std      | 0.46166974902153013   |
| train_0/mu_loss           | 9.411670828620682     |
| train_0/next_q            | -9.407713478919101    |
| train_0/q_grads           | -0.014850126300007104 |
| train_0/q_grads_std       | 0.2780558630824089    |
| train_0/q_loss            | 0.2011723867259604    |
| train_0/reward            | -0.6438341611199576   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.196337890625        |
| train_0/target_q          | -9.728156547220982    |
| train_1/avg_q             | -26.136402057046837   |
| train_1/current_q         | -26.99999999069262    |
| train_1/fw_bonus          | -0.9940053775906563   |
| train_1/fw_loss           | 0.07118276059627533   |
| train_1/mu_grads          | -0.04796074330806732  |
| train_1/mu_grads_std      | 0.5806608200073242    |
| train_1/mu_loss           | 28.0                  |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -27.0                 |
| train_1/q_grads           | -0.042628563940525055 |
| train_1/q_grads_std       | 0.36704298853874207   |
| train_1/q_loss            | 88.74296758633983     |
| train_1/reward            | -2.696914431510595    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0066162109375       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -23.60005603307311    |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 26
Time for epoch 26: 412.92. Rollout time: 217.20, Training time: 195.69
Evaluating epoch 26
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 26                    |
| policy/steps              | 2458315.0             |
| test/episodes             | 675.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -27.0                 |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 2700.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -27.0                 |
| train_0/current_q         | -9.555012866882796    |
| train_0/fw_bonus          | -0.999177274107933    |
| train_0/fw_loss           | 0.006893440778367221  |
| train_0/mu_grads          | -0.013449364528059959 |
| train_0/mu_grads_std      | 0.4661767467856407    |
| train_0/mu_loss           | 9.390818990216314     |
| train_0/next_q            | -9.386478673982085    |
| train_0/q_grads           | -0.01520398531574756  |
| train_0/q_grads_std       | 0.28275234550237655   |
| train_0/q_loss            | 0.1995486695016509    |
| train_0/reward            | -0.6442584291522507   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.2010498046875       |
| train_0/target_q          | -9.709361385900547    |
| train_1/avg_q             | -27.0                 |
| train_1/current_q         | -26.999999999869544   |
| train_1/fw_bonus          | -0.9937046691775322   |
| train_1/fw_loss           | 0.07389822043478489   |
| train_1/mu_grads          | -0.04796074330806732  |
| train_1/mu_grads_std      | 0.5806608200073242    |
| train_1/mu_loss           | 28.0                  |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -27.0                 |
| train_1/q_grads           | -0.042628563940525055 |
| train_1/q_grads_std       | 0.36704298853874207   |
| train_1/q_loss            | 85.0435166420214      |
| train_1/reward            | -2.645368307573153    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0079833984375       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -23.755874655229416   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 27
Time for epoch 27: 416.19. Rollout time: 222.73, Training time: 193.43
Evaluating epoch 27
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 27                    |
| policy/steps              | 2549440.0             |
| test/episodes             | 700.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -27.0                 |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 2800.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -27.0                 |
| train_0/current_q         | -9.723607711574173    |
| train_0/fw_bonus          | -0.9991762414574623   |
| train_0/fw_loss           | 0.006901470827870071  |
| train_0/mu_grads          | -0.012949269847013056 |
| train_0/mu_grads_std      | 0.46985144913196564   |
| train_0/mu_loss           | 9.564379079109006     |
| train_0/next_q            | -9.557292435978217    |
| train_0/q_grads           | -0.015129776229150594 |
| train_0/q_grads_std       | 0.28727310225367547   |
| train_0/q_loss            | 0.19712820000651168   |
| train_0/reward            | -0.648140010013958    |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.2009033203125       |
| train_0/target_q          | -9.881214959674422    |
| train_1/avg_q             | -26.99999999999996    |
| train_1/current_q         | -26.999999999872536   |
| train_1/fw_bonus          | -0.9932781055569648   |
| train_1/fw_loss           | 0.0777502192184329    |
| train_1/mu_grads          | -0.04796074330806732  |
| train_1/mu_grads_std      | 0.5806608200073242    |
| train_1/mu_loss           | 28.0                  |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -27.0                 |
| train_1/q_grads           | -0.042628560215234756 |
| train_1/q_grads_std       | 0.36704298853874207   |
| train_1/q_loss            | 89.74657887472162     |
| train_1/reward            | -2.658463569779633    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0063232421875       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -23.571941108842147   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 28
Time for epoch 28: 420.55. Rollout time: 223.01, Training time: 197.51
Evaluating epoch 28
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 28                    |
| policy/steps              | 2640565.0             |
| test/episodes             | 725.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -27.0                 |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 2900.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -27.0                 |
| train_0/current_q         | -9.445986129795068    |
| train_0/fw_bonus          | -0.9992435961961746   |
| train_0/fw_loss           | 0.006371384661179036  |
| train_0/mu_grads          | -0.012534347106702625 |
| train_0/mu_grads_std      | 0.4729979023337364    |
| train_0/mu_loss           | 9.28874813749         |
| train_0/next_q            | -9.284347884426854    |
| train_0/q_grads           | -0.015973225189372896 |
| train_0/q_grads_std       | 0.29148530289530755   |
| train_0/q_loss            | 0.19299638433214308   |
| train_0/reward            | -0.636639514511262    |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.1964111328125       |
| train_0/target_q          | -9.602373433805573    |
| train_1/avg_q             | -27.0                 |
| train_1/current_q         | -26.99999999998517    |
| train_1/fw_bonus          | -0.9935730472207069   |
| train_1/fw_loss           | 0.07508686147630214   |
| train_1/mu_grads          | -0.04796074330806732  |
| train_1/mu_grads_std      | 0.5806608200073242    |
| train_1/mu_loss           | 28.0                  |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -27.0                 |
| train_1/q_grads           | -0.04262855648994446  |
| train_1/q_grads_std       | 0.36704298853874207   |
| train_1/q_loss            | 91.73980982534859     |
| train_1/reward            | -2.6335434096672543   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0073486328125       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -23.50244721826102    |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 29
Time for epoch 29: 408.60. Rollout time: 214.09, Training time: 194.48
Evaluating epoch 29
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 29                    |
| policy/steps              | 2731690.0             |
| test/episodes             | 750.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -27.0                 |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 3000.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -27.0                 |
| train_0/current_q         | -9.577206118790963    |
| train_0/fw_bonus          | -0.9992662161588669   |
| train_0/fw_loss           | 0.006193484272807836  |
| train_0/mu_grads          | -0.01245941799134016  |
| train_0/mu_grads_std      | 0.47648187801241876   |
| train_0/mu_loss           | 9.422349572299355     |
| train_0/next_q            | -9.41828239408694     |
| train_0/q_grads           | -0.016113112214952707 |
| train_0/q_grads_std       | 0.2947086736559868    |
| train_0/q_loss            | 0.189141148030193     |
| train_0/reward            | -0.6394100577803329   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.201953125           |
| train_0/target_q          | -9.72868978223841     |
| train_1/avg_q             | -27.0                 |
| train_1/current_q         | -26.999999999995488   |
| train_1/fw_bonus          | -0.9937465950846672   |
| train_1/fw_loss           | 0.07351978700608015   |
| train_1/mu_grads          | -0.04796074330806732  |
| train_1/mu_grads_std      | 0.5806608200073242    |
| train_1/mu_loss           | 28.0                  |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -27.0                 |
| train_1/q_grads           | -0.04262855648994446  |
| train_1/q_grads_std       | 0.36704298853874207   |
| train_1/q_loss            | 93.84462283246302     |
| train_1/reward            | -2.668120483234452    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0069091796875       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -23.412347045734467   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 30
Time for epoch 30: 408.14. Rollout time: 221.51, Training time: 186.60
Evaluating epoch 30
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 30                    |
| policy/steps              | 2822815.0             |
| test/episodes             | 775.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -27.0                 |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 3100.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -27.0                 |
| train_0/current_q         | -9.613727026254496    |
| train_0/fw_bonus          | -0.9992883056402206   |
| train_0/fw_loss           | 0.0060195829952135686 |
| train_0/mu_grads          | -0.013469750224612654 |
| train_0/mu_grads_std      | 0.4813107267022133    |
| train_0/mu_loss           | 9.466245283085692     |
| train_0/next_q            | -9.459887968223033    |
| train_0/q_grads           | -0.01645605550147593  |
| train_0/q_grads_std       | 0.2981576532125473    |
| train_0/q_loss            | 0.18484914941084285   |
| train_0/reward            | -0.6390310605635022   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.202685546875        |
| train_0/target_q          | -9.77276299158726     |
| train_1/avg_q             | -27.0                 |
| train_1/current_q         | -26.999999999983896   |
| train_1/fw_bonus          | -0.9936123803257942   |
| train_1/fw_loss           | 0.07473162915557623   |
| train_1/mu_grads          | -0.04796074330806732  |
| train_1/mu_grads_std      | 0.5806608200073242    |
| train_1/mu_loss           | 28.0                  |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -27.0                 |
| train_1/q_grads           | -0.04262855648994446  |
| train_1/q_grads_std       | 0.36704298853874207   |
| train_1/q_loss            | 89.63619697362029     |
| train_1/reward            | -2.6810874239305122   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0065673828125       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -23.570017111430527   |
-----------------------------------------------------
Saving periodic policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_30.pkl ...
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 31
Time for epoch 31: 404.63. Rollout time: 218.70, Training time: 185.89
Evaluating epoch 31
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 31                    |
| policy/steps              | 2913940.0             |
| test/episodes             | 800.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -27.0                 |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 3200.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -27.0                 |
| train_0/current_q         | -9.395641797964881    |
| train_0/fw_bonus          | -0.9992791742086411   |
| train_0/fw_loss           | 0.006091325881425291  |
| train_0/mu_grads          | -0.014671306475065649 |
| train_0/mu_grads_std      | 0.48699837923049927   |
| train_0/mu_loss           | 9.24236144931748      |
| train_0/next_q            | -9.237773845724567    |
| train_0/q_grads           | -0.01664200290106237  |
| train_0/q_grads_std       | 0.30218639224767685   |
| train_0/q_loss            | 0.1872076122658172    |
| train_0/reward            | -0.6339201389102527   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.1965576171875       |
| train_0/target_q          | -9.549477581641948    |
| train_1/avg_q             | -27.0                 |
| train_1/current_q         | -26.999999999988923   |
| train_1/fw_bonus          | -0.9938187927007676   |
| train_1/fw_loss           | 0.07286780457943678   |
| train_1/mu_grads          | -0.04796074330806732  |
| train_1/mu_grads_std      | 0.5806608200073242    |
| train_1/mu_loss           | 28.0                  |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -27.0                 |
| train_1/q_grads           | -0.042628560215234756 |
| train_1/q_grads_std       | 0.36704298853874207   |
| train_1/q_loss            | 92.70064112124304     |
| train_1/reward            | -2.7466358486908575   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0070556640625       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -23.43466075103462    |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 32
Time for epoch 32: 407.85. Rollout time: 218.27, Training time: 189.54
Evaluating epoch 32
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 32                    |
| policy/steps              | 3005065.0             |
| test/episodes             | 825.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -27.0                 |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 3300.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -27.0                 |
| train_0/current_q         | -9.629794290371636    |
| train_0/fw_bonus          | -0.9993146672844887   |
| train_0/fw_loss           | 0.005812084255740047  |
| train_0/mu_grads          | -0.015444313571788371 |
| train_0/mu_grads_std      | 0.4919407345354557    |
| train_0/mu_loss           | 9.475521476779731     |
| train_0/next_q            | -9.47399836544698     |
| train_0/q_grads           | -0.016409241780638694 |
| train_0/q_grads_std       | 0.30521474704146384   |
| train_0/q_loss            | 0.19655649907494258   |
| train_0/reward            | -0.6409968802930962   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.20576171875         |
| train_0/target_q          | -9.784156190242278    |
| train_1/avg_q             | -27.0                 |
| train_1/current_q         | -26.999999999993385   |
| train_1/fw_bonus          | -0.9935798406600952   |
| train_1/fw_loss           | 0.07502545677125454   |
| train_1/mu_grads          | -0.04796074330806732  |
| train_1/mu_grads_std      | 0.5806608200073242    |
| train_1/mu_loss           | 28.0                  |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -27.0                 |
| train_1/q_grads           | -0.042628590017557144 |
| train_1/q_grads_std       | 0.36704298853874207   |
| train_1/q_loss            | 93.06582468100994     |
| train_1/reward            | -2.655536075486816    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.006982421875        |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -23.44562836064308    |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 33
Time for epoch 33: 409.62. Rollout time: 217.26, Training time: 192.32
Evaluating epoch 33
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 33                    |
| policy/steps              | 3096190.0             |
| test/episodes             | 850.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -27.0                 |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 3400.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -27.0                 |
| train_0/current_q         | -9.520135691090209    |
| train_0/fw_bonus          | -0.9992861926555634   |
| train_0/fw_loss           | 0.006036194902844727  |
| train_0/mu_grads          | -0.015589196956716478 |
| train_0/mu_grads_std      | 0.49677109345793724   |
| train_0/mu_loss           | 9.362357188441006     |
| train_0/next_q            | -9.359288486526621    |
| train_0/q_grads           | -0.016872233850881456 |
| train_0/q_grads_std       | 0.3081914894282818    |
| train_0/q_loss            | 0.18859837319624248   |
| train_0/reward            | -0.640513687139537    |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.1978515625          |
| train_0/target_q          | -9.674117429224264    |
| train_1/avg_q             | -27.0                 |
| train_1/current_q         | -26.99999999999726    |
| train_1/fw_bonus          | -0.9940352991223336   |
| train_1/fw_loss           | 0.07091264724731446   |
| train_1/mu_grads          | -0.04796074330806732  |
| train_1/mu_grads_std      | 0.5806608200073242    |
| train_1/mu_loss           | 28.0                  |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -27.0                 |
| train_1/q_grads           | -0.04262878922745585  |
| train_1/q_grads_std       | 0.3670429587364197    |
| train_1/q_loss            | 89.61715817541743     |
| train_1/reward            | -2.7537395376177303   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0067138671875       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -23.550937779805245   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 34
Time for epoch 34: 410.70. Rollout time: 220.06, Training time: 190.61
Evaluating epoch 34
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 34                    |
| policy/steps              | 3187315.0             |
| test/episodes             | 875.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -27.0                 |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 3500.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -27.0                 |
| train_0/current_q         | -9.495593042739534    |
| train_0/fw_bonus          | -0.9993102654814721   |
| train_0/fw_loss           | 0.005846702074632048  |
| train_0/mu_grads          | -0.01706801075488329  |
| train_0/mu_grads_std      | 0.5010353624820709    |
| train_0/mu_loss           | 9.34487001325716      |
| train_0/next_q            | -9.341030259523766    |
| train_0/q_grads           | -0.017590714804828166 |
| train_0/q_grads_std       | 0.31163536086678506   |
| train_0/q_loss            | 0.18575293337326587   |
| train_0/reward            | -0.6376266399995074   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.2027587890625       |
| train_0/target_q          | -9.649426506244096    |
| train_1/avg_q             | -26.99999999999957    |
| train_1/current_q         | -26.999999999997932   |
| train_1/fw_bonus          | -0.9941235065460206   |
| train_1/fw_loss           | 0.07011609999462962   |
| train_1/mu_grads          | -0.04796074330806732  |
| train_1/mu_grads_std      | 0.5806608200073242    |
| train_1/mu_loss           | 28.0                  |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -27.0                 |
| train_1/q_grads           | -0.04262970592826605  |
| train_1/q_grads_std       | 0.36704268380999566   |
| train_1/q_loss            | 94.20085374044902     |
| train_1/reward            | -2.646370231930632    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0078369140625       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -23.404808708493146   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 35
Time for epoch 35: 415.58. Rollout time: 220.72, Training time: 194.83
Evaluating epoch 35
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 35                    |
| policy/steps              | 3278440.0             |
| test/episodes             | 900.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -27.0                 |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 3600.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -27.0                 |
| train_0/current_q         | -9.560752356486075    |
| train_0/fw_bonus          | -0.9993275597691536   |
| train_0/fw_loss           | 0.005710653483401984  |
| train_0/mu_grads          | -0.017258409410715103 |
| train_0/mu_grads_std      | 0.5044960990548134    |
| train_0/mu_loss           | 9.406543793360731     |
| train_0/next_q            | -9.401717884587239    |
| train_0/q_grads           | -0.017947753006592394 |
| train_0/q_grads_std       | 0.3156162314116955    |
| train_0/q_loss            | 0.18283761268096144   |
| train_0/reward            | -0.6370450239344791   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.2063232421875       |
| train_0/target_q          | -9.718804521601495    |
| train_1/avg_q             | -27.0                 |
| train_1/current_q         | -26.999999999999023   |
| train_1/fw_bonus          | -0.9937190353870392   |
| train_1/fw_loss           | 0.07376857362687587   |
| train_1/mu_grads          | -0.04796074330806732  |
| train_1/mu_grads_std      | 0.5806608200073242    |
| train_1/mu_loss           | 28.0                  |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -27.0                 |
| train_1/q_grads           | -0.042633145954459904 |
| train_1/q_grads_std       | 0.3670416034758091    |
| train_1/q_loss            | 97.87835584460505     |
| train_1/reward            | -2.646708567372843    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0071044921875       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -23.263673899404107   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 36
Time for epoch 36: 412.53. Rollout time: 209.04, Training time: 203.46
Evaluating epoch 36
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
----------------------------------------------------
| epoch                     | 36                   |
| policy/steps              | 3369565.0            |
| test/episodes             | 925.0                |
| test/success_rate         | 0.0                  |
| test_0/avg_q              | -27.0                |
| test_1/avg_q              | -26.99999999999932   |
| test_1/n_subgoals         | 675.0                |
| test_1/subgoal_succ_rate  | 0.0                  |
| train/episodes            | 3700.0               |
| train/success_rate        | 0.0                  |
| train_0/avg_q             | -27.0                |
| train_0/current_q         | -9.484976950584265   |
| train_0/fw_bonus          | -0.9993234559893608  |
| train_0/fw_loss           | 0.005742984870448708 |
| train_0/mu_grads          | -0.01744972914457321 |
| train_0/mu_grads_std      | 0.5064948692917823   |
| train_0/mu_loss           | 9.340552102118291    |
| train_0/next_q            | -9.336071841396347   |
| train_0/q_grads           | -0.01912574884481728 |
| train_0/q_grads_std       | 0.31895101964473727  |
| train_0/q_loss            | 0.1786483186079017   |
| train_0/reward            | -0.632832571437757   |
| train_0/reward_-0.0_frac  | 0.0                  |
| train_0/reward_-1.0_frac  | 0.200341796875       |
| train_0/target_q          | -9.640926574899254   |
| train_1/avg_q             | -26.999999999999435  |
| train_1/current_q         | -23.478971133749802  |
| train_1/fw_bonus          | -0.9940319269895553  |
| train_1/fw_loss           | 0.07094308361411095  |
| train_1/mu_grads          | -0.04796074330806732 |
| train_1/mu_grads_std      | 0.5806608200073242   |
| train_1/mu_loss           | 28.0                 |
| train_1/n_subgoals        | 2700.0               |
| train_1/next_q            | -27.0                |
| train_1/q_grads           | -0.04534032810479403 |
| train_1/q_grads_std       | 0.3641727715730667   |
| train_1/q_loss            | 9.7252423100731      |
| train_1/reward            | -2.70029020756665    |
| train_1/reward_-0.0_frac  | 0.0                  |
| train_1/reward_-1.0_frac  | 0.0064208984375      |
| train_1/reward_-27.0_frac | 0.0                  |
| train_1/subgoal_succ_rate | 0.0                  |
| train_1/target_q          | -23.268805832566663  |
----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 37
Time for epoch 37: 417.32. Rollout time: 225.03, Training time: 192.27
Evaluating epoch 37
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-------------------------------------------------------
| epoch                     | 37                      |
| policy/steps              | 3460690.0               |
| test/episodes             | 950.0                   |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -27.0                   |
| test_1/avg_q              | -26.999999999477286     |
| test_1/n_subgoals         | 675.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 3800.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -27.0                   |
| train_0/current_q         | -9.478113644451263      |
| train_0/fw_bonus          | -0.9993344455957412     |
| train_0/fw_loss           | 0.005656458658631891    |
| train_0/mu_grads          | -0.016825481737032532   |
| train_0/mu_grads_std      | 0.5090430989861489      |
| train_0/mu_loss           | 9.3342300717789         |
| train_0/next_q            | -9.331435663863784      |
| train_0/q_grads           | -0.02022753991186619    |
| train_0/q_grads_std       | 0.32252218648791314     |
| train_0/q_loss            | 0.1798968658125209      |
| train_0/reward            | -0.6333113940592738     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.1969970703125         |
| train_0/target_q          | -9.63122517221133       |
| train_1/avg_q             | -26.12643041643778      |
| train_1/current_q         | -2.4190803824640112     |
| train_1/fw_bonus          | -0.9938813090324402     |
| train_1/fw_loss           | 0.07230311203747988     |
| train_1/mu_grads          | -0.04059871658682823    |
| train_1/mu_grads_std      | 0.5834740996360779      |
| train_1/mu_loss           | 1.0                     |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -2.4225485869254027e-30 |
| train_1/q_grads           | -0.043896949663758275   |
| train_1/q_grads_std       | 0.36964761316776273     |
| train_1/q_loss            | 5.772610909374071       |
| train_1/reward            | -2.6897423026548495     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.0061279296875         |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -2.6897423026548495     |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 38
Time for epoch 38: 410.78. Rollout time: 212.96, Training time: 197.79
Evaluating epoch 38
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-------------------------------------------------------
| epoch                     | 38                      |
| policy/steps              | 3551815.0               |
| test/episodes             | 975.0                   |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -27.0                   |
| test_1/avg_q              | -26.999999999999993     |
| test_1/n_subgoals         | 675.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 3900.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -27.0                   |
| train_0/current_q         | -9.514884973357796      |
| train_0/fw_bonus          | -0.9993196576833725     |
| train_0/fw_loss           | 0.005772878765128553    |
| train_0/mu_grads          | -0.016098379530012606   |
| train_0/mu_grads_std      | 0.5111320421099663      |
| train_0/mu_loss           | 9.364550236213441       |
| train_0/next_q            | -9.358023705699779      |
| train_0/q_grads           | -0.02071662377566099    |
| train_0/q_grads_std       | 0.32596084997057917     |
| train_0/q_loss            | 0.18015913992830726     |
| train_0/reward            | -0.637097979264945      |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.1896728515625         |
| train_0/target_q          | -9.669116300639129      |
| train_1/avg_q             | -25.997028193894323     |
| train_1/current_q         | -2.4278202095720767     |
| train_1/fw_bonus          | -0.9936168536543846     |
| train_1/fw_loss           | 0.0746912082657218      |
| train_1/mu_grads          | -0.041399337351322174   |
| train_1/mu_grads_std      | 0.5874417424201965      |
| train_1/mu_loss           | 1.0                     |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -1.8999897004318064e-16 |
| train_1/q_grads           | -0.04316186895594001    |
| train_1/q_grads_std       | 0.371001298725605       |
| train_1/q_loss            | 4.922500602095774       |
| train_1/reward            | -2.670743082084664      |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.0071533203125         |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -2.670743082084664      |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 39
Time for epoch 39: 412.98. Rollout time: 216.20, Training time: 196.75
Evaluating epoch 39
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 39                    |
| policy/steps              | 3642940.0             |
| test/episodes             | 1000.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -27.0                 |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 4000.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -27.0                 |
| train_0/current_q         | -9.366885433487258    |
| train_0/fw_bonus          | -0.9993382558226586   |
| train_0/fw_loss           | 0.005626390036195516  |
| train_0/mu_grads          | -0.016352695133537053 |
| train_0/mu_grads_std      | 0.5141767457127571    |
| train_0/mu_loss           | 9.209065692571556     |
| train_0/next_q            | -9.203374330510778    |
| train_0/q_grads           | -0.021565031539648772 |
| train_0/q_grads_std       | 0.32917757481336596   |
| train_0/q_loss            | 0.18014462817205618   |
| train_0/reward            | -0.6340630816179328   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.188720703125        |
| train_0/target_q          | -9.521147847646393    |
| train_1/avg_q             | -26.589347459547948   |
| train_1/current_q         | -26.999999999999993   |
| train_1/fw_bonus          | -0.9930611923336983   |
| train_1/fw_loss           | 0.07970907092094422   |
| train_1/mu_grads          | -0.04138151556253433  |
| train_1/mu_grads_std      | 0.5998184084892273    |
| train_1/mu_loss           | 28.0                  |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -27.0                 |
| train_1/q_grads           | -0.04169776663184166  |
| train_1/q_grads_std       | 0.37457433342933655   |
| train_1/q_loss            | 89.76823742048039     |
| train_1/reward            | -2.6607713990215416   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.00595703125         |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -23.57037296152156    |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 40
Time for epoch 40: 407.73. Rollout time: 219.57, Training time: 188.12
Evaluating epoch 40
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 40                    |
| policy/steps              | 3734065.0             |
| test/episodes             | 1025.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -27.0                 |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 4100.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -27.0                 |
| train_0/current_q         | -9.598277214714935    |
| train_0/fw_bonus          | -0.9993729904294014   |
| train_0/fw_loss           | 0.005353109096176922  |
| train_0/mu_grads          | -0.017025562236085534 |
| train_0/mu_grads_std      | 0.5172792226076126    |
| train_0/mu_loss           | 9.428915003876595     |
| train_0/next_q            | -9.42423363244904     |
| train_0/q_grads           | -0.021169419679790736 |
| train_0/q_grads_std       | 0.33260994255542753   |
| train_0/q_loss            | 0.18556725097763216   |
| train_0/reward            | -0.6461933444072201   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.1984130859375       |
| train_0/target_q          | -9.75238484999967     |
| train_1/avg_q             | -27.0                 |
| train_1/current_q         | -26.999999999999915   |
| train_1/fw_bonus          | -0.9931185886263847   |
| train_1/fw_loss           | 0.07919066753238439   |
| train_1/mu_grads          | -0.04138151556253433  |
| train_1/mu_grads_std      | 0.5998184084892273    |
| train_1/mu_loss           | 28.0                  |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -27.0                 |
| train_1/q_grads           | -0.04169776663184166  |
| train_1/q_grads_std       | 0.37457433342933655   |
| train_1/q_loss            | 85.77119781468409     |
| train_1/reward            | -2.7069649525426938   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0061767578125       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -23.710881944730207   |
-----------------------------------------------------
Saving periodic policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_40.pkl ...
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 41
Time for epoch 41: 415.87. Rollout time: 220.35, Training time: 195.50
Evaluating epoch 41
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 41                    |
| policy/steps              | 3825190.0             |
| test/episodes             | 1050.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -27.0                 |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 4200.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -27.0                 |
| train_0/current_q         | -9.56166657657253     |
| train_0/fw_bonus          | -0.9993986204266548   |
| train_0/fw_loss           | 0.005151429341640324  |
| train_0/mu_grads          | -0.01831358321942389  |
| train_0/mu_grads_std      | 0.5210312023758888    |
| train_0/mu_loss           | 9.383557816046402     |
| train_0/next_q            | -9.378336974766658    |
| train_0/q_grads           | -0.022432092111557722 |
| train_0/q_grads_std       | 0.33627376407384874   |
| train_0/q_loss            | 0.17376377100876372   |
| train_0/reward            | -0.6476645944374468   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.1702392578125       |
| train_0/target_q          | -9.716145088122566    |
| train_1/avg_q             | -27.0                 |
| train_1/current_q         | -26.99999999999998    |
| train_1/fw_bonus          | -0.9930287629365921   |
| train_1/fw_loss           | 0.08000179547816515   |
| train_1/mu_grads          | -0.04138151556253433  |
| train_1/mu_grads_std      | 0.5998184084892273    |
| train_1/mu_loss           | 28.0                  |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -27.0                 |
| train_1/q_grads           | -0.04169776663184166  |
| train_1/q_grads_std       | 0.37457433342933655   |
| train_1/q_loss            | 78.63829542157671     |
| train_1/reward            | -2.7115567533051945   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0065673828125       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -23.98291612830521    |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 42
Time for epoch 42: 412.36. Rollout time: 214.51, Training time: 197.82
Evaluating epoch 42
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 42                    |
| policy/steps              | 3916315.0             |
| test/episodes             | 1075.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -27.0                 |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 4300.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -27.0                 |
| train_0/current_q         | -9.540559479294698    |
| train_0/fw_bonus          | -0.999360416829586    |
| train_0/fw_loss           | 0.0054520228295587    |
| train_0/mu_grads          | -0.0197455873247236   |
| train_0/mu_grads_std      | 0.5243229731917382    |
| train_0/mu_loss           | 9.356919169682337     |
| train_0/next_q            | -9.351971070671107    |
| train_0/q_grads           | -0.023226782074198128 |
| train_0/q_grads_std       | 0.34029108509421346   |
| train_0/q_loss            | 0.17938595344449312   |
| train_0/reward            | -0.649815690705509    |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.1837646484375       |
| train_0/target_q          | -9.695516901643575    |
| train_1/avg_q             | -27.0                 |
| train_1/current_q         | -26.999999999999908   |
| train_1/fw_bonus          | -0.9933586895465851   |
| train_1/fw_loss           | 0.07702247500419616   |
| train_1/mu_grads          | -0.04138151556253433  |
| train_1/mu_grads_std      | 0.5998184084892273    |
| train_1/mu_loss           | 28.0                  |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -27.0                 |
| train_1/q_grads           | -0.04169776663184166  |
| train_1/q_grads_std       | 0.37457433342933655   |
| train_1/q_loss            | 73.83156793472128     |
| train_1/reward            | -2.690435621902361    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.00703125            |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -24.173035719558627   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 43
Time for epoch 43: 411.77. Rollout time: 215.35, Training time: 196.38
Evaluating epoch 43
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 43                    |
| policy/steps              | 4007440.0             |
| test/episodes             | 1100.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -27.0                 |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 4400.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -27.0                 |
| train_0/current_q         | -9.660155301689672    |
| train_0/fw_bonus          | -0.9993379682302475   |
| train_0/fw_loss           | 0.005628674151375889  |
| train_0/mu_grads          | -0.021263409731909634 |
| train_0/mu_grads_std      | 0.5281274020671844    |
| train_0/mu_loss           | 9.486103440800225     |
| train_0/next_q            | -9.480536533935563    |
| train_0/q_grads           | -0.023644911590963602 |
| train_0/q_grads_std       | 0.3442608408629894    |
| train_0/q_loss            | 0.18536179959592106   |
| train_0/reward            | -0.6503960168025514   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.191455078125        |
| train_0/target_q          | -9.817448156245067    |
| train_1/avg_q             | -27.0                 |
| train_1/current_q         | -26.999999999999876   |
| train_1/fw_bonus          | -0.9934137761592865   |
| train_1/fw_loss           | 0.07652508597820998   |
| train_1/mu_grads          | -0.04138151556253433  |
| train_1/mu_grads_std      | 0.5998184084892273    |
| train_1/mu_loss           | 28.0                  |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -27.0                 |
| train_1/q_grads           | -0.04169776663184166  |
| train_1/q_grads_std       | 0.37457433342933655   |
| train_1/q_loss            | 76.3461567450893      |
| train_1/reward            | -2.7116713161169175   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.005908203125        |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -24.070886159866934   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 44
Time for epoch 44: 410.48. Rollout time: 218.97, Training time: 191.48
Evaluating epoch 44
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 44                    |
| policy/steps              | 4098565.0             |
| test/episodes             | 1125.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -27.0                 |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 4500.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -27.0                 |
| train_0/current_q         | -9.487823793377233    |
| train_0/fw_bonus          | -0.9993328273296356   |
| train_0/fw_loss           | 0.005669319804292172  |
| train_0/mu_grads          | -0.02139158365316689  |
| train_0/mu_grads_std      | 0.5307668685913086    |
| train_0/mu_loss           | 9.319445076104913     |
| train_0/next_q            | -9.316550676977348    |
| train_0/q_grads           | -0.023943188972771166 |
| train_0/q_grads_std       | 0.34953094497323034   |
| train_0/q_loss            | 0.19501946192186698   |
| train_0/reward            | -0.640635204174032    |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.1671630859375       |
| train_0/target_q          | -9.63676254407512     |
| train_1/avg_q             | -27.0                 |
| train_1/current_q         | -26.999999999999943   |
| train_1/fw_bonus          | -0.9928259536623955   |
| train_1/fw_loss           | 0.08183322567492723   |
| train_1/mu_grads          | -0.04138151556253433  |
| train_1/mu_grads_std      | 0.5998184084892273    |
| train_1/mu_loss           | 28.0                  |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -27.0                 |
| train_1/q_grads           | -0.04169776663184166  |
| train_1/q_grads_std       | 0.37457433342933655   |
| train_1/q_loss            | 84.19256518439038     |
| train_1/reward            | -2.720954291935777    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0063232421875       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -23.767507026310792   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 45
Time for epoch 45: 416.34. Rollout time: 220.29, Training time: 196.01
Evaluating epoch 45
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 45                    |
| policy/steps              | 4189690.0             |
| test/episodes             | 1150.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -27.0                 |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 4600.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -27.0                 |
| train_0/current_q         | -9.473300374846295    |
| train_0/fw_bonus          | -0.9993079528212547   |
| train_0/fw_loss           | 0.00586492212023586   |
| train_0/mu_grads          | -0.020824196003377437 |
| train_0/mu_grads_std      | 0.5340770035982132    |
| train_0/mu_loss           | 9.310045712180633     |
| train_0/next_q            | -9.306164493427792    |
| train_0/q_grads           | -0.023612618120387195 |
| train_0/q_grads_std       | 0.35419923812150955   |
| train_0/q_loss            | 0.1960783973626228    |
| train_0/reward            | -0.6384317253232439   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.1565185546875       |
| train_0/target_q          | -9.626723627498015    |
| train_1/avg_q             | -27.0                 |
| train_1/current_q         | -26.99999999991084    |
| train_1/fw_bonus          | -0.9928123563528061   |
| train_1/fw_loss           | 0.081956079415977     |
| train_1/mu_grads          | -0.04138151556253433  |
| train_1/mu_grads_std      | 0.5998184084892273    |
| train_1/mu_loss           | 28.0                  |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -27.0                 |
| train_1/q_grads           | -0.041697729378938675 |
| train_1/q_grads_std       | 0.37457430362701416   |
| train_1/q_loss            | 88.45341473351297     |
| train_1/reward            | -2.6749731081534263   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0060302734375       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -23.616874475340943   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 46
Time for epoch 46: 414.98. Rollout time: 220.84, Training time: 194.11
Evaluating epoch 46
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 46                    |
| policy/steps              | 4280815.0             |
| test/episodes             | 1175.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -27.0                 |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 4700.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -27.0                 |
| train_0/current_q         | -9.564013294680702    |
| train_0/fw_bonus          | -0.9993378832936287   |
| train_0/fw_loss           | 0.005629346775822342  |
| train_0/mu_grads          | -0.020591528387740254 |
| train_0/mu_grads_std      | 0.536810801923275     |
| train_0/mu_loss           | 9.408156487946302     |
| train_0/next_q            | -9.403816187453021    |
| train_0/q_grads           | -0.02350961584597826  |
| train_0/q_grads_std       | 0.35812009274959566   |
| train_0/q_loss            | 0.1837163552803862    |
| train_0/reward            | -0.6377434077578072   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.1758056640625       |
| train_0/target_q          | -9.720615568034285    |
| train_1/avg_q             | -27.0                 |
| train_1/current_q         | -26.999999999950624   |
| train_1/fw_bonus          | -0.9926280036568642   |
| train_1/fw_loss           | 0.08362076859921216   |
| train_1/mu_grads          | -0.04138151556253433  |
| train_1/mu_grads_std      | 0.5998184084892273    |
| train_1/mu_loss           | 28.0                  |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -27.0                 |
| train_1/q_grads           | -0.04169744998216629  |
| train_1/q_grads_std       | 0.3745742440223694    |
| train_1/q_loss            | 94.85624810958281     |
| train_1/reward            | -2.6080935257134117   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0062255859375       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -23.389787861650927   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 47
Time for epoch 47: 402.58. Rollout time: 215.46, Training time: 187.08
Evaluating epoch 47
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 47                    |
| policy/steps              | 4371940.0             |
| test/episodes             | 1200.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -27.0                 |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 4800.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -27.0                 |
| train_0/current_q         | -9.463158212311438    |
| train_0/fw_bonus          | -0.9993812695145607   |
| train_0/fw_loss           | 0.005287886608857662  |
| train_0/mu_grads          | -0.020462137646973133 |
| train_0/mu_grads_std      | 0.5402707770466805    |
| train_0/mu_loss           | 9.308028029636349     |
| train_0/next_q            | -9.303345652687675    |
| train_0/q_grads           | -0.023797405930235983 |
| train_0/q_grads_std       | 0.3613055877387524    |
| train_0/q_loss            | 0.17861025286576457   |
| train_0/reward            | -0.6321878478011058   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.1378662109375       |
| train_0/target_q          | -9.617725386381773    |
| train_1/avg_q             | -27.0                 |
| train_1/current_q         | -26.999999999978492   |
| train_1/fw_bonus          | -0.9931734681129456   |
| train_1/fw_loss           | 0.07869517393410205   |
| train_1/mu_grads          | -0.04138151556253433  |
| train_1/mu_grads_std      | 0.5998184084892273    |
| train_1/mu_loss           | 28.0                  |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -27.0                 |
| train_1/q_grads           | -0.04169627204537392  |
| train_1/q_grads_std       | 0.37457430362701416   |
| train_1/q_loss            | 94.93004823816044     |
| train_1/reward            | -2.6592064509204647   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.005517578125        |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -23.37307119701423    |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 48
Time for epoch 48: 399.84. Rollout time: 215.70, Training time: 184.11
Evaluating epoch 48
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 48                    |
| policy/steps              | 4463065.0             |
| test/episodes             | 1225.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -27.0                 |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 4900.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -27.0                 |
| train_0/current_q         | -9.521699504096052    |
| train_0/fw_bonus          | -0.9994185507297516   |
| train_0/fw_loss           | 0.004994482256006449  |
| train_0/mu_grads          | -0.02057941104285419  |
| train_0/mu_grads_std      | 0.5447602346539497    |
| train_0/mu_loss           | 9.358791260720958     |
| train_0/next_q            | -9.354440585179766    |
| train_0/q_grads           | -0.023829758726060392 |
| train_0/q_grads_std       | 0.36464512571692465   |
| train_0/q_loss            | 0.1669215184695852    |
| train_0/reward            | -0.6373973633664718   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.1381103515625       |
| train_0/target_q          | -9.675875794748862    |
| train_1/avg_q             | -27.0                 |
| train_1/current_q         | -26.999999999950727   |
| train_1/fw_bonus          | -0.9933156207203865   |
| train_1/fw_loss           | 0.07741144727915525   |
| train_1/mu_grads          | -0.04138151556253433  |
| train_1/mu_grads_std      | 0.5998184084892273    |
| train_1/mu_loss           | 28.0                  |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -27.0                 |
| train_1/q_grads           | -0.041691282950341704 |
| train_1/q_grads_std       | 0.3745749890804291    |
| train_1/q_loss            | 93.46257266814175     |
| train_1/reward            | -2.6369214348073      |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.00458984375         |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -23.435411669182315   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 49
Time for epoch 49: 406.50. Rollout time: 216.91, Training time: 189.55
Evaluating epoch 49
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 49                    |
| policy/steps              | 4554190.0             |
| test/episodes             | 1250.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -27.0                 |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 5000.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -27.0                 |
| train_0/current_q         | -9.445788265494633    |
| train_0/fw_bonus          | -0.999444143474102    |
| train_0/fw_loss           | 0.004793161351699382  |
| train_0/mu_grads          | -0.02049020128324628  |
| train_0/mu_grads_std      | 0.548695282638073     |
| train_0/mu_loss           | 9.284001330696857     |
| train_0/next_q            | -9.27876565378746     |
| train_0/q_grads           | -0.024299356807023286 |
| train_0/q_grads_std       | 0.3680903948843479    |
| train_0/q_loss            | 0.1736821119359994    |
| train_0/reward            | -0.6370192368161952   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.1446044921875       |
| train_0/target_q          | -9.597174459411086    |
| train_1/avg_q             | -27.0                 |
| train_1/current_q         | -26.999999999959403   |
| train_1/fw_bonus          | -0.9938370972871781   |
| train_1/fw_loss           | 0.07270236872136593   |
| train_1/mu_grads          | -0.04138151556253433  |
| train_1/mu_grads_std      | 0.5998184084892273    |
| train_1/mu_loss           | 28.0                  |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -27.0                 |
| train_1/q_grads           | -0.041680892929434776 |
| train_1/q_grads_std       | 0.3745780438184738    |
| train_1/q_loss            | 90.49360905848312     |
| train_1/reward            | -2.697421678941828    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.00478515625         |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -23.532733690660592   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 50
Time for epoch 50: 409.11. Rollout time: 215.43, Training time: 193.65
Evaluating epoch 50
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 50                    |
| policy/steps              | 4645315.0             |
| test/episodes             | 1275.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -27.0                 |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 5100.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -27.0                 |
| train_0/current_q         | -9.486963987919852    |
| train_0/fw_bonus          | -0.9994475826621055   |
| train_0/fw_loss           | 0.004766031156759709  |
| train_0/mu_grads          | -0.021381709165871142 |
| train_0/mu_grads_std      | 0.5520363181829453    |
| train_0/mu_loss           | 9.320786549815068     |
| train_0/next_q            | -9.315788841949523    |
| train_0/q_grads           | -0.024706959119066597 |
| train_0/q_grads_std       | 0.3710347831249237    |
| train_0/q_loss            | 0.17152968708923141   |
| train_0/reward            | -0.6400962704221456   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.139208984375        |
| train_0/target_q          | -9.641085085138602    |
| train_1/avg_q             | -27.0                 |
| train_1/current_q         | -26.99999999999625    |
| train_1/fw_bonus          | -0.9934043556451797   |
| train_1/fw_loss           | 0.07661021631211043   |
| train_1/mu_grads          | -0.04138151556253433  |
| train_1/mu_grads_std      | 0.5998184084892273    |
| train_1/mu_loss           | 28.0                  |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -27.0                 |
| train_1/q_grads           | -0.041671911533921954 |
| train_1/q_grads_std       | 0.3745813310146332    |
| train_1/q_loss            | 86.59380863852682     |
| train_1/reward            | -2.576866713257914    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0050537109375       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -23.715150892945427   |
-----------------------------------------------------
Saving periodic policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_50.pkl ...
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 51
Time for epoch 51: 401.39. Rollout time: 217.46, Training time: 183.90
Evaluating epoch 51
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 51                    |
| policy/steps              | 4736440.0             |
| test/episodes             | 1300.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -27.0                 |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 5200.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -27.0                 |
| train_0/current_q         | -9.472335700991183    |
| train_0/fw_bonus          | -0.999422450363636    |
| train_0/fw_loss           | 0.004963927564676851  |
| train_0/mu_grads          | -0.022122744331136347 |
| train_0/mu_grads_std      | 0.5545545309782028    |
| train_0/mu_loss           | 9.31265315480927      |
| train_0/next_q            | -9.306978823708018    |
| train_0/q_grads           | -0.024423630302771927 |
| train_0/q_grads_std       | 0.3741197884082794    |
| train_0/q_loss            | 0.16752254407930942   |
| train_0/reward            | -0.638240618902637    |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.152197265625        |
| train_0/target_q          | -9.626731990469949    |
| train_1/avg_q             | -27.0                 |
| train_1/current_q         | -26.99999999999858    |
| train_1/fw_bonus          | -0.9929144993424416   |
| train_1/fw_loss           | 0.08103363271802663   |
| train_1/mu_grads          | -0.04138151556253433  |
| train_1/mu_grads_std      | 0.5998184084892273    |
| train_1/mu_loss           | 28.0                  |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -27.0                 |
| train_1/q_grads           | -0.041666018404066564 |
| train_1/q_grads_std       | 0.37458363100886344   |
| train_1/q_loss            | 86.74383986256511     |
| train_1/reward            | -2.645298939366694    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.00556640625         |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -23.69055968155421    |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 52
Time for epoch 52: 408.26. Rollout time: 219.84, Training time: 188.39
Evaluating epoch 52
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 52                    |
| policy/steps              | 4827565.0             |
| test/episodes             | 1325.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -27.0                 |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 5300.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -27.0                 |
| train_0/current_q         | -9.562783826281246    |
| train_0/fw_bonus          | -0.9994489088654518   |
| train_0/fw_loss           | 0.004755610460415482  |
| train_0/mu_grads          | -0.021751325065270066 |
| train_0/mu_grads_std      | 0.5587865561246872    |
| train_0/mu_loss           | 9.398721254614232     |
| train_0/next_q            | -9.397051056314574    |
| train_0/q_grads           | -0.02431143866851926  |
| train_0/q_grads_std       | 0.37724435776472093   |
| train_0/q_loss            | 0.1693012804749126    |
| train_0/reward            | -0.6357203806812322   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.130615234375        |
| train_0/target_q          | -9.71380285486315     |
| train_1/avg_q             | -27.0                 |
| train_1/current_q         | -26.999999999999204   |
| train_1/fw_bonus          | -0.9928758054971695   |
| train_1/fw_loss           | 0.0813830815255642    |
| train_1/mu_grads          | -0.04138151556253433  |
| train_1/mu_grads_std      | 0.5998184084892273    |
| train_1/mu_loss           | 28.0                  |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -27.0                 |
| train_1/q_grads           | -0.04166008839383721  |
| train_1/q_grads_std       | 0.37458647787570953   |
| train_1/q_loss            | 95.5623106394007      |
| train_1/reward            | -2.6432018622370377   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0051513671875       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -23.353190631768303   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 53
Time for epoch 53: 412.88. Rollout time: 218.24, Training time: 194.61
Evaluating epoch 53
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 53                    |
| policy/steps              | 4918690.0             |
| test/episodes             | 1350.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -27.0                 |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 5400.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -27.0                 |
| train_0/current_q         | -9.36920358485697     |
| train_0/fw_bonus          | -0.9994471728801727   |
| train_0/fw_loss           | 0.004769203590694815  |
| train_0/mu_grads          | -0.020662721013650298 |
| train_0/mu_grads_std      | 0.561651585996151     |
| train_0/mu_loss           | 9.201170352407749     |
| train_0/next_q            | -9.197719240501794    |
| train_0/q_grads           | -0.024167865933850407 |
| train_0/q_grads_std       | 0.3800865702331066    |
| train_0/q_loss            | 0.16382949377943062   |
| train_0/reward            | -0.6301718532045925   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.102392578125        |
| train_0/target_q          | -9.522593726874941    |
| train_1/avg_q             | -27.0                 |
| train_1/current_q         | -26.999999999999716   |
| train_1/fw_bonus          | -0.9932511761784554   |
| train_1/fw_loss           | 0.07799348365515471   |
| train_1/mu_grads          | -0.04138151556253433  |
| train_1/mu_grads_std      | 0.5998184084892273    |
| train_1/mu_loss           | 28.0                  |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -27.0                 |
| train_1/q_grads           | -0.04165872223675251  |
| train_1/q_grads_std       | 0.374587744474411     |
| train_1/q_loss            | 98.77527095956205     |
| train_1/reward            | -2.5912571278211542   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.004638671875        |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -23.24439824110242    |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 54
Time for epoch 54: 411.71. Rollout time: 220.67, Training time: 191.01
Evaluating epoch 54
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 54                    |
| policy/steps              | 5009785.0             |
| test/episodes             | 1375.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -27.0                 |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 5500.0                |
| train/success_rate        | 0.01                  |
| train_0/avg_q             | -27.0                 |
| train_0/current_q         | -9.531596904360978    |
| train_0/fw_bonus          | -0.9994278609752655   |
| train_0/fw_loss           | 0.004921263002324849  |
| train_0/mu_grads          | -0.0203625135589391   |
| train_0/mu_grads_std      | 0.5647879302501678    |
| train_0/mu_loss           | 9.375563837027261     |
| train_0/next_q            | -9.37284759642391     |
| train_0/q_grads           | -0.024147159419953823 |
| train_0/q_grads_std       | 0.3830763213336468    |
| train_0/q_loss            | 0.1683827284628157    |
| train_0/reward            | -0.631833282952357    |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.1485595703125       |
| train_0/target_q          | -9.688675383717749    |
| train_1/avg_q             | -27.0                 |
| train_1/current_q         | -26.999999999999496   |
| train_1/fw_bonus          | -0.9930217772722244   |
| train_1/fw_loss           | 0.08006499949842691   |
| train_1/mu_grads          | -0.04138151556253433  |
| train_1/mu_grads_std      | 0.5998184084892273    |
| train_1/mu_loss           | 28.0                  |
| train_1/n_subgoals        | 2699.0                |
| train_1/next_q            | -27.0                 |
| train_1/q_grads           | -0.04165775887668133  |
| train_1/q_grads_std       | 0.37458932399749756   |
| train_1/q_loss            | 100.96765137508979    |
| train_1/reward            | -2.6413889887946427   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.00478515625         |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -23.146596996607155   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 55
Time for epoch 55: 409.31. Rollout time: 219.38, Training time: 189.91
Evaluating epoch 55
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
----------------------------------------------------
| epoch                     | 55                   |
| policy/steps              | 5100910.0            |
| test/episodes             | 1400.0               |
| test/success_rate         | 0.0                  |
| test_0/avg_q              | -27.0                |
| test_1/avg_q              | -27.0                |
| test_1/n_subgoals         | 675.0                |
| test_1/subgoal_succ_rate  | 0.0                  |
| train/episodes            | 5600.0               |
| train/success_rate        | 0.0                  |
| train_0/avg_q             | -27.0                |
| train_0/current_q         | -9.494091049299614   |
| train_0/fw_bonus          | -0.9994386494159698  |
| train_0/fw_loss           | 0.004836289072409272 |
| train_0/mu_grads          | -0.02037344886921346 |
| train_0/mu_grads_std      | 0.5685733735561371   |
| train_0/mu_loss           | 9.327785821005046    |
| train_0/next_q            | -9.323650371343945   |
| train_0/q_grads           | -0.02436025640927255 |
| train_0/q_grads_std       | 0.38503977209329604  |
| train_0/q_loss            | 0.16690770449338882  |
| train_0/reward            | -0.6309936967751127  |
| train_0/reward_-0.0_frac  | 0.0                  |
| train_0/reward_-1.0_frac  | 0.149755859375       |
| train_0/target_q          | -9.645881282143831   |
| train_1/avg_q             | -26.999999999974854  |
| train_1/current_q         | -23.204660305097303  |
| train_1/fw_bonus          | -0.9933766558766365  |
| train_1/fw_loss           | 0.07686034478247165  |
| train_1/mu_grads          | -0.04138151556253433 |
| train_1/mu_grads_std      | 0.5998184084892273   |
| train_1/mu_loss           | 28.0                 |
| train_1/n_subgoals        | 2700.0               |
| train_1/next_q            | -27.0                |
| train_1/q_grads           | -0.04511388316750527 |
| train_1/q_grads_std       | 0.3721617363393307   |
| train_1/q_loss            | 10.46322688753049    |
| train_1/reward            | -2.668851974539939   |
| train_1/reward_-0.0_frac  | 0.0                  |
| train_1/reward_-1.0_frac  | 0.00478515625        |
| train_1/reward_-27.0_frac | 0.0                  |
| train_1/subgoal_succ_rate | 0.0                  |
| train_1/target_q          | -22.988659103446203  |
----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 56
Time for epoch 56: 409.11. Rollout time: 218.05, Training time: 191.03
Evaluating epoch 56
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 56                    |
| policy/steps              | 5192035.0             |
| test/episodes             | 1425.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -27.0                 |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 5700.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -27.0                 |
| train_0/current_q         | -9.566366258920052    |
| train_0/fw_bonus          | -0.9994666457176209   |
| train_0/fw_loss           | 0.0046160481753759084 |
| train_0/mu_grads          | -0.020286683365702628 |
| train_0/mu_grads_std      | 0.5716639697551728    |
| train_0/mu_loss           | 9.401139883780626     |
| train_0/next_q            | -9.401311484814613    |
| train_0/q_grads           | -0.02466048952192068  |
| train_0/q_grads_std       | 0.3866854101419449    |
| train_0/q_loss            | 0.17100920561587224   |
| train_0/reward            | -0.6311837982771976   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.140576171875        |
| train_0/target_q          | -9.722187045318595    |
| train_1/avg_q             | -26.999999836256567   |
| train_1/current_q         | -23.282908623912352   |
| train_1/fw_bonus          | -0.9944097220897674   |
| train_1/fw_loss           | 0.0675314124673605    |
| train_1/mu_grads          | -0.04138151556253433  |
| train_1/mu_grads_std      | 0.5998184084892273    |
| train_1/mu_loss           | 28.0                  |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -27.0                 |
| train_1/q_grads           | -0.04255500501021743  |
| train_1/q_grads_std       | 0.37970670163631437   |
| train_1/q_loss            | 6.519576741655665     |
| train_1/reward            | -2.680975480892812    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0041015625          |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -23.087992082455326   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 57
Time for epoch 57: 413.10. Rollout time: 219.07, Training time: 194.00
Evaluating epoch 57
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 57                    |
| policy/steps              | 5283160.0             |
| test/episodes             | 1450.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -27.0                 |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 5800.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -27.0                 |
| train_0/current_q         | -9.48985100487599     |
| train_0/fw_bonus          | -0.9995033770799637   |
| train_0/fw_loss           | 0.004326869209762663  |
| train_0/mu_grads          | -0.020887989597395063 |
| train_0/mu_grads_std      | 0.573986354470253     |
| train_0/mu_loss           | 9.3192847207914       |
| train_0/next_q            | -9.315238807990017    |
| train_0/q_grads           | -0.024817659659311174 |
| train_0/q_grads_std       | 0.38910815864801407   |
| train_0/q_loss            | 0.16848962332649867   |
| train_0/reward            | -0.6370523591343954   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.095556640625        |
| train_0/target_q          | -9.644726268084792    |
| train_1/avg_q             | -26.999999999986386   |
| train_1/current_q         | -23.506072588259045   |
| train_1/fw_bonus          | -0.9944890335202217   |
| train_1/fw_loss           | 0.06681529153138399   |
| train_1/mu_grads          | -0.04138151556253433  |
| train_1/mu_grads_std      | 0.5998184084892273    |
| train_1/mu_loss           | 28.0                  |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -27.0                 |
| train_1/q_grads           | -0.042987754382193086 |
| train_1/q_grads_std       | 0.3862152129411697    |
| train_1/q_loss            | 7.032470540852108     |
| train_1/reward            | -2.728163275905172    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0031982421875       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -23.334146674342684   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 58
Time for epoch 58: 435.65. Rollout time: 233.30, Training time: 202.31
Evaluating epoch 58
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 58                    |
| policy/steps              | 5374285.0             |
| test/episodes             | 1475.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -27.0                 |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 5900.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -27.0                 |
| train_0/current_q         | -9.488994586576581    |
| train_0/fw_bonus          | -0.999510882794857    |
| train_0/fw_loss           | 0.004267844988498836  |
| train_0/mu_grads          | -0.02036172114312649  |
| train_0/mu_grads_std      | 0.5760071858763695    |
| train_0/mu_loss           | 9.310376097295954     |
| train_0/next_q            | -9.306352978812914    |
| train_0/q_grads           | -0.025328241335228086 |
| train_0/q_grads_std       | 0.39180481657385824   |
| train_0/q_loss            | 0.1631072331582562    |
| train_0/reward            | -0.6379331403404649   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.11943359375         |
| train_0/target_q          | -9.644325288154509    |
| train_1/avg_q             | -26.99999999994371    |
| train_1/current_q         | -23.439159802368597   |
| train_1/fw_bonus          | -0.99470936357975     |
| train_1/fw_loss           | 0.06482575861737132   |
| train_1/mu_grads          | -0.04138151556253433  |
| train_1/mu_grads_std      | 0.5998184084892273    |
| train_1/mu_loss           | 28.0                  |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -27.0                 |
| train_1/q_grads           | -0.042327688075602055 |
| train_1/q_grads_std       | 0.39115663543343543   |
| train_1/q_loss            | 5.8133192225022485    |
| train_1/reward            | -2.703352633342365    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0033447265625       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -23.25184237943613    |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 59
Time for epoch 59: 418.64. Rollout time: 218.89, Training time: 199.72
Evaluating epoch 59
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
----------------------------------------------------
| epoch                     | 59                   |
| policy/steps              | 5465410.0            |
| test/episodes             | 1500.0               |
| test/success_rate         | 0.0                  |
| test_0/avg_q              | -27.0                |
| test_1/avg_q              | -26.999999999999947  |
| test_1/n_subgoals         | 675.0                |
| test_1/subgoal_succ_rate  | 0.0                  |
| train/episodes            | 6000.0               |
| train/success_rate        | 0.0                  |
| train_0/avg_q             | -27.0                |
| train_0/current_q         | -9.458857237841118   |
| train_0/fw_bonus          | -0.9995418712496758  |
| train_0/fw_loss           | 0.004023867892101407 |
| train_0/mu_grads          | -0.02122716624289751 |
| train_0/mu_grads_std      | 0.5783222436904907   |
| train_0/mu_loss           | 9.27555831583862     |
| train_0/next_q            | -9.272069253885093   |
| train_0/q_grads           | -0.02564658271148801 |
| train_0/q_grads_std       | 0.39462740421295167  |
| train_0/q_loss            | 0.1636413791673835   |
| train_0/reward            | -0.635230929242607   |
| train_0/reward_-0.0_frac  | 0.0                  |
| train_0/reward_-1.0_frac  | 0.0955322265625      |
| train_0/target_q          | -9.610664266682713   |
| train_1/avg_q             | -27.0                |
| train_1/current_q         | -23.46038307197022   |
| train_1/fw_bonus          | -0.9948960602283478  |
| train_1/fw_loss           | 0.06313977856189013  |
| train_1/mu_grads          | -0.04138151556253433 |
| train_1/mu_grads_std      | 0.5998184084892273   |
| train_1/mu_loss           | 28.0                 |
| train_1/n_subgoals        | 2700.0               |
| train_1/next_q            | -27.0                |
| train_1/q_grads           | -0.04218759192153811 |
| train_1/q_grads_std       | 0.3962827235460281   |
| train_1/q_loss            | 6.002718682842145    |
| train_1/reward            | -2.599193986760656   |
| train_1/reward_-0.0_frac  | 0.0                  |
| train_1/reward_-1.0_frac  | 0.0037353515625      |
| train_1/reward_-27.0_frac | 0.0                  |
| train_1/subgoal_succ_rate | 0.0                  |
| train_1/target_q          | -23.32597865472942   |
----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 60
Time for epoch 60: 411.17. Rollout time: 216.91, Training time: 194.23
Evaluating epoch 60
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 60                    |
| policy/steps              | 5556535.0             |
| test/episodes             | 1525.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -27.0                 |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 6100.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -27.0                 |
| train_0/current_q         | -9.535321724303392    |
| train_0/fw_bonus          | -0.9995194733142853   |
| train_0/fw_loss           | 0.004200116608990357  |
| train_0/mu_grads          | -0.0207376214209944   |
| train_0/mu_grads_std      | 0.5805325284600258    |
| train_0/mu_loss           | 9.36253270364288      |
| train_0/next_q            | -9.360386024971138    |
| train_0/q_grads           | -0.025582735799252986 |
| train_0/q_grads_std       | 0.39723070040345193   |
| train_0/q_loss            | 0.1614476711110672    |
| train_0/reward            | -0.6351337779025925   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.1090087890625       |
| train_0/target_q          | -9.691922858448532    |
| train_1/avg_q             | -26.999999996728235   |
| train_1/current_q         | -23.46206728503734    |
| train_1/fw_bonus          | -0.9947133556008338   |
| train_1/fw_loss           | 0.06478966921567916   |
| train_1/mu_grads          | -0.04138151556253433  |
| train_1/mu_grads_std      | 0.5998184084892273    |
| train_1/mu_loss           | 28.0                  |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -27.0                 |
| train_1/q_grads           | -0.04261302035301924  |
| train_1/q_grads_std       | 0.4011748142540455    |
| train_1/q_loss            | 5.856164029645262     |
| train_1/reward            | -2.642855583718119    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.004296875           |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -23.306978630593136   |
-----------------------------------------------------
Saving periodic policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_60.pkl ...
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 61
Time for epoch 61: 414.12. Rollout time: 218.68, Training time: 195.42
Evaluating epoch 61
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 61                    |
| policy/steps              | 5647655.0             |
| test/episodes             | 1550.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -26.999999999999837   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 6200.0                |
| train/success_rate        | 0.01                  |
| train_0/avg_q             | -27.0                 |
| train_0/current_q         | -9.371501523473725    |
| train_0/fw_bonus          | -0.9995019257068634   |
| train_0/fw_loss           | 0.004338363191345706  |
| train_0/mu_grads          | -0.0204881408251822   |
| train_0/mu_grads_std      | 0.5815632805228234    |
| train_0/mu_loss           | 9.200523647364673     |
| train_0/next_q            | -9.19732946075275     |
| train_0/q_grads           | -0.025510558066889642 |
| train_0/q_grads_std       | 0.40025152564048766   |
| train_0/q_loss            | 0.16856788366538847   |
| train_0/reward            | -0.6295446699121385   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.075244140625        |
| train_0/target_q          | -9.525858663425709    |
| train_1/avg_q             | -26.99947064223127    |
| train_1/current_q         | -23.275507496710286   |
| train_1/fw_bonus          | -0.994023859500885    |
| train_1/fw_loss           | 0.07101591546088457   |
| train_1/mu_grads          | -0.04138151556253433  |
| train_1/mu_grads_std      | 0.5998184084892273    |
| train_1/mu_loss           | 28.0                  |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -27.0                 |
| train_1/q_grads           | -0.042675234563648703 |
| train_1/q_grads_std       | 0.4061411924660206    |
| train_1/q_loss            | 6.43073474977003      |
| train_1/reward            | -2.6492017659864358   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0037109375          |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -23.12469395348645    |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 62
Time for epoch 62: 423.65. Rollout time: 224.58, Training time: 199.04
Evaluating epoch 62
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 62                    |
| policy/steps              | 5738780.0             |
| test/episodes             | 1575.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -26.999997699695104   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 6300.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -27.0                 |
| train_0/current_q         | -9.4061691672196      |
| train_0/fw_bonus          | -0.9994765013456345   |
| train_0/fw_loss           | 0.004538509552367032  |
| train_0/mu_grads          | -0.020243618451058865 |
| train_0/mu_grads_std      | 0.5843046680092812    |
| train_0/mu_loss           | 9.251928015991135     |
| train_0/next_q            | -9.248871646740792    |
| train_0/q_grads           | -0.025521563133224845 |
| train_0/q_grads_std       | 0.4033421091735363    |
| train_0/q_loss            | 0.16827156978106378   |
| train_0/reward            | -0.6263888979872718   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.109130859375        |
| train_0/target_q          | -9.562766383958254    |
| train_1/avg_q             | -26.999964209745823   |
| train_1/current_q         | -23.20772816770008    |
| train_1/fw_bonus          | -0.9928833484649658   |
| train_1/fw_loss           | 0.08131501823663712   |
| train_1/mu_grads          | -0.04138151556253433  |
| train_1/mu_grads_std      | 0.5998184084892273    |
| train_1/mu_loss           | 28.0                  |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -27.0                 |
| train_1/q_grads           | -0.04297474343329668  |
| train_1/q_grads_std       | 0.40977949276566505   |
| train_1/q_loss            | 6.441753872017424     |
| train_1/reward            | -2.62449036601065     |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0045166015625       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -23.075434701948165   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 63
Time for epoch 63: 425.92. Rollout time: 223.95, Training time: 201.94
Evaluating epoch 63
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 63                    |
| policy/steps              | 5829905.0             |
| test/episodes             | 1600.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -26.99999999952821    |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 6400.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -27.0                 |
| train_0/current_q         | -9.418665429138684    |
| train_0/fw_bonus          | -0.9994900315999985   |
| train_0/fw_loss           | 0.0044319398177322    |
| train_0/mu_grads          | -0.02017190186306834  |
| train_0/mu_grads_std      | 0.5867470234632493    |
| train_0/mu_loss           | 9.269222463926619     |
| train_0/next_q            | -9.265816951858413    |
| train_0/q_grads           | -0.025439051305875184 |
| train_0/q_grads_std       | 0.40659490451216695   |
| train_0/q_loss            | 0.16370211030015636   |
| train_0/reward            | -0.6203681132741622   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.100830078125        |
| train_0/target_q          | -9.569865742099662    |
| train_1/avg_q             | -26.999998999104008   |
| train_1/current_q         | -22.993184350942443   |
| train_1/fw_bonus          | -0.9925899043679237   |
| train_1/fw_loss           | 0.08396481182426214   |
| train_1/mu_grads          | -0.04138151556253433  |
| train_1/mu_grads_std      | 0.5998184084892273    |
| train_1/mu_loss           | 28.0                  |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -27.0                 |
| train_1/q_grads           | -0.04354826034978032  |
| train_1/q_grads_std       | 0.41415851190686226   |
| train_1/q_loss            | 6.959866982408569     |
| train_1/reward            | -2.671108901526895    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0035400390625       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -22.81585108902691    |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 64
Time for epoch 64: 416.99. Rollout time: 218.64, Training time: 198.32
Evaluating epoch 64
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 64                    |
| policy/steps              | 5921030.0             |
| test/episodes             | 1625.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -26.995888171623324   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 6500.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -27.0                 |
| train_0/current_q         | -9.405230337578882    |
| train_0/fw_bonus          | -0.9994713678956032   |
| train_0/fw_loss           | 0.004578838648740202  |
| train_0/mu_grads          | -0.02127551711164415  |
| train_0/mu_grads_std      | 0.588230749964714     |
| train_0/mu_loss           | 9.25982702347645      |
| train_0/next_q            | -9.257004313105545    |
| train_0/q_grads           | -0.025201989710330962 |
| train_0/q_grads_std       | 0.4097105339169502    |
| train_0/q_loss            | 0.15649910641240597   |
| train_0/reward            | -0.6168844429401361   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0895263671875       |
| train_0/target_q          | -9.558185989347697    |
| train_1/avg_q             | -26.98377435761059    |
| train_1/current_q         | -22.825914472720047   |
| train_1/fw_bonus          | -0.9921344295144081   |
| train_1/fw_loss           | 0.08807791545987129   |
| train_1/mu_grads          | -0.04138151556253433  |
| train_1/mu_grads_std      | 0.5998184084892273    |
| train_1/mu_loss           | 28.0                  |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -27.0                 |
| train_1/q_grads           | -0.0444306418299675   |
| train_1/q_grads_std       | 0.41838494017720224   |
| train_1/q_loss            | 6.698312202042944     |
| train_1/reward            | -2.6776051380245915   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0038330078125       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -22.671830235680854   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 65
Time for epoch 65: 417.64. Rollout time: 217.18, Training time: 200.43
Evaluating epoch 65
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 65                    |
| policy/steps              | 6012155.0             |
| test/episodes             | 1650.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -27.0                 |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 6600.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -27.0                 |
| train_0/current_q         | -9.404178695569081    |
| train_0/fw_bonus          | -0.9993899464607239   |
| train_0/fw_loss           | 0.005219651898369193  |
| train_0/mu_grads          | -0.020497242640703916 |
| train_0/mu_grads_std      | 0.5905276492238045    |
| train_0/mu_loss           | 9.266795719666874     |
| train_0/next_q            | -9.263163419915685    |
| train_0/q_grads           | -0.025492662470787762 |
| train_0/q_grads_std       | 0.4124281220138073    |
| train_0/q_loss            | 0.16452707795109883   |
| train_0/reward            | -0.6210022710321936   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.115380859375        |
| train_0/target_q          | -9.558950989370677    |
| train_1/avg_q             | -26.998967700859566   |
| train_1/current_q         | -23.15315894286267    |
| train_1/fw_bonus          | -0.9910549655556679   |
| train_1/fw_loss           | 0.09782551135867834   |
| train_1/mu_grads          | -0.04138151556253433  |
| train_1/mu_grads_std      | 0.5998184084892273    |
| train_1/mu_loss           | 28.0                  |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -27.0                 |
| train_1/q_grads           | -0.045679394900798795 |
| train_1/q_grads_std       | 0.4221174269914627    |
| train_1/q_loss            | 5.945558582150813     |
| train_1/reward            | -2.639359203686399    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.006298828125        |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -23.003094066967662   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 66
Time for epoch 66: 407.02. Rollout time: 209.39, Training time: 197.60
Evaluating epoch 66
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 66                    |
| policy/steps              | 6103280.0             |
| test/episodes             | 1675.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -27.0                 |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 6700.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -27.0                 |
| train_0/current_q         | -9.438500066461785    |
| train_0/fw_bonus          | -0.9993840500712394   |
| train_0/fw_loss           | 0.005266013892833143  |
| train_0/mu_grads          | -0.020261005125939847 |
| train_0/mu_grads_std      | 0.5950589388608932    |
| train_0/mu_loss           | 9.294532810011606     |
| train_0/next_q            | -9.28997337594711     |
| train_0/q_grads           | -0.025819591153413056 |
| train_0/q_grads_std       | 0.4164245218038559    |
| train_0/q_loss            | 0.17013147143215285   |
| train_0/reward            | -0.6261826832542283   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.1388671875          |
| train_0/target_q          | -9.594647340938717    |
| train_1/avg_q             | -26.99810780466389    |
| train_1/current_q         | -23.364017011302188   |
| train_1/fw_bonus          | -0.9903257772326469   |
| train_1/fw_loss           | 0.1044102756306529    |
| train_1/mu_grads          | -0.04138151556253433  |
| train_1/mu_grads_std      | 0.5998184084892273    |
| train_1/mu_loss           | 28.0                  |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -27.0                 |
| train_1/q_grads           | -0.04663399178534746  |
| train_1/q_grads_std       | 0.42604047656059263   |
| train_1/q_loss            | 5.7106484608786685    |
| train_1/reward            | -2.662943993330191    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0054443359375       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -23.202389794111458   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 67
Time for epoch 67: 378.61. Rollout time: 195.89, Training time: 182.70
Evaluating epoch 67
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 67                    |
| policy/steps              | 6194405.0             |
| test/episodes             | 1700.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -27.0                 |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 6800.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -27.0                 |
| train_0/current_q         | -9.487533440188402    |
| train_0/fw_bonus          | -0.9993640199303627   |
| train_0/fw_loss           | 0.005423691566102206  |
| train_0/mu_grads          | -0.020277162129059435 |
| train_0/mu_grads_std      | 0.5979353621602058    |
| train_0/mu_loss           | 9.339104784872527     |
| train_0/next_q            | -9.335664151102964    |
| train_0/q_grads           | -0.02599025461822748  |
| train_0/q_grads_std       | 0.4198143623769283    |
| train_0/q_loss            | 0.17157875865365563   |
| train_0/reward            | -0.6261593639945204   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.14697265625         |
| train_0/target_q          | -9.641585336262981    |
| train_1/avg_q             | -26.993355609393415   |
| train_1/current_q         | -23.521680259966274   |
| train_1/fw_bonus          | -0.9895603597164154   |
| train_1/fw_loss           | 0.1113220838829875    |
| train_1/mu_grads          | -0.04138151556253433  |
| train_1/mu_grads_std      | 0.5998184084892273    |
| train_1/mu_loss           | 28.0                  |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -27.0                 |
| train_1/q_grads           | -0.04765237299725413  |
| train_1/q_grads_std       | 0.42930885404348373   |
| train_1/q_loss            | 5.973090351509672     |
| train_1/reward            | -2.6765038603174616   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.005908203125        |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -23.352254836879975   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 68
Time for epoch 68: 375.12. Rollout time: 196.36, Training time: 178.73
Evaluating epoch 68
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 68                    |
| policy/steps              | 6285530.0             |
| test/episodes             | 1725.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -27.0                 |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 6900.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -27.0                 |
| train_0/current_q         | -9.40652442197775     |
| train_0/fw_bonus          | -0.9993507087230682   |
| train_0/fw_loss           | 0.005528432631399483  |
| train_0/mu_grads          | -0.019496722705662252 |
| train_0/mu_grads_std      | 0.6005903884768486    |
| train_0/mu_loss           | 9.260976585529232     |
| train_0/next_q            | -9.259059416666444    |
| train_0/q_grads           | -0.026069400319829582 |
| train_0/q_grads_std       | 0.42213221564888953   |
| train_0/q_loss            | 0.1640115065510227    |
| train_0/reward            | -0.62060794127392     |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.1464599609375       |
| train_0/target_q          | -9.560095478348837    |
| train_1/avg_q             | -26.991723701797767   |
| train_1/current_q         | -23.348266602113775   |
| train_1/fw_bonus          | -0.9893693625926971   |
| train_1/fw_loss           | 0.1130470123142004    |
| train_1/mu_grads          | -0.04138151556253433  |
| train_1/mu_grads_std      | 0.5998184084892273    |
| train_1/mu_loss           | 28.0                  |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -27.0                 |
| train_1/q_grads           | -0.04818309210240841  |
| train_1/q_grads_std       | 0.43181445747613906   |
| train_1/q_loss            | 5.96797124179937      |
| train_1/reward            | -2.6839268098810862   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.006103515625        |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -23.1697549348811     |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 69
Time for epoch 69: 374.07. Rollout time: 197.24, Training time: 176.80
Evaluating epoch 69
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 69                    |
| policy/steps              | 6376655.0             |
| test/episodes             | 1750.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -26.894616587594523   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 7000.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -27.0                 |
| train_0/current_q         | -9.330978314028792    |
| train_0/fw_bonus          | -0.9993406981229782   |
| train_0/fw_loss           | 0.005607206898275763  |
| train_0/mu_grads          | -0.019498831359669566 |
| train_0/mu_grads_std      | 0.6042178645730019    |
| train_0/mu_loss           | 9.189591481303289     |
| train_0/next_q            | -9.184142395528077    |
| train_0/q_grads           | -0.026007159193977714 |
| train_0/q_grads_std       | 0.42523128241300584   |
| train_0/q_loss            | 0.16335984695504985   |
| train_0/reward            | -0.6188659371699032   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.13076171875         |
| train_0/target_q          | -9.481094451451222    |
| train_1/avg_q             | -26.97014264183129    |
| train_1/current_q         | -23.111584334186066   |
| train_1/fw_bonus          | -0.9887637928128242   |
| train_1/fw_loss           | 0.11851536575704813   |
| train_1/mu_grads          | -0.04138151556253433  |
| train_1/mu_grads_std      | 0.5998184084892273    |
| train_1/mu_loss           | 28.0                  |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -27.0                 |
| train_1/q_grads           | -0.0491856231354177   |
| train_1/q_grads_std       | 0.43499005660414697   |
| train_1/q_loss            | 5.730496787560101     |
| train_1/reward            | -2.6517502721409985   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0052490234375       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -22.932151639328513   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 70
Time for epoch 70: 367.00. Rollout time: 193.13, Training time: 173.84
Evaluating epoch 70
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 70                    |
| policy/steps              | 6467780.0             |
| test/episodes             | 1775.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -27.0                 |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 7100.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -27.0                 |
| train_0/current_q         | -9.418296590171064    |
| train_0/fw_bonus          | -0.9993540778756141   |
| train_0/fw_loss           | 0.005501918611116707  |
| train_0/mu_grads          | -0.02023112610913813  |
| train_0/mu_grads_std      | 0.6080569580197335    |
| train_0/mu_loss           | 9.282339273821453     |
| train_0/next_q            | -9.27842121757862     |
| train_0/q_grads           | -0.026626202231273054 |
| train_0/q_grads_std       | 0.4281374715268612    |
| train_0/q_loss            | 0.16751735868577397   |
| train_0/reward            | -0.6176424749777653   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.1490478515625       |
| train_0/target_q          | -9.570523226786582    |
| train_1/avg_q             | -26.988710048996836   |
| train_1/current_q         | -23.089433728372175   |
| train_1/fw_bonus          | -0.9888961419463158   |
| train_1/fw_loss           | 0.11732004918158054   |
| train_1/mu_grads          | -0.04138151556253433  |
| train_1/mu_grads_std      | 0.5998184084892273    |
| train_1/mu_loss           | 28.0                  |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -27.0                 |
| train_1/q_grads           | -0.05052357977256179  |
| train_1/q_grads_std       | 0.4383513547480106    |
| train_1/q_loss            | 6.221432294504815     |
| train_1/reward            | -2.6243541658099274   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.004345703125        |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -22.931241372841193   |
-----------------------------------------------------
Saving periodic policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_70.pkl ...
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 71
Time for epoch 71: 373.43. Rollout time: 199.94, Training time: 173.46
Evaluating epoch 71
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 71                    |
| policy/steps              | 6558905.0             |
| test/episodes             | 1800.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -27.0                 |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 7200.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -27.0                 |
| train_0/current_q         | -9.43594825599082     |
| train_0/fw_bonus          | -0.9993861749768257   |
| train_0/fw_loss           | 0.005249302624724806  |
| train_0/mu_grads          | -0.020504701789468525 |
| train_0/mu_grads_std      | 0.6111388564109802    |
| train_0/mu_loss           | 9.299951067284063     |
| train_0/next_q            | -9.296597436668744    |
| train_0/q_grads           | -0.026393341831862926 |
| train_0/q_grads_std       | 0.4306810714304447    |
| train_0/q_loss            | 0.159857657892266     |
| train_0/reward            | -0.6156180286343442   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.11484375            |
| train_0/target_q          | -9.594160587277027    |
| train_1/avg_q             | -26.95452563491995    |
| train_1/current_q         | -22.72767948385126    |
| train_1/fw_bonus          | -0.9895763292908668   |
| train_1/fw_loss           | 0.11117788814008236   |
| train_1/mu_grads          | -0.04138151556253433  |
| train_1/mu_grads_std      | 0.5998184084892273    |
| train_1/mu_loss           | 28.0                  |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -27.0                 |
| train_1/q_grads           | -0.05163498753681779  |
| train_1/q_grads_std       | 0.44113148748874664   |
| train_1/q_loss            | 5.981527473973965     |
| train_1/reward            | -2.6159548754218123   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0053955078125       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -22.585632121515577   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 72
Time for epoch 72: 371.45. Rollout time: 197.69, Training time: 173.73
Evaluating epoch 72
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 72                    |
| policy/steps              | 6650030.0             |
| test/episodes             | 1825.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -26.99999999970147    |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 7300.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -27.0                 |
| train_0/current_q         | -9.517023736938787    |
| train_0/fw_bonus          | -0.9994442611932755   |
| train_0/fw_loss           | 0.004792151553556323  |
| train_0/mu_grads          | -0.02030055704526603  |
| train_0/mu_grads_std      | 0.6149102091789246    |
| train_0/mu_loss           | 9.389057806248292     |
| train_0/next_q            | -9.385809594138777    |
| train_0/q_grads           | -0.026509725162759422 |
| train_0/q_grads_std       | 0.43255185410380365   |
| train_0/q_loss            | 0.16782335942306398   |
| train_0/reward            | -0.6181594054036396   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.1027099609375       |
| train_0/target_q          | -9.6709065236328      |
| train_1/avg_q             | -26.989383281170404   |
| train_1/current_q         | -22.615080545687526   |
| train_1/fw_bonus          | -0.9901159092783928   |
| train_1/fw_loss           | 0.10630537308752537   |
| train_1/mu_grads          | -0.04138151556253433  |
| train_1/mu_grads_std      | 0.5998184084892273    |
| train_1/mu_loss           | 28.0                  |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -27.0                 |
| train_1/q_grads           | -0.0527280448935926   |
| train_1/q_grads_std       | 0.4452615141868591    |
| train_1/q_loss            | 7.062918475127856     |
| train_1/reward            | -2.6275704824638524   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0052490234375       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -22.461588548870118   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 73
Time for epoch 73: 366.83. Rollout time: 197.04, Training time: 169.75
Evaluating epoch 73
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 73                    |
| policy/steps              | 6741155.0             |
| test/episodes             | 1850.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -26.99999999999986    |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 7400.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -27.0                 |
| train_0/current_q         | -9.414556480365912    |
| train_0/fw_bonus          | -0.9994493409991264   |
| train_0/fw_loss           | 0.004752074007410556  |
| train_0/mu_grads          | -0.019907310232520105 |
| train_0/mu_grads_std      | 0.6173979565501213    |
| train_0/mu_loss           | 9.268152600083516     |
| train_0/next_q            | -9.265968246958352    |
| train_0/q_grads           | -0.02632874371483922  |
| train_0/q_grads_std       | 0.4341880641877651    |
| train_0/q_loss            | 0.17205998514618834   |
| train_0/reward            | -0.6229214209819475   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.1365966796875       |
| train_0/target_q          | -9.568408664065043    |
| train_1/avg_q             | -26.97731370746232    |
| train_1/current_q         | -22.998596441111903   |
| train_1/fw_bonus          | -0.9903045937418937   |
| train_1/fw_loss           | 0.10460160989314318   |
| train_1/mu_grads          | -0.04138151556253433  |
| train_1/mu_grads_std      | 0.5998184084892273    |
| train_1/mu_loss           | 28.0                  |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -27.0                 |
| train_1/q_grads           | -0.053300471417605876 |
| train_1/q_grads_std       | 0.44964052811264993   |
| train_1/q_loss            | 6.702494409124446     |
| train_1/reward            | -2.598888402896409    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0058837890625       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -22.86701584430267    |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 74
Time for epoch 74: 371.62. Rollout time: 197.17, Training time: 174.42
Evaluating epoch 74
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 74                    |
| policy/steps              | 6832280.0             |
| test/episodes             | 1875.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -26.999999999992962   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 7500.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -27.0                 |
| train_0/current_q         | -9.465534098378669    |
| train_0/fw_bonus          | -0.9994947537779808   |
| train_0/fw_loss           | 0.00439474901650101   |
| train_0/mu_grads          | -0.018822920508682728 |
| train_0/mu_grads_std      | 0.6194532155990601    |
| train_0/mu_loss           | 9.306849272669856     |
| train_0/next_q            | -9.303000043512856    |
| train_0/q_grads           | -0.027233033208176494 |
| train_0/q_grads_std       | 0.43609519302845      |
| train_0/q_loss            | 0.16898388901192649   |
| train_0/reward            | -0.6313556226992659   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0866943359375       |
| train_0/target_q          | -9.617779191010147    |
| train_1/avg_q             | -26.978247565804867   |
| train_1/current_q         | -23.467380035197124   |
| train_1/fw_bonus          | -0.9906439900398254   |
| train_1/fw_loss           | 0.10153677742928266   |
| train_1/mu_grads          | -0.04138151556253433  |
| train_1/mu_grads_std      | 0.5998184084892273    |
| train_1/mu_loss           | 28.0                  |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -27.0                 |
| train_1/q_grads           | -0.05363484714180231  |
| train_1/q_grads_std       | 0.4535594418644905    |
| train_1/q_loss            | 6.360791929747083     |
| train_1/reward            | -2.7163072371029555   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0052978515625       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -23.304202744915468   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 75
Time for epoch 75: 376.66. Rollout time: 200.24, Training time: 176.39
Evaluating epoch 75
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 75                    |
| policy/steps              | 6923405.0             |
| test/episodes             | 1900.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -26.999999999999428   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 7600.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -27.0                 |
| train_0/current_q         | -9.48205660492619     |
| train_0/fw_bonus          | -0.999502569437027    |
| train_0/fw_loss           | 0.004333262360887602  |
| train_0/mu_grads          | -0.018526053382083775 |
| train_0/mu_grads_std      | 0.6206122756004333    |
| train_0/mu_loss           | 9.327579619170333     |
| train_0/next_q            | -9.321653335365568    |
| train_0/q_grads           | -0.02792422561906278  |
| train_0/q_grads_std       | 0.43815495893359185   |
| train_0/q_loss            | 0.1784546500094682    |
| train_0/reward            | -0.6352760330511955   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.1303466796875       |
| train_0/target_q          | -9.635584014809325    |
| train_1/avg_q             | -26.978254920684282   |
| train_1/current_q         | -23.480405016883076   |
| train_1/fw_bonus          | -0.9908146187663078   |
| train_1/fw_loss           | 0.0999959172680974    |
| train_1/mu_grads          | -0.04138151556253433  |
| train_1/mu_grads_std      | 0.5998184084892273    |
| train_1/mu_loss           | 28.0                  |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -27.0                 |
| train_1/q_grads           | -0.054506377410143615 |
| train_1/q_grads_std       | 0.4572105288505554    |
| train_1/q_loss            | 6.507777818078743     |
| train_1/reward            | -2.689891156235899    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0048095703125       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -23.321068402329665   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 76
Time for epoch 76: 373.83. Rollout time: 199.06, Training time: 174.74
Evaluating epoch 76
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 76                    |
| policy/steps              | 7014530.0             |
| test/episodes             | 1925.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -27.0                 |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 7700.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -27.0                 |
| train_0/current_q         | -9.389348252042073    |
| train_0/fw_bonus          | -0.9995350807905197   |
| train_0/fw_loss           | 0.004077407123986632  |
| train_0/mu_grads          | -0.018156794272363184 |
| train_0/mu_grads_std      | 0.6217744171619415    |
| train_0/mu_loss           | 9.223642710726224     |
| train_0/next_q            | -9.220557951519423    |
| train_0/q_grads           | -0.028300707368180154 |
| train_0/q_grads_std       | 0.4395075708627701    |
| train_0/q_loss            | 0.17466649782448856   |
| train_0/reward            | -0.6361996876217745   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.083203125           |
| train_0/target_q          | -9.540470635767658    |
| train_1/avg_q             | -26.9997343754132     |
| train_1/current_q         | -23.587543251016903   |
| train_1/fw_bonus          | -0.9908729523420334   |
| train_1/fw_loss           | 0.09946915321052074   |
| train_1/mu_grads          | -0.04138151556253433  |
| train_1/mu_grads_std      | 0.5998184084892273    |
| train_1/mu_loss           | 28.0                  |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -27.0                 |
| train_1/q_grads           | -0.05393816288560629  |
| train_1/q_grads_std       | 0.46083945855498315   |
| train_1/q_loss            | 6.198178393122968     |
| train_1/reward            | -2.68963108254502     |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.004248046875        |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -23.433211648951282   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 77
Time for epoch 77: 368.38. Rollout time: 198.67, Training time: 169.68
Evaluating epoch 77
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 77                    |
| policy/steps              | 7105655.0             |
| test/episodes             | 1950.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -27.0                 |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 7800.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -27.0                 |
| train_0/current_q         | -9.476744993567976    |
| train_0/fw_bonus          | -0.9995440065860748   |
| train_0/fw_loss           | 0.004007053532404825  |
| train_0/mu_grads          | -0.018692757561802865 |
| train_0/mu_grads_std      | 0.6225701972842217    |
| train_0/mu_loss           | 9.309017513325363     |
| train_0/next_q            | -9.304501657133143    |
| train_0/q_grads           | -0.028071872889995575 |
| train_0/q_grads_std       | 0.4405439756810665    |
| train_0/q_loss            | 0.16553004879693956   |
| train_0/reward            | -0.6371906919972389   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.087939453125        |
| train_0/target_q          | -9.627894014598295    |
| train_1/avg_q             | -26.973702175310276   |
| train_1/current_q         | -23.617564715342223   |
| train_1/fw_bonus          | -0.9911164566874504   |
| train_1/fw_loss           | 0.09727033711969853   |
| train_1/mu_grads          | -0.04138151556253433  |
| train_1/mu_grads_std      | 0.5998184084892273    |
| train_1/mu_loss           | 28.0                  |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -27.0                 |
| train_1/q_grads           | -0.05364242680370808  |
| train_1/q_grads_std       | 0.46431752368807794   |
| train_1/q_loss            | 6.640365236803007     |
| train_1/reward            | -2.7206001590795497   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0033203125          |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -23.438340881735815   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 78
Time for epoch 78: 366.92. Rollout time: 194.47, Training time: 172.42
Evaluating epoch 78
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 78                    |
| policy/steps              | 7196780.0             |
| test/episodes             | 1975.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -27.0                 |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 7900.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -27.0                 |
| train_0/current_q         | -9.406983074178417    |
| train_0/fw_bonus          | -0.999529555439949    |
| train_0/fw_loss           | 0.0041209119372069836 |
| train_0/mu_grads          | -0.01923114601522684  |
| train_0/mu_grads_std      | 0.6240751892328262    |
| train_0/mu_loss           | 9.242475201840449     |
| train_0/next_q            | -9.240060750875841    |
| train_0/q_grads           | -0.027687818743288517 |
| train_0/q_grads_std       | 0.4426925003528595    |
| train_0/q_loss            | 0.17385446751316874   |
| train_0/reward            | -0.6314741847643746   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.1144287109375       |
| train_0/target_q          | -9.5594833932869      |
| train_1/avg_q             | -26.987234309939087   |
| train_1/current_q         | -23.423212578905105   |
| train_1/fw_bonus          | -0.9908911198377609   |
| train_1/fw_loss           | 0.0993051091209054    |
| train_1/mu_grads          | -0.04138151556253433  |
| train_1/mu_grads_std      | 0.5998184084892273    |
| train_1/mu_loss           | 28.0                  |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -27.0                 |
| train_1/q_grads           | -0.05335772288963199  |
| train_1/q_grads_std       | 0.4666648395359516    |
| train_1/q_loss            | 5.48011922626722      |
| train_1/reward            | -2.6564202916895736   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0026611328125       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -23.25723572137709    |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 79
Time for epoch 79: 369.35. Rollout time: 195.10, Training time: 174.22
Evaluating epoch 79
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 79                    |
| policy/steps              | 7287905.0             |
| test/episodes             | 2000.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -26.999730955421658   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 8000.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -27.0                 |
| train_0/current_q         | -9.472739370369169    |
| train_0/fw_bonus          | -0.9995309501886368   |
| train_0/fw_loss           | 0.004109855595743284  |
| train_0/mu_grads          | -0.020088830264285207 |
| train_0/mu_grads_std      | 0.625836917757988     |
| train_0/mu_loss           | 9.297708475418336     |
| train_0/next_q            | -9.296031205023022    |
| train_0/q_grads           | -0.028112333128228785 |
| train_0/q_grads_std       | 0.4450704112648964    |
| train_0/q_loss            | 0.16699641472932214   |
| train_0/reward            | -0.6341478757509321   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0837646484375       |
| train_0/target_q          | -9.624654763801436    |
| train_1/avg_q             | -26.960255563369742   |
| train_1/current_q         | -23.246733169550033   |
| train_1/fw_bonus          | -0.9904615864157676   |
| train_1/fw_loss           | 0.10318399183452129   |
| train_1/mu_grads          | -0.04138151556253433  |
| train_1/mu_grads_std      | 0.5998184084892273    |
| train_1/mu_loss           | 28.0                  |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -27.0                 |
| train_1/q_grads           | -0.053994841128587726 |
| train_1/q_grads_std       | 0.46972639486193657   |
| train_1/q_loss            | 5.561268255030937     |
| train_1/reward            | -2.6659808731183148   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0035888671875       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -23.067183509837083   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 80
Time for epoch 80: 375.17. Rollout time: 196.21, Training time: 178.92
Evaluating epoch 80
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 80                    |
| policy/steps              | 7379030.0             |
| test/episodes             | 2025.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -27.0                 |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 8100.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -27.0                 |
| train_0/current_q         | -9.529879572735396    |
| train_0/fw_bonus          | -0.9995429456233978   |
| train_0/fw_loss           | 0.004015500727109611  |
| train_0/mu_grads          | -0.020103627163916827 |
| train_0/mu_grads_std      | 0.6273788437247276    |
| train_0/mu_loss           | 9.368925192439354     |
| train_0/next_q            | -9.364470533401246    |
| train_0/q_grads           | -0.02834950969554484  |
| train_0/q_grads_std       | 0.4473003357648849    |
| train_0/q_loss            | 0.16370393761677066   |
| train_0/reward            | -0.6319352967133455   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.08759765625         |
| train_0/target_q          | -9.683752248456816    |
| train_1/avg_q             | -26.953661222353553   |
| train_1/current_q         | -23.210065459899333   |
| train_1/fw_bonus          | -0.9907952308654785   |
| train_1/fw_loss           | 0.10017114039510489   |
| train_1/mu_grads          | -0.04138151556253433  |
| train_1/mu_grads_std      | 0.5998184084892273    |
| train_1/mu_loss           | 28.0                  |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -27.0                 |
| train_1/q_grads           | -0.05527982674539089  |
| train_1/q_grads_std       | 0.4736155360937119    |
| train_1/q_loss            | 5.46349184418978      |
| train_1/reward            | -2.704131911130389    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0034423828125       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -23.018125075192906   |
-----------------------------------------------------
Saving periodic policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_80.pkl ...
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 81
Time for epoch 81: 371.83. Rollout time: 195.32, Training time: 176.48
Evaluating epoch 81
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 81                    |
| policy/steps              | 7470155.0             |
| test/episodes             | 2050.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -27.0                 |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 8200.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -27.0                 |
| train_0/current_q         | -9.47737555557585     |
| train_0/fw_bonus          | -0.9995225131511688   |
| train_0/fw_loss           | 0.004176339897094294  |
| train_0/mu_grads          | -0.02116212248802185  |
| train_0/mu_grads_std      | 0.6307281032204628    |
| train_0/mu_loss           | 9.316405500988257     |
| train_0/next_q            | -9.31449000930957     |
| train_0/q_grads           | -0.029105735803022982 |
| train_0/q_grads_std       | 0.4499343499541283    |
| train_0/q_loss            | 0.16218132852196368   |
| train_0/reward            | -0.6302273067474744   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.1064453125          |
| train_0/target_q          | -9.631359202892423    |
| train_1/avg_q             | -26.955544151885757   |
| train_1/current_q         | -23.195259096686673   |
| train_1/fw_bonus          | -0.9900420159101486   |
| train_1/fw_loss           | 0.10697271544486284   |
| train_1/mu_grads          | -0.04138151556253433  |
| train_1/mu_grads_std      | 0.5998184084892273    |
| train_1/mu_loss           | 28.0                  |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -27.0                 |
| train_1/q_grads           | -0.055407194700092074 |
| train_1/q_grads_std       | 0.47766185700893404   |
| train_1/q_loss            | 5.436692964734877     |
| train_1/reward            | -2.7114692435148755   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0047607421875       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -22.99445459507739    |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 82
Time for epoch 82: 374.28. Rollout time: 197.82, Training time: 176.43
Evaluating epoch 82
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 82                    |
| policy/steps              | 7561280.0             |
| test/episodes             | 2075.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -26.999999977887555   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 8300.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -27.0                 |
| train_0/current_q         | -9.507157830707545    |
| train_0/fw_bonus          | -0.9995133325457572   |
| train_0/fw_loss           | 0.004248550831107423  |
| train_0/mu_grads          | -0.02123510749079287  |
| train_0/mu_grads_std      | 0.633855339884758     |
| train_0/mu_loss           | 9.35365784330132      |
| train_0/next_q            | -9.347622085474919    |
| train_0/q_grads           | -0.029349073488265275 |
| train_0/q_grads_std       | 0.4528598144650459    |
| train_0/q_loss            | 0.16663764200639736   |
| train_0/reward            | -0.6312137824821548   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0877197265625       |
| train_0/target_q          | -9.658891901354622    |
| train_1/avg_q             | -26.99730269291778    |
| train_1/current_q         | -23.272384558398677   |
| train_1/fw_bonus          | -0.9897266119718552   |
| train_1/fw_loss           | 0.10982079487293958   |
| train_1/mu_grads          | -0.04138151556253433  |
| train_1/mu_grads_std      | 0.5998184084892273    |
| train_1/mu_loss           | 28.0                  |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -27.0                 |
| train_1/q_grads           | -0.05686843264847994  |
| train_1/q_grads_std       | 0.48143097683787345   |
| train_1/q_loss            | 6.680577886648943     |
| train_1/reward            | -2.6766552872428293   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0043212890625       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -23.094653822399096   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 83
Time for epoch 83: 375.81. Rollout time: 197.21, Training time: 178.57
Evaluating epoch 83
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 83                    |
| policy/steps              | 7652405.0             |
| test/episodes             | 2100.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -26.98470485248769    |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 8400.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -27.0                 |
| train_0/current_q         | -9.466769526758647    |
| train_0/fw_bonus          | -0.9995161786675453   |
| train_0/fw_loss           | 0.004226111201569438  |
| train_0/mu_grads          | -0.022666432010009886 |
| train_0/mu_grads_std      | 0.6370003342628479    |
| train_0/mu_loss           | 9.307569418700735     |
| train_0/next_q            | -9.300688530373002    |
| train_0/q_grads           | -0.030361654283478857 |
| train_0/q_grads_std       | 0.4557025745511055    |
| train_0/q_loss            | 0.15781218784302747   |
| train_0/reward            | -0.6339610771654407   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0949951171875       |
| train_0/target_q          | -9.619257359054414    |
| train_1/avg_q             | -26.946582816594326   |
| train_1/current_q         | -23.4952073705545     |
| train_1/fw_bonus          | -0.9895497918128967   |
| train_1/fw_loss           | 0.11141756735742092   |
| train_1/mu_grads          | -0.04138151556253433  |
| train_1/mu_grads_std      | 0.5998184084892273    |
| train_1/mu_loss           | 28.0                  |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -27.0                 |
| train_1/q_grads           | -0.05774158025160432  |
| train_1/q_grads_std       | 0.4848046742379665    |
| train_1/q_loss            | 6.087734515250654     |
| train_1/reward            | -2.704260006304685    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.004638671875        |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -23.3128273891172     |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 84
Time for epoch 84: 383.71. Rollout time: 199.77, Training time: 183.92
Evaluating epoch 84
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 84                    |
| policy/steps              | 7743530.0             |
| test/episodes             | 2125.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -26.97940318811255    |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 8500.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -27.0                 |
| train_0/current_q         | -9.525613244685962    |
| train_0/fw_bonus          | -0.9994912207126617   |
| train_0/fw_loss           | 0.004422642890131101  |
| train_0/mu_grads          | -0.022632441064342857 |
| train_0/mu_grads_std      | 0.6401008665561676    |
| train_0/mu_loss           | 9.370348141860683     |
| train_0/next_q            | -9.366807083876001    |
| train_0/q_grads           | -0.029757225513458253 |
| train_0/q_grads_std       | 0.45804487243294717   |
| train_0/q_loss            | 0.16880422541333143   |
| train_0/reward            | -0.6335153814019577   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.1005859375          |
| train_0/target_q          | -9.682906378635371    |
| train_1/avg_q             | -26.88601146987537    |
| train_1/current_q         | -23.359477770057314   |
| train_1/fw_bonus          | -0.9895271107554435   |
| train_1/fw_loss           | 0.11162236444652081   |
| train_1/mu_grads          | -0.04138151556253433  |
| train_1/mu_grads_std      | 0.5998184084892273    |
| train_1/mu_loss           | 28.0                  |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -27.0                 |
| train_1/q_grads           | -0.058372149243950844 |
| train_1/q_grads_std       | 0.48721447959542274   |
| train_1/q_loss            | 6.008704189040133     |
| train_1/reward            | -2.6594850510355172   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.00537109375         |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -23.206036808848033   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 85
Time for epoch 85: 374.09. Rollout time: 195.89, Training time: 178.17
Evaluating epoch 85
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 85                    |
| policy/steps              | 7834655.0             |
| test/episodes             | 2150.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -26.938712779506222   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 8600.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -27.0                 |
| train_0/current_q         | -9.51071096673017     |
| train_0/fw_bonus          | -0.9994486525654793   |
| train_0/fw_loss           | 0.004757581604644656  |
| train_0/mu_grads          | -0.022537709027528764 |
| train_0/mu_grads_std      | 0.6425995975732803    |
| train_0/mu_loss           | 9.35671359033812      |
| train_0/next_q            | -9.35245794048792     |
| train_0/q_grads           | -0.030315932631492615 |
| train_0/q_grads_std       | 0.46095039173960684   |
| train_0/q_loss            | 0.17451997058699337   |
| train_0/reward            | -0.6375513727689395   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.1372314453125       |
| train_0/target_q          | -9.662056030320553    |
| train_1/avg_q             | -26.918870520946932   |
| train_1/current_q         | -23.603363990063386   |
| train_1/fw_bonus          | -0.9884245321154594   |
| train_1/fw_loss           | 0.12157888263463974   |
| train_1/mu_grads          | -0.04138151556253433  |
| train_1/mu_grads_std      | 0.5998184084892273    |
| train_1/mu_loss           | 28.0                  |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -27.0                 |
| train_1/q_grads           | -0.05903102764859795  |
| train_1/q_grads_std       | 0.4899351865053177    |
| train_1/q_loss            | 5.791628619650934     |
| train_1/reward            | -2.6722831345075972   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0052001953125       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -23.43588957982011    |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 86
Time for epoch 86: 374.73. Rollout time: 193.83, Training time: 180.87
Evaluating epoch 86
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 86                    |
| policy/steps              | 7925780.0             |
| test/episodes             | 2175.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -26.99707426767117    |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 8700.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -27.0                 |
| train_0/current_q         | -9.47801996649319     |
| train_0/fw_bonus          | -0.9994333431124687   |
| train_0/fw_loss           | 0.004878096270840615  |
| train_0/mu_grads          | -0.02118860078044236  |
| train_0/mu_grads_std      | 0.6441081821918487    |
| train_0/mu_loss           | 9.31765043848955      |
| train_0/next_q            | -9.31560969995039     |
| train_0/q_grads           | -0.030480542732402683 |
| train_0/q_grads_std       | 0.46412546262145044   |
| train_0/q_loss            | 0.1705385310426641    |
| train_0/reward            | -0.6344164412152168   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.1375244140625       |
| train_0/target_q          | -9.628593169658199    |
| train_1/avg_q             | -26.867693805669806   |
| train_1/current_q         | -23.56659077347291    |
| train_1/fw_bonus          | -0.987875871360302    |
| train_1/fw_loss           | 0.12653353605419398   |
| train_1/mu_grads          | -0.04138151556253433  |
| train_1/mu_grads_std      | 0.5998184084892273    |
| train_1/mu_loss           | 28.0                  |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -27.0                 |
| train_1/q_grads           | -0.05989812910556793  |
| train_1/q_grads_std       | 0.4931871831417084    |
| train_1/q_loss            | 6.109287448313763     |
| train_1/reward            | -2.72812130801758     |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0052001953125       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -23.390952362705093   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 87
Time for epoch 87: 378.96. Rollout time: 196.23, Training time: 182.70
Evaluating epoch 87
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
----------------------------------------------------
| epoch                     | 87                   |
| policy/steps              | 8016905.0            |
| test/episodes             | 2200.0               |
| test/success_rate         | 0.0                  |
| test_0/avg_q              | -27.0                |
| test_1/avg_q              | -26.954900841633783  |
| test_1/n_subgoals         | 675.0                |
| test_1/subgoal_succ_rate  | 0.0                  |
| train/episodes            | 8800.0               |
| train/success_rate        | 0.0                  |
| train_0/avg_q             | -27.0                |
| train_0/current_q         | -9.503642559229883   |
| train_0/fw_bonus          | -0.9994306892156601  |
| train_0/fw_loss           | 0.004899029643274844 |
| train_0/mu_grads          | -0.02173643670976162 |
| train_0/mu_grads_std      | 0.645313972234726    |
| train_0/mu_loss           | 9.3577029908765      |
| train_0/next_q            | -9.353301334218703   |
| train_0/q_grads           | -0.03097916110418737 |
| train_0/q_grads_std       | 0.4661788731813431   |
| train_0/q_loss            | 0.16715148265620783  |
| train_0/reward            | -0.6271647996014508  |
| train_0/reward_-0.0_frac  | 0.0                  |
| train_0/reward_-1.0_frac  | 0.1462158203125      |
| train_0/target_q          | -9.659215661270395   |
| train_1/avg_q             | -26.84496220616783   |
| train_1/current_q         | -23.390363846192916  |
| train_1/fw_bonus          | -0.98758355230093    |
| train_1/fw_loss           | 0.12917306628078223  |
| train_1/mu_grads          | -0.04138151556253433 |
| train_1/mu_grads_std      | 0.5998184084892273   |
| train_1/mu_loss           | 28.0                 |
| train_1/n_subgoals        | 2700.0               |
| train_1/next_q            | -27.0                |
| train_1/q_grads           | -0.06014921069145203 |
| train_1/q_grads_std       | 0.4975184604525566   |
| train_1/q_loss            | 6.073484162993202    |
| train_1/reward            | -2.7189112687494346  |
| train_1/reward_-0.0_frac  | 0.0                  |
| train_1/reward_-1.0_frac  | 0.0051025390625      |
| train_1/reward_-27.0_frac | 0.0                  |
| train_1/subgoal_succ_rate | 0.0                  |
| train_1/target_q          | -23.21636732343695   |
----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 88
Time for epoch 88: 377.27. Rollout time: 198.22, Training time: 179.02
Evaluating epoch 88
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 88                    |
| policy/steps              | 8108030.0             |
| test/episodes             | 2225.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -26.938000770412906   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 8900.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -27.0                 |
| train_0/current_q         | -9.509594415153657    |
| train_0/fw_bonus          | -0.9994322821497917   |
| train_0/fw_loss           | 0.0048864483716897665 |
| train_0/mu_grads          | -0.02209916734136641  |
| train_0/mu_grads_std      | 0.6472140476107597    |
| train_0/mu_loss           | 9.373391366503906     |
| train_0/next_q            | -9.369361446436693    |
| train_0/q_grads           | -0.03230712348595262  |
| train_0/q_grads_std       | 0.46828521266579626   |
| train_0/q_loss            | 0.16507771268539875   |
| train_0/reward            | -0.6275314100916148   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.1147705078125       |
| train_0/target_q          | -9.66785613763686     |
| train_1/avg_q             | -26.87159003706002    |
| train_1/current_q         | -23.43598322245948    |
| train_1/fw_bonus          | -0.9868348553776741   |
| train_1/fw_loss           | 0.1359338952228427    |
| train_1/mu_grads          | -0.04138151556253433  |
| train_1/mu_grads_std      | 0.5998184084892273    |
| train_1/mu_loss           | 28.0                  |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -27.0                 |
| train_1/q_grads           | -0.060576492361724375 |
| train_1/q_grads_std       | 0.5009251236915588    |
| train_1/q_loss            | 5.010812823357542     |
| train_1/reward            | -2.6237106681557636   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.006298828125        |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -23.292355687687028   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 89
Time for epoch 89: 369.55. Rollout time: 198.67, Training time: 170.85
Evaluating epoch 89
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 89                    |
| policy/steps              | 8199155.0             |
| test/episodes             | 2250.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -26.89563551637372    |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 9000.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -27.0                 |
| train_0/current_q         | -9.420337352695622    |
| train_0/fw_bonus          | -0.9994434863328934   |
| train_0/fw_loss           | 0.004798287758603692  |
| train_0/mu_grads          | -0.021746549196541308 |
| train_0/mu_grads_std      | 0.6487821608781814    |
| train_0/mu_loss           | 9.276097039633399     |
| train_0/next_q            | -9.271274177903498    |
| train_0/q_grads           | -0.03180275782942772  |
| train_0/q_grads_std       | 0.4705271765589714    |
| train_0/q_loss            | 0.1721889233262905    |
| train_0/reward            | -0.6271313786193786   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.1098876953125       |
| train_0/target_q          | -9.572884924936456    |
| train_1/avg_q             | -26.802311772758806   |
| train_1/current_q         | -23.423465308307506   |
| train_1/fw_bonus          | -0.9865382149815559   |
| train_1/fw_loss           | 0.13861280102282764   |
| train_1/mu_grads          | -0.04138151556253433  |
| train_1/mu_grads_std      | 0.5998184084892273    |
| train_1/mu_loss           | 28.0                  |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -27.0                 |
| train_1/q_grads           | -0.06089930823072791  |
| train_1/q_grads_std       | 0.5046890616416931    |
| train_1/q_loss            | 5.928424787503327     |
| train_1/reward            | -2.7029944121728477   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.00673828125         |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -23.276032009829112   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 90
Time for epoch 90: 366.25. Rollout time: 196.08, Training time: 170.15
Evaluating epoch 90
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
----------------------------------------------------
| epoch                     | 90                   |
| policy/steps              | 8290280.0            |
| test/episodes             | 2275.0               |
| test/success_rate         | 0.0                  |
| test_0/avg_q              | -27.0                |
| test_1/avg_q              | -26.74384042125235   |
| test_1/n_subgoals         | 675.0                |
| test_1/subgoal_succ_rate  | 0.0                  |
| train/episodes            | 9100.0               |
| train/success_rate        | 0.0                  |
| train_0/avg_q             | -27.0                |
| train_0/current_q         | -9.487454887034286   |
| train_0/fw_bonus          | -0.9994437679648399  |
| train_0/fw_loss           | 0.00479606845183298  |
| train_0/mu_grads          | -0.0223819425329566  |
| train_0/mu_grads_std      | 0.650421042740345    |
| train_0/mu_loss           | 9.342956931647942    |
| train_0/next_q            | -9.339354268077901   |
| train_0/q_grads           | -0.03230570899322629 |
| train_0/q_grads_std       | 0.4726525589823723   |
| train_0/q_loss            | 0.165560166596947    |
| train_0/reward            | -0.628321634565873   |
| train_0/reward_-0.0_frac  | 0.0                  |
| train_0/reward_-1.0_frac  | 0.1322509765625      |
| train_0/target_q          | -9.642453409838385   |
| train_1/avg_q             | -26.771929302442672  |
| train_1/current_q         | -23.451377956888912  |
| train_1/fw_bonus          | -0.9871638521552086  |
| train_1/fw_loss           | 0.132963103428483    |
| train_1/mu_grads          | -0.04138151556253433 |
| train_1/mu_grads_std      | 0.5998184084892273   |
| train_1/mu_loss           | 28.0                 |
| train_1/n_subgoals        | 2700.0               |
| train_1/next_q            | -27.0                |
| train_1/q_grads           | -0.06162807997316122 |
| train_1/q_grads_std       | 0.5078146129846572   |
| train_1/q_loss            | 6.1425511750900785   |
| train_1/reward            | -2.6951124946608616  |
| train_1/reward_-0.0_frac  | 0.0                  |
| train_1/reward_-1.0_frac  | 0.005859375          |
| train_1/reward_-27.0_frac | 0.0                  |
| train_1/subgoal_succ_rate | 0.0                  |
| train_1/target_q          | -23.315953803254626  |
----------------------------------------------------
Saving periodic policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_90.pkl ...
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 91
Time for epoch 91: 415.45. Rollout time: 216.22, Training time: 199.20
Evaluating epoch 91
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 91                    |
| policy/steps              | 8381405.0             |
| test/episodes             | 2300.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -26.425287584787466   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 9200.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -27.0                 |
| train_0/current_q         | -9.548525694376224    |
| train_0/fw_bonus          | -0.9994356915354728   |
| train_0/fw_loss           | 0.004859585885424167  |
| train_0/mu_grads          | -0.022413405915722252 |
| train_0/mu_grads_std      | 0.6526140168309211    |
| train_0/mu_loss           | 9.402056496835275     |
| train_0/next_q            | -9.396969625709758    |
| train_0/q_grads           | -0.031878802180290225 |
| train_0/q_grads_std       | 0.4747088268399239    |
| train_0/q_loss            | 0.1767619561683182    |
| train_0/reward            | -0.6342888553332159   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.1091796875          |
| train_0/target_q          | -9.703230337697647    |
| train_1/avg_q             | -26.657416037565106   |
| train_1/current_q         | -23.504597206627384   |
| train_1/fw_bonus          | -0.9869088739156723   |
| train_1/fw_loss           | 0.13526559732854365   |
| train_1/mu_grads          | -0.04138151556253433  |
| train_1/mu_grads_std      | 0.5998184084892273    |
| train_1/mu_loss           | 28.0                  |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -27.0                 |
| train_1/q_grads           | -0.06262508518993855  |
| train_1/q_grads_std       | 0.5113272994756699    |
| train_1/q_loss            | 5.658052327828726     |
| train_1/reward            | -2.7237413777649637   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0069091796875       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -23.34522868245248    |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 92
Time for epoch 92: 437.59. Rollout time: 231.28, Training time: 206.28
Evaluating epoch 92
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 92                    |
| policy/steps              | 8472530.0             |
| test/episodes             | 2325.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -26.426416631670964   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 9300.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -27.0                 |
| train_0/current_q         | -9.362048275216058    |
| train_0/fw_bonus          | -0.9994530946016311   |
| train_0/fw_loss           | 0.004722659441176802  |
| train_0/mu_grads          | -0.021764769172295927 |
| train_0/mu_grads_std      | 0.6555305033922195    |
| train_0/mu_loss           | 9.205713059080674     |
| train_0/next_q            | -9.202807115584       |
| train_0/q_grads           | -0.031835004407912496 |
| train_0/q_grads_std       | 0.47600380778312684   |
| train_0/q_loss            | 0.16964452799385912   |
| train_0/reward            | -0.6323889079096261   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.1208740234375       |
| train_0/target_q          | -9.512976947663237    |
| train_1/avg_q             | -26.732753590579794   |
| train_1/current_q         | -23.647333013570442   |
| train_1/fw_bonus          | -0.9874189332127571   |
| train_1/fw_loss           | 0.13065972942858933   |
| train_1/mu_grads          | -0.04138151556253433  |
| train_1/mu_grads_std      | 0.5998184084892273    |
| train_1/mu_loss           | 28.0                  |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -27.0                 |
| train_1/q_grads           | -0.06337895654141903  |
| train_1/q_grads_std       | 0.5127696499228478    |
| train_1/q_loss            | 5.126827382079236     |
| train_1/reward            | -2.784806390392987    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.00625               |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -23.445699456799254   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 93
Time for epoch 93: 428.04. Rollout time: 224.98, Training time: 203.02
Evaluating epoch 93
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 93                    |
| policy/steps              | 8563655.0             |
| test/episodes             | 2350.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -26.896510267065164   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 9400.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -27.0                 |
| train_0/current_q         | -9.405975522267786    |
| train_0/fw_bonus          | -0.9994390681385994   |
| train_0/fw_loss           | 0.004833011759910732  |
| train_0/mu_grads          | -0.020960620371624827 |
| train_0/mu_grads_std      | 0.6586242705583573    |
| train_0/mu_loss           | 9.24425846262829      |
| train_0/next_q            | -9.238761830564197    |
| train_0/q_grads           | -0.032652299478650094 |
| train_0/q_grads_std       | 0.4778578050434589    |
| train_0/q_loss            | 0.1651090053047241    |
| train_0/reward            | -0.6366517599028156   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.112353515625        |
| train_0/target_q          | -9.558789795510332    |
| train_1/avg_q             | -26.4707814238609     |
| train_1/current_q         | -23.722749616578      |
| train_1/fw_bonus          | -0.9865973368287086   |
| train_1/fw_loss           | 0.13807875476777554   |
| train_1/mu_grads          | -0.04138151556253433  |
| train_1/mu_grads_std      | 0.5998184084892273    |
| train_1/mu_loss           | 28.0                  |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -27.0                 |
| train_1/q_grads           | -0.06394536532461643  |
| train_1/q_grads_std       | 0.5150045782327652    |
| train_1/q_loss            | 5.460587539215663     |
| train_1/reward            | -2.7283326845310514   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.007421875           |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -23.549432782187317   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 94
Time for epoch 94: 438.51. Rollout time: 229.84, Training time: 208.65
Evaluating epoch 94
Data_dir: data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 94                    |
| policy/steps              | 8654780.0             |
| test/episodes             | 2375.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -26.499035158564823   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 9500.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -27.0                 |
| train_0/current_q         | -9.491909608455966    |
| train_0/fw_bonus          | -0.9994059383869172   |
| train_0/fw_loss           | 0.005093735689297318  |
| train_0/mu_grads          | -0.019430529372766615 |
| train_0/mu_grads_std      | 0.6608042895793915    |
| train_0/mu_loss           | 9.337519005910586     |
| train_0/next_q            | -9.332659286695579    |
| train_0/q_grads           | -0.03273788010701537  |
| train_0/q_grads_std       | 0.4793967954814434    |
| train_0/q_loss            | 0.17533447868210433   |
| train_0/reward            | -0.6366984097257955   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.1504150390625       |
| train_0/target_q          | -9.649491751104659    |
| train_1/avg_q             | -26.524983462916225   |
| train_1/current_q         | -23.4333621650347     |
| train_1/fw_bonus          | -0.9856937497854232   |
| train_1/fw_loss           | 0.1462384108453989    |
| train_1/mu_grads          | -0.04138151556253433  |
| train_1/mu_grads_std      | 0.5998184084892273    |
| train_1/mu_loss           | 28.0                  |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -27.0                 |
| train_1/q_grads           | -0.06458877250552178  |
| train_1/q_grads_std       | 0.5165227308869362    |
| train_1/q_loss            | 4.241474789806096     |
| train_1/reward            | -2.6545166988849815   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0082275390625       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -23.303781835603747   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/AntReacherEnv-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 95
