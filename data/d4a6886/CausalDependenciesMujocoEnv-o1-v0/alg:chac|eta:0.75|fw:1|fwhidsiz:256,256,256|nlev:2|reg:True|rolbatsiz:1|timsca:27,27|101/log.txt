Starting process id: 62706
T: 50
alg: chac
algorithm: src.chac
atomic_noise: 0.2
base_logdir: data
batch_size: 1024
bind_core: 0
buffer_size: 500
chac_params: {}
early_stop_data_column: test/success_rate
early_stop_threshold: 100.0
env_name: CausalDependenciesMujocoEnv-o1-v0
eta: 0.75
fw: 1
fw_hidden_size: 256,256,256
fw_lr: 0.001
gamma: 0.98
graph: 1
info: 
make_env: <function prepare_params.<locals>.make_env at 0x7fd119e71440>
max_try_idx: 199
mu_hidden_size: 64
mu_lr: 0.001
n_episodes: 100
n_levels: 2
n_pre_episodes: 30
n_test_rollouts: 25
n_train_batches: 40
n_train_rollouts: 100
num_threads: 1
q_hidden_size: 64
q_lr: 0.001
random_action_perc: 0.3
regularization: True
render: 0
rollout_batch_size: 1
subgoal_noise: 0.2
subgoal_test_perc: 0.3
time_scales: 27,27
try_start_idx: 100
use_mpi: False
verbose: False

*** Warning ***
You are running src.chac with just a single MPI worker. This will work, but the HER experiments that we report in Plappert et al. (2018, https://arxiv.org/abs/1802.09464) were obtained with --num_cpu 19. This makes a significant difference and if you are looking to reproduce those results, be aware of this. Please also refer to https://github.com/openai/baselines/issues/314 for further details.
****************

dims: action = 2, subgoal = 2, end_goal = 2
subgoal_bounds: symmetric [0.2 0.2], offset [0.922 0.25 ]
Running on CPU ...
Creating a CHAC agent

Hierarchy Level 0 with time scale 27
Actor(
  (fc1): Linear(in_features=35, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
  (fc4): Linear(in_features=64, out_features=2, bias=True)
)
Critic(
  (fc1): Linear(in_features=37, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
  (fc4): Linear(in_features=64, out_features=1, bias=True)
  (mse_loss): MSELoss()
)
ForwardModel(
  (mlp): Sequential(
    (0): Linear(in_features=35, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=256, bias=True)
    (5): ReLU()
    (6): Linear(in_features=256, out_features=33, bias=True)
    (7): Identity()
  )
  (mse_loss): MSELoss()
)

Hierarchy Level 1 with time scale 27
Actor(
  (fc1): Linear(in_features=35, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
  (fc4): Linear(in_features=64, out_features=2, bias=True)
)
Critic(
  (fc1): Linear(in_features=37, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
  (fc4): Linear(in_features=64, out_features=1, bias=True)
  (mse_loss): MSELoss()
)
ForwardModel(
  (mlp): Sequential(
    (0): Linear(in_features=35, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=256, bias=True)
    (5): ReLU()
    (6): Linear(in_features=256, out_features=33, bias=True)
    (7): Identity()
  )
  (mse_loss): MSELoss()
)
Training epoch 0
Time for epoch 0: 833.07. Rollout time: 508.73, Training time: 324.27
Evaluating epoch 0
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
------------------------------------------------------
| epoch                     | 0                      |
| policy/steps              | 87719.0                |
| test/episodes             | 25.0                   |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -26.99999999998454     |
| test_1/avg_q              | -19.911792398205005    |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 100.0                  |
| train/success_rate        | 0.02                   |
| train_0/avg_q             | -15.659861259874734    |
| train_0/current_q         | -6.822265887573093     |
| train_0/fw_bonus          | -0.9995327830314636    |
| train_0/fw_loss           | 0.00011043006416002754 |
| train_0/mu_grads          | -0.0035527052474208175 |
| train_0/mu_grads_std      | 0.15226441696286203    |
| train_0/mu_loss           | 6.712809784969996      |
| train_0/next_q            | -6.709351091941504     |
| train_0/q_grads           | 0.020248769829049707   |
| train_0/q_grads_std       | 0.14509620927274228    |
| train_0/q_loss            | 0.23661327156269235    |
| train_0/reward            | -0.5824027251495864    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.0039306640625        |
| train_0/target_q          | -6.8959112579652215    |
| train_1/avg_q             | -10.803524039273714    |
| train_1/current_q         | -3.1518861583666054    |
| train_1/fw_bonus          | -0.9972306743264199    |
| train_1/fw_loss           | 0.001319238735595718   |
| train_1/mu_grads          | -2.377602825163194e-05 |
| train_1/mu_grads_std      | 0.11574075017124415    |
| train_1/mu_loss           | 1.0723129320565135     |
| train_1/n_subgoals        | 2685.0                 |
| train_1/next_q            | -1.0676684848899145    |
| train_1/q_grads           | 0.014691808610223234   |
| train_1/q_grads_std       | 0.21200735121965408    |
| train_1/q_loss            | 4.192075041643669      |
| train_1/reward            | -2.4789829573950555    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0059814453125        |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.04618249534450652    |
| train_1/target_q          | -3.1275193939174577    |
------------------------------------------------------
Saving periodic policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_0.pkl ...
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Training epoch 1
Time for epoch 1: 16338.59. Rollout time: 7705.66, Training time: 8632.77
Evaluating epoch 1
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 1                     |
| policy/steps              | 178844.0              |
| test/episodes             | 50.0                  |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -20.01230784291067    |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 200.0                 |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.9999999998029     |
| train_0/current_q         | -8.067475247180814    |
| train_0/fw_bonus          | -0.9996530637145042   |
| train_0/fw_loss           | 8.27771999865945e-05  |
| train_0/mu_grads          | -0.011752092000097036 |
| train_0/mu_grads_std      | 0.1747863382101059    |
| train_0/mu_loss           | 8.019413616839069     |
| train_0/next_q            | -8.006063358936562    |
| train_0/q_grads           | 0.02005948666483164   |
| train_0/q_grads_std       | 0.16219519153237344   |
| train_0/q_loss            | 0.2027461557682746    |
| train_0/reward            | -0.5736254241255665   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0107421875          |
| train_0/target_q          | -8.188012134469393    |
| train_1/avg_q             | -19.937641698516263   |
| train_1/current_q         | -2.907101024541878    |
| train_1/fw_bonus          | -0.9982322975993156   |
| train_1/fw_loss           | 0.0010653966237441637 |
| train_1/mu_grads          | 0.0014959300751797855 |
| train_1/mu_grads_std      | 0.13610598258674145   |
| train_1/mu_loss           | 0.7105352738581596    |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -0.6864350809307134   |
| train_1/q_grads           | 0.01648885039612651   |
| train_1/q_grads_std       | 0.24891799427568911   |
| train_1/q_loss            | 1.7043599953174031    |
| train_1/reward            | -2.470700658452915    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0061767578125       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -2.8931700887588954   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Training epoch 2
Time for epoch 2: 610.00. Rollout time: 411.59, Training time: 198.34
Evaluating epoch 2
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 2                     |
| policy/steps              | 269969.0              |
| test/episodes             | 75.0                  |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -26.999999999999996   |
| test_1/avg_q              | -21.191431561449594   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 300.0                 |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.999999999795104   |
| train_0/current_q         | -8.570928662294444    |
| train_0/fw_bonus          | -0.9996922716498375   |
| train_0/fw_loss           | 7.376123112408095e-05 |
| train_0/mu_grads          | -0.01622791087720543  |
| train_0/mu_grads_std      | 0.20266091376543044   |
| train_0/mu_loss           | 8.526314394273584     |
| train_0/next_q            | -8.514389474365384    |
| train_0/q_grads           | 0.02118094633333385   |
| train_0/q_grads_std       | 0.18286829330027105   |
| train_0/q_loss            | 0.18767655506196915   |
| train_0/reward            | -0.5698083245511952   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0095947265625       |
| train_0/target_q          | -8.681591133515008    |
| train_1/avg_q             | -19.946607164446224   |
| train_1/current_q         | -3.707139156882473    |
| train_1/fw_bonus          | -0.9985820755362511   |
| train_1/fw_loss           | 0.0009767573763383552 |
| train_1/mu_grads          | -0.000885230016137939 |
| train_1/mu_grads_std      | 0.14073831625282765   |
| train_1/mu_loss           | 2.5838175743656273    |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -2.6308562242344413   |
| train_1/q_grads           | 0.013375118211843073  |
| train_1/q_grads_std       | 0.29124796465039254   |
| train_1/q_loss            | 3.22057086845358      |
| train_1/reward            | -2.450902844682787    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.005078125           |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -3.7043254681131215   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Training epoch 3
Time for epoch 3: 6311.42. Rollout time: 2311.11, Training time: 4000.06
Evaluating epoch 3
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 3                     |
| policy/steps              | 361094.0              |
| test/episodes             | 100.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -26.999994066148044   |
| test_1/avg_q              | -19.53647612585932    |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 400.0                 |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.99999999945559    |
| train_0/current_q         | -8.758872339982604    |
| train_0/fw_bonus          | -0.9997262641787529   |
| train_0/fw_loss           | 6.594805909116985e-05 |
| train_0/mu_grads          | -0.01769791361875832  |
| train_0/mu_grads_std      | 0.21994509361684322   |
| train_0/mu_loss           | 8.657095241379556     |
| train_0/next_q            | -8.649342257314526    |
| train_0/q_grads           | 0.025402288325130938  |
| train_0/q_grads_std       | 0.2074446462094784    |
| train_0/q_loss            | 0.11299684833969378   |
| train_0/reward            | -0.5722338133542507   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0210205078125       |
| train_0/target_q          | -8.88789941104028     |
| train_1/avg_q             | -20.415731097247367   |
| train_1/current_q         | -11.580867326773111   |
| train_1/fw_bonus          | -0.9983043149113655   |
| train_1/fw_loss           | 0.0010471507630427368 |
| train_1/mu_grads          | -0.003976675379090011 |
| train_1/mu_grads_std      | 0.15133190490305423   |
| train_1/mu_loss           | 14.95045582745792     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -14.88726739634078    |
| train_1/q_grads           | 0.011664940137416124  |
| train_1/q_grads_std       | 0.3336035326123238    |
| train_1/q_loss            | 18.55912660270838     |
| train_1/reward            | -2.4688143597406453   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0053466796875       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -11.35514429928897    |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 4
Time for epoch 4: 7826.57. Rollout time: 7438.52, Training time: 387.84
Evaluating epoch 4
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 4                     |
| policy/steps              | 452219.0              |
| test/episodes             | 125.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -25.59138097352987    |
| test_1/avg_q              | -20.0019749696441     |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 500.0                 |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -24.37492528380397    |
| train_0/current_q         | -8.83974223011641     |
| train_0/fw_bonus          | -0.9997178599238395   |
| train_0/fw_loss           | 6.788238124499912e-05 |
| train_0/mu_grads          | -0.01748540378175676  |
| train_0/mu_grads_std      | 0.23505820371210576   |
| train_0/mu_loss           | 8.800880729578685     |
| train_0/next_q            | -8.7871617958487      |
| train_0/q_grads           | 0.021841735113412142  |
| train_0/q_grads_std       | 0.2219188552349806    |
| train_0/q_loss            | 0.18045429033448918   |
| train_0/reward            | -0.573147237291414    |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.020947265625        |
| train_0/target_q          | -8.963520363297249    |
| train_1/avg_q             | -20.12937243549535    |
| train_1/current_q         | -3.4915277759891965   |
| train_1/fw_bonus          | -0.9982065826654434   |
| train_1/fw_loss           | 0.0010719183119363151 |
| train_1/mu_grads          | -0.008199787209741771 |
| train_1/mu_grads_std      | 0.15266207829117776   |
| train_1/mu_loss           | 1.7838274584084217    |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -1.5942840542969712   |
| train_1/q_grads           | 0.010692764911800623  |
| train_1/q_grads_std       | 0.3581473045051098    |
| train_1/q_loss            | 1.6897435836305497    |
| train_1/reward            | -2.4859652184186416   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0058349609375       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -3.5654152898759697   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 5
Time for epoch 5: 710.05. Rollout time: 479.67, Training time: 230.29
Evaluating epoch 5
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-----------------------------------------------------
| epoch                     | 5                     |
| policy/steps              | 543344.0              |
| test/episodes             | 150.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -26.955301814994343   |
| test_1/avg_q              | -20.538515923067212   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 600.0                 |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -23.74063496579654    |
| train_0/current_q         | -9.296293262606074    |
| train_0/fw_bonus          | -0.9997772753238678   |
| train_0/fw_loss           | 5.422323074526503e-05 |
| train_0/mu_grads          | -0.01819579303264618  |
| train_0/mu_grads_std      | 0.2391826633363962    |
| train_0/mu_loss           | 9.274983741805862     |
| train_0/next_q            | -9.273474521611792    |
| train_0/q_grads           | 0.01239177044481039   |
| train_0/q_grads_std       | 0.23823820054531097   |
| train_0/q_loss            | 0.18699058769931212   |
| train_0/reward            | -0.5709465150051983   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0376953125          |
| train_0/target_q          | -9.43411351259412     |
| train_1/avg_q             | -20.378050536336275   |
| train_1/current_q         | -3.9307955759596522   |
| train_1/fw_bonus          | -0.9987926736474038   |
| train_1/fw_loss           | 0.0009233884396962821 |
| train_1/mu_grads          | -0.008768015610985457 |
| train_1/mu_grads_std      | 0.15696993246674537   |
| train_1/mu_loss           | 2.0544197902322585    |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -1.9608254681062796   |
| train_1/q_grads           | 0.012097682524472474  |
| train_1/q_grads_std       | 0.35905388444662095   |
| train_1/q_loss            | 0.3578208030625207    |
| train_1/reward            | -2.5311923088360344   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0052490234375       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -3.877091064165561    |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 6
Time for epoch 6: 735.03. Rollout time: 499.98, Training time: 234.95
Evaluating epoch 6
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
------------------------------------------------------
| epoch                     | 6                      |
| policy/steps              | 634469.0               |
| test/episodes             | 175.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -26.99999910941051     |
| test_1/avg_q              | -20.368504796172072    |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 700.0                  |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -26.97564813920636     |
| train_0/current_q         | -9.434324227824163     |
| train_0/fw_bonus          | -0.9998023703694343    |
| train_0/fw_loss           | 4.8451190878040507e-05 |
| train_0/mu_grads          | -0.019639439601451158  |
| train_0/mu_grads_std      | 0.2393685195595026     |
| train_0/mu_loss           | 9.427152232332128      |
| train_0/next_q            | -9.408955984565909     |
| train_0/q_grads           | 0.013595797005109489   |
| train_0/q_grads_std       | 0.24500945396721363    |
| train_0/q_loss            | 0.1689211770116565     |
| train_0/reward            | -0.5713739346436342    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.0720703125           |
| train_0/target_q          | -9.570179215045455     |
| train_1/avg_q             | -20.489641774851588    |
| train_1/current_q         | -3.4359587793409645    |
| train_1/fw_bonus          | -0.9991101965308189    |
| train_1/fw_loss           | 0.000842918957641814   |
| train_1/mu_grads          | -0.007709952304139733  |
| train_1/mu_grads_std      | 0.1597479648888111     |
| train_1/mu_loss           | 1.3998125638650651     |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -1.3731837278028665    |
| train_1/q_grads           | 0.009668051823973655   |
| train_1/q_grads_std       | 0.3678056441247463     |
| train_1/q_loss            | 0.5752721683804193     |
| train_1/reward            | -2.5453341900327358    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.00498046875          |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -3.419638832972031     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 7
Time for epoch 7: 872.91. Rollout time: 628.37, Training time: 244.45
Evaluating epoch 7
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
------------------------------------------------------
| epoch                     | 7                      |
| policy/steps              | 725594.0               |
| test/episodes             | 200.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -27.0                  |
| test_1/avg_q              | -20.475986873711292    |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 800.0                  |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -26.598167550375333    |
| train_0/current_q         | -9.234467969797725     |
| train_0/fw_bonus          | -0.9998327165842056    |
| train_0/fw_loss           | 4.147726058363332e-05  |
| train_0/mu_grads          | -0.01951423888094723   |
| train_0/mu_grads_std      | 0.24828998409211636    |
| train_0/mu_loss           | 9.221351323276625      |
| train_0/next_q            | -9.216308841965892     |
| train_0/q_grads           | 0.015048970747739077   |
| train_0/q_grads_std       | 0.2517416477203369     |
| train_0/q_loss            | 0.18885550376299345    |
| train_0/reward            | -0.5724252198113391    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.080712890625         |
| train_0/target_q          | -9.449505943990035     |
| train_1/avg_q             | -20.487272132107837    |
| train_1/current_q         | -2.692304420142816     |
| train_1/fw_bonus          | -0.9992709770798683    |
| train_1/fw_loss           | 0.000802175646822434   |
| train_1/mu_grads          | -0.0075549947330728175 |
| train_1/mu_grads_std      | 0.16670785285532475    |
| train_1/mu_loss           | 0.29673047662200336    |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -0.2981134609561814    |
| train_1/q_grads           | 0.007745531690306962   |
| train_1/q_grads_std       | 0.3763177700340748     |
| train_1/q_loss            | 0.1765260565260543     |
| train_1/reward            | -2.5269722852837733    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0043212890625        |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -2.727406836320364     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 8
Time for epoch 8: 940.78. Rollout time: 656.90, Training time: 283.77
Evaluating epoch 8
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
------------------------------------------------------
| epoch                     | 8                      |
| policy/steps              | 816719.0               |
| test/episodes             | 225.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -27.0                  |
| test_1/avg_q              | -20.571437330944473    |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 900.0                  |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -27.0                  |
| train_0/current_q         | -9.245940953235728     |
| train_0/fw_bonus          | -0.9998573675751686    |
| train_0/fw_loss           | 3.5808083885058296e-05 |
| train_0/mu_grads          | -0.02040299107320607   |
| train_0/mu_grads_std      | 0.2522869534790516     |
| train_0/mu_loss           | 9.227679238350529      |
| train_0/next_q            | -9.226437116992534     |
| train_0/q_grads           | 0.016582789504900573   |
| train_0/q_grads_std       | 0.2611688241362572     |
| train_0/q_loss            | 0.17246588860795561    |
| train_0/reward            | -0.5663339728038409    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.1129638671875        |
| train_0/target_q          | -9.392387969437749     |
| train_1/avg_q             | -20.461817505605943    |
| train_1/current_q         | -2.5238137628595103    |
| train_1/fw_bonus          | -0.9994548350572586    |
| train_1/fw_loss           | 0.000755580207624007   |
| train_1/mu_grads          | -0.005093243042938411  |
| train_1/mu_grads_std      | 0.17037379667162894    |
| train_1/mu_loss           | 0.009472631061976423   |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -0.009758873139888088  |
| train_1/q_grads           | 0.007381536532193422   |
| train_1/q_grads_std       | 0.37808996960520747    |
| train_1/q_loss            | 0.07488532587616432    |
| train_1/reward            | -2.5187418812009126    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.004931640625         |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -2.5241472771607767    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 9
Time for epoch 9: 888.75. Rollout time: 639.93, Training time: 248.69
Evaluating epoch 9
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
------------------------------------------------------
| epoch                     | 9                      |
| policy/steps              | 907844.0               |
| test/episodes             | 250.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -27.0                  |
| test_1/avg_q              | -20.564709574907766    |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 1000.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -26.78431030764308     |
| train_0/current_q         | -6.201890134992267     |
| train_0/fw_bonus          | -0.9998564094305038    |
| train_0/fw_loss           | 3.6032377147421356e-05 |
| train_0/mu_grads          | -0.02209832933731377   |
| train_0/mu_grads_std      | 0.257919330149889      |
| train_0/mu_loss           | 6.066754867279305      |
| train_0/next_q            | -6.051247267661367     |
| train_0/q_grads           | 0.013512942846864462   |
| train_0/q_grads_std       | 0.2645573318004608     |
| train_0/q_loss            | 0.18370105200264342    |
| train_0/reward            | -0.5651984749616531    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.129345703125         |
| train_0/target_q          | -6.230154292074937     |
| train_1/avg_q             | -20.49395922425617     |
| train_1/current_q         | -2.4893827971664195    |
| train_1/fw_bonus          | -0.9994039222598076    |
| train_1/fw_loss           | 0.0007684837037231773  |
| train_1/mu_grads          | -0.0023086102795787156 |
| train_1/mu_grads_std      | 0.17401203252375125    |
| train_1/mu_loss           | 0.0007344864172819078  |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -0.0007293201353975062 |
| train_1/q_grads           | 0.006367423036135733   |
| train_1/q_grads_std       | 0.37870521247386935    |
| train_1/q_loss            | 0.061797141922048146   |
| train_1/reward            | -2.4850194373608248    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.004052734375         |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -2.4853656509147344    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 10
Time for epoch 10: 1186.28. Rollout time: 846.37, Training time: 339.76
Evaluating epoch 10
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-------------------------------------------------------
| epoch                     | 10                      |
| policy/steps              | 998969.0                |
| test/episodes             | 275.0                   |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -27.0                   |
| test_1/avg_q              | -20.534412680822175     |
| test_1/n_subgoals         | 675.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 1100.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -27.0                   |
| train_0/current_q         | -9.223937544151005      |
| train_0/fw_bonus          | -0.9998649895191193     |
| train_0/fw_loss           | 3.4056981439789526e-05  |
| train_0/mu_grads          | -0.02008031290024519    |
| train_0/mu_grads_std      | 0.26489405855536463     |
| train_0/mu_loss           | 9.171846623793387       |
| train_0/next_q            | -9.172413925201406      |
| train_0/q_grads           | 0.014615021809004248    |
| train_0/q_grads_std       | 0.27206700667738914     |
| train_0/q_loss            | 0.14189150888092886     |
| train_0/reward            | -0.5702360236908135     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.1737060546875         |
| train_0/target_q          | -9.367730392536691      |
| train_1/avg_q             | -20.48431788541499      |
| train_1/current_q         | -2.4592599697259145     |
| train_1/fw_bonus          | -0.9993958532810211     |
| train_1/fw_loss           | 0.0007705291282036342   |
| train_1/mu_grads          | -0.0010173888556892052  |
| train_1/mu_grads_std      | 0.17666730284690857     |
| train_1/mu_loss           | 1.0290161710339199e-05  |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -1.0408657697400054e-05 |
| train_1/q_grads           | 0.0045650128624401985   |
| train_1/q_grads_std       | 0.38301508575677873     |
| train_1/q_loss            | 0.0403791584981068      |
| train_1/reward            | -2.4498245475784644     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.0041015625            |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -2.4498301572146213     |
-------------------------------------------------------
Saving periodic policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_10.pkl ...
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 11
Time for epoch 11: 1542.63. Rollout time: 1115.18, Training time: 427.31
Evaluating epoch 11
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-------------------------------------------------------
| epoch                     | 11                      |
| policy/steps              | 1090094.0               |
| test/episodes             | 300.0                   |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -27.0                   |
| test_1/avg_q              | -20.509934920127808     |
| test_1/n_subgoals         | 675.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 1200.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -27.0                   |
| train_0/current_q         | -9.27962288949138       |
| train_0/fw_bonus          | -0.9998297572135926     |
| train_0/fw_loss           | 4.215709927848366e-05   |
| train_0/mu_grads          | -0.02075932640582323    |
| train_0/mu_grads_std      | 0.2676809996366501      |
| train_0/mu_loss           | 9.260902830463014       |
| train_0/next_q            | -9.26416581475045       |
| train_0/q_grads           | 0.014566845470108091    |
| train_0/q_grads_std       | 0.2757981508970261      |
| train_0/q_loss            | 0.17501493116174868     |
| train_0/reward            | -0.577752849657918      |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.1438720703125         |
| train_0/target_q          | -9.454374855480074      |
| train_1/avg_q             | -20.488458045529946     |
| train_1/current_q         | -2.5512918166466334     |
| train_1/fw_bonus          | -0.9991500437259674     |
| train_1/fw_loss           | 0.0008328218376846052   |
| train_1/mu_grads          | -0.0008511632899171673  |
| train_1/mu_grads_std      | 0.1776564735919237      |
| train_1/mu_loss           | 1.5321115551684247e-06  |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -1.6000665143226757e-06 |
| train_1/q_grads           | 0.0017057294986443593   |
| train_1/q_grads_std       | 0.39230813533067704     |
| train_1/q_loss            | 0.03279459767954574     |
| train_1/reward            | -2.5471568964236213     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.004638671875          |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -2.5471575844818624     |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 12
Time for epoch 12: 1321.10. Rollout time: 994.06, Training time: 326.86
Evaluating epoch 12
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-------------------------------------------------------
| epoch                     | 12                      |
| policy/steps              | 1181219.0               |
| test/episodes             | 325.0                   |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -27.0                   |
| test_1/avg_q              | -20.491892408104434     |
| test_1/n_subgoals         | 675.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 1300.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -27.0                   |
| train_0/current_q         | -9.25753718947853       |
| train_0/fw_bonus          | -0.9998338669538498     |
| train_0/fw_loss           | 4.1211626285075906e-05  |
| train_0/mu_grads          | -0.021347248693928123   |
| train_0/mu_grads_std      | 0.27210367247462275     |
| train_0/mu_loss           | 9.208160774097133       |
| train_0/next_q            | -9.208476760371994      |
| train_0/q_grads           | 0.01512138822581619     |
| train_0/q_grads_std       | 0.27579892948269846     |
| train_0/q_loss            | 0.1522166816566879      |
| train_0/reward            | -0.5802807333562668     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.079931640625          |
| train_0/target_q          | -9.40666024664517       |
| train_1/avg_q             | -20.472207589017692     |
| train_1/current_q         | -2.4969398108947876     |
| train_1/fw_bonus          | -0.9989416480064393     |
| train_1/fw_loss           | 0.000885634683072567    |
| train_1/mu_grads          | -0.00027013221115339546 |
| train_1/mu_grads_std      | 0.18264530189335346     |
| train_1/mu_loss           | 1.4570419022873204e-05  |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -1.0822276953279228e-05 |
| train_1/q_grads           | -0.002125984925078228   |
| train_1/q_grads_std       | 0.4023600906133652      |
| train_1/q_loss            | 0.026910179358212178    |
| train_1/reward            | -2.492722471925299      |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.0045654296875         |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -2.492726675612821      |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 13
Time for epoch 13: 3344.31. Rollout time: 2935.59, Training time: 408.49
Evaluating epoch 13
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-------------------------------------------------------
| epoch                     | 13                      |
| policy/steps              | 1272344.0               |
| test/episodes             | 350.0                   |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -27.0                   |
| test_1/avg_q              | -20.579896226027564     |
| test_1/n_subgoals         | 675.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 1400.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -27.0                   |
| train_0/current_q         | -9.401079898795695      |
| train_0/fw_bonus          | -0.9998250097036362     |
| train_0/fw_loss           | 4.324706833358505e-05   |
| train_0/mu_grads          | -0.02229289971292019    |
| train_0/mu_grads_std      | 0.2750937379896641      |
| train_0/mu_loss           | 9.35457105176403        |
| train_0/next_q            | -9.3555897898796        |
| train_0/q_grads           | 0.01492963507771492     |
| train_0/q_grads_std       | 0.27537678852677344     |
| train_0/q_loss            | 0.1586228360795235      |
| train_0/reward            | -0.5856251995861385     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.0696533203125         |
| train_0/target_q          | -9.555269882308533      |
| train_1/avg_q             | -20.488001781780362     |
| train_1/current_q         | -2.4791260933801116     |
| train_1/fw_bonus          | -0.9988564774394035     |
| train_1/fw_loss           | 0.0009072194341570138   |
| train_1/mu_grads          | -0.00018366554322710727 |
| train_1/mu_grads_std      | 0.18465656489133836     |
| train_1/mu_loss           | 1.3150615542257285e-07  |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -8.366124036451557e-08  |
| train_1/q_grads           | -0.002966859861044213   |
| train_1/q_grads_std       | 0.40916797146201134     |
| train_1/q_loss            | 0.02385519657125424     |
| train_1/reward            | -2.475287847654181      |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.003857421875          |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -2.4752878816343227     |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 14
Time for epoch 14: 1369.35. Rollout time: 1033.16, Training time: 335.96
Evaluating epoch 14
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-------------------------------------------------------
| epoch                     | 14                      |
| policy/steps              | 1363469.0               |
| test/episodes             | 375.0                   |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -27.0                   |
| test_1/avg_q              | -20.54271666520943      |
| test_1/n_subgoals         | 675.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 1500.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -27.0                   |
| train_0/current_q         | -9.431764589403874      |
| train_0/fw_bonus          | -0.9998293802142143     |
| train_0/fw_loss           | 4.224234689900186e-05   |
| train_0/mu_grads          | -0.026706713996827603   |
| train_0/mu_grads_std      | 0.2743811376392841      |
| train_0/mu_loss           | 9.380046110292849       |
| train_0/next_q            | -9.372507023379352      |
| train_0/q_grads           | 0.015491982595995069    |
| train_0/q_grads_std       | 0.27866519838571546     |
| train_0/q_loss            | 0.15764407461076324     |
| train_0/reward            | -0.5871460675996787     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.054150390625          |
| train_0/target_q          | -9.57431233364004       |
| train_1/avg_q             | -20.524364272319186     |
| train_1/current_q         | -2.4660894830511424     |
| train_1/fw_bonus          | -0.9993519961833954     |
| train_1/fw_loss           | 0.0007816427896614186   |
| train_1/mu_grads          | -0.00011686937741615112 |
| train_1/mu_grads_std      | 0.18516531586647034     |
| train_1/mu_loss           | 3.054213557954912e-11   |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -1.2345086914807047e-11 |
| train_1/q_grads           | -0.004883037065155804   |
| train_1/q_grads_std       | 0.41617835462093355     |
| train_1/q_loss            | 0.018185431489945104    |
| train_1/reward            | -2.462814072274341      |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.0043701171875         |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -2.4628140722828307     |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 15
Time for epoch 15: 1337.57. Rollout time: 992.42, Training time: 344.93
Evaluating epoch 15
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-------------------------------------------------------
| epoch                     | 15                      |
| policy/steps              | 1454594.0               |
| test/episodes             | 400.0                   |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -27.0                   |
| test_1/avg_q              | -20.42234060551757      |
| test_1/n_subgoals         | 675.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 1600.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -27.0                   |
| train_0/current_q         | -9.405740006202558      |
| train_0/fw_bonus          | -0.9998561486601829     |
| train_0/fw_loss           | 3.6090426920054594e-05  |
| train_0/mu_grads          | -0.028434044495224953   |
| train_0/mu_grads_std      | 0.285511976480484       |
| train_0/mu_loss           | 9.32483541000956        |
| train_0/next_q            | -9.31373938189516       |
| train_0/q_grads           | 0.015542596625164152    |
| train_0/q_grads_std       | 0.2841877341270447      |
| train_0/q_loss            | 0.12839212626874447     |
| train_0/reward            | -0.5840216541353584     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.10048828125           |
| train_0/target_q          | -9.544437416168781      |
| train_1/avg_q             | -20.492565750445205     |
| train_1/current_q         | -2.5138026087522904     |
| train_1/fw_bonus          | -0.99947549700737       |
| train_1/fw_loss           | 0.0007503415516111999   |
| train_1/mu_grads          | -0.00011690598803397734 |
| train_1/mu_grads_std      | 0.18516531586647034     |
| train_1/mu_loss           | 1.7556140128586984e-11  |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -1.9698293012104687e-11 |
| train_1/q_grads           | -0.0066327483742497865  |
| train_1/q_grads_std       | 0.42318532392382624     |
| train_1/q_loss            | 0.021822368221755307    |
| train_1/reward            | -2.5101410927185497     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.004296875             |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -2.5101410927274324     |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 16
Time for epoch 16: 1469.04. Rollout time: 1096.68, Training time: 372.20
Evaluating epoch 16
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
------------------------------------------------------
| epoch                     | 16                     |
| policy/steps              | 1545719.0              |
| test/episodes             | 425.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -27.0                  |
| test_1/avg_q              | -20.30055682889652     |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 1700.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -27.0                  |
| train_0/current_q         | -9.482161477564759     |
| train_0/fw_bonus          | -0.9998605057597161    |
| train_0/fw_loss           | 3.5091112385998714e-05 |
| train_0/mu_grads          | -0.030325477104634048  |
| train_0/mu_grads_std      | 0.2936503320932388     |
| train_0/mu_loss           | 9.376908158482752      |
| train_0/next_q            | -9.362217526805933     |
| train_0/q_grads           | 0.01546955038793385    |
| train_0/q_grads_std       | 0.287767605483532      |
| train_0/q_loss            | 0.1098052943792801     |
| train_0/reward            | -0.5878679555884446    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.09755859375          |
| train_0/target_q          | -9.623327295828577     |
| train_1/avg_q             | -20.46064211883256     |
| train_1/current_q         | -2.431070607456589     |
| train_1/fw_bonus          | -0.9997042879462242    |
| train_1/fw_loss           | 0.0006923625376657582  |
| train_1/mu_grads          | -0.0001481633553339634 |
| train_1/mu_grads_std      | 0.1854274518787861     |
| train_1/mu_loss           | 6.172777886869991e-10  |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -6.530410372463915e-10 |
| train_1/q_grads           | -0.0077408267767168585 |
| train_1/q_grads_std       | 0.43158604130148887    |
| train_1/q_loss            | 0.03197559344603439    |
| train_1/reward            | -2.429888232471785     |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0040283203125        |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -2.429888232773572     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 17
Time for epoch 17: 6625.40. Rollout time: 6223.37, Training time: 401.77
Evaluating epoch 17
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-------------------------------------------------------
| epoch                     | 17                      |
| policy/steps              | 1636844.0               |
| test/episodes             | 450.0                   |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -27.0                   |
| test_1/avg_q              | -20.44057459380608      |
| test_1/n_subgoals         | 675.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 1800.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -27.0                   |
| train_0/current_q         | -9.425036852464448      |
| train_0/fw_bonus          | -0.9998630002140999     |
| train_0/fw_loss           | 3.451558191045478e-05   |
| train_0/mu_grads          | -0.03169462773948908    |
| train_0/mu_grads_std      | 0.3009237140417099      |
| train_0/mu_loss           | 9.341101470098483       |
| train_0/next_q            | -9.328336202234642      |
| train_0/q_grads           | 0.014947709301486612    |
| train_0/q_grads_std       | 0.28989142179489136     |
| train_0/q_loss            | 0.11641825024102866     |
| train_0/reward            | -0.5842247826585663     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.1190185546875         |
| train_0/target_q          | -9.55524909373523       |
| train_1/avg_q             | -20.423735557731757     |
| train_1/current_q         | -2.516353256281203      |
| train_1/fw_bonus          | -0.9996962666511535     |
| train_1/fw_loss           | 0.0006943951666471548   |
| train_1/mu_grads          | -0.00015039394202176481 |
| train_1/mu_grads_std      | 0.18545151501893997     |
| train_1/mu_loss           | 3.6012500949850084e-10  |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -3.36415077877e-10      |
| train_1/q_grads           | -0.00882996350992471    |
| train_1/q_grads_std       | 0.4403393119573593      |
| train_1/q_loss            | 0.02987801857137078     |
| train_1/reward            | -2.5151401313512904     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.0045654296875         |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -2.5151401315314326     |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 18
Time for epoch 18: 1493.95. Rollout time: 1106.98, Training time: 386.75
Evaluating epoch 18
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
------------------------------------------------------
| epoch                     | 18                     |
| policy/steps              | 1727969.0              |
| test/episodes             | 475.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -27.0                  |
| test_1/avg_q              | -20.474807596626654    |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 1900.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -27.0                  |
| train_0/current_q         | -9.488842207374603     |
| train_0/fw_bonus          | -0.9998662620782852    |
| train_0/fw_loss           | 3.376785116415704e-05  |
| train_0/mu_grads          | -0.031224597711116076  |
| train_0/mu_grads_std      | 0.31058595776557923    |
| train_0/mu_loss           | 9.40011292063165       |
| train_0/next_q            | -9.391193354713774     |
| train_0/q_grads           | 0.014735310338437557   |
| train_0/q_grads_std       | 0.2939979150891304     |
| train_0/q_loss            | 0.12173836039397638    |
| train_0/reward            | -0.5864961036237218    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.120361328125         |
| train_0/target_q          | -9.621523983904577     |
| train_1/avg_q             | -20.483439232508815    |
| train_1/current_q         | -2.4487875922487072    |
| train_1/fw_bonus          | -1.000133568048477     |
| train_1/fw_loss           | 0.0005835698582814075  |
| train_1/mu_grads          | -8.799607585388002e-05 |
| train_1/mu_grads_std      | 0.18627739362418652    |
| train_1/mu_loss           | 7.79371591424777e-10   |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -7.857986633206777e-10 |
| train_1/q_grads           | -0.008618468930944801  |
| train_1/q_grads_std       | 0.4485763795673847     |
| train_1/q_loss            | 0.013844941359557989   |
| train_1/reward            | -2.447042672005773     |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.003857421875         |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -2.447042672457018     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 19
Time for epoch 19: 1573.18. Rollout time: 1141.95, Training time: 431.07
Evaluating epoch 19
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
------------------------------------------------------
| epoch                     | 19                     |
| policy/steps              | 1819094.0              |
| test/episodes             | 500.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -27.0                  |
| test_1/avg_q              | -20.433201408712083    |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 2000.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -27.0                  |
| train_0/current_q         | -9.458053557578612     |
| train_0/fw_bonus          | -0.9998722985386849    |
| train_0/fw_loss           | 3.237653827454778e-05  |
| train_0/mu_grads          | -0.03345746584236622   |
| train_0/mu_grads_std      | 0.31912925764918326    |
| train_0/mu_loss           | 9.37192840479914       |
| train_0/next_q            | -9.360452527405906     |
| train_0/q_grads           | 0.014943821262568236   |
| train_0/q_grads_std       | 0.2996682673692703     |
| train_0/q_loss            | 0.11915596354149875    |
| train_0/reward            | -0.5833298920759262    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.127001953125         |
| train_0/target_q          | -9.594757921404774     |
| train_1/avg_q             | -20.491904427725213    |
| train_1/current_q         | -2.4881185125400065    |
| train_1/fw_bonus          | -1.000221784412861     |
| train_1/fw_loss           | 0.0005612214998109266  |
| train_1/mu_grads          | 0.0011719457106664777  |
| train_1/mu_grads_std      | 0.18974927067756653    |
| train_1/mu_loss           | 6.976447156385956e-14  |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -7.255724417197692e-14 |
| train_1/q_grads           | -0.008926648972555995  |
| train_1/q_grads_std       | 0.4585133455693722     |
| train_1/q_loss            | 0.01690709189994952    |
| train_1/reward            | -2.485223898742697     |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0042724609375        |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -2.485223898742741     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 20
Time for epoch 20: 1508.54. Rollout time: 1089.54, Training time: 418.79
Evaluating epoch 20
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
------------------------------------------------------
| epoch                     | 20                     |
| policy/steps              | 1910219.0              |
| test/episodes             | 525.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -27.0                  |
| test_1/avg_q              | -20.54292240262405     |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 2100.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -27.0                  |
| train_0/current_q         | -9.476706841711259     |
| train_0/fw_bonus          | -0.9998779609799385    |
| train_0/fw_loss           | 3.107895663561067e-05  |
| train_0/mu_grads          | -0.032882350776344535  |
| train_0/mu_grads_std      | 0.3291480153799057     |
| train_0/mu_loss           | 9.397304145463156      |
| train_0/next_q            | -9.385475446720081     |
| train_0/q_grads           | 0.014719850942492485   |
| train_0/q_grads_std       | 0.30291105434298515    |
| train_0/q_loss            | 0.12144996121956095    |
| train_0/reward            | -0.5856018110174773    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.1342529296875        |
| train_0/target_q          | -9.615228043676705     |
| train_1/avg_q             | -20.48970748981631     |
| train_1/current_q         | -2.465773572321985     |
| train_1/fw_bonus          | -1.0003201618790627    |
| train_1/fw_loss           | 0.0005362853560654912  |
| train_1/mu_grads          | 0.0011719458270817995  |
| train_1/mu_grads_std      | 0.18974927067756653    |
| train_1/mu_loss           | 4.644336728765757e-15  |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -4.913567689759655e-15 |
| train_1/q_grads           | -0.010027613630518318  |
| train_1/q_grads_std       | 0.46893446072936057    |
| train_1/q_loss            | 0.025355409162250125   |
| train_1/reward            | -2.4632026002280325    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.003271484375         |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -2.4632026002280356    |
------------------------------------------------------
Saving periodic policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_20.pkl ...
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 21
Time for epoch 21: 2271.74. Rollout time: 919.67, Training time: 1351.93
Evaluating epoch 21
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
------------------------------------------------------
| epoch                     | 21                     |
| policy/steps              | 2001344.0              |
| test/episodes             | 550.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -27.0                  |
| test_1/avg_q              | -20.552123191519524    |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 2200.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -27.0                  |
| train_0/current_q         | -9.352282172801363     |
| train_0/fw_bonus          | -0.999868854880333     |
| train_0/fw_loss           | 3.317057785352517e-05  |
| train_0/mu_grads          | -0.034202569723129274  |
| train_0/mu_grads_std      | 0.3336999773979187     |
| train_0/mu_loss           | 9.263155211689831      |
| train_0/next_q            | -9.250699944040555     |
| train_0/q_grads           | 0.014870740193873643   |
| train_0/q_grads_std       | 0.3042453058063984     |
| train_0/q_loss            | 0.12186149997236433    |
| train_0/reward            | -0.5872108646075503    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.1190185546875        |
| train_0/target_q          | -9.49221641827655      |
| train_1/avg_q             | -20.49509085955635     |
| train_1/current_q         | -2.554879579887129     |
| train_1/fw_bonus          | -1.0004562884569168    |
| train_1/fw_loss           | 0.0005017842617235146  |
| train_1/mu_grads          | 0.0011719458270817995  |
| train_1/mu_grads_std      | 0.18974927067756653    |
| train_1/mu_loss           | 4.365499718040874e-18  |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -4.338408440350092e-18 |
| train_1/q_grads           | -0.011494337790645659  |
| train_1/q_grads_std       | 0.4773195683956146     |
| train_1/q_loss            | 0.02382786686254636    |
| train_1/reward            | -2.554825990448444     |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0042236328125        |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -2.554825990448444     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 22
Time for epoch 22: 1449.39. Rollout time: 1057.06, Training time: 392.15
Evaluating epoch 22
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
------------------------------------------------------
| epoch                     | 22                     |
| policy/steps              | 2092469.0              |
| test/episodes             | 575.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -27.0                  |
| test_1/avg_q              | -20.565125959025785    |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 2300.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -27.0                  |
| train_0/current_q         | -9.378384885468972     |
| train_0/fw_bonus          | -0.9998765736818314    |
| train_0/fw_loss           | 3.139253299195843e-05  |
| train_0/mu_grads          | -0.03663031924515962   |
| train_0/mu_grads_std      | 0.34009755998849867    |
| train_0/mu_loss           | 9.285464793093517      |
| train_0/next_q            | -9.274006599927835     |
| train_0/q_grads           | 0.014515604102052748   |
| train_0/q_grads_std       | 0.30674514546990395    |
| train_0/q_loss            | 0.11976829455570978    |
| train_0/reward            | -0.5864071339252405    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.1408935546875        |
| train_0/target_q          | -9.514441972514133     |
| train_1/avg_q             | -20.53730933723329     |
| train_1/current_q         | -2.4924355889586383    |
| train_1/fw_bonus          | -1.0005707174539566    |
| train_1/fw_loss           | 0.00047278926067519935 |
| train_1/mu_grads          | 0.0011733605293557047  |
| train_1/mu_grads_std      | 0.18976371884346008    |
| train_1/mu_loss           | 2.175081889240467e-12  |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -2.201876128292301e-12 |
| train_1/q_grads           | -0.01265398000832647   |
| train_1/q_grads_std       | 0.48570539653301237    |
| train_1/q_loss            | 0.01937630255244111    |
| train_1/reward            | -2.490254984139392     |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0039794921875        |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -2.490254984140672     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 23
Time for epoch 23: 1893.00. Rollout time: 1334.87, Training time: 557.91
Evaluating epoch 23
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-------------------------------------------------------
| epoch                     | 23                      |
| policy/steps              | 2183594.0               |
| test/episodes             | 600.0                   |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -27.0                   |
| test_1/avg_q              | -20.473903055911645     |
| test_1/n_subgoals         | 675.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 2400.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -27.0                   |
| train_0/current_q         | -9.437823179205122      |
| train_0/fw_bonus          | -0.999869191646576      |
| train_0/fw_loss           | 3.309330468255211e-05   |
| train_0/mu_grads          | -0.03903403524309397    |
| train_0/mu_grads_std      | 0.346725445240736       |
| train_0/mu_loss           | 9.347399346488363       |
| train_0/next_q            | -9.334061463743746      |
| train_0/q_grads           | 0.0145359544781968      |
| train_0/q_grads_std       | 0.30891913920640945     |
| train_0/q_loss            | 0.11912061582341091     |
| train_0/reward            | -0.5882363392112893     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.14189453125           |
| train_0/target_q          | -9.583647040787625      |
| train_1/avg_q             | -20.524219918151243     |
| train_1/current_q         | -2.5397388145898376     |
| train_1/fw_bonus          | -1.0004619017243386     |
| train_1/fw_loss           | 0.0005003672486054711   |
| train_1/mu_grads          | 0.0011754792649298906   |
| train_1/mu_grads_std      | 0.18978556990623474     |
| train_1/mu_loss           | 2.61945909259497e-21    |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -1.9121952284774777e-21 |
| train_1/q_grads           | -0.01351592862047255    |
| train_1/q_grads_std       | 0.49608089253306387     |
| train_1/q_loss            | 0.01578080586099674     |
| train_1/reward            | -2.5392268026531384     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.003564453125          |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -2.5392268026531384     |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 24
Time for epoch 24: 2264.89. Rollout time: 1502.84, Training time: 761.17
Evaluating epoch 24
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
------------------------------------------------------
| epoch                     | 24                     |
| policy/steps              | 2274719.0              |
| test/episodes             | 625.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -27.0                  |
| test_1/avg_q              | -20.3608143900333      |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 2500.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -27.0                  |
| train_0/current_q         | -9.361375264124693     |
| train_0/fw_bonus          | -0.9998692229390145    |
| train_0/fw_loss           | 3.3081624042097244e-05 |
| train_0/mu_grads          | -0.03939597299322486   |
| train_0/mu_grads_std      | 0.35469986125826836    |
| train_0/mu_loss           | 9.258415459563022      |
| train_0/next_q            | -9.249823097404066     |
| train_0/q_grads           | 0.014303441625088453   |
| train_0/q_grads_std       | 0.3094128049910069     |
| train_0/q_loss            | 0.12163979006348705    |
| train_0/reward            | -0.5876870354008134    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.12294921875          |
| train_0/target_q          | -9.503099732555148     |
| train_1/avg_q             | -20.470359599404205    |
| train_1/current_q         | -2.4479254952910723    |
| train_1/fw_bonus          | -1.0007918179035187    |
| train_1/fw_loss           | 0.00041675568427308464 |
| train_1/mu_grads          | 0.0011754792649298906  |
| train_1/mu_grads_std      | 0.18978556990623474    |
| train_1/mu_loss           | 3.632747038105896e-20  |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -4.155730160553977e-20 |
| train_1/q_grads           | -0.015181988151744008  |
| train_1/q_grads_std       | 0.5033161163330078     |
| train_1/q_loss            | 0.014256401961583587   |
| train_1/reward            | -2.447005818937396     |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.003466796875         |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -2.447005818937396     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 25
Time for epoch 25: 2245.68. Rollout time: 1534.07, Training time: 710.93
Evaluating epoch 25
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
------------------------------------------------------
| epoch                     | 25                     |
| policy/steps              | 2365844.0              |
| test/episodes             | 650.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -27.0                  |
| test_1/avg_q              | -20.4899122003111      |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 2600.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -27.0                  |
| train_0/current_q         | -9.487944860536732     |
| train_0/fw_bonus          | -0.9998948454856873    |
| train_0/fw_loss           | 2.7192568359168946e-05 |
| train_0/mu_grads          | -0.04095227848738432   |
| train_0/mu_grads_std      | 0.36290112510323524    |
| train_0/mu_loss           | 9.384946996130449      |
| train_0/next_q            | -9.37542367215028      |
| train_0/q_grads           | 0.013937744288705289   |
| train_0/q_grads_std       | 0.3096568189561367     |
| train_0/q_loss            | 0.11073212736552311    |
| train_0/reward            | -0.5888286936024087    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.1787109375           |
| train_0/target_q          | -9.635600940462712     |
| train_1/avg_q             | -20.429244066019823    |
| train_1/current_q         | -2.549733188469166     |
| train_1/fw_bonus          | -1.0007211208343505    |
| train_1/fw_loss           | 0.000434674014832126   |
| train_1/mu_grads          | 0.0011754792649298906  |
| train_1/mu_grads_std      | 0.18978556990623474    |
| train_1/mu_loss           | 5.5127486910924e-24    |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -4.663772545672834e-24 |
| train_1/q_grads           | -0.016125207766890526  |
| train_1/q_grads_std       | 0.5087616845965386     |
| train_1/q_loss            | 0.008421963476006687   |
| train_1/reward            | -2.547897564117375     |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0027587890625        |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -2.547897564117375     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 26
Time for epoch 26: 1592.44. Rollout time: 1151.51, Training time: 440.60
Evaluating epoch 26
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-------------------------------------------------------
| epoch                     | 26                      |
| policy/steps              | 2456969.0               |
| test/episodes             | 675.0                   |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -27.0                   |
| test_1/avg_q              | -20.64399456400993      |
| test_1/n_subgoals         | 675.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 2700.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -27.0                   |
| train_0/current_q         | -9.4291825924312        |
| train_0/fw_bonus          | -0.9999038308858872     |
| train_0/fw_loss           | 2.5127751177933534e-05  |
| train_0/mu_grads          | -0.042495331261307      |
| train_0/mu_grads_std      | 0.368120451271534       |
| train_0/mu_loss           | 9.349701307557428       |
| train_0/next_q            | -9.339750480504085      |
| train_0/q_grads           | 0.013535162969492375    |
| train_0/q_grads_std       | 0.30963820666074754     |
| train_0/q_loss            | 0.12098475294526442     |
| train_0/reward            | -0.5826419741988502     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.20869140625           |
| train_0/target_q          | -9.563065724478628      |
| train_1/avg_q             | -20.497941782162272     |
| train_1/current_q         | -2.4684573795200384     |
| train_1/fw_bonus          | -1.000706773996353      |
| train_1/fw_loss           | 0.00043831297516589985  |
| train_1/mu_grads          | 0.0011754792649298906   |
| train_1/mu_grads_std      | 0.18978556990623474     |
| train_1/mu_loss           | 5.049065666442606e-17   |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -1.1665396851943278e-16 |
| train_1/q_grads           | -0.01661968594416976    |
| train_1/q_grads_std       | 0.5138649746775628      |
| train_1/q_loss            | 0.04076407611509632     |
| train_1/reward            | -2.470623466559482      |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.00419921875           |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -2.470623466559482      |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 27
Time for epoch 27: 1424.39. Rollout time: 1015.37, Training time: 408.74
Evaluating epoch 27
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
------------------------------------------------------
| epoch                     | 27                     |
| policy/steps              | 2548094.0              |
| test/episodes             | 700.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -27.0                  |
| test_1/avg_q              | -20.4921755475925      |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 2800.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -27.0                  |
| train_0/current_q         | -9.48801392400588      |
| train_0/fw_bonus          | -0.9998999014496803    |
| train_0/fw_loss           | 2.6029326772913918e-05 |
| train_0/mu_grads          | -0.04403064241632819   |
| train_0/mu_grads_std      | 0.3735900163650513     |
| train_0/mu_loss           | 9.394421022050338      |
| train_0/next_q            | -9.387487574896676     |
| train_0/q_grads           | 0.013647583429701626   |
| train_0/q_grads_std       | 0.3105490170419216     |
| train_0/q_loss            | 0.11376134096554011    |
| train_0/reward            | -0.5817803979436575    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.199658203125         |
| train_0/target_q          | -9.63220608282897      |
| train_1/avg_q             | -20.522571779197918    |
| train_1/current_q         | -2.473707839826529     |
| train_1/fw_bonus          | -1.0008421748876573    |
| train_1/fw_loss           | 0.00040399418576271274 |
| train_1/mu_grads          | 0.001229259735555388   |
| train_1/mu_grads_std      | 0.18985793702304363    |
| train_1/mu_loss           | 7.856825092582222e-13  |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -7.421303906580359e-13 |
| train_1/q_grads           | -0.017125745443627237  |
| train_1/q_grads_std       | 0.5196564435958863     |
| train_1/q_loss            | 0.0087768870869742     |
| train_1/reward            | -2.4723235816374656    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.003271484375         |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -2.472323581637794     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 28
Time for epoch 28: 1441.95. Rollout time: 1014.72, Training time: 426.97
Evaluating epoch 28
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
------------------------------------------------------
| epoch                     | 28                     |
| policy/steps              | 2639219.0              |
| test/episodes             | 725.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -27.0                  |
| test_1/avg_q              | -20.52395750114946     |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 2900.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -27.0                  |
| train_0/current_q         | -9.373856572049501     |
| train_0/fw_bonus          | -0.9999203354120254    |
| train_0/fw_loss           | 2.1336654458536942e-05 |
| train_0/mu_grads          | -0.046207085531204936  |
| train_0/mu_grads_std      | 0.37974114790558816    |
| train_0/mu_loss           | 9.28181747352075       |
| train_0/next_q            | -9.275571280811459     |
| train_0/q_grads           | 0.013186730304732918   |
| train_0/q_grads_std       | 0.3101856462657452     |
| train_0/q_loss            | 0.11380797131959661    |
| train_0/reward            | -0.5751937043161888    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.24677734375          |
| train_0/target_q          | -9.51709951878848      |
| train_1/avg_q             | -20.490266613568263    |
| train_1/current_q         | -2.500900382967566     |
| train_1/fw_bonus          | -1.0007983952760697    |
| train_1/fw_loss           | 0.0004150920838583261  |
| train_1/mu_grads          | 0.0013868415262550116  |
| train_1/mu_grads_std      | 0.19012288749217987    |
| train_1/mu_loss           | 6.69638604170359e-17   |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -5.679815750372188e-17 |
| train_1/q_grads           | -0.01845546583645046   |
| train_1/q_grads_std       | 0.5242113620042801     |
| train_1/q_loss            | 0.050914425464098675   |
| train_1/reward            | -2.5052716184691235    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0033203125           |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -2.5052716184691235    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 29
Time for epoch 29: 1525.18. Rollout time: 1008.05, Training time: 516.87
Evaluating epoch 29
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
------------------------------------------------------
| epoch                     | 29                     |
| policy/steps              | 2730344.0              |
| test/episodes             | 750.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -27.0                  |
| test_1/avg_q              | -20.4587422664881      |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 3000.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -27.0                  |
| train_0/current_q         | -9.330684137624436     |
| train_0/fw_bonus          | -0.9999344542622566    |
| train_0/fw_loss           | 1.8088339606947558e-05 |
| train_0/mu_grads          | -0.04809790337458253   |
| train_0/mu_grads_std      | 0.3853815749287605     |
| train_0/mu_loss           | 9.235459002856237      |
| train_0/next_q            | -9.231551855664069     |
| train_0/q_grads           | 0.012903717276640237   |
| train_0/q_grads_std       | 0.3089544244110584     |
| train_0/q_loss            | 0.11257043171613168    |
| train_0/reward            | -0.5712970903452514    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.2918701171875        |
| train_0/target_q          | -9.471449357655647     |
| train_1/avg_q             | -20.489683491495203    |
| train_1/current_q         | -2.500113691734988     |
| train_1/fw_bonus          | -1.0006847262382508    |
| train_1/fw_loss           | 0.00044389793838490733 |
| train_1/mu_grads          | 0.0013868436217308044  |
| train_1/mu_grads_std      | 0.19012288749217987    |
| train_1/mu_loss           | 2.993752417617232e-28  |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -2.810017201434973e-28 |
| train_1/q_grads           | -0.020437098620459438  |
| train_1/q_grads_std       | 0.5280161365866661     |
| train_1/q_loss            | 0.019785769680386527   |
| train_1/reward            | -2.499713416457598     |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0032470703125        |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -2.499713416457598     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 30
Time for epoch 30: 1617.44. Rollout time: 1031.02, Training time: 586.10
Evaluating epoch 30
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-------------------------------------------------------
| epoch                     | 30                      |
| policy/steps              | 2821469.0               |
| test/episodes             | 775.0                   |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -27.0                   |
| test_1/avg_q              | -20.488584174717168     |
| test_1/n_subgoals         | 675.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 3100.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -27.0                   |
| train_0/current_q         | -9.47867009617153       |
| train_0/fw_bonus          | -0.9999355182051659     |
| train_0/fw_loss           | 1.7846031187218615e-05  |
| train_0/mu_grads          | -0.04908044645562768    |
| train_0/mu_grads_std      | 0.3904053449630737      |
| train_0/mu_loss           | 9.378921760773597       |
| train_0/next_q            | -9.374139688267515      |
| train_0/q_grads           | 0.012510347925126553    |
| train_0/q_grads_std       | 0.30831312015652657     |
| train_0/q_loss            | 0.11014318513155592     |
| train_0/reward            | -0.5753439845932007     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.3104736328125         |
| train_0/target_q          | -9.631914860435224      |
| train_1/avg_q             | -20.50283362546476      |
| train_1/current_q         | -2.4589304391138507     |
| train_1/fw_bonus          | -1.0007991373538971     |
| train_1/fw_loss           | 0.00041489977520541287  |
| train_1/mu_grads          | 0.0013868436217308044   |
| train_1/mu_grads_std      | 0.19012288749217987     |
| train_1/mu_loss           | 1.777067616861916e-28   |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -1.8399688894487254e-28 |
| train_1/q_grads           | -0.02179214861243963    |
| train_1/q_grads_std       | 0.532120119035244       |
| train_1/q_loss            | 0.012604952970832578    |
| train_1/reward            | -2.458817950991579      |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.0035400390625         |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -2.458817950991579      |
-------------------------------------------------------
Saving periodic policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_30.pkl ...
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 31
Time for epoch 31: 1591.79. Rollout time: 1018.30, Training time: 573.17
Evaluating epoch 31
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
------------------------------------------------------
| epoch                     | 31                     |
| policy/steps              | 2912594.0              |
| test/episodes             | 800.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -27.0                  |
| test_1/avg_q              | -20.40037440255555     |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 3200.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -27.0                  |
| train_0/current_q         | -9.35701376267548      |
| train_0/fw_bonus          | -0.9999392345547676    |
| train_0/fw_loss           | 1.6989652385746014e-05 |
| train_0/mu_grads          | -0.04999831756576896   |
| train_0/mu_grads_std      | 0.39705755636096       |
| train_0/mu_loss           | 9.248733790713706      |
| train_0/next_q            | -9.245668711988092     |
| train_0/q_grads           | 0.01223730391357094    |
| train_0/q_grads_std       | 0.3092165686190128     |
| train_0/q_loss            | 0.10457610110608777    |
| train_0/reward            | -0.5683685976047854    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.281591796875         |
| train_0/target_q          | -9.50427385278625      |
| train_1/avg_q             | -20.496782991696247    |
| train_1/current_q         | -2.5265929249318715    |
| train_1/fw_bonus          | -1.0009240597486495    |
| train_1/fw_loss           | 0.0003832431997579988  |
| train_1/mu_grads          | 0.0013868436217308044  |
| train_1/mu_grads_std      | 0.19012288749217987    |
| train_1/mu_loss           | 5.6121772895488634e-30 |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -5.669033959245077e-30 |
| train_1/q_grads           | -0.023227067245170475  |
| train_1/q_grads_std       | 0.5361447185277939     |
| train_1/q_loss            | 0.013983566663886804   |
| train_1/reward            | -2.525939024154286     |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.003369140625         |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -2.525939024154286     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 32
Time for epoch 32: 1588.70. Rollout time: 1058.41, Training time: 530.04
Evaluating epoch 32
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
------------------------------------------------------
| epoch                     | 32                     |
| policy/steps              | 3003719.0              |
| test/episodes             | 825.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -27.0                  |
| test_1/avg_q              | -20.501852309404434    |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 3300.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -27.0                  |
| train_0/current_q         | -9.314542175745846     |
| train_0/fw_bonus          | -0.9999532580375672    |
| train_0/fw_loss           | 1.3766303345619235e-05 |
| train_0/mu_grads          | -0.051399476174265143  |
| train_0/mu_grads_std      | 0.3996887572109699     |
| train_0/mu_loss           | 9.21271478239738       |
| train_0/next_q            | -9.207573638351175     |
| train_0/q_grads           | 0.012096004141494632   |
| train_0/q_grads_std       | 0.31021947264671323    |
| train_0/q_loss            | 0.09302069248101222    |
| train_0/reward            | -0.5663396324318455    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.347119140625         |
| train_0/target_q          | -9.466168457020107     |
| train_1/avg_q             | -20.46624327279369     |
| train_1/current_q         | -2.4792248172670694    |
| train_1/fw_bonus          | -1.0008805632591247    |
| train_1/fw_loss           | 0.000394268095260486   |
| train_1/mu_grads          | 0.0013868436217308044  |
| train_1/mu_grads_std      | 0.19012288749217987    |
| train_1/mu_loss           | 4.623562412887915e-36  |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -6.113842007417075e-36 |
| train_1/q_grads           | -0.02467100266367197   |
| train_1/q_grads_std       | 0.5397151201963425     |
| train_1/q_loss            | 0.01392773303972133    |
| train_1/reward            | -2.480060228666116     |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.003857421875         |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -2.480060228666116     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 33
Time for epoch 33: 1595.88. Rollout time: 1054.61, Training time: 540.98
Evaluating epoch 33
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
------------------------------------------------------
| epoch                     | 33                     |
| policy/steps              | 3094844.0              |
| test/episodes             | 850.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -27.0                  |
| test_1/avg_q              | -20.560597999640077    |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 3400.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -27.0                  |
| train_0/current_q         | -9.268453199876479     |
| train_0/fw_bonus          | -0.999955752491951     |
| train_0/fw_loss           | 1.3191408697821316e-05 |
| train_0/mu_grads          | -0.052474561240524055  |
| train_0/mu_grads_std      | 0.4017545819282532     |
| train_0/mu_loss           | 9.173336956483377      |
| train_0/next_q            | -9.16915832957952      |
| train_0/q_grads           | 0.012104471353814006   |
| train_0/q_grads_std       | 0.31008814424276354    |
| train_0/q_loss            | 0.08805314013609136    |
| train_0/reward            | -0.5648021715580398    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.3609130859375        |
| train_0/target_q          | -9.42181181280919      |
| train_1/avg_q             | -20.499359678297722    |
| train_1/current_q         | -2.462328315235511     |
| train_1/fw_bonus          | -1.000991988182068     |
| train_1/fw_loss           | 0.00036602749023586514 |
| train_1/mu_grads          | 0.0013868436217308044  |
| train_1/mu_grads_std      | 0.19012288749217987    |
| train_1/mu_loss           | 6.31183362437831e-22   |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -6.418934190565186e-22 |
| train_1/q_grads           | -0.02562135918997228   |
| train_1/q_grads_std       | 0.5447291761636734     |
| train_1/q_loss            | 0.013449457119785905   |
| train_1/reward            | -2.461814162704468     |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.003369140625         |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -2.461814162704468     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 34
Time for epoch 34: 1528.89. Rollout time: 1078.04, Training time: 450.52
Evaluating epoch 34
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
-------------------------------------------------------
| epoch                     | 34                      |
| policy/steps              | 3185969.0               |
| test/episodes             | 875.0                   |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -27.0                   |
| test_1/avg_q              | -20.49592876428327      |
| test_1/n_subgoals         | 675.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 3500.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -27.0                   |
| train_0/current_q         | -9.381881818068758      |
| train_0/fw_bonus          | -0.999952246248722      |
| train_0/fw_loss           | 1.39979706318627e-05    |
| train_0/mu_grads          | -0.05368939395993948    |
| train_0/mu_grads_std      | 0.4042630881071091      |
| train_0/mu_loss           | 9.289295947936195       |
| train_0/next_q            | -9.286234859483296      |
| train_0/q_grads           | 0.012186773587018251    |
| train_0/q_grads_std       | 0.30978474766016006     |
| train_0/q_loss            | 0.08926133630371288     |
| train_0/reward            | -0.5671084233101282     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.3592041015625         |
| train_0/target_q          | -9.535069542415922      |
| train_1/avg_q             | -20.51971803503775      |
| train_1/current_q         | -2.491822095464169      |
| train_1/fw_bonus          | -1.0010950237512588     |
| train_1/fw_loss           | 0.00033991621166933327  |
| train_1/mu_grads          | 0.0014504512946587056   |
| train_1/mu_grads_std      | 0.19013407230377197     |
| train_1/mu_loss           | 1.230847974710169e-09   |
| train_1/n_subgoals        | 2700.0                  |
| train_1/next_q            | -1.1279430408949755e-09 |
| train_1/q_grads           | -0.026607579737901687   |
| train_1/q_grads_std       | 0.5508987516164779      |
| train_1/q_loss            | 0.011461248138081647    |
| train_1/reward            | -2.4917044209210872     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.002783203125          |
| train_1/reward_-27.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -2.491704421460157      |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 35
Time for epoch 35: 1465.70. Rollout time: 1028.07, Training time: 437.31
Evaluating epoch 35
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
------------------------------------------------------
| epoch                     | 35                     |
| policy/steps              | 3277094.0              |
| test/episodes             | 900.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -27.0                  |
| test_1/avg_q              | -20.642318227706408    |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 3600.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -27.0                  |
| train_0/current_q         | -9.233082760470872     |
| train_0/fw_bonus          | -0.9999563127756119    |
| train_0/fw_loss           | 1.3064740346635518e-05 |
| train_0/mu_grads          | -0.05450781872496009   |
| train_0/mu_grads_std      | 0.4061169855296612     |
| train_0/mu_loss           | 9.145451671608543      |
| train_0/next_q            | -9.141909658449153     |
| train_0/q_grads           | 0.012240351131185889   |
| train_0/q_grads_std       | 0.3100865975022316     |
| train_0/q_loss            | 0.0868587808138562     |
| train_0/reward            | -0.5639600242000598    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.3517333984375        |
| train_0/target_q          | -9.383761323492092     |
| train_1/avg_q             | -20.541751970953886    |
| train_1/current_q         | -15.927829887290198    |
| train_1/fw_bonus          | -1.0010946393013       |
| train_1/fw_loss           | 0.0003400202302145772  |
| train_1/mu_grads          | 0.012328471057116985   |
| train_1/mu_grads_std      | 0.20191536843776703    |
| train_1/mu_loss           | 27.0                   |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -27.0                  |
| train_1/q_grads           | -0.02852244391106069   |
| train_1/q_grads_std       | 0.5588504776358605     |
| train_1/q_loss            | 19.110331165287448     |
| train_1/reward            | -2.466765354717063     |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.002685546875         |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -16.181908421123318    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 36
Time for epoch 36: 1336.43. Rollout time: 967.17, Training time: 369.03
Evaluating epoch 36
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
------------------------------------------------------
| epoch                     | 36                     |
| policy/steps              | 3368219.0              |
| test/episodes             | 925.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -27.0                  |
| test_1/avg_q              | -20.656520985167504    |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 3700.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -27.0                  |
| train_0/current_q         | -9.338177067896257     |
| train_0/fw_bonus          | -0.9999592944979667    |
| train_0/fw_loss           | 1.2377944915442641e-05 |
| train_0/mu_grads          | -0.05477293645963073   |
| train_0/mu_grads_std      | 0.40692307725548743    |
| train_0/mu_loss           | 9.257554980542112      |
| train_0/next_q            | -9.254824684145282     |
| train_0/q_grads           | 0.012265064218081535   |
| train_0/q_grads_std       | 0.3100886806845665     |
| train_0/q_loss            | 0.0880061643046535     |
| train_0/reward            | -0.5648213666929223    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.362451171875         |
| train_0/target_q          | -9.490676308421062     |
| train_1/avg_q             | -20.536117150022864    |
| train_1/current_q         | -16.115927544707464    |
| train_1/fw_bonus          | -1.001150718331337     |
| train_1/fw_loss           | 0.00032580088736722246 |
| train_1/mu_grads          | 0.012328471057116985   |
| train_1/mu_grads_std      | 0.20191536843776703    |
| train_1/mu_loss           | 27.0                   |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -27.0                  |
| train_1/q_grads           | -0.027982916822656988  |
| train_1/q_grads_std       | 0.5700495481491089     |
| train_1/q_loss            | 17.35751261634124      |
| train_1/reward            | -2.4941170285383123    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0027099609375        |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -16.336521325413322    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 37
Time for epoch 37: 1117.21. Rollout time: 816.18, Training time: 300.91
Evaluating epoch 37
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
------------------------------------------------------
| epoch                     | 37                     |
| policy/steps              | 3459344.0              |
| test/episodes             | 950.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -27.0                  |
| test_1/avg_q              | -20.657065655197748    |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 3800.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -27.0                  |
| train_0/current_q         | -9.271013722482703     |
| train_0/fw_bonus          | -0.999960595369339     |
| train_0/fw_loss           | 1.2078650388502866e-05 |
| train_0/mu_grads          | -0.05617102039977908   |
| train_0/mu_grads_std      | 0.4076281890273094     |
| train_0/mu_loss           | 9.190594664918715      |
| train_0/next_q            | -9.186473445174997     |
| train_0/q_grads           | 0.012375142588280142   |
| train_0/q_grads_std       | 0.3102236598730087     |
| train_0/q_loss            | 0.08771279097594915    |
| train_0/reward            | -0.5631554322902957    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.3795654296875        |
| train_0/target_q          | -9.422892102936284     |
| train_1/avg_q             | -20.557245260797714    |
| train_1/current_q         | -16.01815261974583     |
| train_1/fw_bonus          | -1.0011427134275437    |
| train_1/fw_loss           | 0.0003278296244388912  |
| train_1/mu_grads          | 0.012328471057116985   |
| train_1/mu_grads_std      | 0.20191536843776703    |
| train_1/mu_loss           | 27.0                   |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -27.0                  |
| train_1/q_grads           | -0.025028263591229914  |
| train_1/q_grads_std       | 0.5857274144887924     |
| train_1/q_loss            | 15.1529945215137       |
| train_1/reward            | -2.4848865094634673    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0028076171875        |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -16.176127720400974    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 38
Time for epoch 38: 1040.10. Rollout time: 753.83, Training time: 286.15
Evaluating epoch 38
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
------------------------------------------------------
| epoch                     | 38                     |
| policy/steps              | 3550469.0              |
| test/episodes             | 975.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -27.0                  |
| test_1/avg_q              | -20.568079541715797    |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 3900.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -27.0                  |
| train_0/current_q         | -9.359575891648703     |
| train_0/fw_bonus          | -0.9999662652611733    |
| train_0/fw_loss           | 1.0773835310828871e-05 |
| train_0/mu_grads          | -0.056064109224826096  |
| train_0/mu_grads_std      | 0.40942104533314705    |
| train_0/mu_loss           | 9.27627258411771       |
| train_0/next_q            | -9.272149606139973     |
| train_0/q_grads           | 0.012374953972175718   |
| train_0/q_grads_std       | 0.30981998816132544    |
| train_0/q_loss            | 0.08604273299338412    |
| train_0/reward            | -0.5661048495181603    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.3779296875           |
| train_0/target_q          | -9.51499414633005      |
| train_1/avg_q             | -20.573195079788096    |
| train_1/current_q         | -15.9382442145892      |
| train_1/fw_bonus          | -1.0012542217969895    |
| train_1/fw_loss           | 0.0002995732280396624  |
| train_1/mu_grads          | 0.012328471057116985   |
| train_1/mu_grads_std      | 0.20191536843776703    |
| train_1/mu_loss           | 27.0                   |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -27.0                  |
| train_1/q_grads           | -0.020225464180111886  |
| train_1/q_grads_std       | 0.6175957545638084     |
| train_1/q_loss            | 10.721932952883922     |
| train_1/reward            | -2.4668052557783087    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0022216796875        |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -16.129622638590813    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 39
Time for epoch 39: 1079.03. Rollout time: 779.68, Training time: 299.22
Evaluating epoch 39
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
------------------------------------------------------
| epoch                     | 39                     |
| policy/steps              | 3641594.0              |
| test/episodes             | 1000.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -27.0                  |
| test_1/avg_q              | -20.594646514257438    |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 4000.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -27.0                  |
| train_0/current_q         | -9.247404235364538     |
| train_0/fw_bonus          | -0.999961893260479     |
| train_0/fw_loss           | 1.1779505643971789e-05 |
| train_0/mu_grads          | -0.05637342175468803   |
| train_0/mu_grads_std      | 0.4109838016331196     |
| train_0/mu_loss           | 9.164872359980473      |
| train_0/next_q            | -9.160978832389592     |
| train_0/q_grads           | 0.012328915018588304   |
| train_0/q_grads_std       | 0.3100317850708961     |
| train_0/q_loss            | 0.08629986866978549    |
| train_0/reward            | -0.5625148885821545    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.3631103515625        |
| train_0/target_q          | -9.398622849651446     |
| train_1/avg_q             | -20.603441608077965    |
| train_1/current_q         | -15.8739900056782      |
| train_1/fw_bonus          | -1.0012177646160125    |
| train_1/fw_loss           | 0.00030881214188411833 |
| train_1/mu_grads          | 0.012328471057116985   |
| train_1/mu_grads_std      | 0.20191536843776703    |
| train_1/mu_loss           | 27.0                   |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -27.0                  |
| train_1/q_grads           | -0.019013243447989225  |
| train_1/q_grads_std       | 0.6436316430568695     |
| train_1/q_loss            | 7.247383170620561      |
| train_1/reward            | -2.4798332512429626    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.002490234375         |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -16.07934301686797     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 40
Time for epoch 40: 1038.54. Rollout time: 757.78, Training time: 280.67
Evaluating epoch 40
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
------------------------------------------------------
| epoch                     | 40                     |
| policy/steps              | 3732719.0              |
| test/episodes             | 1025.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -27.0                  |
| test_1/avg_q              | -20.655438546111075    |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 4100.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -27.0                  |
| train_0/current_q         | -9.262066841119601     |
| train_0/fw_bonus          | -0.9999726787209511    |
| train_0/fw_loss           | 9.301094746660964e-06  |
| train_0/mu_grads          | -0.05619840379804373   |
| train_0/mu_grads_std      | 0.4129711963236332     |
| train_0/mu_loss           | 9.1801397325909        |
| train_0/next_q            | -9.17662947505166      |
| train_0/q_grads           | 0.012147981976158916   |
| train_0/q_grads_std       | 0.3107292838394642     |
| train_0/q_loss            | 0.08495599862240995    |
| train_0/reward            | -0.5604851129181043    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.381103515625         |
| train_0/target_q          | -9.413543084400455     |
| train_1/avg_q             | -20.579916269280975    |
| train_1/current_q         | -15.646958573192444    |
| train_1/fw_bonus          | -1.0013127267360686    |
| train_1/fw_loss           | 0.00028474835635279306 |
| train_1/mu_grads          | 0.012328471057116985   |
| train_1/mu_grads_std      | 0.20191536843776703    |
| train_1/mu_loss           | 27.0                   |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -27.0                  |
| train_1/q_grads           | -0.018277565762400628  |
| train_1/q_grads_std       | 0.6623153835535049     |
| train_1/q_loss            | 5.979465284252962      |
| train_1/reward            | -2.5444565967285597    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.00224609375          |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -15.893965874072318    |
------------------------------------------------------
Saving periodic policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_40.pkl ...
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 41
Time for epoch 41: 1071.25. Rollout time: 781.76, Training time: 289.36
Evaluating epoch 41
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
------------------------------------------------------
| epoch                     | 41                     |
| policy/steps              | 3823844.0              |
| test/episodes             | 1050.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -27.0                  |
| test_1/avg_q              | -20.3467370321036      |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 4200.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -27.0                  |
| train_0/current_q         | -9.266148789545225     |
| train_0/fw_bonus          | -0.9999592408537865    |
| train_0/fw_loss           | 1.2389470020934823e-05 |
| train_0/mu_grads          | -0.056122071109712124  |
| train_0/mu_grads_std      | 0.4150014452636242     |
| train_0/mu_loss           | 9.179428299070821      |
| train_0/next_q            | -9.175744752532081     |
| train_0/q_grads           | 0.012018161010928452   |
| train_0/q_grads_std       | 0.3113086298108101     |
| train_0/q_loss            | 0.08514467725881311    |
| train_0/reward            | -0.5636861002240039    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.352783203125         |
| train_0/target_q          | -9.417696686941682     |
| train_1/avg_q             | -20.632854327121077    |
| train_1/current_q         | -15.811232258409959    |
| train_1/fw_bonus          | -1.001206600666046     |
| train_1/fw_loss           | 0.0003116410160146188  |
| train_1/mu_grads          | 0.012328471057116985   |
| train_1/mu_grads_std      | 0.20191536843776703    |
| train_1/mu_loss           | 27.0                   |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -27.0                  |
| train_1/q_grads           | -0.017907135561108588  |
| train_1/q_grads_std       | 0.6787705466151237     |
| train_1/q_loss            | 5.214011349372578      |
| train_1/reward            | -2.491640761893359     |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0021484375           |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -16.089212539237117    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 42
Time for epoch 42: 1095.28. Rollout time: 786.95, Training time: 308.22
Evaluating epoch 42
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
------------------------------------------------------
| epoch                     | 42                     |
| policy/steps              | 3914969.0              |
| test/episodes             | 1075.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -27.0                  |
| test_1/avg_q              | -20.56800788385172     |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 4300.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -27.0                  |
| train_0/current_q         | -9.329679334587931     |
| train_0/fw_bonus          | -0.9999621614813805    |
| train_0/fw_loss           | 1.171926487586461e-05  |
| train_0/mu_grads          | -0.0563838129863143    |
| train_0/mu_grads_std      | 0.4175286293029785     |
| train_0/mu_loss           | 9.24459288326851       |
| train_0/next_q            | -9.241077954831553     |
| train_0/q_grads           | 0.011961256014183163   |
| train_0/q_grads_std       | 0.31142818331718447    |
| train_0/q_loss            | 0.0854631591211267     |
| train_0/reward            | -0.5647305301827146    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.3740966796875        |
| train_0/target_q          | -9.484325720334514     |
| train_1/avg_q             | -20.537642068293277    |
| train_1/current_q         | -15.89821958529257     |
| train_1/fw_bonus          | -1.0012423008680345    |
| train_1/fw_loss           | 0.00030259570812631866 |
| train_1/mu_grads          | 0.012328471057116985   |
| train_1/mu_grads_std      | 0.20191536843776703    |
| train_1/mu_loss           | 27.0                   |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -27.0                  |
| train_1/q_grads           | -0.017609929060563446  |
| train_1/q_grads_std       | 0.6912179827690125     |
| train_1/q_loss            | 4.530567569597359      |
| train_1/reward            | -2.4323235130395915    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0021728515625        |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -16.186872341164598    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 43
Time for epoch 43: 1082.32. Rollout time: 781.93, Training time: 300.23
Evaluating epoch 43
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
------------------------------------------------------
| epoch                     | 43                     |
| policy/steps              | 4006094.0              |
| test/episodes             | 1100.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -27.0                  |
| test_1/avg_q              | -20.368050891330824    |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 4400.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -27.0                  |
| train_0/current_q         | -9.33163716244144      |
| train_0/fw_bonus          | -0.999964340031147     |
| train_0/fw_loss           | 1.121958628118591e-05  |
| train_0/mu_grads          | -0.0566871402785182    |
| train_0/mu_grads_std      | 0.4196243815124035     |
| train_0/mu_loss           | 9.247319584439737      |
| train_0/next_q            | -9.243317942856374     |
| train_0/q_grads           | 0.011973745375871658   |
| train_0/q_grads_std       | 0.31166132017970083    |
| train_0/q_loss            | 0.08653471054334329    |
| train_0/reward            | -0.5649141792593582    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.3845458984375        |
| train_0/target_q          | -9.483948981871553     |
| train_1/avg_q             | -20.47482528361596     |
| train_1/current_q         | -2.497851607396517     |
| train_1/fw_bonus          | -1.0014144480228424    |
| train_1/fw_loss           | 0.00025896767656377053 |
| train_1/mu_grads          | 0.012119348160922527   |
| train_1/mu_grads_std      | 0.20091062784194946    |
| train_1/mu_loss           | 0.0                    |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | 0.0                    |
| train_1/q_grads           | -0.018587699579074978  |
| train_1/q_grads_std       | 0.6934644475579261     |
| train_1/q_loss            | 0.13166751215773603    |
| train_1/reward            | -2.5758846182252455    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0021484375           |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -2.5758846182252455    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 44
Time for epoch 44: 7782.40. Rollout time: 789.33, Training time: 6992.97
Evaluating epoch 44
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
------------------------------------------------------
| epoch                     | 44                     |
| policy/steps              | 4097219.0              |
| test/episodes             | 1125.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -27.0                  |
| test_1/avg_q              | -20.46017973029864     |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 4500.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -27.0                  |
| train_0/current_q         | -9.209277530512457     |
| train_0/fw_bonus          | -0.9999650359153748    |
| train_0/fw_loss           | 1.1057285814786155e-05 |
| train_0/mu_grads          | -0.05665211509913206   |
| train_0/mu_grads_std      | 0.4216868706047535     |
| train_0/mu_loss           | 9.124183611235148      |
| train_0/next_q            | -9.120843894286342     |
| train_0/q_grads           | 0.012039899360388517   |
| train_0/q_grads_std       | 0.3117451354861259     |
| train_0/q_loss            | 0.08414895311508087    |
| train_0/reward            | -0.5612156251601845    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.3780029296875        |
| train_0/target_q          | -9.359659709800017     |
| train_1/avg_q             | -20.40402762784026     |
| train_1/current_q         | -2.5160594812592927    |
| train_1/fw_bonus          | -1.0013762176036836    |
| train_1/fw_loss           | 0.0002686506064492278  |
| train_1/mu_grads          | 0.012119348160922527   |
| train_1/mu_grads_std      | 0.20091062784194946    |
| train_1/mu_loss           | 0.0                    |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | 0.0                    |
| train_1/q_grads           | -0.019183338806033134  |
| train_1/q_grads_std       | 0.6952541053295136     |
| train_1/q_loss            | 0.04831964838349566    |
| train_1/reward            | -2.520027898960689     |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.00234375             |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -2.520027898960689     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 45
