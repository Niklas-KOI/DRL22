Starting process id: 45837
T: 50
alg: chac
algorithm: src.chac
atomic_noise: 0.2
base_logdir: data
batch_size: 1024
bind_core: 0
buffer_size: 500
chac_params: {}
early_stop_data_column: test/success_rate
early_stop_threshold: 100.0
env_name: CausalDependenciesMujocoEnv-o1-v0
eta: 0.5
fw: 1
fw_hidden_size: 256,256,256
fw_lr: 0.001
gamma: 0.98
graph: 1
info: 
make_env: <function prepare_params.<locals>.make_env at 0x7f833214ee60>
max_try_idx: 199
mu_hidden_size: 64
mu_lr: 0.001
n_episodes: 100
n_levels: 2
n_pre_episodes: 30
n_test_rollouts: 25
n_train_batches: 40
n_train_rollouts: 100
num_threads: 1
q_hidden_size: 64
q_lr: 0.001
random_action_perc: 0.3
regularization: True
render: 0
rollout_batch_size: 1
subgoal_noise: 0.2
subgoal_test_perc: 0.3
time_scales: 25,10
try_start_idx: 100
use_mpi: False
verbose: False

*** Warning ***
You are running src.chac with just a single MPI worker. This will work, but the HER experiments that we report in Plappert et al. (2018, https://arxiv.org/abs/1802.09464) were obtained with --num_cpu 19. This makes a significant difference and if you are looking to reproduce those results, be aware of this. Please also refer to https://github.com/openai/baselines/issues/314 for further details.
****************

dims: action = 2, subgoal = 2, end_goal = 2
subgoal_bounds: symmetric [0.2 0.2], offset [0.922 0.25 ]
Running on CPU ...
Creating a CHAC agent

Hierarchy Level 0 with time scale 25
Actor(
  (fc1): Linear(in_features=35, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
  (fc4): Linear(in_features=64, out_features=2, bias=True)
)
Critic(
  (fc1): Linear(in_features=37, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
  (fc4): Linear(in_features=64, out_features=1, bias=True)
  (mse_loss): MSELoss()
)
ForwardModel(
  (mlp): Sequential(
    (0): Linear(in_features=35, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=256, bias=True)
    (5): ReLU()
    (6): Linear(in_features=256, out_features=33, bias=True)
    (7): Identity()
  )
  (mse_loss): MSELoss()
)

Hierarchy Level 1 with time scale 10
Actor(
  (fc1): Linear(in_features=35, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
  (fc4): Linear(in_features=64, out_features=2, bias=True)
)
Critic(
  (fc1): Linear(in_features=37, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
  (fc4): Linear(in_features=64, out_features=1, bias=True)
  (mse_loss): MSELoss()
)
ForwardModel(
  (mlp): Sequential(
    (0): Linear(in_features=35, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=256, bias=True)
    (5): ReLU()
    (6): Linear(in_features=256, out_features=33, bias=True)
    (7): Identity()
  )
  (mse_loss): MSELoss()
)
Training epoch 0
Time for epoch 0: 593.70. Rollout time: 242.53, Training time: 351.07
Evaluating epoch 0
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
-----------------------------------------------------
| epoch                     | 0                     |
| policy/steps              | 29736.0               |
| test/episodes             | 25.0                  |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -25.0                 |
| test_1/avg_q              | -4.087839614238626    |
| test_1/n_subgoals         | 250.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 100.0                 |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -16.641174361306565   |
| train_0/current_q         | -9.316669948066826    |
| train_0/fw_bonus          | -0.9996154174208641   |
| train_0/fw_loss           | 8.628220639366191e-05 |
| train_0/mu_grads          | 0.006506400508806109  |
| train_0/mu_grads_std      | 0.11783380433917046   |
| train_0/mu_loss           | 7.9846285630745255    |
| train_0/next_q            | -9.31521760140329     |
| train_0/q_grads           | 0.038394985254853964  |
| train_0/q_grads_std       | 0.1997710879892111    |
| train_0/q_loss            | 0.8142750813688402    |
| train_0/reward            | -0.7417266514494258   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0021240234375       |
| train_0/target_q          | -9.49820334797713     |
| train_1/avg_q             | -2.9672660673695135   |
| train_1/current_q         | -3.534858746517975    |
| train_1/fw_bonus          | -1.0113101959228517   |
| train_1/fw_loss           | 0.000890771254489664  |
| train_1/mu_grads          | 0.0037935562781058253 |
| train_1/mu_grads_std      | 0.11493037547916174   |
| train_1/mu_loss           | 4.03995422406892      |
| train_1/n_subgoals        | 1000.0                |
| train_1/next_q            | -3.0336023783111132   |
| train_1/q_grads           | 0.01871397364884615   |
| train_1/q_grads_std       | 0.1658079158514738    |
| train_1/q_loss            | 1.413051811691158     |
| train_1/reward            | -1.3087068541353801   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0002197265625       |
| train_1/reward_-10.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.076                 |
| train_1/target_q          | -3.5205810750453437   |
-----------------------------------------------------
Saving periodic policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_0.pkl ...
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Training epoch 1
Time for epoch 1: 353.53. Rollout time: 147.88, Training time: 205.61
Evaluating epoch 1
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
-----------------------------------------------------
| epoch                     | 1                     |
| policy/steps              | 60986.0               |
| test/episodes             | 50.0                  |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -25.0                 |
| test_1/avg_q              | -6.052527085183929    |
| test_1/n_subgoals         | 250.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 200.0                 |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -25.0                 |
| train_0/current_q         | -9.846988342553987    |
| train_0/fw_bonus          | -0.9996853351593018   |
| train_0/fw_loss           | 7.103216094037635e-05 |
| train_0/mu_grads          | 0.008902457566000522  |
| train_0/mu_grads_std      | 0.13165234848856927   |
| train_0/mu_loss           | 8.427261471157715     |
| train_0/next_q            | -9.846196150302724    |
| train_0/q_grads           | 0.04095825357362628   |
| train_0/q_grads_std       | 0.21968333534896373   |
| train_0/q_loss            | 0.7038416045519915    |
| train_0/reward            | -0.7421888653596398   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0025390625          |
| train_0/target_q          | -10.011795751020127   |
| train_1/avg_q             | -5.082949694935949    |
| train_1/current_q         | -4.8358918516223905   |
| train_1/fw_bonus          | -1.0118048161268234   |
| train_1/fw_loss           | 0.000774896341317799  |
| train_1/mu_grads          | 0.0006278965607634746 |
| train_1/mu_grads_std      | 0.11786453183740378   |
| train_1/mu_loss           | 5.817196182257029     |
| train_1/n_subgoals        | 1000.0                |
| train_1/next_q            | -4.793055878365783    |
| train_1/q_grads           | 0.006293479248415679  |
| train_1/q_grads_std       | 0.21810429878532886   |
| train_1/q_loss            | 1.3207248816201758    |
| train_1/reward            | -1.329193421304808    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 9.765625e-05          |
| train_1/reward_-10.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -4.786994899398465    |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Training epoch 2
Time for epoch 2: 345.13. Rollout time: 144.05, Training time: 201.04
Evaluating epoch 2
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
-----------------------------------------------------
| epoch                     | 2                     |
| policy/steps              | 92236.0               |
| test/episodes             | 75.0                  |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -25.0                 |
| test_1/avg_q              | -9.999995502717528    |
| test_1/n_subgoals         | 250.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 300.0                 |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -25.0                 |
| train_0/current_q         | -10.022888546595478   |
| train_0/fw_bonus          | -0.999708816409111    |
| train_0/fw_loss           | 6.591377714357805e-05 |
| train_0/mu_grads          | 0.011316174268722534  |
| train_0/mu_grads_std      | 0.14506289400160313   |
| train_0/mu_loss           | 8.716315229605476     |
| train_0/next_q            | -10.022555932935187   |
| train_0/q_grads           | 0.04018859742209315   |
| train_0/q_grads_std       | 0.22430177703499793   |
| train_0/q_loss            | 0.7557309232875745    |
| train_0/reward            | -0.7430795078413212   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0046875             |
| train_0/target_q          | -10.229629631776112   |
| train_1/avg_q             | -7.230244196249005    |
| train_1/current_q         | -8.301148734633285    |
| train_1/fw_bonus          | -1.0119656026363373   |
| train_1/fw_loss           | 0.0007372335327090696 |
| train_1/mu_grads          | 1.872660704975715e-05 |
| train_1/mu_grads_std      | 0.11777990516275168   |
| train_1/mu_loss           | 10.999997817098071    |
| train_1/n_subgoals        | 1000.0                |
| train_1/next_q            | -9.99999785140786     |
| train_1/q_grads           | 0.009168167505413294  |
| train_1/q_grads_std       | 0.27695552781224253   |
| train_1/q_loss            | 4.463826148190883     |
| train_1/reward            | -1.3138929450899013   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0001708984375       |
| train_1/reward_-10.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -8.406449998190705    |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Training epoch 3
Time for epoch 3: 354.54. Rollout time: 147.81, Training time: 206.68
Evaluating epoch 3
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 3                      |
| policy/steps              | 123486.0               |
| test/episodes             | 100.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -9.999999997279344     |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 400.0                  |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -9.00908370814316      |
| train_0/fw_bonus          | -0.9997093468904495    |
| train_0/fw_loss           | 6.579705950571224e-05  |
| train_0/mu_grads          | 0.011643484001979232   |
| train_0/mu_grads_std      | 0.14437390938401223    |
| train_0/mu_loss           | 8.32390531227218       |
| train_0/next_q            | -9.00704402122978      |
| train_0/q_grads           | 0.03566365083679557    |
| train_0/q_grads_std       | 0.22500110790133476    |
| train_0/q_loss            | 0.38450845464959726    |
| train_0/reward            | -0.7427257714683947    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.0026611328125        |
| train_0/target_q          | -9.320434232290074     |
| train_1/avg_q             | -9.99958303575676      |
| train_1/current_q         | -8.272256185752797     |
| train_1/fw_bonus          | -1.012064555287361     |
| train_1/fw_loss           | 0.0007140444562537595  |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 10.999999999930008     |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -9.999999999926327     |
| train_1/q_grads           | 0.019456325145438314   |
| train_1/q_grads_std       | 0.320906275510788      |
| train_1/q_loss            | 3.8831323090380905     |
| train_1/reward            | -1.316383843350195     |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 4.8828125e-05          |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.406549858916934     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 4
Time for epoch 4: 329.29. Rollout time: 137.89, Training time: 191.37
Evaluating epoch 4
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 4                      |
| policy/steps              | 154736.0               |
| test/episodes             | 125.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -9.999999999945272     |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 500.0                  |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.249562873716688    |
| train_0/fw_bonus          | -0.9997151896357537    |
| train_0/fw_loss           | 6.45229430119798e-05   |
| train_0/mu_grads          | 0.007736527291126549   |
| train_0/mu_grads_std      | 0.13885636813938618    |
| train_0/mu_loss           | 9.00155892676822       |
| train_0/next_q            | -10.243272470890275    |
| train_0/q_grads           | 0.03878998178988695    |
| train_0/q_grads_std       | 0.23049530424177647    |
| train_0/q_loss            | 0.6008577386494076     |
| train_0/reward            | -0.7445719546587497    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.0075927734375        |
| train_0/target_q          | -10.406356556648223    |
| train_1/avg_q             | -9.998092368564672     |
| train_1/current_q         | -8.26030341182533      |
| train_1/fw_bonus          | -1.011925819516182     |
| train_1/fw_loss           | 0.0007465467570000328  |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.02599869528785348    |
| train_1/q_grads_std       | 0.36205668821930886    |
| train_1/q_loss            | 3.3767719349265013     |
| train_1/reward            | -1.3188047085794097    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.000146484375         |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.412320333579412     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 5
Time for epoch 5: 301.72. Rollout time: 128.61, Training time: 173.08
Evaluating epoch 5
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 5                      |
| policy/steps              | 185872.0               |
| test/episodes             | 150.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -9.999999999735486     |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 600.0                  |
| train/success_rate        | 0.01                   |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.290399627722502    |
| train_0/fw_bonus          | -0.999761775135994     |
| train_0/fw_loss           | 5.436522169475211e-05  |
| train_0/mu_grads          | 0.005154387431684881   |
| train_0/mu_grads_std      | 0.13477949760854244    |
| train_0/mu_loss           | 9.151882508193678      |
| train_0/next_q            | -10.29178733254452     |
| train_0/q_grads           | 0.038914299197494985   |
| train_0/q_grads_std       | 0.23228590674698352    |
| train_0/q_loss            | 0.7789781070319861     |
| train_0/reward            | -0.7460967106788303    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.020458984375         |
| train_0/target_q          | -10.49574206836235     |
| train_1/avg_q             | -9.999869004693506     |
| train_1/current_q         | -8.270345986613652     |
| train_1/fw_bonus          | -1.011946052312851     |
| train_1/fw_loss           | 0.0007418074033921585  |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 996.0                  |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.027390526747331022   |
| train_1/q_grads_std       | 0.38456504642963407    |
| train_1/q_loss            | 2.8944176914905833     |
| train_1/reward            | -1.3331947065984422    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 7.32421875e-05         |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.445372440973445     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 6
Time for epoch 6: 297.52. Rollout time: 120.87, Training time: 176.62
Evaluating epoch 6
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 6                      |
| policy/steps              | 217122.0               |
| test/episodes             | 175.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -9.99999999999625      |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 700.0                  |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -9.568392855945714     |
| train_0/fw_bonus          | -0.999766343832016     |
| train_0/fw_loss           | 5.3370113437267716e-05 |
| train_0/mu_grads          | 0.002891679498134181   |
| train_0/mu_grads_std      | 0.1357129842042923     |
| train_0/mu_loss           | 8.60344526313827       |
| train_0/next_q            | -9.562124181910653     |
| train_0/q_grads           | 0.03715586392208934    |
| train_0/q_grads_std       | 0.23421216011047363    |
| train_0/q_loss            | 0.4972503618131968     |
| train_0/reward            | -0.7414603204349988    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.0191650390625        |
| train_0/target_q          | -9.875002433996336     |
| train_1/avg_q             | -9.999972840126796     |
| train_1/current_q         | -8.262316326973783     |
| train_1/fw_bonus          | -1.0119164794683457    |
| train_1/fw_loss           | 0.0007487369613954798  |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.02894249469973147    |
| train_1/q_grads_std       | 0.40324451476335527    |
| train_1/q_loss            | 2.7288643957681282     |
| train_1/reward            | -1.3193981782038464    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 9.765625e-05           |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.458133529766348     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 7
Time for epoch 7: 297.40. Rollout time: 119.78, Training time: 177.59
Evaluating epoch 7
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
-------------------------------------------------------
| epoch                     | 7                       |
| policy/steps              | 248372.0                |
| test/episodes             | 200.0                   |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -25.0                   |
| test_1/avg_q              | -9.99999999999999       |
| test_1/n_subgoals         | 250.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 800.0                   |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -25.0                   |
| train_0/current_q         | -10.241270847590531     |
| train_0/fw_bonus          | -0.9998119667172432     |
| train_0/fw_loss           | 4.342006791375752e-05   |
| train_0/mu_grads          | -0.00024690528007340615 |
| train_0/mu_grads_std      | 0.13982209786772729     |
| train_0/mu_loss           | 9.201897381311513       |
| train_0/next_q            | -10.23868963184228      |
| train_0/q_grads           | 0.03847666615620256     |
| train_0/q_grads_std       | 0.236988914757967       |
| train_0/q_loss            | 0.8235273663232883      |
| train_0/reward            | -0.7445471003993589     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.095849609375          |
| train_0/target_q          | -10.412435992806945     |
| train_1/avg_q             | -9.997807109160961      |
| train_1/current_q         | -8.304518651608834      |
| train_1/fw_bonus          | -1.012479129433632      |
| train_1/fw_loss           | 0.0006169206768390723   |
| train_1/mu_grads          | 1.8710847143665887e-05  |
| train_1/mu_grads_std      | 0.1177799254655838      |
| train_1/mu_loss           | 11.0                    |
| train_1/n_subgoals        | 1000.0                  |
| train_1/next_q            | -10.0                   |
| train_1/q_grads           | 0.032022590097039935    |
| train_1/q_grads_std       | 0.42254461869597437     |
| train_1/q_loss            | 3.054768458687685       |
| train_1/reward            | -1.3189104682001926     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.0001220703125         |
| train_1/reward_-10.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -8.492816718200196      |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 8
Time for epoch 8: 373.52. Rollout time: 150.69, Training time: 222.79
Evaluating epoch 8
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 8                      |
| policy/steps              | 279622.0               |
| test/episodes             | 225.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -9.999999999999996     |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 900.0                  |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.229278190536167    |
| train_0/fw_bonus          | -0.9998008862137795    |
| train_0/fw_loss           | 4.5836600702386934e-05 |
| train_0/mu_grads          | -0.0028565687709487973 |
| train_0/mu_grads_std      | 0.142673284932971      |
| train_0/mu_loss           | 9.237963046914242      |
| train_0/next_q            | -10.222705288468726    |
| train_0/q_grads           | 0.037465921975672244   |
| train_0/q_grads_std       | 0.23941751308739184    |
| train_0/q_loss            | 0.5644237013549681     |
| train_0/reward            | -0.7437114812739309    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.0767333984375        |
| train_0/target_q          | -10.399396765693261    |
| train_1/avg_q             | -9.995707975209891     |
| train_1/current_q         | -8.265112783177113     |
| train_1/fw_bonus          | -1.012400135397911     |
| train_1/fw_loss           | 0.0006354286306304857  |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.033395635150372985   |
| train_1/q_grads_std       | 0.43704139441251755    |
| train_1/q_loss            | 2.8269984044933354     |
| train_1/reward            | -1.3104481150112406    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0001220703125        |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.465213740011244     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 9
Time for epoch 9: 373.56. Rollout time: 155.37, Training time: 218.13
Evaluating epoch 9
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 9                      |
| policy/steps              | 310866.0               |
| test/episodes             | 250.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 1000.0                 |
| train/success_rate        | 0.01                   |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.361819631693905    |
| train_0/fw_bonus          | -0.999784903228283     |
| train_0/fw_loss           | 4.932503679810907e-05  |
| train_0/mu_grads          | -0.004111914290115237  |
| train_0/mu_grads_std      | 0.14530498832464217    |
| train_0/mu_loss           | 9.479607532055626      |
| train_0/next_q            | -10.361295564654245    |
| train_0/q_grads           | 0.0351933728903532     |
| train_0/q_grads_std       | 0.2416899487376213     |
| train_0/q_loss            | 0.6292998632109575     |
| train_0/reward            | -0.7441551425254147    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.0508056640625        |
| train_0/target_q          | -10.55511239524528     |
| train_1/avg_q             | -9.998589734317193     |
| train_1/current_q         | -8.272051412471217     |
| train_1/fw_bonus          | -1.012563082575798     |
| train_1/fw_loss           | 0.0005972529106657021  |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.03375078309327364    |
| train_1/q_grads_std       | 0.45555078983306885    |
| train_1/q_loss            | 2.730460620744507      |
| train_1/reward            | -1.3299143583033584    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0                    |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.44592021767836      |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 10
Time for epoch 10: 365.32. Rollout time: 153.51, Training time: 211.76
Evaluating epoch 10
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 10                     |
| policy/steps              | 342116.0               |
| test/episodes             | 275.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -9.999999999999861     |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 1100.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.268225444172243    |
| train_0/fw_bonus          | -0.9997414246201515    |
| train_0/fw_loss           | 5.8804410946322606e-05 |
| train_0/mu_grads          | -0.005776371143292635  |
| train_0/mu_grads_std      | 0.147381304949522      |
| train_0/mu_loss           | 9.377571207646316      |
| train_0/next_q            | -10.266591253474422    |
| train_0/q_grads           | 0.03468022653833032    |
| train_0/q_grads_std       | 0.24441176205873488    |
| train_0/q_loss            | 0.7538199153030074     |
| train_0/reward            | -0.7448780357277428    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.0659912109375        |
| train_0/target_q          | -10.460945390624833    |
| train_1/avg_q             | -9.999999999914987     |
| train_1/current_q         | -8.280239974951115     |
| train_1/fw_bonus          | -1.0126651734113694    |
| train_1/fw_loss           | 0.0005733360449085012  |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.03393195988610387    |
| train_1/q_grads_std       | 0.4663319803774357     |
| train_1/q_loss            | 2.7953762897473644     |
| train_1/reward            | -1.3117036852017918    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 4.8828125e-05          |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.474125560201795     |
------------------------------------------------------
Saving periodic policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_10.pkl ...
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 11
Time for epoch 11: 384.37. Rollout time: 164.85, Training time: 219.40
Evaluating epoch 11
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 11                     |
| policy/steps              | 373349.0               |
| test/episodes             | 300.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 1200.0                 |
| train/success_rate        | 0.01                   |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.278582271397866    |
| train_0/fw_bonus          | -0.9997915953397751    |
| train_0/fw_loss           | 4.786545409842802e-05  |
| train_0/mu_grads          | -0.006365200993604958  |
| train_0/mu_grads_std      | 0.15105402953922747    |
| train_0/mu_loss           | 9.535126751861814      |
| train_0/next_q            | -10.278789754364215    |
| train_0/q_grads           | 0.03396301884204149    |
| train_0/q_grads_std       | 0.2474606666713953     |
| train_0/q_loss            | 0.7019820351344704     |
| train_0/reward            | -0.7427790948735492    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.0932861328125        |
| train_0/target_q          | -10.461712412170055    |
| train_1/avg_q             | -9.999999348674683     |
| train_1/current_q         | -8.269067648803292     |
| train_1/fw_bonus          | -1.012999963760376     |
| train_1/fw_loss           | 0.0004949087960994802  |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.03225780175998807    |
| train_1/q_grads_std       | 0.47364596799016       |
| train_1/q_loss            | 2.619932535653628      |
| train_1/reward            | -1.3288998796371743    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 7.32421875e-05         |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.437488746824679     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 12
Time for epoch 12: 373.55. Rollout time: 157.08, Training time: 216.41
Evaluating epoch 12
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 12                     |
| policy/steps              | 404599.0               |
| test/episodes             | 325.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 1300.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.312979981633035    |
| train_0/fw_bonus          | -0.999820151925087     |
| train_0/fw_loss           | 4.163957214586844e-05  |
| train_0/mu_grads          | -0.006294529070146382  |
| train_0/mu_grads_std      | 0.1548709139227867     |
| train_0/mu_loss           | 9.617163671045681      |
| train_0/next_q            | -10.309906425567945    |
| train_0/q_grads           | 0.03353742742910981    |
| train_0/q_grads_std       | 0.2503731608390808     |
| train_0/q_loss            | 0.6594234859787227     |
| train_0/reward            | -0.7437821400177199    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.1732421875           |
| train_0/target_q          | -10.507977797125864    |
| train_1/avg_q             | -9.999919284971481     |
| train_1/current_q         | -8.24795581215029      |
| train_1/fw_bonus          | -1.0130463302135468    |
| train_1/fw_loss           | 0.0004840447225433309  |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.03310300363227725    |
| train_1/q_grads_std       | 0.47797081544995307    |
| train_1/q_loss            | 2.643545190054385      |
| train_1/reward            | -1.3253952345439757    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 4.8828125e-05          |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.412690156418979     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 13
Time for epoch 13: 383.09. Rollout time: 164.40, Training time: 218.64
Evaluating epoch 13
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 13                     |
| policy/steps              | 435753.0               |
| test/episodes             | 350.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 1400.0                 |
| train/success_rate        | 0.01                   |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.362874165238484    |
| train_0/fw_bonus          | -0.9997912898659707    |
| train_0/fw_loss           | 4.793205584974203e-05  |
| train_0/mu_grads          | -0.007292556366883219  |
| train_0/mu_grads_std      | 0.15970755629241468    |
| train_0/mu_loss           | 9.780424323118675      |
| train_0/next_q            | -10.358718043487524    |
| train_0/q_grads           | 0.03207339905202389    |
| train_0/q_grads_std       | 0.251713290810585      |
| train_0/q_loss            | 0.48053822833670035    |
| train_0/reward            | -0.7452813834432164    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.0740478515625        |
| train_0/target_q          | -10.551538248280744    |
| train_1/avg_q             | -9.993759535906463     |
| train_1/current_q         | -8.252828343567556     |
| train_1/fw_bonus          | -1.0130686730146408    |
| train_1/fw_loss           | 0.00047880729180178606 |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 997.0                  |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.03394306916743517    |
| train_1/q_grads_std       | 0.48598589599132536    |
| train_1/q_loss            | 2.490371787169871      |
| train_1/reward            | -1.2953893229248934    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 2.44140625e-05         |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.464271158862397     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 14
Time for epoch 14: 385.75. Rollout time: 163.52, Training time: 222.19
Evaluating epoch 14
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 14                     |
| policy/steps              | 467003.0               |
| test/episodes             | 375.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 1500.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.096497516232983    |
| train_0/fw_bonus          | -0.9997379183769226    |
| train_0/fw_loss           | 5.9571384872469936e-05 |
| train_0/mu_grads          | -0.008059458225034177  |
| train_0/mu_grads_std      | 0.1641540013253689     |
| train_0/mu_loss           | 9.49821121973786       |
| train_0/next_q            | -10.089508590066435    |
| train_0/q_grads           | 0.030078460229560733   |
| train_0/q_grads_std       | 0.25455085262656213    |
| train_0/q_loss            | 0.5526683571615114     |
| train_0/reward            | -0.744021784873621     |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.05087890625          |
| train_0/target_q          | -10.20929435615766     |
| train_1/avg_q             | -9.99062476068623      |
| train_1/current_q         | -8.299845162177728     |
| train_1/fw_bonus          | -1.0132028967142106    |
| train_1/fw_loss           | 0.00044736596319125963 |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.03508996181190014    |
| train_1/q_grads_std       | 0.48946180418133733    |
| train_1/q_loss            | 2.5864966201564643     |
| train_1/reward            | -1.3171849431280862    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 2.44140625e-05         |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.49085193531559      |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 15
Time for epoch 15: 381.03. Rollout time: 162.71, Training time: 218.27
Evaluating epoch 15
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 15                     |
| policy/steps              | 498253.0               |
| test/episodes             | 400.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 1600.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.098981814665676    |
| train_0/fw_bonus          | -0.9998017892241478    |
| train_0/fw_loss           | 4.5641353381142835e-05 |
| train_0/mu_grads          | -0.008010686794295906  |
| train_0/mu_grads_std      | 0.16976170614361763    |
| train_0/mu_loss           | 9.699886059981733      |
| train_0/next_q            | -10.097244227883307    |
| train_0/q_grads           | 0.029648726945742963   |
| train_0/q_grads_std       | 0.25653224214911463    |
| train_0/q_loss            | 0.5789268457921043     |
| train_0/reward            | -0.7474106593152101    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.1304443359375        |
| train_0/target_q          | -10.338323380940215    |
| train_1/avg_q             | -9.999999838559143     |
| train_1/current_q         | -8.27847999656782      |
| train_1/fw_bonus          | -1.0134094417095185    |
| train_1/fw_loss           | 0.0003989787554019131  |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.03629131084308028    |
| train_1/q_grads_std       | 0.49323693066835406    |
| train_1/q_loss            | 2.4475621056581383     |
| train_1/reward            | -1.3041168282579747    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0                    |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.485200812632979     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 16
Time for epoch 16: 388.89. Rollout time: 163.51, Training time: 225.32
Evaluating epoch 16
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 16                     |
| policy/steps              | 529491.0               |
| test/episodes             | 425.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 1700.0                 |
| train/success_rate        | 0.01                   |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -9.784808900034333     |
| train_0/fw_bonus          | -0.9998112410306931    |
| train_0/fw_loss           | 4.358086862339405e-05  |
| train_0/mu_grads          | -0.008014328102581203  |
| train_0/mu_grads_std      | 0.17530660107731819    |
| train_0/mu_loss           | 9.460659908497792      |
| train_0/next_q            | -9.77462939071881      |
| train_0/q_grads           | 0.029220737610012294   |
| train_0/q_grads_std       | 0.258848462253809      |
| train_0/q_loss            | 0.558257267098538      |
| train_0/reward            | -0.7425813995832868    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.1440673828125        |
| train_0/target_q          | -10.00629084826705     |
| train_1/avg_q             | -9.999999999997721     |
| train_1/current_q         | -8.282861818133032     |
| train_1/fw_bonus          | -1.0133341938257217    |
| train_1/fw_loss           | 0.0004166033693763893  |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.03469882691279054    |
| train_1/q_grads_std       | 0.4993018664419651     |
| train_1/q_loss            | 2.696253087493012      |
| train_1/reward            | -1.2960713994078106    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0001220703125        |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.487682727532814     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 17
Time for epoch 17: 385.13. Rollout time: 163.77, Training time: 221.30
Evaluating epoch 17
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 17                     |
| policy/steps              | 560741.0               |
| test/episodes             | 450.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 1800.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.201697024735342    |
| train_0/fw_bonus          | -0.9998324394226075    |
| train_0/fw_loss           | 3.8957957667662416e-05 |
| train_0/mu_grads          | -0.00781058482825756   |
| train_0/mu_grads_std      | 0.18090440891683102    |
| train_0/mu_loss           | 9.478061385841087      |
| train_0/next_q            | -10.199160824769354    |
| train_0/q_grads           | 0.03148804935626685    |
| train_0/q_grads_std       | 0.26325407698750497    |
| train_0/q_loss            | 0.4274238838752636     |
| train_0/reward            | -0.7424211013225431    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.153369140625         |
| train_0/target_q          | -10.391899391844762    |
| train_1/avg_q             | -9.998941443012471     |
| train_1/current_q         | -8.270310440338926     |
| train_1/fw_bonus          | -1.0134601593017578    |
| train_1/fw_loss           | 0.00038709837390342726 |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.03263913970440626    |
| train_1/q_grads_std       | 0.5067110314965249     |
| train_1/q_loss            | 2.5485503970927637     |
| train_1/reward            | -1.3188272425431933    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 2.44140625e-05         |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.452777437855696     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 18
Time for epoch 18: 393.53. Rollout time: 171.41, Training time: 222.06
Evaluating epoch 18
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 18                     |
| policy/steps              | 591991.0               |
| test/episodes             | 475.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 1900.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.30260718267349     |
| train_0/fw_bonus          | -0.9998283714056015    |
| train_0/fw_loss           | 3.984471068179119e-05  |
| train_0/mu_grads          | -0.008471395773813128  |
| train_0/mu_grads_std      | 0.18751380927860736    |
| train_0/mu_loss           | 9.432221040335524      |
| train_0/next_q            | -10.29882360985406     |
| train_0/q_grads           | 0.03214482516050339    |
| train_0/q_grads_std       | 0.269038599729538      |
| train_0/q_loss            | 0.679455933770695      |
| train_0/reward            | -0.7427852790468024    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.1182373046875        |
| train_0/target_q          | -10.488504624140159    |
| train_1/avg_q             | -9.999941621673486     |
| train_1/current_q         | -8.31433842482096      |
| train_1/fw_bonus          | -1.0135229259729386    |
| train_1/fw_loss           | 0.0003723927802639082  |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.032211856078356506   |
| train_1/q_grads_std       | 0.5127805903553962     |
| train_1/q_loss            | 2.4863961921877342     |
| train_1/reward            | -1.3139514021626382    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 7.32421875e-05         |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.50963011310014      |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 19
Time for epoch 19: 396.20. Rollout time: 165.25, Training time: 230.90
Evaluating epoch 19
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 19                     |
| policy/steps              | 623241.0               |
| test/episodes             | 500.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 2000.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.450079834822917    |
| train_0/fw_bonus          | -0.999840684235096     |
| train_0/fw_loss           | 3.716176984198682e-05  |
| train_0/mu_grads          | -0.009508190979249775  |
| train_0/mu_grads_std      | 0.194424844160676      |
| train_0/mu_loss           | 9.462446789968032      |
| train_0/next_q            | -10.443703236962728    |
| train_0/q_grads           | 0.031138621782884002   |
| train_0/q_grads_std       | 0.2717136278748512     |
| train_0/q_loss            | 0.7445919534514359     |
| train_0/reward            | -0.7448300051379192    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.193505859375         |
| train_0/target_q          | -10.645103636899204    |
| train_1/avg_q             | -9.999999999989464     |
| train_1/current_q         | -8.284929818765841     |
| train_1/fw_bonus          | -1.0135078698396682    |
| train_1/fw_loss           | 0.000375916637131013   |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.032376731652766465   |
| train_1/q_grads_std       | 0.5236051306128502     |
| train_1/q_loss            | 2.7376111384102506     |
| train_1/reward            | -1.323123568570736     |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 2.44140625e-05         |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.44750345138324      |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 20
Time for epoch 20: 402.17. Rollout time: 168.16, Training time: 233.96
Evaluating epoch 20
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 20                     |
| policy/steps              | 654491.0               |
| test/episodes             | 525.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 2100.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.533955766981283    |
| train_0/fw_bonus          | -0.9998365253210068    |
| train_0/fw_loss           | 3.8066988599894104e-05 |
| train_0/mu_grads          | -0.010195450065657497  |
| train_0/mu_grads_std      | 0.2025335218757391     |
| train_0/mu_loss           | 9.55456854915449       |
| train_0/next_q            | -10.537148954949256    |
| train_0/q_grads           | 0.031162609113380313   |
| train_0/q_grads_std       | 0.27482849135994913    |
| train_0/q_loss            | 0.6797654420806595     |
| train_0/reward            | -0.7467932613952144    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.1849609375           |
| train_0/target_q          | -10.746735559051803    |
| train_1/avg_q             | -9.990901922044355     |
| train_1/current_q         | -8.30346799952614      |
| train_1/fw_bonus          | -1.0135787099599838    |
| train_1/fw_loss           | 0.0003593240340705961  |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.03314281515777111    |
| train_1/q_grads_std       | 0.5312259122729301     |
| train_1/q_loss            | 2.5113845244065227     |
| train_1/reward            | -1.3123435515561142    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 9.765625e-05           |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.499887496868618     |
------------------------------------------------------
Saving periodic policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_20.pkl ...
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 21
Time for epoch 21: 418.77. Rollout time: 180.81, Training time: 237.90
Evaluating epoch 21
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 21                     |
| policy/steps              | 685741.0               |
| test/episodes             | 550.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 2200.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.281310830813567    |
| train_0/fw_bonus          | -0.9998672008514404    |
| train_0/fw_loss           | 3.1376615197586943e-05 |
| train_0/mu_grads          | -0.011555347079411148  |
| train_0/mu_grads_std      | 0.209952212870121      |
| train_0/mu_loss           | 9.640399699829686      |
| train_0/next_q            | -10.277472944692846    |
| train_0/q_grads           | 0.030699908593669535   |
| train_0/q_grads_std       | 0.276354606449604      |
| train_0/q_loss            | 0.5334265327188724     |
| train_0/reward            | -0.7434150473978661    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.2218505859375        |
| train_0/target_q          | -10.462246558576053    |
| train_1/avg_q             | -9.99061118908545      |
| train_1/current_q         | -8.31766094715344      |
| train_1/fw_bonus          | -1.0137353628873824    |
| train_1/fw_loss           | 0.0003226221480872482  |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.03360311593860388    |
| train_1/q_grads_std       | 0.5388909921050071     |
| train_1/q_loss            | 2.7903813483826214     |
| train_1/reward            | -1.3359326260659146    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0                    |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.491416024503417     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 22
Time for epoch 22: 430.02. Rollout time: 180.57, Training time: 249.36
Evaluating epoch 22
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 22                     |
| policy/steps              | 716954.0               |
| test/episodes             | 575.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 2300.0                 |
| train/success_rate        | 0.01                   |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.156721171782928    |
| train_0/fw_bonus          | -0.9998515173792839    |
| train_0/fw_loss           | 3.479694864836347e-05  |
| train_0/mu_grads          | -0.011808753432705998  |
| train_0/mu_grads_std      | 0.21515363603830337    |
| train_0/mu_loss           | 9.992487883545627      |
| train_0/next_q            | -10.151059802650414    |
| train_0/q_grads           | 0.029731486458331347   |
| train_0/q_grads_std       | 0.27762395814061164    |
| train_0/q_loss            | 0.5965948021565556     |
| train_0/reward            | -0.743517068138317     |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.1566650390625        |
| train_0/target_q          | -10.324372166295143    |
| train_1/avg_q             | -9.9813319285139       |
| train_1/current_q         | -8.282230863540477     |
| train_1/fw_bonus          | -1.0135716766119003    |
| train_1/fw_loss           | 0.00036097005868214184 |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 999.0                  |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.03431181162595749    |
| train_1/q_grads_std       | 0.5463363036513329     |
| train_1/q_loss            | 2.7116724417716718     |
| train_1/reward            | -1.3355333985135076    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0                    |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.432159375076012     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 23
Time for epoch 23: 425.88. Rollout time: 189.15, Training time: 236.66
Evaluating epoch 23
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 23                     |
| policy/steps              | 748119.0               |
| test/episodes             | 600.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 2400.0                 |
| train/success_rate        | 0.01                   |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.378659925121948    |
| train_0/fw_bonus          | -0.9998485162854195    |
| train_0/fw_loss           | 3.5452824113235694e-05 |
| train_0/mu_grads          | -0.012081600492820144  |
| train_0/mu_grads_std      | 0.21912398114800452    |
| train_0/mu_loss           | 10.1693018349845       |
| train_0/next_q            | -10.379060935736888    |
| train_0/q_grads           | 0.02913768175058067    |
| train_0/q_grads_std       | 0.28029866963624955    |
| train_0/q_loss            | 0.643313273067617      |
| train_0/reward            | -0.7452001346420729    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.1534423828125        |
| train_0/target_q          | -10.58413052821128     |
| train_1/avg_q             | -9.990535153290258     |
| train_1/current_q         | -8.2615103430885       |
| train_1/fw_bonus          | -1.0137226969003676    |
| train_1/fw_loss           | 0.00032558916354901156 |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 997.0                  |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.03463309928774834    |
| train_1/q_grads_std       | 0.5544021770358085     |
| train_1/q_loss            | 2.5956760660593665     |
| train_1/reward            | -1.3178476803630474    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0                    |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.437203149113051     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 24
Time for epoch 24: 356.56. Rollout time: 148.82, Training time: 207.68
Evaluating epoch 24
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 24                     |
| policy/steps              | 779369.0               |
| test/episodes             | 625.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 2500.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.439796649965862    |
| train_0/fw_bonus          | -0.9998690605163574    |
| train_0/fw_loss           | 3.0973189359428944e-05 |
| train_0/mu_grads          | -0.012776541500352323  |
| train_0/mu_grads_std      | 0.22188055627048014    |
| train_0/mu_loss           | 10.20425034728842      |
| train_0/next_q            | -10.438683334282917    |
| train_0/q_grads           | 0.029391463659703732   |
| train_0/q_grads_std       | 0.28248831406235697    |
| train_0/q_loss            | 0.7926465143170603     |
| train_0/reward            | -0.7437333553061762    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.175537109375         |
| train_0/target_q          | -10.644398333749757    |
| train_1/avg_q             | -9.994805899863534     |
| train_1/current_q         | -8.213667533777999     |
| train_1/fw_bonus          | -1.0139151692390442    |
| train_1/fw_loss           | 0.000280497745188768   |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.03526089293882251    |
| train_1/q_grads_std       | 0.5625802755355835     |
| train_1/q_loss            | 2.455318173099715      |
| train_1/reward            | -1.323205148735724     |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0                    |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.382985422173226     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 25
Time for epoch 25: 7601.36. Rollout time: 7086.28, Training time: 514.87
Evaluating epoch 25
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 25                     |
| policy/steps              | 810619.0               |
| test/episodes             | 650.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 2600.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.269625204111176    |
| train_0/fw_bonus          | -0.9998867884278297    |
| train_0/fw_loss           | 2.710757498789462e-05  |
| train_0/mu_grads          | -0.013438718463294207  |
| train_0/mu_grads_std      | 0.22424700558185579    |
| train_0/mu_loss           | 10.021129413145895     |
| train_0/next_q            | -10.267561767567754    |
| train_0/q_grads           | 0.02969652656465769    |
| train_0/q_grads_std       | 0.2855688326060772     |
| train_0/q_loss            | 0.7842768629649026     |
| train_0/reward            | -0.7437178049687645    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.2533447265625        |
| train_0/target_q          | -10.47111109346871     |
| train_1/avg_q             | -9.999999999990818     |
| train_1/current_q         | -8.229923655380393     |
| train_1/fw_bonus          | -1.013751021027565     |
| train_1/fw_loss           | 0.0003189558497979306  |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.03471623277291656    |
| train_1/q_grads_std       | 0.5704882547259331     |
| train_1/q_loss            | 2.591894576461841      |
| train_1/reward            | -1.3198149627547537    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0                    |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.405674337754757     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 26
Time for epoch 26: 14478.78. Rollout time: 1244.86, Training time: 13233.78
Evaluating epoch 26
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 26                     |
| policy/steps              | 841700.0               |
| test/episodes             | 675.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 2700.0                 |
| train/success_rate        | 0.01                   |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.008758981764498    |
| train_0/fw_bonus          | -0.9998730629682541    |
| train_0/fw_loss           | 3.009987649420509e-05  |
| train_0/mu_grads          | -0.014079646649770438  |
| train_0/mu_grads_std      | 0.22664619721472262    |
| train_0/mu_loss           | 9.99168335977849       |
| train_0/next_q            | -10.007353712051943    |
| train_0/q_grads           | 0.02976403278298676    |
| train_0/q_grads_std       | 0.28851408511400223    |
| train_0/q_loss            | 0.4206064239253526     |
| train_0/reward            | -0.7435400452603063    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.205859375            |
| train_0/target_q          | -10.195567555212767    |
| train_1/avg_q             | -9.99999999999993      |
| train_1/current_q         | -8.230845666107204     |
| train_1/fw_bonus          | -1.0138921558856964    |
| train_1/fw_loss           | 0.0002858892799849855  |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 994.0                  |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.03523711040616036    |
| train_1/q_grads_std       | 0.5795938089489937     |
| train_1/q_loss            | 2.543606571674689      |
| train_1/reward            | -1.3235354380762145    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 2.44140625e-05         |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.404370399013718     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 27
Time for epoch 27: 349.29. Rollout time: 144.85, Training time: 204.39
Evaluating epoch 27
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 27                     |
| policy/steps              | 872950.0               |
| test/episodes             | 700.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 2800.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.273028017270962    |
| train_0/fw_bonus          | -0.9998608022928238    |
| train_0/fw_loss           | 3.2773281236586624e-05 |
| train_0/mu_grads          | -0.014413852361030877  |
| train_0/mu_grads_std      | 0.2293980974704027     |
| train_0/mu_loss           | 10.25569563479456      |
| train_0/next_q            | -10.274711859444562    |
| train_0/q_grads           | 0.030643830308690667   |
| train_0/q_grads_std       | 0.2917982444167137     |
| train_0/q_loss            | 0.6516180728879997     |
| train_0/reward            | -0.7430944620617084    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.1642333984375        |
| train_0/target_q          | -10.438083431070329    |
| train_1/avg_q             | -9.992148536444514     |
| train_1/current_q         | -8.271566216062986     |
| train_1/fw_bonus          | -1.013963559269905     |
| train_1/fw_loss           | 0.0002691643090656726  |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.034269300382584335   |
| train_1/q_grads_std       | 0.5872478350996971     |
| train_1/q_loss            | 2.870500062192637      |
| train_1/reward            | -1.308947111015732     |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0                    |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.452467618828235     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 28
Time for epoch 28: 4847.09. Rollout time: 2001.89, Training time: 2845.13
Evaluating epoch 28
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 28                     |
| policy/steps              | 904200.0               |
| test/episodes             | 725.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 2900.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.31093653105614     |
| train_0/fw_bonus          | -0.9998436689376831    |
| train_0/fw_loss           | 3.650839016700047e-05  |
| train_0/mu_grads          | -0.013998941937461495  |
| train_0/mu_grads_std      | 0.230799912661314      |
| train_0/mu_loss           | 10.294418495937624     |
| train_0/next_q            | -10.30972960065109     |
| train_0/q_grads           | 0.030685562547296284   |
| train_0/q_grads_std       | 0.2942732870578766     |
| train_0/q_loss            | 0.7081263868507256     |
| train_0/reward            | -0.7431225189793622    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.1284912109375        |
| train_0/target_q          | -10.513591955085849    |
| train_1/avg_q             | -9.999999995809272     |
| train_1/current_q         | -8.278821629625105     |
| train_1/fw_bonus          | -1.013913205265999     |
| train_1/fw_loss           | 0.0002809602221532259  |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.03330396246165037    |
| train_1/q_grads_std       | 0.5947490230202674     |
| train_1/q_loss            | 2.448454635832548      |
| train_1/reward            | -1.3225327745960385    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0                    |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.460071837096041     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 29
Time for epoch 29: 1361.53. Rollout time: 638.46, Training time: 722.80
Evaluating epoch 29
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 29                     |
| policy/steps              | 935437.0               |
| test/episodes             | 750.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 3000.0                 |
| train/success_rate        | 0.01                   |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.500177188078364    |
| train_0/fw_bonus          | -0.9998897239565849    |
| train_0/fw_loss           | 2.6465725522939464e-05 |
| train_0/mu_grads          | -0.01393241211771965   |
| train_0/mu_grads_std      | 0.23312588669359685    |
| train_0/mu_loss           | 10.584379392936734     |
| train_0/next_q            | -10.497922878322449    |
| train_0/q_grads           | 0.03130400227382779    |
| train_0/q_grads_std       | 0.2977505996823311     |
| train_0/q_loss            | 0.4783223677396773     |
| train_0/reward            | -0.7446592180589505    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.2146484375           |
| train_0/target_q          | -10.683944096898056    |
| train_1/avg_q             | -9.999999999735577     |
| train_1/current_q         | -8.25675197991641      |
| train_1/fw_bonus          | -1.0139015197753907    |
| train_1/fw_loss           | 0.0002836919218680123  |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.032309741526842115   |
| train_1/q_grads_std       | 0.604163010418415      |
| train_1/q_loss            | 2.7555063815875416     |
| train_1/reward            | -1.312932218358037     |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0                    |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.441857999608041     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 30
Time for epoch 30: 1617.21. Rollout time: 782.58, Training time: 834.35
Evaluating epoch 30
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 30                     |
| policy/steps              | 966687.0               |
| test/episodes             | 775.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 3100.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.10921231716328     |
| train_0/fw_bonus          | -0.9998863026499748    |
| train_0/fw_loss           | 2.7214278998144436e-05 |
| train_0/mu_grads          | -0.01393097611144185   |
| train_0/mu_grads_std      | 0.2361164029687643     |
| train_0/mu_loss           | 10.23070751399833      |
| train_0/next_q            | -10.105443416919107    |
| train_0/q_grads           | 0.02988863466307521    |
| train_0/q_grads_std       | 0.3002363033592701     |
| train_0/q_loss            | 0.3634545049976108     |
| train_0/reward            | -0.7437908025858633    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.229638671875         |
| train_0/target_q          | -10.252491384550927    |
| train_1/avg_q             | -9.999999987216677     |
| train_1/current_q         | -8.266798057403703     |
| train_1/fw_bonus          | -1.014094814658165     |
| train_1/fw_loss           | 0.00023841352558520156 |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.031556913256645204   |
| train_1/q_grads_std       | 0.6139109253883361     |
| train_1/q_loss            | 2.4949185447664672     |
| train_1/reward            | -1.3165200661991547    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0                    |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.458844284949157     |
------------------------------------------------------
Saving periodic policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_30.pkl ...
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 31
Time for epoch 31: 6712.48. Rollout time: 6473.34, Training time: 239.05
Evaluating epoch 31
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 31                     |
| policy/steps              | 997937.0               |
| test/episodes             | 800.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 3200.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.297192400386454    |
| train_0/fw_bonus          | -0.9998907715082168    |
| train_0/fw_loss           | 2.6238724399263446e-05 |
| train_0/mu_grads          | -0.013834693538956343  |
| train_0/mu_grads_std      | 0.23960912562906742    |
| train_0/mu_loss           | 10.259552676932476     |
| train_0/next_q            | -10.290705161729292    |
| train_0/q_grads           | 0.030522336764261127   |
| train_0/q_grads_std       | 0.3040593914687634     |
| train_0/q_loss            | 0.6504062241825476     |
| train_0/reward            | -0.7411929418645741    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.2226806640625        |
| train_0/target_q          | -10.518396878458528    |
| train_1/avg_q             | -9.999979771666364     |
| train_1/current_q         | -8.22596344792772      |
| train_1/fw_bonus          | -1.0140715688467026    |
| train_1/fw_loss           | 0.00024385702417930588 |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.03153103180229664    |
| train_1/q_grads_std       | 0.6235122397542        |
| train_1/q_loss            | 2.4667925742869414     |
| train_1/reward            | -1.3183272957590817    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0                    |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.410168116071585     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 32
Time for epoch 32: 397.34. Rollout time: 165.74, Training time: 231.49
Evaluating epoch 32
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 32                     |
| policy/steps              | 1029187.0              |
| test/episodes             | 825.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 3300.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.44240998637918     |
| train_0/fw_bonus          | -0.999903267621994     |
| train_0/fw_loss           | 2.3513445057687932e-05 |
| train_0/mu_grads          | -0.01344137389678508   |
| train_0/mu_grads_std      | 0.24750535376369953    |
| train_0/mu_loss           | 10.337311967698025     |
| train_0/next_q            | -10.44531005371        |
| train_0/q_grads           | 0.031011086562648414   |
| train_0/q_grads_std       | 0.30756414234638213    |
| train_0/q_loss            | 0.7895811236850587     |
| train_0/reward            | -0.7447392297035549    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.257421875            |
| train_0/target_q          | -10.63486021526593     |
| train_1/avg_q             | -9.999999999985086     |
| train_1/current_q         | -8.275749262925094     |
| train_1/fw_bonus          | -1.0141966104507447    |
| train_1/fw_loss           | 0.00021456694812513888 |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.03045120444148779    |
| train_1/q_grads_std       | 0.6331355929374695     |
| train_1/q_loss            | 2.436965955067418      |
| train_1/reward            | -1.333099281239265     |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 4.8828125e-05          |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.452454749989268     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 33
Time for epoch 33: 396.79. Rollout time: 163.79, Training time: 232.93
Evaluating epoch 33
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 33                     |
| policy/steps              | 1060437.0              |
| test/episodes             | 850.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 3400.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.417876838827642    |
| train_0/fw_bonus          | -0.9999006524682045    |
| train_0/fw_loss           | 2.4083834000521163e-05 |
| train_0/mu_grads          | -0.014089083299040794  |
| train_0/mu_grads_std      | 0.255131646245718      |
| train_0/mu_loss           | 9.710820279854662      |
| train_0/next_q            | -10.417881073153984    |
| train_0/q_grads           | 0.03198759537190199    |
| train_0/q_grads_std       | 0.31122113168239596    |
| train_0/q_loss            | 0.5831489397306117     |
| train_0/reward            | -0.7439322578378779    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.2020751953125        |
| train_0/target_q          | -10.61742678474145     |
| train_1/avg_q             | -9.999999999999957     |
| train_1/current_q         | -8.262723742597537     |
| train_1/fw_bonus          | -1.0142025828361512    |
| train_1/fw_loss           | 0.0002131667384674074  |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.030113565176725386   |
| train_1/q_grads_std       | 0.640921114385128      |
| train_1/q_loss            | 2.6520970709139737     |
| train_1/reward            | -1.339791723453527     |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 7.32421875e-05         |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.41799484845353      |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 34
Time for epoch 34: 379.15. Rollout time: 161.77, Training time: 217.31
Evaluating epoch 34
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 34                     |
| policy/steps              | 1091687.0              |
| test/episodes             | 875.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 3500.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.154369301556537    |
| train_0/fw_bonus          | -0.9999115109443665    |
| train_0/fw_loss           | 2.1714747549594905e-05 |
| train_0/mu_grads          | -0.016487461095675827  |
| train_0/mu_grads_std      | 0.2623975194990635     |
| train_0/mu_loss           | 10.077587197367915     |
| train_0/next_q            | -10.157296980936561    |
| train_0/q_grads           | 0.03271875567734241    |
| train_0/q_grads_std       | 0.31475368440151213    |
| train_0/q_loss            | 0.6976223364284675     |
| train_0/reward            | -0.7416183540473866    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.278564453125         |
| train_0/target_q          | -10.345893085793303    |
| train_1/avg_q             | -10.0                  |
| train_1/current_q         | -8.274733886927649     |
| train_1/fw_bonus          | -1.0140281945466996    |
| train_1/fw_loss           | 0.00025402264691365417 |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.029542275704443455   |
| train_1/q_grads_std       | 0.6511345103383064     |
| train_1/q_loss            | 2.5128180396006323     |
| train_1/reward            | -1.3556224942265545    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 2.44140625e-05         |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.427604916101558     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 35
Time for epoch 35: 442.07. Rollout time: 195.91, Training time: 246.05
Evaluating epoch 35
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 35                     |
| policy/steps              | 1122937.0              |
| test/episodes             | 900.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 3600.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.389797407508294    |
| train_0/fw_bonus          | -0.9999012365937233    |
| train_0/fw_loss           | 2.39558384009797e-05   |
| train_0/mu_grads          | -0.017425579112023117  |
| train_0/mu_grads_std      | 0.2708112023770809     |
| train_0/mu_loss           | 10.362841766582278     |
| train_0/next_q            | -10.381536000083846    |
| train_0/q_grads           | 0.03296362366527319    |
| train_0/q_grads_std       | 0.3178934819996357     |
| train_0/q_loss            | 0.8287563031466822     |
| train_0/reward            | -0.7419428050015995    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.2130126953125        |
| train_0/target_q          | -10.560381696324756    |
| train_1/avg_q             | -9.981007514905444     |
| train_1/current_q         | -8.268635840052676     |
| train_1/fw_bonus          | -1.0141550540924071    |
| train_1/fw_loss           | 0.00022429960772569758 |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.027806539461016654   |
| train_1/q_grads_std       | 0.6613638907670975     |
| train_1/q_loss            | 2.526374428275136      |
| train_1/reward            | -1.3394017519756745    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0                    |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.431960345725678     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 36
Time for epoch 36: 485.20. Rollout time: 227.60, Training time: 257.45
Evaluating epoch 36
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 36                     |
| policy/steps              | 1154187.0              |
| test/episodes             | 925.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 3700.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.29488235478775     |
| train_0/fw_bonus          | -0.9999214425683022    |
| train_0/fw_loss           | 1.9547360761862364e-05 |
| train_0/mu_grads          | -0.01810912457294762   |
| train_0/mu_grads_std      | 0.27889650985598563    |
| train_0/mu_loss           | 9.847514909328728      |
| train_0/next_q            | -10.295216137086447    |
| train_0/q_grads           | 0.03297093091532588    |
| train_0/q_grads_std       | 0.3198124147951603     |
| train_0/q_loss            | 0.5430752183337871     |
| train_0/reward            | -0.7424167806806509    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.2937255859375        |
| train_0/target_q          | -10.463789286118573    |
| train_1/avg_q             | -9.999978957383872     |
| train_1/current_q         | -8.237071691033659     |
| train_1/fw_bonus          | -1.01421160697937      |
| train_1/fw_loss           | 0.0002110547233314719  |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.028401938639581202   |
| train_1/q_grads_std       | 0.6684946089982986     |
| train_1/q_loss            | 2.52436517129458       |
| train_1/reward            | -1.3268811915033438    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 2.44140625e-05         |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.417525722753348     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 37
Time for epoch 37: 406.83. Rollout time: 181.40, Training time: 225.34
Evaluating epoch 37
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 37                     |
| policy/steps              | 1185437.0              |
| test/episodes             | 950.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 3800.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.115115697983127    |
| train_0/fw_bonus          | -0.9999086201190949    |
| train_0/fw_loss           | 2.234497305835248e-05  |
| train_0/mu_grads          | -0.01676518488675356   |
| train_0/mu_grads_std      | 0.2872003696858883     |
| train_0/mu_loss           | 9.558035956210428      |
| train_0/next_q            | -10.116992275480985    |
| train_0/q_grads           | 0.032404749747365715   |
| train_0/q_grads_std       | 0.32177000045776366    |
| train_0/q_loss            | 0.9747099285619301     |
| train_0/reward            | -0.7428742329837406    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.27216796875          |
| train_0/target_q          | -10.37462229870614     |
| train_1/avg_q             | -9.990264240552532     |
| train_1/current_q         | -8.251664720089039     |
| train_1/fw_bonus          | -1.0142672121524812    |
| train_1/fw_loss           | 0.00019802168826572598 |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.02793033830821514    |
| train_1/q_grads_std       | 0.6751827716827392     |
| train_1/q_loss            | 2.398621808265729      |
| train_1/reward            | -1.3223801997868576    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 2.44140625e-05         |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.43671125447436      |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 38
Time for epoch 38: 482.65. Rollout time: 213.96, Training time: 268.59
Evaluating epoch 38
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 38                     |
| policy/steps              | 1216687.0              |
| test/episodes             | 975.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 3900.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.380436697020647    |
| train_0/fw_bonus          | -0.9998861744999885    |
| train_0/fw_loss           | 2.724074870457116e-05  |
| train_0/mu_grads          | -0.018153683841228486  |
| train_0/mu_grads_std      | 0.295989865064621      |
| train_0/mu_loss           | 9.788923941976952      |
| train_0/next_q            | -10.378440627243007    |
| train_0/q_grads           | 0.033010983001440765   |
| train_0/q_grads_std       | 0.3248648479580879     |
| train_0/q_loss            | 0.6947743885554287     |
| train_0/reward            | -0.741300507507549     |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.2128662109375        |
| train_0/target_q          | -10.566048764980458    |
| train_1/avg_q             | -9.990101993577252     |
| train_1/current_q         | -8.233121587273368     |
| train_1/fw_bonus          | -1.0142630875110625    |
| train_1/fw_loss           | 0.0001989940385101363  |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.027669529989361764   |
| train_1/q_grads_std       | 0.6820684880018234     |
| train_1/q_loss            | 2.578384651144555      |
| train_1/reward            | -1.3264674792771984    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 2.44140625e-05         |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.406106151152203     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 39
Time for epoch 39: 557.19. Rollout time: 261.77, Training time: 295.29
Evaluating epoch 39
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 39                     |
| policy/steps              | 1247927.0              |
| test/episodes             | 1000.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 4000.0                 |
| train/success_rate        | 0.01                   |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.370075453007697    |
| train_0/fw_bonus          | -0.9999122008681297    |
| train_0/fw_loss           | 2.1566986106336117e-05 |
| train_0/mu_grads          | -0.018471000762656332  |
| train_0/mu_grads_std      | 0.3046168014407158     |
| train_0/mu_loss           | 9.966929594440467      |
| train_0/next_q            | -10.372215531667768    |
| train_0/q_grads           | 0.03249771688133478    |
| train_0/q_grads_std       | 0.3279298827052116     |
| train_0/q_loss            | 1.0267051920172439     |
| train_0/reward            | -0.7427783607090532    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.2511474609375        |
| train_0/target_q          | -10.568435480988512    |
| train_1/avg_q             | -9.991057648439375     |
| train_1/current_q         | -8.258749298374997     |
| train_1/fw_bonus          | -1.0141957074403762    |
| train_1/fw_loss           | 0.00021477402187883854 |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.026448920788243412   |
| train_1/q_grads_std       | 0.6874805808067321     |
| train_1/q_loss            | 2.3793265428684074     |
| train_1/reward            | -1.3298029410230812    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 2.44140625e-05         |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.434802941023085     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 40
Time for epoch 40: 452.21. Rollout time: 211.46, Training time: 240.67
Evaluating epoch 40
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 40                     |
| policy/steps              | 1279177.0              |
| test/episodes             | 1025.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 4100.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.230701831189577    |
| train_0/fw_bonus          | -0.9999242007732392    |
| train_0/fw_loss           | 1.8946886257253936e-05 |
| train_0/mu_grads          | -0.01802460146136582   |
| train_0/mu_grads_std      | 0.31188385784626005    |
| train_0/mu_loss           | 9.320005145981565      |
| train_0/next_q            | -10.230012572289606    |
| train_0/q_grads           | 0.03257898194715381    |
| train_0/q_grads_std       | 0.3294725053012371     |
| train_0/q_loss            | 0.53049097376998       |
| train_0/reward            | -0.7422838802827755    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.3144287109375        |
| train_0/target_q          | -10.398434643398563    |
| train_1/avg_q             | -9.997879312649017     |
| train_1/current_q         | -8.233538877547181     |
| train_1/fw_bonus          | -1.0143291026353836    |
| train_1/fw_loss           | 0.00018352952793065924 |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.02631757645867765    |
| train_1/q_grads_std       | 0.6937320277094841     |
| train_1/q_loss            | 2.3534049240944896     |
| train_1/reward            | -1.325865719162539     |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0                    |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.412203609787543     |
------------------------------------------------------
Saving periodic policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_40.pkl ...
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 41
Time for epoch 41: 477.14. Rollout time: 217.72, Training time: 259.30
Evaluating epoch 41
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 41                     |
| policy/steps              | 1310427.0              |
| test/episodes             | 1050.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 4200.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.394598465230816    |
| train_0/fw_bonus          | -0.9999004319310189    |
| train_0/fw_loss           | 2.4131409986694052e-05 |
| train_0/mu_grads          | -0.01796345254406333   |
| train_0/mu_grads_std      | 0.31877151057124137    |
| train_0/mu_loss           | 9.521286343036065      |
| train_0/next_q            | -10.39316014492853     |
| train_0/q_grads           | 0.03193013146519661    |
| train_0/q_grads_std       | 0.3311151094734669     |
| train_0/q_loss            | 0.44756017565455863    |
| train_0/reward            | -0.7423818556395417    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.165966796875         |
| train_0/target_q          | -10.55657200983389     |
| train_1/avg_q             | -10.0                  |
| train_1/current_q         | -8.293397056992616     |
| train_1/fw_bonus          | -1.0143236130475999    |
| train_1/fw_loss           | 0.0001848088581027696  |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.02568839625455439    |
| train_1/q_grads_std       | 0.7002797320485115     |
| train_1/q_loss            | 2.513403424967587      |
| train_1/reward            | -1.3354455074426368    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 2.44140625e-05         |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.45408320275514      |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 42
Time for epoch 42: 611.49. Rollout time: 283.22, Training time: 328.18
Evaluating epoch 42
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 42                     |
| policy/steps              | 1341677.0              |
| test/episodes             | 1075.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 4300.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.26263689857918     |
| train_0/fw_bonus          | -0.9999200060963631    |
| train_0/fw_loss           | 1.9861984378621854e-05 |
| train_0/mu_grads          | -0.018057471280917524  |
| train_0/mu_grads_std      | 0.32537279576063155    |
| train_0/mu_loss           | 9.246569684175025      |
| train_0/next_q            | -10.257251966234923    |
| train_0/q_grads           | 0.03174481596797705    |
| train_0/q_grads_std       | 0.3322340726852417     |
| train_0/q_loss            | 1.083769579608076      |
| train_0/reward            | -0.7427578560869733    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.2819580078125        |
| train_0/target_q          | -10.463445390303036    |
| train_1/avg_q             | -9.990712344727354     |
| train_1/current_q         | -8.247931132561993     |
| train_1/fw_bonus          | -1.0143762916326522    |
| train_1/fw_loss           | 0.00017247153773496392 |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.0250428460072726     |
| train_1/q_grads_std       | 0.7063123837113381     |
| train_1/q_loss            | 2.485979069338284      |
| train_1/reward            | -1.3249615854889272    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0                    |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.42661197611393      |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 43
Time for epoch 43: 750.79. Rollout time: 360.68, Training time: 389.97
Evaluating epoch 43
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 43                     |
| policy/steps              | 1372927.0              |
| test/episodes             | 1100.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 4400.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.189312964911196    |
| train_0/fw_bonus          | -0.9999259248375892    |
| train_0/fw_loss           | 1.8573171496427675e-05 |
| train_0/mu_grads          | -0.017552967788651584  |
| train_0/mu_grads_std      | 0.3330055058002472     |
| train_0/mu_loss           | 9.376216746640955      |
| train_0/next_q            | -10.186462225594278    |
| train_0/q_grads           | 0.031914154160767795   |
| train_0/q_grads_std       | 0.3352773658931255     |
| train_0/q_loss            | 0.6347089000385389     |
| train_0/reward            | -0.7432612997945398    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.29794921875          |
| train_0/target_q          | -10.375137010004746    |
| train_1/avg_q             | -9.990112861691825     |
| train_1/current_q         | -8.260945461496528     |
| train_1/fw_bonus          | -1.0142455250024796    |
| train_1/fw_loss           | 0.00020310756299295462 |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.023913027439266443   |
| train_1/q_grads_std       | 0.7141945794224739     |
| train_1/q_loss            | 2.4791392989166297     |
| train_1/reward            | -1.350079305383406     |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0                    |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.41177364132091      |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 44
Time for epoch 44: 713.01. Rollout time: 340.95, Training time: 371.86
Evaluating epoch 44
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 44                     |
| policy/steps              | 1404177.0              |
| test/episodes             | 1125.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 4500.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.475844674880358    |
| train_0/fw_bonus          | -0.9999344199895859    |
| train_0/fw_loss           | 1.6719589211788843e-05 |
| train_0/mu_grads          | -0.01479535058606416   |
| train_0/mu_grads_std      | 0.3371154569089413     |
| train_0/mu_loss           | 9.285133558589186      |
| train_0/next_q            | -10.47394608689316     |
| train_0/q_grads           | 0.03255690122023225    |
| train_0/q_grads_std       | 0.338849763572216      |
| train_0/q_loss            | 0.5109822530634054     |
| train_0/reward            | -0.7453163311547542    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.32861328125          |
| train_0/target_q          | -10.63621436636589     |
| train_1/avg_q             | -9.99999999999999      |
| train_1/current_q         | -8.242102729770503     |
| train_1/fw_bonus          | -1.0141764163970948    |
| train_1/fw_loss           | 0.00021929889408056624 |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.02250769534148276    |
| train_1/q_grads_std       | 0.7220655307173729     |
| train_1/q_loss            | 2.5470755971329067     |
| train_1/reward            | -1.3334798073425191    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0                    |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.403069651092522     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 45
Time for epoch 45: 925.09. Rollout time: 440.72, Training time: 484.21
Evaluating epoch 45
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 45                     |
| policy/steps              | 1435427.0              |
| test/episodes             | 1150.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 4600.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.347550738049435    |
| train_0/fw_bonus          | -0.999914002418518     |
| train_0/fw_loss           | 2.11738880352641e-05   |
| train_0/mu_grads          | -0.013546757237054407  |
| train_0/mu_grads_std      | 0.3433461368083954     |
| train_0/mu_loss           | 9.436485397010422      |
| train_0/next_q            | -10.341831030584189    |
| train_0/q_grads           | 0.031962848734110594   |
| train_0/q_grads_std       | 0.339849853515625      |
| train_0/q_loss            | 0.4707783607859911     |
| train_0/reward            | -0.7450375628905022    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.2687744140625        |
| train_0/target_q          | -10.500679695284045    |
| train_1/avg_q             | -9.991197861829097     |
| train_1/current_q         | -8.249998934088314     |
| train_1/fw_bonus          | -1.014321517944336     |
| train_1/fw_loss           | 0.0001853020574344555  |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.021371628763154148   |
| train_1/q_grads_std       | 0.7294523298740387     |
| train_1/q_loss            | 2.4607137424540246     |
| train_1/reward            | -1.331916623502184     |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0                    |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.422082639127186     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 46
Time for epoch 46: 713.92. Rollout time: 359.66, Training time: 354.11
Evaluating epoch 46
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 46                     |
| policy/steps              | 1466677.0              |
| test/episodes             | 1175.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 4700.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.509037426694949    |
| train_0/fw_bonus          | -0.999922651052475     |
| train_0/fw_loss           | 1.9283246911072637e-05 |
| train_0/mu_grads          | -0.01499245841987431   |
| train_0/mu_grads_std      | 0.35008271113038064    |
| train_0/mu_loss           | 9.6212451158241        |
| train_0/next_q            | -10.50931819451982     |
| train_0/q_grads           | 0.031701257452368736   |
| train_0/q_grads_std       | 0.3416001424193382     |
| train_0/q_loss            | 0.680283696935492      |
| train_0/reward            | -0.7426126989412296    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.32802734375          |
| train_0/target_q          | -10.694568961732992    |
| train_1/avg_q             | -9.990497804290321     |
| train_1/current_q         | -8.228203596844551     |
| train_1/fw_bonus          | -1.0142237365245819    |
| train_1/fw_loss           | 0.00020821746184083168 |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.020672096917405725   |
| train_1/q_grads_std       | 0.7375966414809227     |
| train_1/q_loss            | 2.5048942307205158     |
| train_1/reward            | -1.3300488682805736    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0                    |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.398920938593076     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 47
Time for epoch 47: 681.49. Rollout time: 313.92, Training time: 367.45
Evaluating epoch 47
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 47                     |
| policy/steps              | 1497927.0              |
| test/episodes             | 1200.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 4800.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.255170001913061    |
| train_0/fw_bonus          | -0.9999143078923225    |
| train_0/fw_loss           | 2.11073971058795e-05   |
| train_0/mu_grads          | -0.017629245296120644  |
| train_0/mu_grads_std      | 0.3559422709047794     |
| train_0/mu_loss           | 9.672717054737058      |
| train_0/next_q            | -10.254277345966504    |
| train_0/q_grads           | 0.031972544733434916   |
| train_0/q_grads_std       | 0.34379917159676554    |
| train_0/q_loss            | 0.5225369455636969     |
| train_0/reward            | -0.741827266242035     |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.3072021484375        |
| train_0/target_q          | -10.412209156209238    |
| train_1/avg_q             | -9.999999984657348     |
| train_1/current_q         | -8.29388248090911      |
| train_1/fw_bonus          | -1.0143389999866486    |
| train_1/fw_loss           | 0.00018120861604984385 |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.019655936304479836   |
| train_1/q_grads_std       | 0.7473828926682472     |
| train_1/q_loss            | 2.4830127180248676     |
| train_1/reward            | -1.3374551451292063    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0                    |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.46901276231671      |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 48
Time for epoch 48: 2623.75. Rollout time: 386.98, Training time: 2236.64
Evaluating epoch 48
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 48                     |
| policy/steps              | 1529177.0              |
| test/episodes             | 1225.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 4900.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.344581497956531    |
| train_0/fw_bonus          | -0.9999286532402039    |
| train_0/fw_loss           | 1.7976215053749912e-05 |
| train_0/mu_grads          | -0.01878852555528283   |
| train_0/mu_grads_std      | 0.3614943377673626     |
| train_0/mu_loss           | 9.180393869544286      |
| train_0/next_q            | -10.342539440785966    |
| train_0/q_grads           | 0.03265503952279687    |
| train_0/q_grads_std       | 0.3459932379424572     |
| train_0/q_loss            | 0.6199986444664256     |
| train_0/reward            | -0.7424325903630233    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.2768798828125        |
| train_0/target_q          | -10.563504888098581    |
| train_1/avg_q             | -9.99999898273982      |
| train_1/current_q         | -8.310211524712923     |
| train_1/fw_bonus          | -1.0143004477024078    |
| train_1/fw_loss           | 0.00019023802524316126 |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.01904161893762648    |
| train_1/q_grads_std       | 0.7575654581189155     |
| train_1/q_loss            | 2.43421824243295       |
| train_1/reward            | -1.3447967136715306    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0                    |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.479464682421533     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 49
Time for epoch 49: 783.66. Rollout time: 377.84, Training time: 405.59
Evaluating epoch 49
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 49                     |
| policy/steps              | 1560427.0              |
| test/episodes             | 1250.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 5000.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.185270343757049    |
| train_0/fw_bonus          | -0.9999210014939308    |
| train_0/fw_loss           | 1.9645317911454187e-05 |
| train_0/mu_grads          | -0.02057264088653028   |
| train_0/mu_grads_std      | 0.36662106662988664    |
| train_0/mu_loss           | 9.174776602874823      |
| train_0/next_q            | -10.184543727194436    |
| train_0/q_grads           | 0.03291650805622339    |
| train_0/q_grads_std       | 0.3478683322668076     |
| train_0/q_loss            | 0.4995573378146204     |
| train_0/reward            | -0.7421235872898251    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.2988037109375        |
| train_0/target_q          | -10.385403917734221    |
| train_1/avg_q             | -9.999999869611894     |
| train_1/current_q         | -8.255336055933261     |
| train_1/fw_bonus          | -1.0143108785152435    |
| train_1/fw_loss           | 0.00018779704696498812 |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.019173416821286083   |
| train_1/q_grads_std       | 0.7651012375950813     |
| train_1/q_loss            | 2.5346827830562804     |
| train_1/reward            | -1.3334493843707604    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0                    |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.425050946870764     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 50
Time for epoch 50: 792.21. Rollout time: 389.97, Training time: 402.03
Evaluating epoch 50
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 50                     |
| policy/steps              | 1591677.0              |
| test/episodes             | 1275.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 5100.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.491181566342522    |
| train_0/fw_bonus          | -0.9999012261629104    |
| train_0/fw_loss           | 2.396065167431516e-05  |
| train_0/mu_grads          | -0.01977883051149547   |
| train_0/mu_grads_std      | 0.3732760936021805     |
| train_0/mu_loss           | 9.420476130812709      |
| train_0/next_q            | -10.49396974126535     |
| train_0/q_grads           | 0.032904007472097874   |
| train_0/q_grads_std       | 0.3492568895220757     |
| train_0/q_loss            | 0.5215658770058335     |
| train_0/reward            | -0.7442133019474568    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.21123046875          |
| train_0/target_q          | -10.677048024092265    |
| train_1/avg_q             | -9.99999587531402      |
| train_1/current_q         | -8.313440216467182     |
| train_1/fw_bonus          | -1.0143522500991822    |
| train_1/fw_loss           | 0.00017810045792430174 |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.018327192589640617   |
| train_1/q_grads_std       | 0.773539912700653      |
| train_1/q_loss            | 2.3900568391009296     |
| train_1/reward            | -1.326805040564068     |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 2.44140625e-05         |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.50071129056407      |
------------------------------------------------------
Saving periodic policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_50.pkl ...
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 51
Time for epoch 51: 644.06. Rollout time: 308.85, Training time: 335.00
Evaluating epoch 51
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
-------------------------------------------------------
| epoch                     | 51                      |
| policy/steps              | 1622904.0               |
| test/episodes             | 1300.0                  |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -25.0                   |
| test_1/avg_q              | -10.0                   |
| test_1/n_subgoals         | 250.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 5200.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -24.99894154587355      |
| train_0/current_q         | -10.33188591415632      |
| train_0/fw_bonus          | -0.9999333828687668     |
| train_0/fw_loss           | 1.6945835159276613e-05  |
| train_0/mu_grads          | -0.02071412596851587    |
| train_0/mu_grads_std      | 0.3802978344261646      |
| train_0/mu_loss           | 9.190914564361373       |
| train_0/next_q            | -10.326439433396349     |
| train_0/q_grads           | 0.03306079907342792     |
| train_0/q_grads_std       | 0.35093641877174375     |
| train_0/q_loss            | 0.6140407612897557      |
| train_0/reward            | -0.7414461834596295     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.3424072265625         |
| train_0/target_q          | -10.482465501113571     |
| train_1/avg_q             | -9.994795687495456      |
| train_1/current_q         | -8.311906290695443      |
| train_1/fw_bonus          | -1.014373505115509      |
| train_1/fw_loss           | 0.00017312462405243422  |
| train_1/mu_grads          | -0.00016732786025386304 |
| train_1/mu_grads_std      | 0.11784094572067261     |
| train_1/mu_loss           | 11.0                    |
| train_1/n_subgoals        | 1000.0                  |
| train_1/next_q            | -10.0                   |
| train_1/q_grads           | 0.018244517082348467    |
| train_1/q_grads_std       | 0.7811093538999557      |
| train_1/q_loss            | 2.366968921549141       |
| train_1/reward            | -1.3263884147490899     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 2.44140625e-05          |
| train_1/reward_-10.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.001                   |
| train_1/target_q          | -8.503883531936593      |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 52
Time for epoch 52: 686.22. Rollout time: 331.85, Training time: 354.20
Evaluating epoch 52
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
-------------------------------------------------------
| epoch                     | 52                      |
| policy/steps              | 1654154.0               |
| test/episodes             | 1325.0                  |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -25.0                   |
| test_1/avg_q              | -10.0                   |
| test_1/n_subgoals         | 250.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 5300.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -24.989707012272213     |
| train_0/current_q         | -10.331261373805727     |
| train_0/fw_bonus          | -0.9999309867620468     |
| train_0/fw_loss           | 1.7469278145654243e-05  |
| train_0/mu_grads          | -0.019854523008689284   |
| train_0/mu_grads_std      | 0.38819524720311166     |
| train_0/mu_loss           | 9.123706352236562       |
| train_0/next_q            | -10.329802346095756     |
| train_0/q_grads           | 0.03326644850894809     |
| train_0/q_grads_std       | 0.3522295609116554      |
| train_0/q_loss            | 0.46449693280067483     |
| train_0/reward            | -0.7421774050351815     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.32822265625           |
| train_0/target_q          | -10.52310794029702      |
| train_1/avg_q             | -9.999985506361012      |
| train_1/current_q         | -8.32343226836746       |
| train_1/fw_bonus          | -1.0144320636987687     |
| train_1/fw_loss           | 0.00015940550620143767  |
| train_1/mu_grads          | -0.00016732786025386304 |
| train_1/mu_grads_std      | 0.11784094572067261     |
| train_1/mu_loss           | 11.0                    |
| train_1/n_subgoals        | 1000.0                  |
| train_1/next_q            | -10.0                   |
| train_1/q_grads           | 0.018805312039330602    |
| train_1/q_grads_std       | 0.7886515513062478      |
| train_1/q_loss            | 2.478758337424113       |
| train_1/reward            | -1.325062714388332      |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.0                     |
| train_1/reward_-10.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -8.511888886263336      |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 53
Time for epoch 53: 719.76. Rollout time: 343.09, Training time: 376.44
Evaluating epoch 53
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
-------------------------------------------------------
| epoch                     | 53                      |
| policy/steps              | 1685404.0               |
| test/episodes             | 1350.0                  |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -25.0                   |
| test_1/avg_q              | -10.0                   |
| test_1/n_subgoals         | 250.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 5400.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -25.0                   |
| train_0/current_q         | -10.237878187069418     |
| train_0/fw_bonus          | -0.9999360248446465     |
| train_0/fw_loss           | 1.6369977925023704e-05  |
| train_0/mu_grads          | -0.022357838274911047   |
| train_0/mu_grads_std      | 0.39490080773830416     |
| train_0/mu_loss           | 9.486035916427705       |
| train_0/next_q            | -10.241391965103187     |
| train_0/q_grads           | 0.03357531195506454     |
| train_0/q_grads_std       | 0.3539786070585251      |
| train_0/q_loss            | 0.44560037840431177     |
| train_0/reward            | -0.744157464732416      |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.33349609375           |
| train_0/target_q          | -10.355205512604162     |
| train_1/avg_q             | -9.999999999999867      |
| train_1/current_q         | -8.312626565857432      |
| train_1/fw_bonus          | -1.0143262892961502     |
| train_1/fw_loss           | 0.00018418688232486603  |
| train_1/mu_grads          | -0.00016732793301343918 |
| train_1/mu_grads_std      | 0.11784094572067261     |
| train_1/mu_loss           | 11.0                    |
| train_1/n_subgoals        | 1000.0                  |
| train_1/next_q            | -10.0                   |
| train_1/q_grads           | 0.01844646641984582     |
| train_1/q_grads_std       | 0.7963393568992615      |
| train_1/q_loss            | 2.5593959820623007      |
| train_1/reward            | -1.3094702731643337     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.0                     |
| train_1/reward_-10.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -8.517111874726837      |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 54
Time for epoch 54: 845.28. Rollout time: 416.02, Training time: 429.11
Evaluating epoch 54
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 54                     |
| policy/steps              | 1716654.0              |
| test/episodes             | 1375.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 5500.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -24.999999795228042    |
| train_0/current_q         | -10.338806515080433    |
| train_0/fw_bonus          | -0.9999182254076004    |
| train_0/fw_loss           | 2.0251444607310986e-05 |
| train_0/mu_grads          | -0.024110058369114996  |
| train_0/mu_grads_std      | 0.40098299011588096    |
| train_0/mu_loss           | 9.45607575555482       |
| train_0/next_q            | -10.336963887232018    |
| train_0/q_grads           | 0.03329297248274088    |
| train_0/q_grads_std       | 0.35539553612470626    |
| train_0/q_loss            | 0.4937282716354202     |
| train_0/reward            | -0.7468829397395893    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.3296875              |
| train_0/target_q          | -10.51073034453933     |
| train_1/avg_q             | -10.0                  |
| train_1/current_q         | -8.273061942836947     |
| train_1/fw_bonus          | -1.0144034504890442    |
| train_1/fw_loss           | 0.00016611135397397446 |
| train_1/mu_grads          | -0.0001673269143793732 |
| train_1/mu_grads_std      | 0.1178409531712532     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.01790090133436024    |
| train_1/q_grads_std       | 0.8020266622304917     |
| train_1/q_loss            | 2.343865819193456      |
| train_1/reward            | -1.31293902740872      |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0                    |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.472968324283723     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 55
Time for epoch 55: 672.07. Rollout time: 334.93, Training time: 336.98
Evaluating epoch 55
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
-------------------------------------------------------
| epoch                     | 55                      |
| policy/steps              | 1747896.0               |
| test/episodes             | 1400.0                  |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -25.0                   |
| test_1/avg_q              | -10.0                   |
| test_1/n_subgoals         | 250.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 5600.0                  |
| train/success_rate        | 0.01                    |
| train_0/avg_q             | -24.997155561486583     |
| train_0/current_q         | -10.489235990932624     |
| train_0/fw_bonus          | -0.999931700527668      |
| train_0/fw_loss           | 1.7314983392680005e-05  |
| train_0/mu_grads          | -0.02492663487792015    |
| train_0/mu_grads_std      | 0.4060882404446602      |
| train_0/mu_loss           | 10.332833875273534      |
| train_0/next_q            | -10.488393777323674     |
| train_0/q_grads           | 0.03355541378259659     |
| train_0/q_grads_std       | 0.3571468755602837      |
| train_0/q_loss            | 0.4873829627175034      |
| train_0/reward            | -0.7447534425024059     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.3308837890625         |
| train_0/target_q          | -10.686245666330787     |
| train_1/avg_q             | -9.990442292632462      |
| train_1/current_q         | -8.319501894241302      |
| train_1/fw_bonus          | -1.0143838793039321     |
| train_1/fw_loss           | 0.0001706909344648011   |
| train_1/mu_grads          | -0.00016731767391320318 |
| train_1/mu_grads_std      | 0.1178409606218338      |
| train_1/mu_loss           | 10.999999999999995      |
| train_1/n_subgoals        | 1000.0                  |
| train_1/next_q            | -9.999999999999998      |
| train_1/q_grads           | 0.01767491828650236     |
| train_1/q_grads_std       | 0.8085528522729873      |
| train_1/q_loss            | 2.340072860287022       |
| train_1/reward            | -1.310365776348044      |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.0                     |
| train_1/reward_-10.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -8.527816948223045      |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 56
Time for epoch 56: 5862.97. Rollout time: 2158.70, Training time: 3704.12
Evaluating epoch 56
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
-------------------------------------------------------
| epoch                     | 56                      |
| policy/steps              | 1779146.0               |
| test/episodes             | 1425.0                  |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -25.0                   |
| test_1/avg_q              | -10.0                   |
| test_1/n_subgoals         | 250.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 5700.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -25.0                   |
| train_0/current_q         | -9.673324626505812      |
| train_0/fw_bonus          | -0.9999320402741432     |
| train_0/fw_loss           | 1.723855491491122e-05   |
| train_0/mu_grads          | -0.025734803220257164   |
| train_0/mu_grads_std      | 0.4077538773417473      |
| train_0/mu_loss           | 9.25371342533941        |
| train_0/next_q            | -9.677435588683693      |
| train_0/q_grads           | 0.03270529042929411     |
| train_0/q_grads_std       | 0.35811507999897        |
| train_0/q_loss            | 0.5427565889578133      |
| train_0/reward            | -0.741640824778733      |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.3562255859375         |
| train_0/target_q          | -10.014926533950836     |
| train_1/avg_q             | -9.999343800488795      |
| train_1/current_q         | -8.285407870321698      |
| train_1/fw_bonus          | -1.0144720047712326     |
| train_1/fw_loss           | 0.00015004916713223793  |
| train_1/mu_grads          | -0.00016731767391320318 |
| train_1/mu_grads_std      | 0.1178409606218338      |
| train_1/mu_loss           | 11.0                    |
| train_1/n_subgoals        | 1000.0                  |
| train_1/next_q            | -10.0                   |
| train_1/q_grads           | 0.017350517213344574    |
| train_1/q_grads_std       | 0.814573447406292       |
| train_1/q_loss            | 2.3770611348444595      |
| train_1/reward            | -1.3166110034457232     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.0                     |
| train_1/reward_-10.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -8.480946940945726      |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 57
Time for epoch 57: 810.36. Rollout time: 402.01, Training time: 408.23
Evaluating epoch 57
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
-------------------------------------------------------
| epoch                     | 57                      |
| policy/steps              | 1810291.0               |
| test/episodes             | 1450.0                  |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -25.0                   |
| test_1/avg_q              | -10.0                   |
| test_1/n_subgoals         | 250.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 5800.0                  |
| train/success_rate        | 0.01                    |
| train_0/avg_q             | -25.0                   |
| train_0/current_q         | -10.185926027673792     |
| train_0/fw_bonus          | -0.9999373555183411     |
| train_0/fw_loss           | 1.6078411977105134e-05  |
| train_0/mu_grads          | -0.026534525351598857   |
| train_0/mu_grads_std      | 0.41162349209189414     |
| train_0/mu_loss           | 9.640656613380937       |
| train_0/next_q            | -10.186668837339713     |
| train_0/q_grads           | 0.03352392436936498     |
| train_0/q_grads_std       | 0.3597031943500042      |
| train_0/q_loss            | 0.5974718305586002      |
| train_0/reward            | -0.7457694614175125     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.3550537109375         |
| train_0/target_q          | -10.426544368132705     |
| train_1/avg_q             | -9.999999999999986      |
| train_1/current_q         | -8.293366095648972      |
| train_1/fw_bonus          | -1.0144511312246323     |
| train_1/fw_loss           | 0.0001549407283164328   |
| train_1/mu_grads          | -0.00016731767391320318 |
| train_1/mu_grads_std      | 0.1178409606218338      |
| train_1/mu_loss           | 11.0                    |
| train_1/n_subgoals        | 996.0                   |
| train_1/next_q            | -10.0                   |
| train_1/q_grads           | 0.01682775462977588     |
| train_1/q_grads_std       | 0.8211945116519928      |
| train_1/q_loss            | 2.5935934568284744      |
| train_1/reward            | -1.3138417752561509     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 2.44140625e-05          |
| train_1/reward_-10.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -8.476742165881154      |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 58
Time for epoch 58: 802.21. Rollout time: 386.92, Training time: 415.05
Evaluating epoch 58
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
-------------------------------------------------------
| epoch                     | 58                      |
| policy/steps              | 1841541.0               |
| test/episodes             | 1475.0                  |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -25.0                   |
| test_1/avg_q              | -10.0                   |
| test_1/n_subgoals         | 250.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 5900.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -24.999080060027847     |
| train_0/current_q         | -10.500901081418593     |
| train_0/fw_bonus          | -0.9999352097511292     |
| train_0/fw_loss           | 1.6547587370041585e-05  |
| train_0/mu_grads          | -0.02822763007134199    |
| train_0/mu_grads_std      | 0.4159561194479465      |
| train_0/mu_loss           | 9.846129240830345       |
| train_0/next_q            | -10.502832739581478     |
| train_0/q_grads           | 0.03468337841331959     |
| train_0/q_grads_std       | 0.3611724816262722      |
| train_0/q_loss            | 0.5338462578921708      |
| train_0/reward            | -0.7461590105995128     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.3356689453125         |
| train_0/target_q          | -10.691804063084103     |
| train_1/avg_q             | -9.999999998204606      |
| train_1/current_q         | -8.284570330551853      |
| train_1/fw_bonus          | -1.014412698149681      |
| train_1/fw_loss           | 0.00016394567755924073  |
| train_1/mu_grads          | -0.00016731767391320318 |
| train_1/mu_grads_std      | 0.1178409606218338      |
| train_1/mu_loss           | 11.0                    |
| train_1/n_subgoals        | 1000.0                  |
| train_1/next_q            | -10.0                   |
| train_1/q_grads           | 0.015503530157729983    |
| train_1/q_grads_std       | 0.8284469425678254      |
| train_1/q_loss            | 2.873358088450268       |
| train_1/reward            | -1.3222454069873493     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 2.44140625e-05          |
| train_1/reward_-10.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -8.458827438237353      |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 59
Time for epoch 59: 739.85. Rollout time: 363.69, Training time: 375.97
Evaluating epoch 59
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
-------------------------------------------------------
| epoch                     | 59                      |
| policy/steps              | 1872791.0               |
| test/episodes             | 1500.0                  |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -25.0                   |
| test_1/avg_q              | -10.0                   |
| test_1/n_subgoals         | 250.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 6000.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -25.0                   |
| train_0/current_q         | -10.196104034070155     |
| train_0/fw_bonus          | -0.9999395579099655     |
| train_0/fw_loss           | 1.559865800118132e-05   |
| train_0/mu_grads          | -0.02855816106311977    |
| train_0/mu_grads_std      | 0.41963670402765274     |
| train_0/mu_loss           | 9.676215251643765       |
| train_0/next_q            | -10.194747870932273     |
| train_0/q_grads           | 0.03396211452782154     |
| train_0/q_grads_std       | 0.3625516042113304      |
| train_0/q_loss            | 0.5347636381987041      |
| train_0/reward            | -0.7434268113662256     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.3393310546875         |
| train_0/target_q          | -10.455634167433043     |
| train_1/avg_q             | -9.990973397749203      |
| train_1/current_q         | -8.266369746155084      |
| train_1/fw_bonus          | -1.0144671887159347     |
| train_1/fw_loss           | 0.00015118016344786155  |
| train_1/mu_grads          | -0.00016731767391320318 |
| train_1/mu_grads_std      | 0.1178409606218338      |
| train_1/mu_loss           | 11.0                    |
| train_1/n_subgoals        | 1000.0                  |
| train_1/next_q            | -10.0                   |
| train_1/q_grads           | 0.014352687983773649    |
| train_1/q_grads_std       | 0.8347456648945808      |
| train_1/q_loss            | 2.400821471281182       |
| train_1/reward            | -1.335480660583562      |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.0                     |
| train_1/reward_-10.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -8.439045113708564      |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 60
Time for epoch 60: 793.26. Rollout time: 384.15, Training time: 408.91
Evaluating epoch 60
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
-------------------------------------------------------
| epoch                     | 60                      |
| policy/steps              | 1904041.0               |
| test/episodes             | 1525.0                  |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -25.0                   |
| test_1/avg_q              | -10.0                   |
| test_1/n_subgoals         | 250.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 6100.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -24.999999999999943     |
| train_0/current_q         | -10.3907453377426       |
| train_0/fw_bonus          | -0.999925172328949      |
| train_0/fw_loss           | 1.8735575440587127e-05  |
| train_0/mu_grads          | -0.029429463529959322   |
| train_0/mu_grads_std      | 0.42293196320533755     |
| train_0/mu_loss           | 10.237480437573215      |
| train_0/next_q            | -10.390837434928594     |
| train_0/q_grads           | 0.034060201980173586    |
| train_0/q_grads_std       | 0.3642440505325794      |
| train_0/q_loss            | 0.4089673899847684      |
| train_0/reward            | -0.7430412006287952     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.3201904296875         |
| train_0/target_q          | -10.546018520188701     |
| train_1/avg_q             | -9.99010786150023       |
| train_1/current_q         | -8.298755083506535      |
| train_1/fw_bonus          | -1.0145195305347443     |
| train_1/fw_loss           | 0.00013891234339098447  |
| train_1/mu_grads          | -0.00016731767391320318 |
| train_1/mu_grads_std      | 0.1178409606218338      |
| train_1/mu_loss           | 11.0                    |
| train_1/n_subgoals        | 1000.0                  |
| train_1/next_q            | -10.0                   |
| train_1/q_grads           | 0.01292689663823694     |
| train_1/q_grads_std       | 0.8407172963023186      |
| train_1/q_loss            | 2.4833378115717357      |
| train_1/reward            | -1.3186733469512546     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.0                     |
| train_1/reward_-10.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -8.487076667263757      |
-------------------------------------------------------
Saving periodic policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_60.pkl ...
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 61
Time for epoch 61: 852.44. Rollout time: 415.61, Training time: 436.62
Evaluating epoch 61
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
-------------------------------------------------------
| epoch                     | 61                      |
| policy/steps              | 1935291.0               |
| test/episodes             | 1550.0                  |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -25.0                   |
| test_1/avg_q              | -10.0                   |
| test_1/n_subgoals         | 250.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 6200.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -25.0                   |
| train_0/current_q         | -10.581207977239188     |
| train_0/fw_bonus          | -0.9999403685331345     |
| train_0/fw_loss           | 1.5424307548528304e-05  |
| train_0/mu_grads          | -0.02924524350091815    |
| train_0/mu_grads_std      | 0.42601975724101065     |
| train_0/mu_loss           | 10.338588570141553      |
| train_0/next_q            | -10.584932019758662     |
| train_0/q_grads           | 0.034974119253456594    |
| train_0/q_grads_std       | 0.36541223526000977     |
| train_0/q_loss            | 0.5177040603330545      |
| train_0/reward            | -0.744818812792073      |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.3217529296875         |
| train_0/target_q          | -10.766554865411226     |
| train_1/avg_q             | -9.999996449918173      |
| train_1/current_q         | -8.301607794189211      |
| train_1/fw_bonus          | -1.0144691824913026     |
| train_1/fw_loss           | 0.00015070980480231811  |
| train_1/mu_grads          | -0.00016731767391320318 |
| train_1/mu_grads_std      | 0.1178409606218338      |
| train_1/mu_loss           | 11.0                    |
| train_1/n_subgoals        | 1000.0                  |
| train_1/next_q            | -10.0                   |
| train_1/q_grads           | 0.011946602747775615    |
| train_1/q_grads_std       | 0.8469323173165322      |
| train_1/q_loss            | 2.371731240408118       |
| train_1/reward            | -1.3205524648496065     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.0                     |
| train_1/reward_-10.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -8.499961644537109      |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 62
Time for epoch 62: 753.54. Rollout time: 376.27, Training time: 377.14
Evaluating epoch 62
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
-------------------------------------------------------
| epoch                     | 62                      |
| policy/steps              | 1966541.0               |
| test/episodes             | 1575.0                  |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -25.0                   |
| test_1/avg_q              | -10.0                   |
| test_1/n_subgoals         | 250.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 6300.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -25.0                   |
| train_0/current_q         | -10.118885415230324     |
| train_0/fw_bonus          | -0.9999248698353768     |
| train_0/fw_loss           | 1.8801890450959036e-05  |
| train_0/mu_grads          | -0.02906543416902423    |
| train_0/mu_grads_std      | 0.42922854647040365     |
| train_0/mu_loss           | 10.11415105019861       |
| train_0/next_q            | -10.11844757385423      |
| train_0/q_grads           | 0.033671384863555434    |
| train_0/q_grads_std       | 0.36661134734749795     |
| train_0/q_loss            | 0.8093708469455729      |
| train_0/reward            | -0.7455068689712789     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.323486328125          |
| train_0/target_q          | -10.419908273939466     |
| train_1/avg_q             | -10.0                   |
| train_1/current_q         | -8.350330820410957      |
| train_1/fw_bonus          | -1.014409738779068      |
| train_1/fw_loss           | 0.00016463732899865135  |
| train_1/mu_grads          | -0.00016731767391320318 |
| train_1/mu_grads_std      | 0.1178409606218338      |
| train_1/mu_loss           | 11.0                    |
| train_1/n_subgoals        | 1000.0                  |
| train_1/next_q            | -10.0                   |
| train_1/q_grads           | 0.011251441016793252    |
| train_1/q_grads_std       | 0.853631554543972       |
| train_1/q_loss            | 2.4425041782660744      |
| train_1/reward            | -1.3355495949392207     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 2.44140625e-05          |
| train_1/reward_-10.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -8.531946079314224      |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 63
Time for epoch 63: 819.02. Rollout time: 404.49, Training time: 414.35
Evaluating epoch 63
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
-------------------------------------------------------
| epoch                     | 63                      |
| policy/steps              | 1997791.0               |
| test/episodes             | 1600.0                  |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -25.0                   |
| test_1/avg_q              | -10.0                   |
| test_1/n_subgoals         | 250.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 6400.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -25.0                   |
| train_0/current_q         | -10.476504820365307     |
| train_0/fw_bonus          | -0.9999341905117035     |
| train_0/fw_loss           | 1.6768058003435726e-05  |
| train_0/mu_grads          | -0.030129280965775253   |
| train_0/mu_grads_std      | 0.43275782093405724     |
| train_0/mu_loss           | 10.310042854220459      |
| train_0/next_q            | -10.477332316541961     |
| train_0/q_grads           | 0.034542111400514844    |
| train_0/q_grads_std       | 0.368119265884161       |
| train_0/q_loss            | 0.5126079809054114      |
| train_0/reward            | -0.7418372159831051     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.3226318359375         |
| train_0/target_q          | -10.652240461213312     |
| train_1/avg_q             | -9.990157537980103      |
| train_1/current_q         | -8.264355682378172      |
| train_1/fw_bonus          | -1.014446485042572      |
| train_1/fw_loss           | 0.00015602649800712244  |
| train_1/mu_grads          | -0.00016731767391320318 |
| train_1/mu_grads_std      | 0.1178409606218338      |
| train_1/mu_loss           | 11.0                    |
| train_1/n_subgoals        | 1000.0                  |
| train_1/next_q            | -10.0                   |
| train_1/q_grads           | 0.01106674368493259     |
| train_1/q_grads_std       | 0.8591562882065773      |
| train_1/q_loss            | 3.0067448584307988      |
| train_1/reward            | -1.3171109420014546     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.0                     |
| train_1/reward_-10.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -8.438858988876458      |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 64
Time for epoch 64: 654.99. Rollout time: 319.21, Training time: 335.55
Evaluating epoch 64
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
-------------------------------------------------------
| epoch                     | 64                      |
| policy/steps              | 2029041.0               |
| test/episodes             | 1625.0                  |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -25.0                   |
| test_1/avg_q              | -10.0                   |
| test_1/n_subgoals         | 250.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 6500.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -25.0                   |
| train_0/current_q         | -10.467569243338545     |
| train_0/fw_bonus          | -0.9999410197138786     |
| train_0/fw_loss           | 1.528069210507965e-05   |
| train_0/mu_grads          | -0.0315820143558085     |
| train_0/mu_grads_std      | 0.43437184393405914     |
| train_0/mu_loss           | 10.630970544635053      |
| train_0/next_q            | -10.466052814301728     |
| train_0/q_grads           | 0.03397796768695116     |
| train_0/q_grads_std       | 0.3690892316401005      |
| train_0/q_loss            | 0.5919082859209412      |
| train_0/reward            | -0.7419993179319135     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.33046875              |
| train_0/target_q          | -10.619800575325298     |
| train_1/avg_q             | -9.991650518100244      |
| train_1/current_q         | -8.252973822592073      |
| train_1/fw_bonus          | -1.0144501745700836     |
| train_1/fw_loss           | 0.00015516269486397504  |
| train_1/mu_grads          | -0.00016731767391320318 |
| train_1/mu_grads_std      | 0.1178409606218338      |
| train_1/mu_loss           | 11.0                    |
| train_1/n_subgoals        | 1000.0                  |
| train_1/next_q            | -10.0                   |
| train_1/q_grads           | 0.010417326563037932    |
| train_1/q_grads_std       | 0.8655402779579162      |
| train_1/q_loss            | 2.351025392912268       |
| train_1/reward            | -1.3236191327952838     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.0                     |
| train_1/reward_-10.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -8.436514640607786      |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 65
Time for epoch 65: 1667.73. Rollout time: 1329.06, Training time: 338.55
Evaluating epoch 65
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
-------------------------------------------------------
| epoch                     | 65                      |
| policy/steps              | 2060291.0               |
| test/episodes             | 1650.0                  |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -25.0                   |
| test_1/avg_q              | -10.0                   |
| test_1/n_subgoals         | 250.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 6600.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -24.999993199150573     |
| train_0/current_q         | -10.519609319704855     |
| train_0/fw_bonus          | -0.999840846657753      |
| train_0/fw_loss           | 3.712332509167027e-05   |
| train_0/mu_grads          | -0.03220738917589187    |
| train_0/mu_grads_std      | 0.4375820368528366      |
| train_0/mu_loss           | 10.543547488144037      |
| train_0/next_q            | -10.521186341579028     |
| train_0/q_grads           | 0.03434381261467934     |
| train_0/q_grads_std       | 0.3694968782365322      |
| train_0/q_loss            | 0.5022986980347662      |
| train_0/reward            | -0.7436093900061678     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.1409423828125         |
| train_0/target_q          | -10.714600525570496     |
| train_1/avg_q             | -9.990904794889106      |
| train_1/current_q         | -8.209280126825153      |
| train_1/fw_bonus          | -1.0143481135368346     |
| train_1/fw_loss           | 0.00017907398141687737  |
| train_1/mu_grads          | -0.00016731767391320318 |
| train_1/mu_grads_std      | 0.1178409606218338      |
| train_1/mu_loss           | 11.0                    |
| train_1/n_subgoals        | 1000.0                  |
| train_1/next_q            | -10.0                   |
| train_1/q_grads           | 0.009357848903164268    |
| train_1/q_grads_std       | 0.8699615135788917      |
| train_1/q_loss            | 2.289944703500249       |
| train_1/reward            | -1.3057214197724534     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.0                     |
| train_1/reward_-10.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -8.408568099459956      |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 66
Time for epoch 66: 680.40. Rollout time: 330.06, Training time: 350.15
Evaluating epoch 66
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
-------------------------------------------------------
| epoch                     | 66                      |
| policy/steps              | 2091541.0               |
| test/episodes             | 1675.0                  |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -25.0                   |
| test_1/avg_q              | -10.0                   |
| test_1/n_subgoals         | 250.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 6700.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -25.0                   |
| train_0/current_q         | -10.334359626907325     |
| train_0/fw_bonus          | -0.999928955733776      |
| train_0/fw_loss           | 1.7910249357555586e-05  |
| train_0/mu_grads          | -0.03364698179066181    |
| train_0/mu_grads_std      | 0.43784986808896065     |
| train_0/mu_loss           | 10.33195450592496       |
| train_0/next_q            | -10.331351034697688     |
| train_0/q_grads           | 0.03386281607672572     |
| train_0/q_grads_std       | 0.37110433503985407     |
| train_0/q_loss            | 0.6622948309690468      |
| train_0/reward            | -0.7437511010859452     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.277685546875          |
| train_0/target_q          | -10.544697925964245     |
| train_1/avg_q             | -9.999999999656469      |
| train_1/current_q         | -8.19144238164844       |
| train_1/fw_bonus          | -1.0143543630838394     |
| train_1/fw_loss           | 0.00017761239578248933  |
| train_1/mu_grads          | -0.00016731767391320318 |
| train_1/mu_grads_std      | 0.1178409606218338      |
| train_1/mu_loss           | 11.0                    |
| train_1/n_subgoals        | 1000.0                  |
| train_1/next_q            | -10.0                   |
| train_1/q_grads           | 0.008827105700038373    |
| train_1/q_grads_std       | 0.873065185546875       |
| train_1/q_loss            | 2.3185121856230375      |
| train_1/reward            | -1.3132806916248227     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.0                     |
| train_1/reward_-10.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -8.377606863499826      |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 67
Time for epoch 67: 804.49. Rollout time: 383.11, Training time: 421.21
Evaluating epoch 67
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
-------------------------------------------------------
| epoch                     | 67                      |
| policy/steps              | 2122791.0               |
| test/episodes             | 1700.0                  |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -25.0                   |
| test_1/avg_q              | -10.0                   |
| test_1/n_subgoals         | 250.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 6800.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -24.999999999987214     |
| train_0/current_q         | -10.165682897102904     |
| train_0/fw_bonus          | -0.9999262720346451     |
| train_0/fw_loss           | 1.8494407004254754e-05  |
| train_0/mu_grads          | -0.038476118072867396   |
| train_0/mu_grads_std      | 0.4340848743915558      |
| train_0/mu_loss           | 10.24487185550847       |
| train_0/next_q            | -10.166249266255283     |
| train_0/q_grads           | 0.03402759609743953     |
| train_0/q_grads_std       | 0.37187072187662124     |
| train_0/q_loss            | 0.635052881794197       |
| train_0/reward            | -0.7436643123743124     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.337744140625          |
| train_0/target_q          | -10.395521892844494     |
| train_1/avg_q             | -9.990429995029618      |
| train_1/current_q         | -8.214537827353094      |
| train_1/fw_bonus          | -1.0143429070711136     |
| train_1/fw_loss           | 0.00018029189959634095  |
| train_1/mu_grads          | -0.00016731767391320318 |
| train_1/mu_grads_std      | 0.1178409606218338      |
| train_1/mu_loss           | 11.0                    |
| train_1/n_subgoals        | 1000.0                  |
| train_1/next_q            | -10.0                   |
| train_1/q_grads           | 0.008852891041897237    |
| train_1/q_grads_std       | 0.8782373398542405      |
| train_1/q_loss            | 2.662401707322968       |
| train_1/reward            | -1.3230283908662386     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.0                     |
| train_1/reward_-10.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -8.38472272680374       |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 68
Time for epoch 68: 864.43. Rollout time: 399.21, Training time: 465.05
Evaluating epoch 68
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
-------------------------------------------------------
| epoch                     | 68                      |
| policy/steps              | 2154041.0               |
| test/episodes             | 1725.0                  |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -25.0                   |
| test_1/avg_q              | -10.0                   |
| test_1/n_subgoals         | 250.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 6900.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -25.0                   |
| train_0/current_q         | -10.447254584662932     |
| train_0/fw_bonus          | -0.9999077782034874     |
| train_0/fw_loss           | 2.252780313938274e-05   |
| train_0/mu_grads          | -0.03990816492587328    |
| train_0/mu_grads_std      | 0.4322053030133247      |
| train_0/mu_loss           | 10.584766880711296      |
| train_0/next_q            | -10.450179510879911     |
| train_0/q_grads           | 0.034158424101769926    |
| train_0/q_grads_std       | 0.37226864099502566     |
| train_0/q_loss            | 0.7869130203909371      |
| train_0/reward            | -0.7449978372103943     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.2978515625            |
| train_0/target_q          | -10.641368185112123     |
| train_1/avg_q             | -10.0                   |
| train_1/current_q         | -8.232510622769002      |
| train_1/fw_bonus          | -1.0144512325525283     |
| train_1/fw_loss           | 0.00015491867779928725  |
| train_1/mu_grads          | -0.00016731767391320318 |
| train_1/mu_grads_std      | 0.1178409606218338      |
| train_1/mu_loss           | 11.0                    |
| train_1/n_subgoals        | 1000.0                  |
| train_1/next_q            | -10.0                   |
| train_1/q_grads           | 0.008714348054490983    |
| train_1/q_grads_std       | 0.8832059964537621      |
| train_1/q_loss            | 2.3717250188752623      |
| train_1/reward            | -1.3089101810153807     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.0                     |
| train_1/reward_-10.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -8.425394556015386      |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 69
Time for epoch 69: 942.67. Rollout time: 456.81, Training time: 485.57
Evaluating epoch 69
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
-------------------------------------------------------
| epoch                     | 69                      |
| policy/steps              | 2185291.0               |
| test/episodes             | 1750.0                  |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -25.0                   |
| test_1/avg_q              | -10.0                   |
| test_1/n_subgoals         | 250.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 7000.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -25.0                   |
| train_0/current_q         | -10.472558259727291     |
| train_0/fw_bonus          | -0.9999224424362183     |
| train_0/fw_loss           | 1.9330782060933414e-05  |
| train_0/mu_grads          | -0.04001559112221002    |
| train_0/mu_grads_std      | 0.4297761470079422      |
| train_0/mu_loss           | 10.527112251126773      |
| train_0/next_q            | -10.467737097288301     |
| train_0/q_grads           | 0.033845850452780726    |
| train_0/q_grads_std       | 0.37344675064086913     |
| train_0/q_loss            | 0.6246326689478627      |
| train_0/reward            | -0.7434670821901819     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.3185546875            |
| train_0/target_q          | -10.641912641988096     |
| train_1/avg_q             | -9.999993580243215      |
| train_1/current_q         | -8.222978039821994      |
| train_1/fw_bonus          | -1.0144234031438828     |
| train_1/fw_loss           | 0.0001614319106010953   |
| train_1/mu_grads          | -0.00016731767391320318 |
| train_1/mu_grads_std      | 0.1178409606218338      |
| train_1/mu_loss           | 11.0                    |
| train_1/n_subgoals        | 1000.0                  |
| train_1/next_q            | -10.0                   |
| train_1/q_grads           | 0.00922924350015819     |
| train_1/q_grads_std       | 0.8874301671981811      |
| train_1/q_loss            | 2.5264518267621967      |
| train_1/reward            | -1.3228489165507198     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.0                     |
| train_1/reward_-10.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -8.408947549363223      |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 70
Time for epoch 70: 859.46. Rollout time: 414.95, Training time: 444.34
Evaluating epoch 70
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
-------------------------------------------------------
| epoch                     | 70                      |
| policy/steps              | 2216524.0               |
| test/episodes             | 1775.0                  |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -25.0                   |
| test_1/avg_q              | -10.0                   |
| test_1/n_subgoals         | 250.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 7100.0                  |
| train/success_rate        | 0.01                    |
| train_0/avg_q             | -25.0                   |
| train_0/current_q         | -10.336097845691661     |
| train_0/fw_bonus          | -0.9999326825141907     |
| train_0/fw_loss           | 1.7099274987231184e-05  |
| train_0/mu_grads          | -0.04002019194886088    |
| train_0/mu_grads_std      | 0.4269587241113186      |
| train_0/mu_loss           | 10.365101442879237      |
| train_0/next_q            | -10.336136656458141     |
| train_0/q_grads           | 0.03355065044015646     |
| train_0/q_grads_std       | 0.3737978063523769      |
| train_0/q_loss            | 0.855539443079946       |
| train_0/reward            | -0.744424347266613      |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.319384765625          |
| train_0/target_q          | -10.546302982639919     |
| train_1/avg_q             | -9.999999999857149      |
| train_1/current_q         | -8.233573463760989      |
| train_1/fw_bonus          | -1.014417764544487      |
| train_1/fw_loss           | 0.00016275911475531756  |
| train_1/mu_grads          | -0.00016731767391320318 |
| train_1/mu_grads_std      | 0.1178409606218338      |
| train_1/mu_loss           | 11.0                    |
| train_1/n_subgoals        | 1000.0                  |
| train_1/next_q            | -10.0                   |
| train_1/q_grads           | 0.008177492138929664    |
| train_1/q_grads_std       | 0.8906477093696594      |
| train_1/q_loss            | 2.275932923583904       |
| train_1/reward            | -1.3034491112455726     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 4.8828125e-05           |
| train_1/reward_-10.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -8.437638564370577      |
-------------------------------------------------------
Saving periodic policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_70.pkl ...
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 71
Time for epoch 71: 1389.43. Rollout time: 594.85, Training time: 793.84
Evaluating epoch 71
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
-------------------------------------------------------
| epoch                     | 71                      |
| policy/steps              | 2247774.0               |
| test/episodes             | 1800.0                  |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -25.0                   |
| test_1/avg_q              | -10.0                   |
| test_1/n_subgoals         | 250.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 7200.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -24.99999998866845      |
| train_0/current_q         | -10.415554329317487     |
| train_0/fw_bonus          | -0.9990611508488655     |
| train_0/fw_loss           | 0.00020714287402370246  |
| train_0/mu_grads          | -0.03871499672532082    |
| train_0/mu_grads_std      | 0.42468110248446467     |
| train_0/mu_loss           | 10.437675067597134      |
| train_0/next_q            | -10.412556941430193     |
| train_0/q_grads           | 0.03407911602407694     |
| train_0/q_grads_std       | 0.3746253877878189      |
| train_0/q_loss            | 0.9850081180007152      |
| train_0/reward            | -0.743500304895133      |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 4.8828125e-05           |
| train_0/target_q          | -10.62230433152665      |
| train_1/avg_q             | -9.999999999077762      |
| train_1/current_q         | -8.231799746525141      |
| train_1/fw_bonus          | -1.014342901110649      |
| train_1/fw_loss           | 0.00018029245438810904  |
| train_1/mu_grads          | -0.00016731767391320318 |
| train_1/mu_grads_std      | 0.1178409606218338      |
| train_1/mu_loss           | 11.0                    |
| train_1/n_subgoals        | 1000.0                  |
| train_1/next_q            | -10.0                   |
| train_1/q_grads           | 0.00782257723622024     |
| train_1/q_grads_std       | 0.896076574921608       |
| train_1/q_loss            | 2.400451925408332       |
| train_1/reward            | -1.330096260659775      |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.0                     |
| train_1/reward_-10.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -8.403514229409778      |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 72
Time for epoch 72: 1185.51. Rollout time: 555.77, Training time: 629.31
Evaluating epoch 72
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
-------------------------------------------------------
| epoch                     | 72                      |
| policy/steps              | 2279024.0               |
| test/episodes             | 1825.0                  |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -25.0                   |
| test_1/avg_q              | -10.0                   |
| test_1/n_subgoals         | 250.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 7300.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -25.0                   |
| train_0/current_q         | -10.139778006639602     |
| train_0/fw_bonus          | -0.9999216049909592     |
| train_0/fw_loss           | 1.9514219775373932e-05  |
| train_0/mu_grads          | -0.037642941903322936   |
| train_0/mu_grads_std      | 0.4236073262989521      |
| train_0/mu_loss           | 10.17597319065796       |
| train_0/next_q            | -10.136871460494035     |
| train_0/q_grads           | 0.033297743368893865    |
| train_0/q_grads_std       | 0.3758825160562992      |
| train_0/q_loss            | 0.7051607532266803      |
| train_0/reward            | -0.7445164659591683     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.281103515625          |
| train_0/target_q          | -10.407072439081722     |
| train_1/avg_q             | -9.995791618399341      |
| train_1/current_q         | -8.204461348984609      |
| train_1/fw_bonus          | -1.0144639700651168     |
| train_1/fw_loss           | 0.00015193242288660258  |
| train_1/mu_grads          | -0.00016731767391320318 |
| train_1/mu_grads_std      | 0.1178409606218338      |
| train_1/mu_loss           | 11.0                    |
| train_1/n_subgoals        | 1000.0                  |
| train_1/next_q            | -10.0                   |
| train_1/q_grads           | 0.007121803774498403    |
| train_1/q_grads_std       | 0.9012011244893074      |
| train_1/q_loss            | 2.3963152811779933      |
| train_1/reward            | -1.3323418474617938     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 2.44140625e-05          |
| train_1/reward_-10.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -8.372024464649297      |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 73
Time for epoch 73: 1239.97. Rollout time: 570.53, Training time: 668.75
Evaluating epoch 73
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
-------------------------------------------------------
| epoch                     | 73                      |
| policy/steps              | 2310271.0               |
| test/episodes             | 1850.0                  |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -25.0                   |
| test_1/avg_q              | -10.0                   |
| test_1/n_subgoals         | 250.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 7400.0                  |
| train/success_rate        | 0.01                    |
| train_0/avg_q             | -25.0                   |
| train_0/current_q         | -10.543728110063755     |
| train_0/fw_bonus          | -0.9999334424734115     |
| train_0/fw_loss           | 1.6931587219914944e-05  |
| train_0/mu_grads          | -0.03776099681854248    |
| train_0/mu_grads_std      | 0.42312092110514643     |
| train_0/mu_loss           | 10.558027369759664      |
| train_0/next_q            | -10.546540151591339     |
| train_0/q_grads           | 0.03359663262963295     |
| train_0/q_grads_std       | 0.37693586349487307     |
| train_0/q_loss            | 0.6071542454050747      |
| train_0/reward            | -0.7444735596487589     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.32548828125           |
| train_0/target_q          | -10.738143024389712     |
| train_1/avg_q             | -9.999989018653755      |
| train_1/current_q         | -8.277303365262751      |
| train_1/fw_bonus          | -1.0144864588975906     |
| train_1/fw_loss           | 0.0001466634505050024   |
| train_1/mu_grads          | -0.00016731767391320318 |
| train_1/mu_grads_std      | 0.1178409606218338      |
| train_1/mu_loss           | 11.0                    |
| train_1/n_subgoals        | 1000.0                  |
| train_1/next_q            | -10.0                   |
| train_1/q_grads           | 0.006639191892463714    |
| train_1/q_grads_std       | 0.907918532192707       |
| train_1/q_loss            | 2.764817053231554       |
| train_1/reward            | -1.3383833667023282     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.0                     |
| train_1/reward_-10.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -8.434052312014831      |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 74
Time for epoch 74: 867.40. Rollout time: 401.31, Training time: 465.70
Evaluating epoch 74
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
-------------------------------------------------------
| epoch                     | 74                      |
| policy/steps              | 2341521.0               |
| test/episodes             | 1875.0                  |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -25.0                   |
| test_1/avg_q              | -10.0                   |
| test_1/n_subgoals         | 250.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 7500.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -25.0                   |
| train_0/current_q         | -10.336112814775952     |
| train_0/fw_bonus          | -0.9999384433031082     |
| train_0/fw_loss           | 1.584581525548856e-05   |
| train_0/mu_grads          | -0.03695413963869214    |
| train_0/mu_grads_std      | 0.42294340804219244     |
| train_0/mu_loss           | 10.357030560886741      |
| train_0/next_q            | -10.336407421355961     |
| train_0/q_grads           | 0.03301516948267817     |
| train_0/q_grads_std       | 0.37723238989710806     |
| train_0/q_loss            | 0.4912114434262653      |
| train_0/reward            | -0.7437192136283557     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.35771484375           |
| train_0/target_q          | -10.518182397129005     |
| train_1/avg_q             | -9.97408150571361       |
| train_1/current_q         | -8.300527960262277      |
| train_1/fw_bonus          | -1.0143985003232956     |
| train_1/fw_loss           | 0.00016726571957406122  |
| train_1/mu_grads          | -0.00016731767391320318 |
| train_1/mu_grads_std      | 0.1178409606218338      |
| train_1/mu_loss           | 11.0                    |
| train_1/n_subgoals        | 1000.0                  |
| train_1/next_q            | -10.0                   |
| train_1/q_grads           | 0.006228117435239255    |
| train_1/q_grads_std       | 0.9132737368345261      |
| train_1/q_loss            | 2.4266257366112542      |
| train_1/reward            | -1.336496133437322      |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.0                     |
| train_1/reward_-10.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -8.472599649062326      |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 75
Time for epoch 75: 810.27. Rollout time: 381.39, Training time: 428.69
Evaluating epoch 75
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
-------------------------------------------------------
| epoch                     | 75                      |
| policy/steps              | 2372771.0               |
| test/episodes             | 1900.0                  |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -25.0                   |
| test_1/avg_q              | -10.0                   |
| test_1/n_subgoals         | 250.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 7600.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -25.0                   |
| train_0/current_q         | -10.441658773034117     |
| train_0/fw_bonus          | -0.9999297663569451     |
| train_0/fw_loss           | 1.7734524772095028e-05  |
| train_0/mu_grads          | -0.03730786005035043    |
| train_0/mu_grads_std      | 0.4229415409266949      |
| train_0/mu_loss           | 10.426352648389527      |
| train_0/next_q            | -10.44481686026568      |
| train_0/q_grads           | 0.033590920455753806    |
| train_0/q_grads_std       | 0.37891570255160334     |
| train_0/q_loss            | 0.6062377176702881      |
| train_0/reward            | -0.7459731835064304     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.329541015625          |
| train_0/target_q          | -10.657232229728816     |
| train_1/avg_q             | -9.993367095161393      |
| train_1/current_q         | -8.286203059311896      |
| train_1/fw_bonus          | -1.0142961621284485     |
| train_1/fw_loss           | 0.00019124730133626144  |
| train_1/mu_grads          | -0.00016731767391320318 |
| train_1/mu_grads_std      | 0.1178409606218338      |
| train_1/mu_loss           | 11.0                    |
| train_1/n_subgoals        | 1000.0                  |
| train_1/next_q            | -10.0                   |
| train_1/q_grads           | 0.005630803166422993    |
| train_1/q_grads_std       | 0.9197923436760902      |
| train_1/q_loss            | 2.407551314919523       |
| train_1/reward            | -1.325800419384177      |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.0                     |
| train_1/reward_-10.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -8.46860315375918       |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 76
Time for epoch 76: 724.19. Rollout time: 345.27, Training time: 378.66
Evaluating epoch 76
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
-------------------------------------------------------
| epoch                     | 76                      |
| policy/steps              | 2404021.0               |
| test/episodes             | 1925.0                  |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -25.0                   |
| test_1/avg_q              | -10.0                   |
| test_1/n_subgoals         | 250.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 7700.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -25.0                   |
| train_0/current_q         | -10.505413776359372     |
| train_0/fw_bonus          | -0.999922414124012      |
| train_0/fw_loss           | 1.9337005096531356e-05  |
| train_0/mu_grads          | -0.036111058201640844   |
| train_0/mu_grads_std      | 0.4241385065019131      |
| train_0/mu_loss           | 10.555595079747093      |
| train_0/next_q            | -10.507449120123157     |
| train_0/q_grads           | 0.034021188504993916    |
| train_0/q_grads_std       | 0.3792103186249733      |
| train_0/q_loss            | 0.7893303266766998      |
| train_0/reward            | -0.7445901145489188     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.30576171875           |
| train_0/target_q          | -10.70300325961807      |
| train_1/avg_q             | -9.999999990824591      |
| train_1/current_q         | -8.26323830884622       |
| train_1/fw_bonus          | -1.0143298506736755     |
| train_1/fw_loss           | 0.0001833519148931373   |
| train_1/mu_grads          | -0.00016731767391320318 |
| train_1/mu_grads_std      | 0.1178409606218338      |
| train_1/mu_loss           | 11.0                    |
| train_1/n_subgoals        | 1000.0                  |
| train_1/next_q            | -10.0                   |
| train_1/q_grads           | 0.003919326810864732    |
| train_1/q_grads_std       | 0.9249499246478081      |
| train_1/q_loss            | 3.6302121564066923      |
| train_1/reward            | -1.3351678559411084     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.0                     |
| train_1/reward_-10.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -8.42437684031611       |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 77
Time for epoch 77: 770.94. Rollout time: 364.16, Training time: 406.64
Evaluating epoch 77
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
-------------------------------------------------------
| epoch                     | 77                      |
| policy/steps              | 2435271.0               |
| test/episodes             | 1950.0                  |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -25.0                   |
| test_1/avg_q              | -10.0                   |
| test_1/n_subgoals         | 250.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 7800.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -25.0                   |
| train_0/current_q         | -10.543186929976434     |
| train_0/fw_bonus          | -0.999939675629139      |
| train_0/fw_loss           | 1.55719233362106e-05    |
| train_0/mu_grads          | -0.0360117987729609     |
| train_0/mu_grads_std      | 0.4254301801323891      |
| train_0/mu_loss           | 10.614023302357884      |
| train_0/next_q            | -10.53975149070575      |
| train_0/q_grads           | 0.03341523818671703     |
| train_0/q_grads_std       | 0.378645721077919       |
| train_0/q_loss            | 0.4119292784996125      |
| train_0/reward            | -0.7455508962171734     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.3153564453125         |
| train_0/target_q          | -10.721574549383536     |
| train_1/avg_q             | -9.999999999998101      |
| train_1/current_q         | -8.249265887504915      |
| train_1/fw_bonus          | -1.0144537299871446     |
| train_1/fw_loss           | 0.0001543296912132064   |
| train_1/mu_grads          | -0.00016731767391320318 |
| train_1/mu_grads_std      | 0.1178409606218338      |
| train_1/mu_loss           | 11.0                    |
| train_1/n_subgoals        | 1000.0                  |
| train_1/next_q            | -10.0                   |
| train_1/q_grads           | 0.0025253019877709447   |
| train_1/q_grads_std       | 0.92878737449646        |
| train_1/q_loss            | 2.6770306298869975      |
| train_1/reward            | -1.3399905398342526     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.0                     |
| train_1/reward_-10.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -8.412451477334255      |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 78
Time for epoch 78: 728.69. Rollout time: 339.43, Training time: 389.03
Evaluating epoch 78
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
-------------------------------------------------------
| epoch                     | 78                      |
| policy/steps              | 2466521.0               |
| test/episodes             | 1975.0                  |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -25.0                   |
| test_1/avg_q              | -10.0                   |
| test_1/n_subgoals         | 250.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 7900.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -25.0                   |
| train_0/current_q         | -10.435891747154491     |
| train_0/fw_bonus          | -0.9999315738677979     |
| train_0/fw_loss           | 1.7339827240903106e-05  |
| train_0/mu_grads          | -0.03588796677067876    |
| train_0/mu_grads_std      | 0.42494321092963216     |
| train_0/mu_loss           | 10.530264518126948      |
| train_0/next_q            | -10.432519547607352     |
| train_0/q_grads           | 0.033065239805728196    |
| train_0/q_grads_std       | 0.37895009368658067     |
| train_0/q_loss            | 0.5783599720561655      |
| train_0/reward            | -0.743984342499607      |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.318798828125          |
| train_0/target_q          | -10.591794707173253     |
| train_1/avg_q             | -10.0                   |
| train_1/current_q         | -8.208426690160703      |
| train_1/fw_bonus          | -1.014449980854988      |
| train_1/fw_loss           | 0.00015521052373514976  |
| train_1/mu_grads          | -0.00016731767391320318 |
| train_1/mu_grads_std      | 0.1178409606218338      |
| train_1/mu_loss           | 11.0                    |
| train_1/n_subgoals        | 1000.0                  |
| train_1/next_q            | -10.0                   |
| train_1/q_grads           | 0.001713560416828841    |
| train_1/q_grads_std       | 0.9342867761850357      |
| train_1/q_loss            | 2.401435987335139       |
| train_1/reward            | -1.3343612185031817     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.0                     |
| train_1/reward_-10.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -8.375479382565684      |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 79
Time for epoch 79: 792.79. Rollout time: 374.14, Training time: 418.33
Evaluating epoch 79
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
-------------------------------------------------------
| epoch                     | 79                      |
| policy/steps              | 2497771.0               |
| test/episodes             | 2000.0                  |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -25.0                   |
| test_1/avg_q              | -10.0                   |
| test_1/n_subgoals         | 250.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 8000.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -25.0                   |
| train_0/current_q         | -10.477500483260425     |
| train_0/fw_bonus          | -0.9999405801296234     |
| train_0/fw_loss           | 1.5376901455965707e-05  |
| train_0/mu_grads          | -0.03636917639523744    |
| train_0/mu_grads_std      | 0.4250430420041084      |
| train_0/mu_loss           | 10.604735294746968      |
| train_0/next_q            | -10.471224907524208     |
| train_0/q_grads           | 0.033264451380819085    |
| train_0/q_grads_std       | 0.3800856299698353      |
| train_0/q_loss            | 0.5111875274154407      |
| train_0/reward            | -0.7431709721422521     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.3491943359375         |
| train_0/target_q          | -10.663607595035517     |
| train_1/avg_q             | -9.99087833029509       |
| train_1/current_q         | -8.164661062740958      |
| train_1/fw_bonus          | -1.0145538955926896     |
| train_1/fw_loss           | 0.00013086634717183188  |
| train_1/mu_grads          | -0.00016731767391320318 |
| train_1/mu_grads_std      | 0.1178409606218338      |
| train_1/mu_loss           | 11.0                    |
| train_1/n_subgoals        | 1000.0                  |
| train_1/next_q            | -10.0                   |
| train_1/q_grads           | 0.0015473347157239914   |
| train_1/q_grads_std       | 0.9389603659510612      |
| train_1/q_loss            | 2.3733611029290316      |
| train_1/reward            | -1.3303848554176512     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.0                     |
| train_1/reward_-10.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -8.330829191355154      |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 80
Time for epoch 80: 738.64. Rollout time: 347.67, Training time: 390.83
Evaluating epoch 80
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
-------------------------------------------------------
| epoch                     | 80                      |
| policy/steps              | 2529021.0               |
| test/episodes             | 2025.0                  |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -25.0                   |
| test_1/avg_q              | -10.0                   |
| test_1/n_subgoals         | 250.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 8100.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -25.0                   |
| train_0/current_q         | -10.459696612067962     |
| train_0/fw_bonus          | -0.9999301761388779     |
| train_0/fw_loss           | 1.7645344792072136e-05  |
| train_0/mu_grads          | -0.035740206204354766   |
| train_0/mu_grads_std      | 0.42554118409752845     |
| train_0/mu_loss           | 10.486528127685377      |
| train_0/next_q            | -10.457542662538831     |
| train_0/q_grads           | 0.033005587942898276    |
| train_0/q_grads_std       | 0.3804877854883671      |
| train_0/q_loss            | 0.552446150883408       |
| train_0/reward            | -0.7429338373265637     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.3185302734375         |
| train_0/target_q          | -10.656679833299185     |
| train_1/avg_q             | -10.0                   |
| train_1/current_q         | -8.227694653547198      |
| train_1/fw_bonus          | -1.0143465667963028     |
| train_1/fw_loss           | 0.00017943200218724088  |
| train_1/mu_grads          | -0.00016731767391320318 |
| train_1/mu_grads_std      | 0.1178409606218338      |
| train_1/mu_loss           | 11.0                    |
| train_1/n_subgoals        | 1000.0                  |
| train_1/next_q            | -10.0                   |
| train_1/q_grads           | 0.0014234020665753633   |
| train_1/q_grads_std       | 0.9420595526695251      |
| train_1/q_loss            | 2.365837816429983       |
| train_1/reward            | -1.3316850114082626     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.0                     |
| train_1/reward_-10.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -8.400317823908265      |
-------------------------------------------------------
Saving periodic policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_80.pkl ...
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 81
Time for epoch 81: 749.02. Rollout time: 350.10, Training time: 398.61
Evaluating epoch 81
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
-------------------------------------------------------
| epoch                     | 81                      |
| policy/steps              | 2560271.0               |
| test/episodes             | 2050.0                  |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -25.0                   |
| test_1/avg_q              | -10.0                   |
| test_1/n_subgoals         | 250.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 8200.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -24.993750030774297     |
| train_0/current_q         | -10.592487380989809     |
| train_0/fw_bonus          | -0.9999332040548324     |
| train_0/fw_loss           | 1.698553996902774e-05   |
| train_0/mu_grads          | -0.03851830484345555    |
| train_0/mu_grads_std      | 0.4250757023692131      |
| train_0/mu_loss           | 10.65522722635607       |
| train_0/next_q            | -10.590243492147252     |
| train_0/q_grads           | 0.03277826625853777     |
| train_0/q_grads_std       | 0.3810032457113266      |
| train_0/q_loss            | 0.6353618405146437      |
| train_0/reward            | -0.7444734361917653     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.3350830078125         |
| train_0/target_q          | -10.795832109571759     |
| train_1/avg_q             | -9.99992618757733       |
| train_1/current_q         | -8.224677184513855      |
| train_1/fw_bonus          | -1.01443153321743       |
| train_1/fw_loss           | 0.00015953390993672656  |
| train_1/mu_grads          | -0.00016731767391320318 |
| train_1/mu_grads_std      | 0.1178409606218338      |
| train_1/mu_loss           | 11.0                    |
| train_1/n_subgoals        | 1000.0                  |
| train_1/next_q            | -10.0                   |
| train_1/q_grads           | 0.0009458016385906376   |
| train_1/q_grads_std       | 0.9456075131893158      |
| train_1/q_loss            | 2.3379501734087698      |
| train_1/reward            | -1.3162489641064894     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.0                     |
| train_1/reward_-10.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -8.414071229731494      |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 82
Time for epoch 82: 751.47. Rollout time: 355.82, Training time: 395.43
Evaluating epoch 82
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
-------------------------------------------------------
| epoch                     | 82                      |
| policy/steps              | 2591521.0               |
| test/episodes             | 2075.0                  |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -25.0                   |
| test_1/avg_q              | -10.0                   |
| test_1/n_subgoals         | 250.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 8300.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -25.0                   |
| train_0/current_q         | -10.1414584897478       |
| train_0/fw_bonus          | -0.9999424919486046     |
| train_0/fw_loss           | 1.495939795859158e-05   |
| train_0/mu_grads          | -0.03787303548306227    |
| train_0/mu_grads_std      | 0.42504367232322693     |
| train_0/mu_loss           | 10.224022878923655      |
| train_0/next_q            | -10.137626146590808     |
| train_0/q_grads           | 0.032428779639303684    |
| train_0/q_grads_std       | 0.3817309081554413      |
| train_0/q_loss            | 0.36604171026065996     |
| train_0/reward            | -0.7435503475848236     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.346337890625          |
| train_0/target_q          | -10.308950068302016     |
| train_1/avg_q             | -9.999999999904464      |
| train_1/current_q         | -8.255736116643964      |
| train_1/fw_bonus          | -1.0144380241632462     |
| train_1/fw_loss           | 0.00015800951478013304  |
| train_1/mu_grads          | -0.00016731767391320318 |
| train_1/mu_grads_std      | 0.1178409606218338      |
| train_1/mu_loss           | 11.0                    |
| train_1/n_subgoals        | 1000.0                  |
| train_1/next_q            | -10.0                   |
| train_1/q_grads           | 0.0006764576784917154   |
| train_1/q_grads_std       | 0.9487825766205787      |
| train_1/q_loss            | 2.530920810337451       |
| train_1/reward            | -1.3063279009060351     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.0                     |
| train_1/reward_-10.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -8.456069111843538      |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 83
Time for epoch 83: 737.22. Rollout time: 315.01, Training time: 422.03
Evaluating epoch 83
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 83                     |
| policy/steps              | 2619195.0              |
| test/episodes             | 2100.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -7.96267611570695      |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 3696.0                 |
| test_1/subgoal_succ_rate  | 0.9710497835497836     |
| train/episodes            | 8400.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -24.380946202117908    |
| train_0/current_q         | -10.536576075889013    |
| train_0/fw_bonus          | -0.9999439999461174    |
| train_0/fw_loss           | 1.4631359886152494e-05 |
| train_0/mu_grads          | -0.038334562443196774  |
| train_0/mu_grads_std      | 0.4254854775965214     |
| train_0/mu_loss           | 10.560444535465924     |
| train_0/next_q            | -10.536206777138887    |
| train_0/q_grads           | 0.033047763723880054   |
| train_0/q_grads_std       | 0.3829182371497154     |
| train_0/q_loss            | 0.4212021955987719     |
| train_0/reward            | -0.7429529552980967    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.3280517578125        |
| train_0/target_q          | -10.751318606838101    |
| train_1/avg_q             | -9.999999999999995     |
| train_1/current_q         | -8.220943699995855     |
| train_1/fw_bonus          | -1.0143840163946152    |
| train_1/fw_loss           | 0.00017065986794477793 |
| train_1/mu_grads          | -0.0006047802569810301 |
| train_1/mu_grads_std      | 0.1171876460313797     |
| train_1/mu_loss           | 10.084715447169632     |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -9.073420200939605     |
| train_1/q_grads           | -0.001681607571663335  |
| train_1/q_grads_std       | 0.9545239835977555     |
| train_1/q_loss            | 5.992443758419329      |
| train_1/reward            | -1.299074338994251     |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0                    |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.158                  |
| train_1/target_q          | -7.96137528468976      |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 84
Time for epoch 84: 471.49. Rollout time: 75.98, Training time: 395.08
Evaluating epoch 84
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 84                     |
| policy/steps              | 2630192.0              |
| test/episodes             | 2125.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -1.2201045790131264    |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 6030.0                 |
| test_1/subgoal_succ_rate  | 0.9983416252072969     |
| train/episodes            | 8500.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -17.71550044689823     |
| train_0/current_q         | -10.34953911761074     |
| train_0/fw_bonus          | -0.9999419197440147    |
| train_0/fw_loss           | 1.5086141434039747e-05 |
| train_0/mu_grads          | -0.038191089686006305  |
| train_0/mu_grads_std      | 0.4251404546201229     |
| train_0/mu_loss           | 10.401915524630862     |
| train_0/next_q            | -10.348753680824439    |
| train_0/q_grads           | 0.03295951001346111    |
| train_0/q_grads_std       | 0.38377762511372565    |
| train_0/q_loss            | 0.5436155278079887     |
| train_0/reward            | -0.7406936117440637    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.336083984375         |
| train_0/target_q          | -10.518737924891969    |
| train_1/avg_q             | -9.998484478814163     |
| train_1/current_q         | -7.6488607177682635    |
| train_1/fw_bonus          | -1.014114961028099     |
| train_1/fw_loss           | 0.00023369337650365197 |
| train_1/mu_grads          | -0.0005110019978019409 |
| train_1/mu_grads_std      | 0.11635437663644552    |
| train_1/mu_loss           | 8.83625738735985       |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -7.818198032527792     |
| train_1/q_grads           | -0.002634200605098158  |
| train_1/q_grads_std       | 0.9585858538746834     |
| train_1/q_loss            | 8.224509991516488      |
| train_1/reward            | -1.2124651984988304    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 2.44140625e-05         |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.862                  |
| train_1/target_q          | -7.219541455646936     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 85
Time for epoch 85: 413.41. Rollout time: 33.60, Training time: 379.52
Evaluating epoch 85
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 85                     |
| policy/steps              | 2638777.0              |
| test/episodes             | 2150.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -1.9972369222057722    |
| test_1/avg_q              | -9.999999999312617     |
| test_1/n_subgoals         | 4894.0                 |
| test_1/subgoal_succ_rate  | 0.9903964037597057     |
| train/episodes            | 8600.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -11.81893581186488     |
| train_0/current_q         | -10.14758164768343     |
| train_0/fw_bonus          | -0.9999278843402862    |
| train_0/fw_loss           | 1.8143014608540396e-05 |
| train_0/mu_grads          | -0.03832256644964218   |
| train_0/mu_grads_std      | 0.42536870688199996    |
| train_0/mu_loss           | 10.167629388145787     |
| train_0/next_q            | -10.146185229029323    |
| train_0/q_grads           | 0.03274366315454245    |
| train_0/q_grads_std       | 0.38424528390169144    |
| train_0/q_loss            | 0.3359081123764387     |
| train_0/reward            | -0.7416631649801275    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.3113525390625        |
| train_0/target_q          | -10.346883179637341    |
| train_1/avg_q             | -9.969776253447346     |
| train_1/current_q         | -6.916381790705307     |
| train_1/fw_bonus          | -1.01381097137928      |
| train_1/fw_loss           | 0.00030491184297716246 |
| train_1/mu_grads          | -0.0002676656600669958 |
| train_1/mu_grads_std      | 0.11911842953413725    |
| train_1/mu_loss           | 7.600077381558063      |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -6.573812134388537     |
| train_1/q_grads           | -0.005379434686619789  |
| train_1/q_grads_std       | 0.9590190693736076     |
| train_1/q_loss            | 9.518010643917986      |
| train_1/reward            | -1.1041466946451692    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 4.8828125e-05          |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.958                  |
| train_1/target_q          | -6.345174597941771     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 86
Time for epoch 86: 490.09. Rollout time: 66.48, Training time: 423.35
Evaluating epoch 86
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
-------------------------------------------------------
| epoch                     | 86                      |
| policy/steps              | 2649250.0               |
| test/episodes             | 2175.0                  |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -8.86962797076864       |
| test_1/avg_q              | -9.99996799415665       |
| test_1/n_subgoals         | 3581.0                  |
| test_1/subgoal_succ_rate  | 0.9673275621334823      |
| train/episodes            | 8700.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -17.029495389580784     |
| train_0/current_q         | -10.363189855664853     |
| train_0/fw_bonus          | -0.9999510258436203     |
| train_0/fw_loss           | 1.3098307613290672e-05  |
| train_0/mu_grads          | -0.03908230224624276    |
| train_0/mu_grads_std      | 0.4250103861093521      |
| train_0/mu_loss           | 10.393414763826229      |
| train_0/next_q            | -10.361222220988143     |
| train_0/q_grads           | 0.032640129886567594    |
| train_0/q_grads_std       | 0.38456292971968653     |
| train_0/q_loss            | 0.5345679640673762      |
| train_0/reward            | -0.740661548616481      |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.3512939453125         |
| train_0/target_q          | -10.558299217595417     |
| train_1/avg_q             | -9.940661900371818      |
| train_1/current_q         | -6.499104045039604      |
| train_1/fw_bonus          | -1.0136275917291642     |
| train_1/fw_loss           | 0.0003478724531305488   |
| train_1/mu_grads          | -0.00021741590717283543 |
| train_1/mu_grads_std      | 0.12403437420725823     |
| train_1/mu_loss           | 6.746520912573773       |
| train_1/n_subgoals        | 1000.0                  |
| train_1/next_q            | -5.722319991083038      |
| train_1/q_grads           | -0.007380112295504659   |
| train_1/q_grads_std       | 0.9612178057432175      |
| train_1/q_loss            | 11.438998244551124      |
| train_1/reward            | -1.0168845940694156     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.0                     |
| train_1/reward_-10.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.889                   |
| train_1/target_q          | -5.719379167528001      |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 87
Time for epoch 87: 472.60. Rollout time: 73.24, Training time: 399.15
Evaluating epoch 87
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 87                     |
| policy/steps              | 2660373.0              |
| test/episodes             | 2200.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -10.726577640593296    |
| test_1/avg_q              | -9.999999766232177     |
| test_1/n_subgoals         | 3409.0                 |
| test_1/subgoal_succ_rate  | 0.965679084775594      |
| train/episodes            | 8800.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -15.918620166596549    |
| train_0/current_q         | -10.317053408899       |
| train_0/fw_bonus          | -0.9999279484152794    |
| train_0/fw_loss           | 1.8131440469915105e-05 |
| train_0/mu_grads          | -0.038575968146324156  |
| train_0/mu_grads_std      | 0.4245190016925335     |
| train_0/mu_loss           | 10.332733624365357     |
| train_0/next_q            | -10.312797179429591    |
| train_0/q_grads           | 0.03265618896111846    |
| train_0/q_grads_std       | 0.38572215512394903    |
| train_0/q_loss            | 0.5632774510379617     |
| train_0/reward            | -0.7401006916210463    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.2100830078125        |
| train_0/target_q          | -10.52712811947973     |
| train_1/avg_q             | -9.918128588919586     |
| train_1/current_q         | -6.150137227466752     |
| train_1/fw_bonus          | -1.0134646028280259    |
| train_1/fw_loss           | 0.00038605513764196077 |
| train_1/mu_grads          | -5.993878494336968e-05 |
| train_1/mu_grads_std      | 0.12803693898022175    |
| train_1/mu_loss           | 6.0438182712942154     |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -4.97978636643852      |
| train_1/q_grads           | -0.009019188676029443  |
| train_1/q_grads_std       | 0.9635492414236069     |
| train_1/q_loss            | 13.672858228084243     |
| train_1/reward            | -0.9309266370582918    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.000146484375         |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.864                  |
| train_1/target_q          | -5.106741388483013     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 88
Time for epoch 88: 561.29. Rollout time: 169.15, Training time: 391.90
Evaluating epoch 88
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 88                     |
| policy/steps              | 2678983.0              |
| test/episodes             | 2225.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -11.725496021813992    |
| test_1/avg_q              | -4.014227008716091     |
| test_1/n_subgoals         | 1384.0                 |
| test_1/subgoal_succ_rate  | 0.8526011560693642     |
| train/episodes            | 8900.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -18.666265050565485    |
| train_0/current_q         | -10.070053455782439    |
| train_0/fw_bonus          | -0.9999544158577919    |
| train_0/fw_loss           | 1.2358532853795622e-05 |
| train_0/mu_grads          | -0.028170500043779613  |
| train_0/mu_grads_std      | 0.42927939519286157    |
| train_0/mu_loss           | 9.413997271356074      |
| train_0/next_q            | -10.063865612719233    |
| train_0/q_grads           | 0.03184089818969369    |
| train_0/q_grads_std       | 0.3865840457379818     |
| train_0/q_loss            | 0.6950697847073963     |
| train_0/reward            | -0.7385270143589878    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.3536376953125        |
| train_0/target_q          | -10.242552351525676    |
| train_1/avg_q             | -8.69541711316003      |
| train_1/current_q         | -3.0382052333463005    |
| train_1/fw_bonus          | -1.013580819964409     |
| train_1/fw_loss           | 0.0003588287239836063  |
| train_1/mu_grads          | -0.0006564515642821789 |
| train_1/mu_grads_std      | 0.12661340720951558    |
| train_1/mu_loss           | 3.86583806486066       |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -2.8729087158101225    |
| train_1/q_grads           | -0.013637537509202958  |
| train_1/q_grads_std       | 0.9595361679792405     |
| train_1/q_loss            | 1.7856600520726023     |
| train_1/reward            | -0.8747713668730285    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 2.44140625e-05         |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.579                  |
| train_1/target_q          | -2.980621398479092     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 89
Time for epoch 89: 645.96. Rollout time: 229.93, Training time: 415.72
Evaluating epoch 89
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 89                     |
| policy/steps              | 2700308.0              |
| test/episodes             | 2250.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -16.33931203165675     |
| test_1/avg_q              | -9.749731338254827     |
| test_1/n_subgoals         | 1525.0                 |
| test_1/subgoal_succ_rate  | 0.8695081967213115     |
| train/episodes            | 9000.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -19.05489834824618     |
| train_0/current_q         | -10.331750821839984    |
| train_0/fw_bonus          | -0.9999539673328399    |
| train_0/fw_loss           | 1.245818120878539e-05  |
| train_0/mu_grads          | -0.026748761255294084  |
| train_0/mu_grads_std      | 0.4327087372541428     |
| train_0/mu_loss           | 8.858291284663158      |
| train_0/next_q            | -10.331347946633894    |
| train_0/q_grads           | 0.032172860112041235   |
| train_0/q_grads_std       | 0.3876828864216805     |
| train_0/q_loss            | 0.7057849110334624     |
| train_0/reward            | -0.7426527624716982    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.376708984375         |
| train_0/target_q          | -10.511141464469972    |
| train_1/avg_q             | -7.203971027368174     |
| train_1/current_q         | -4.317782121793836     |
| train_1/fw_bonus          | -1.0136612236499787    |
| train_1/fw_loss           | 0.0003399924324185122  |
| train_1/mu_grads          | -0.0008315449100336991 |
| train_1/mu_grads_std      | 0.12770180478692056    |
| train_1/mu_loss           | 5.287616487146237      |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -4.1596918004544134    |
| train_1/q_grads           | -0.013341183215379715  |
| train_1/q_grads_std       | 0.9551479443907738     |
| train_1/q_loss            | 1.6280090914838383     |
| train_1/reward            | -0.9069404749243404    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 2.44140625e-05         |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.464                  |
| train_1/target_q          | -4.457272019151321     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 90
Time for epoch 90: 547.81. Rollout time: 143.28, Training time: 404.22
Evaluating epoch 90
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 90                     |
| policy/steps              | 2716332.0              |
| test/episodes             | 2275.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -7.573865816865161     |
| test_1/avg_q              | -8.458985554931735     |
| test_1/n_subgoals         | 3550.0                 |
| test_1/subgoal_succ_rate  | 0.9692957746478873     |
| train/episodes            | 9100.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -17.559726549471694    |
| train_0/current_q         | -9.687821383756443     |
| train_0/fw_bonus          | -0.9999451622366905    |
| train_0/fw_loss           | 1.4376593526321813e-05 |
| train_0/mu_grads          | -0.026939290668815376  |
| train_0/mu_grads_std      | 0.43523831367492677    |
| train_0/mu_loss           | 8.606297308047072      |
| train_0/next_q            | -9.682390409352532     |
| train_0/q_grads           | 0.03156144209206104    |
| train_0/q_grads_std       | 0.388976963609457      |
| train_0/q_loss            | 0.4199452036395517     |
| train_0/reward            | -0.7373138909380941    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.326025390625         |
| train_0/target_q          | -9.843594128071999     |
| train_1/avg_q             | -9.25236044071366      |
| train_1/current_q         | -4.2455870174827       |
| train_1/fw_bonus          | -1.0135325372219086    |
| train_1/fw_loss           | 0.0003701358684338629  |
| train_1/mu_grads          | -0.0017201226466568187 |
| train_1/mu_grads_std      | 0.13113458566367625    |
| train_1/mu_loss           | 5.2630079164579495     |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -4.163598565503646     |
| train_1/q_grads           | -0.014402678445912897  |
| train_1/q_grads_std       | 0.956747616827488      |
| train_1/q_loss            | 1.9452685462279327     |
| train_1/reward            | -0.9647008475069014    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 4.8828125e-05          |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.675                  |
| train_1/target_q          | -4.317358170780044     |
------------------------------------------------------
Saving periodic policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_90.pkl ...
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 91
Time for epoch 91: 549.48. Rollout time: 146.96, Training time: 402.16
Evaluating epoch 91
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 91                     |
| policy/steps              | 2733223.0              |
| test/episodes             | 2300.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -10.803694642948       |
| test_1/avg_q              | -6.637407399004881     |
| test_1/n_subgoals         | 575.0                  |
| test_1/subgoal_succ_rate  | 0.5860869565217391     |
| train/episodes            | 9200.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -17.388246056949026    |
| train_0/current_q         | -10.14889268064671     |
| train_0/fw_bonus          | -0.9999178931117058    |
| train_0/fw_loss           | 2.0324948218330972e-05 |
| train_0/mu_grads          | -0.02709874100983143   |
| train_0/mu_grads_std      | 0.43521535992622373    |
| train_0/mu_loss           | 8.541598171017158      |
| train_0/next_q            | -10.149140978908198    |
| train_0/q_grads           | 0.03140932796522975    |
| train_0/q_grads_std       | 0.39032087251544       |
| train_0/q_loss            | 0.7359610003828764     |
| train_0/reward            | -0.7408280836098129    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.290185546875         |
| train_0/target_q          | -10.316276436202022    |
| train_1/avg_q             | -8.603717577953423     |
| train_1/current_q         | -3.704062865473        |
| train_1/fw_bonus          | -1.0136376857757567    |
| train_1/fw_loss           | 0.0003455052166827954  |
| train_1/mu_grads          | -0.0024300552322529256 |
| train_1/mu_grads_std      | 0.1346344981342554     |
| train_1/mu_loss           | 4.943578677403106      |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -3.8797684457841797    |
| train_1/q_grads           | -0.01712772171013057   |
| train_1/q_grads_std       | 0.9603178411722183     |
| train_1/q_loss            | 2.957842020991449      |
| train_1/reward            | -1.0024243145649963    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0                    |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.638                  |
| train_1/target_q          | -3.8117215189033424    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 92
Time for epoch 92: 606.93. Rollout time: 195.90, Training time: 410.78
Evaluating epoch 92
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 92                     |
| policy/steps              | 2753052.0              |
| test/episodes             | 2325.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -3.881511557613439     |
| test_1/avg_q              | -8.659819087767925     |
| test_1/n_subgoals         | 3730.0                 |
| test_1/subgoal_succ_rate  | 0.9739946380697051     |
| train/episodes            | 9300.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -17.32640039336464     |
| train_0/current_q         | -9.98367835132061      |
| train_0/fw_bonus          | -0.9999542161822319    |
| train_0/fw_loss           | 1.2402648690112982e-05 |
| train_0/mu_grads          | -0.028022201638668774  |
| train_0/mu_grads_std      | 0.43521794900298116    |
| train_0/mu_loss           | 8.618113477919598      |
| train_0/next_q            | -9.985137816609125     |
| train_0/q_grads           | 0.03147403411567211    |
| train_0/q_grads_std       | 0.3916926451027393     |
| train_0/q_loss            | 0.5173900660590276     |
| train_0/reward            | -0.7387344326496532    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.3646484375           |
| train_0/target_q          | -10.16333114224876     |
| train_1/avg_q             | -7.8228055768067115    |
| train_1/current_q         | -4.7653352042550665    |
| train_1/fw_bonus          | -1.0138087064027785    |
| train_1/fw_loss           | 0.000305439157091314   |
| train_1/mu_grads          | -0.0023457753530237825 |
| train_1/mu_grads_std      | 0.1365193475037813     |
| train_1/mu_loss           | 5.8241729802016575     |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -4.621241964651939     |
| train_1/q_grads           | -0.01852903780527413   |
| train_1/q_grads_std       | 0.9668436780571937     |
| train_1/q_loss            | 2.0150574300115185     |
| train_1/reward            | -1.0636548461356141    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 2.44140625e-05         |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.532                  |
| train_1/target_q          | -4.795446558722365     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 93
Time for epoch 93: 492.11. Rollout time: 97.94, Training time: 393.98
Evaluating epoch 93
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 93                     |
| policy/steps              | 2766240.0              |
| test/episodes             | 2350.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -4.507338533591591     |
| test_1/avg_q              | -8.62085557941603      |
| test_1/n_subgoals         | 3907.0                 |
| test_1/subgoal_succ_rate  | 0.9741489633990273     |
| train/episodes            | 9400.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -13.68592813009911     |
| train_0/current_q         | -9.69010827701916      |
| train_0/fw_bonus          | -0.9999470055103302    |
| train_0/fw_loss           | 1.3973893578622664e-05 |
| train_0/mu_grads          | -0.028003207826986908  |
| train_0/mu_grads_std      | 0.4352319911122322     |
| train_0/mu_loss           | 8.34823721355595       |
| train_0/next_q            | -9.68843722967081      |
| train_0/q_grads           | 0.03138001970946789    |
| train_0/q_grads_std       | 0.3933885209262371     |
| train_0/q_loss            | 0.6970121085627538     |
| train_0/reward            | -0.7358499706162547    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.3150634765625        |
| train_0/target_q          | -9.875190595208343     |
| train_1/avg_q             | -8.991815396618083     |
| train_1/current_q         | -4.644652873606992     |
| train_1/fw_bonus          | -1.0137705117464066    |
| train_1/fw_loss           | 0.0003143871981592383  |
| train_1/mu_grads          | -0.0025075689307413997 |
| train_1/mu_grads_std      | 0.13887569569051267    |
| train_1/mu_loss           | 5.630992715921051      |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -4.434078076492078     |
| train_1/q_grads           | -0.022021098621189596  |
| train_1/q_grads_std       | 0.9703618615865708     |
| train_1/q_loss            | 1.7000798138840074     |
| train_1/reward            | -1.0485966866304808    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0                    |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.794                  |
| train_1/target_q          | -4.68000990013369      |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 94
Time for epoch 94: 506.61. Rollout time: 83.08, Training time: 423.21
Evaluating epoch 94
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 94                     |
| policy/steps              | 2777988.0              |
| test/episodes             | 2375.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -0.7980661982388558    |
| test_1/avg_q              | -8.604352715217749     |
| test_1/n_subgoals         | 5633.0                 |
| test_1/subgoal_succ_rate  | 0.9955618675661282     |
| train/episodes            | 9500.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -12.827100014149188    |
| train_0/current_q         | -9.436838431687677     |
| train_0/fw_bonus          | -0.9999403089284897    |
| train_0/fw_loss           | 1.5436148373737524e-05 |
| train_0/mu_grads          | -0.028200828889384865  |
| train_0/mu_grads_std      | 0.43522521778941153    |
| train_0/mu_loss           | 8.229595768833926      |
| train_0/next_q            | -9.436920252333445     |
| train_0/q_grads           | 0.03125849864445627    |
| train_0/q_grads_std       | 0.3954804837703705     |
| train_0/q_loss            | 0.6502494069132325     |
| train_0/reward            | -0.7380927114667429    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.3609375              |
| train_0/target_q          | -9.649365076282953     |
| train_1/avg_q             | -9.184241845242513     |
| train_1/current_q         | -4.533142600735549     |
| train_1/fw_bonus          | -1.013753291964531     |
| train_1/fw_loss           | 0.0003184229113685433  |
| train_1/mu_grads          | -0.002899879647884518  |
| train_1/mu_grads_std      | 0.1443646013736725     |
| train_1/mu_loss           | 5.544285681361951      |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -4.364464315643893     |
| train_1/q_grads           | -0.024529091129079462  |
| train_1/q_grads_std       | 0.9767412796616555     |
| train_1/q_loss            | 1.606043516704466      |
| train_1/reward            | -0.9847941278123471    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 2.44140625e-05         |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.844                  |
| train_1/target_q          | -4.610946442083105     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 95
Time for epoch 95: 518.80. Rollout time: 109.91, Training time: 408.59
Evaluating epoch 95
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 95                     |
| policy/steps              | 2791577.0              |
| test/episodes             | 2400.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -3.0952638278962077    |
| test_1/avg_q              | -8.625655448166183     |
| test_1/n_subgoals         | 3139.0                 |
| test_1/subgoal_succ_rate  | 0.9579483912073908     |
| train/episodes            | 9600.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -12.829881246378621    |
| train_0/current_q         | -9.584228106632642     |
| train_0/fw_bonus          | -0.9999538138508797    |
| train_0/fw_loss           | 1.2492274095166067e-05 |
| train_0/mu_grads          | -0.02764244950376451   |
| train_0/mu_grads_std      | 0.4351459972560406     |
| train_0/mu_loss           | 7.379766697611691      |
| train_0/next_q            | -9.583468778433623     |
| train_0/q_grads           | 0.03155250838026404    |
| train_0/q_grads_std       | 0.3975869543850422     |
| train_0/q_loss            | 0.6136651347611712     |
| train_0/reward            | -0.7356702623379533    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.3670166015625        |
| train_0/target_q          | -9.788971862337316     |
| train_1/avg_q             | -9.152108560537553     |
| train_1/current_q         | -4.538584241730538     |
| train_1/fw_bonus          | -1.0137710094451904    |
| train_1/fw_loss           | 0.0003142720539472066  |
| train_1/mu_grads          | -0.0031874498818069696 |
| train_1/mu_grads_std      | 0.15009392201900482    |
| train_1/mu_loss           | 5.509722218412945      |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -4.346917302684785     |
| train_1/q_grads           | -0.02511547217145562   |
| train_1/q_grads_std       | 0.9862543165683746     |
| train_1/q_loss            | 1.5032113712526567     |
| train_1/reward            | -0.9612556235289957    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 2.44140625e-05         |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.769                  |
| train_1/target_q          | -4.636431292112169     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 96
Time for epoch 96: 514.33. Rollout time: 112.12, Training time: 401.98
Evaluating epoch 96
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 96                     |
| policy/steps              | 2805692.0              |
| test/episodes             | 2425.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -5.321759664884743     |
| test_1/avg_q              | -8.55313920784854      |
| test_1/n_subgoals         | 2481.0                 |
| test_1/subgoal_succ_rate  | 0.9367190648931882     |
| train/episodes            | 9700.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -11.469239291411014    |
| train_0/current_q         | -9.61574743068935      |
| train_0/fw_bonus          | -0.9999573603272438    |
| train_0/fw_loss           | 1.1718932080384548e-05 |
| train_0/mu_grads          | -0.026624372601509093  |
| train_0/mu_grads_std      | 0.4351093254983425     |
| train_0/mu_loss           | 7.389676322039065      |
| train_0/next_q            | -9.617166276052895     |
| train_0/q_grads           | 0.03161820024251938    |
| train_0/q_grads_std       | 0.39994976967573165    |
| train_0/q_loss            | 0.7476641678448491     |
| train_0/reward            | -0.7344757466933516    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.36630859375          |
| train_0/target_q          | -9.798203487122024     |
| train_1/avg_q             | -9.240155840728843     |
| train_1/current_q         | -4.449880122517494     |
| train_1/fw_bonus          | -1.0138141363859177    |
| train_1/fw_loss           | 0.0003041699805180542  |
| train_1/mu_grads          | -0.003390937321819365  |
| train_1/mu_grads_std      | 0.15554829649627208    |
| train_1/mu_loss           | 5.445559761207447      |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -4.284516397215133     |
| train_1/q_grads           | -0.02542341356165707   |
| train_1/q_grads_std       | 0.9942812621593475     |
| train_1/q_loss            | 1.5968086927915508     |
| train_1/reward            | -0.9547586294786015    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0                    |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.748                  |
| train_1/target_q          | -4.5222040422931205    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 97
Time for epoch 97: 642.68. Rollout time: 229.34, Training time: 413.09
Evaluating epoch 97
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 97                     |
| policy/steps              | 2827678.0              |
| test/episodes             | 2450.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -13.029307189528556    |
| test_1/avg_q              | -6.664040601221527     |
| test_1/n_subgoals         | 958.0                  |
| test_1/subgoal_succ_rate  | 0.767223382045929      |
| train/episodes            | 9800.0                 |
| train/success_rate        | 0.01                   |
| train_0/avg_q             | -15.586429772732213    |
| train_0/current_q         | -9.693197618311242     |
| train_0/fw_bonus          | -0.9999363645911217    |
| train_0/fw_loss           | 1.6295508464736487e-05 |
| train_0/mu_grads          | -0.02679570927284658   |
| train_0/mu_grads_std      | 0.4342095755040646     |
| train_0/mu_loss           | 7.670367892691682      |
| train_0/next_q            | -9.698434316343706     |
| train_0/q_grads           | 0.03165822243317962    |
| train_0/q_grads_std       | 0.40131521970033646    |
| train_0/q_loss            | 0.8374820531700277     |
| train_0/reward            | -0.7357836119103013    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.3730712890625        |
| train_0/target_q          | -9.856870521078896     |
| train_1/avg_q             | -7.184416721342831     |
| train_1/current_q         | -3.88532759853249      |
| train_1/fw_bonus          | -1.013870170712471     |
| train_1/fw_loss           | 0.0002910427734605037  |
| train_1/mu_grads          | -0.0038796789071056993 |
| train_1/mu_grads_std      | 0.16092320084571837    |
| train_1/mu_loss           | 4.88774386077157       |
| train_1/n_subgoals        | 998.0                  |
| train_1/next_q            | -3.746353238899694     |
| train_1/q_grads           | -0.02458679019473493   |
| train_1/q_grads_std       | 0.9946937680244445     |
| train_1/q_loss            | 1.666274883395905      |
| train_1/reward            | -0.9705825452576391    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0                    |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.41783567134268534    |
| train_1/target_q          | -3.9002281311265192    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 98
Time for epoch 98: 656.35. Rollout time: 259.87, Training time: 396.27
Evaluating epoch 98
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 98                     |
| policy/steps              | 2851993.0              |
| test/episodes             | 2475.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -15.501667312852621    |
| test_1/avg_q              | -6.3064249039500595    |
| test_1/n_subgoals         | 700.0                  |
| test_1/subgoal_succ_rate  | 0.6685714285714286     |
| train/episodes            | 9900.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -17.275428383142128    |
| train_0/current_q         | -9.222620930544995     |
| train_0/fw_bonus          | -0.9999431997537613    |
| train_0/fw_loss           | 1.4808352761974675e-05 |
| train_0/mu_grads          | -0.026413214951753618  |
| train_0/mu_grads_std      | 0.43399227485060693    |
| train_0/mu_loss           | 7.333376672031127      |
| train_0/next_q            | -9.222176605077859     |
| train_0/q_grads           | 0.03162370761856437    |
| train_0/q_grads_std       | 0.402912013232708      |
| train_0/q_loss            | 0.5249630021181038     |
| train_0/reward            | -0.7309530375889153    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.3529296875           |
| train_0/target_q          | -9.362513413669747     |
| train_1/avg_q             | -6.2719992605542645    |
| train_1/current_q         | -3.980482543425431     |
| train_1/fw_bonus          | -1.013911834359169     |
| train_1/fw_loss           | 0.0002812862421706086  |
| train_1/mu_grads          | -0.004053232108708471  |
| train_1/mu_grads_std      | 0.16516750268638133    |
| train_1/mu_loss           | 4.983952819267039      |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -3.893490275994593     |
| train_1/q_grads           | -0.02245878675021231   |
| train_1/q_grads_std       | 0.9965695306658745     |
| train_1/q_loss            | 1.9542411373221262     |
| train_1/reward            | -1.0130105946242112    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0                    |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.336                  |
| train_1/target_q          | -3.985684462217513     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 99
Time for epoch 99: 667.53. Rollout time: 254.27, Training time: 413.07
Evaluating epoch 99
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 99                     |
| policy/steps              | 2876240.0              |
| test/episodes             | 2500.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -13.798451176835838    |
| test_1/avg_q              | -7.246549214855067     |
| test_1/n_subgoals         | 1021.0                 |
| test_1/subgoal_succ_rate  | 0.7855044074436827     |
| train/episodes            | 10000.0                |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -17.38749094799287     |
| train_0/current_q         | -8.832593821081032     |
| train_0/fw_bonus          | -0.9999491721391678    |
| train_0/fw_loss           | 1.3500820887202281e-05 |
| train_0/mu_grads          | -0.028042899910360575  |
| train_0/mu_grads_std      | 0.43212649077177046    |
| train_0/mu_loss           | 10.153526082572924     |
| train_0/next_q            | -8.829933184118639     |
| train_0/q_grads           | 0.031028822949156164   |
| train_0/q_grads_std       | 0.404015289992094      |
| train_0/q_loss            | 1.2448410531176637     |
| train_0/reward            | -0.733336406849412     |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.3616943359375        |
| train_0/target_q          | -8.993003479014195     |
| train_1/avg_q             | -6.244403857266201     |
| train_1/current_q         | -4.050877728259822     |
| train_1/fw_bonus          | -1.0139719724655152    |
| train_1/fw_loss           | 0.0002671948361239629  |
| train_1/mu_grads          | -0.0046590696787461635 |
| train_1/mu_grads_std      | 0.16976651325821876    |
| train_1/mu_loss           | 5.08664456806551       |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -3.8820696778530306    |
| train_1/q_grads           | -0.02226009713485837   |
| train_1/q_grads_std       | 1.0057332545518876     |
| train_1/q_loss            | 1.916720705402534      |
| train_1/reward            | -1.0706090296735056    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0                    |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.323                  |
| train_1/target_q          | -4.082626141541917     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
All epochs are finished. Stopping the training now.
