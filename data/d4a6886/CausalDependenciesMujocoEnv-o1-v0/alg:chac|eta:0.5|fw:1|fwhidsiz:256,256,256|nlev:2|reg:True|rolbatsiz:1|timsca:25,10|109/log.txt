Starting process id: 45837
T: 50
alg: chac
algorithm: src.chac
atomic_noise: 0.2
base_logdir: data
batch_size: 1024
bind_core: 0
buffer_size: 500
chac_params: {}
early_stop_data_column: test/success_rate
early_stop_threshold: 100.0
env_name: CausalDependenciesMujocoEnv-o1-v0
eta: 0.5
fw: 1
fw_hidden_size: 256,256,256
fw_lr: 0.001
gamma: 0.98
graph: 1
info: 
make_env: <function prepare_params.<locals>.make_env at 0x7f833214ee60>
max_try_idx: 199
mu_hidden_size: 64
mu_lr: 0.001
n_episodes: 100
n_levels: 2
n_pre_episodes: 30
n_test_rollouts: 25
n_train_batches: 40
n_train_rollouts: 100
num_threads: 1
q_hidden_size: 64
q_lr: 0.001
random_action_perc: 0.3
regularization: True
render: 0
rollout_batch_size: 1
subgoal_noise: 0.2
subgoal_test_perc: 0.3
time_scales: 25,10
try_start_idx: 100
use_mpi: False
verbose: False

*** Warning ***
You are running src.chac with just a single MPI worker. This will work, but the HER experiments that we report in Plappert et al. (2018, https://arxiv.org/abs/1802.09464) were obtained with --num_cpu 19. This makes a significant difference and if you are looking to reproduce those results, be aware of this. Please also refer to https://github.com/openai/baselines/issues/314 for further details.
****************

dims: action = 2, subgoal = 2, end_goal = 2
subgoal_bounds: symmetric [0.2 0.2], offset [0.922 0.25 ]
Running on CPU ...
Creating a CHAC agent

Hierarchy Level 0 with time scale 25
Actor(
  (fc1): Linear(in_features=35, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
  (fc4): Linear(in_features=64, out_features=2, bias=True)
)
Critic(
  (fc1): Linear(in_features=37, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
  (fc4): Linear(in_features=64, out_features=1, bias=True)
  (mse_loss): MSELoss()
)
ForwardModel(
  (mlp): Sequential(
    (0): Linear(in_features=35, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=256, bias=True)
    (5): ReLU()
    (6): Linear(in_features=256, out_features=33, bias=True)
    (7): Identity()
  )
  (mse_loss): MSELoss()
)

Hierarchy Level 1 with time scale 10
Actor(
  (fc1): Linear(in_features=35, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
  (fc4): Linear(in_features=64, out_features=2, bias=True)
)
Critic(
  (fc1): Linear(in_features=37, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
  (fc4): Linear(in_features=64, out_features=1, bias=True)
  (mse_loss): MSELoss()
)
ForwardModel(
  (mlp): Sequential(
    (0): Linear(in_features=35, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=256, bias=True)
    (5): ReLU()
    (6): Linear(in_features=256, out_features=33, bias=True)
    (7): Identity()
  )
  (mse_loss): MSELoss()
)
Training epoch 0
Time for epoch 0: 593.70. Rollout time: 242.53, Training time: 351.07
Evaluating epoch 0
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
-----------------------------------------------------
| epoch                     | 0                     |
| policy/steps              | 29736.0               |
| test/episodes             | 25.0                  |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -25.0                 |
| test_1/avg_q              | -4.087839614238626    |
| test_1/n_subgoals         | 250.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 100.0                 |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -16.641174361306565   |
| train_0/current_q         | -9.316669948066826    |
| train_0/fw_bonus          | -0.9996154174208641   |
| train_0/fw_loss           | 8.628220639366191e-05 |
| train_0/mu_grads          | 0.006506400508806109  |
| train_0/mu_grads_std      | 0.11783380433917046   |
| train_0/mu_loss           | 7.9846285630745255    |
| train_0/next_q            | -9.31521760140329     |
| train_0/q_grads           | 0.038394985254853964  |
| train_0/q_grads_std       | 0.1997710879892111    |
| train_0/q_loss            | 0.8142750813688402    |
| train_0/reward            | -0.7417266514494258   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0021240234375       |
| train_0/target_q          | -9.49820334797713     |
| train_1/avg_q             | -2.9672660673695135   |
| train_1/current_q         | -3.534858746517975    |
| train_1/fw_bonus          | -1.0113101959228517   |
| train_1/fw_loss           | 0.000890771254489664  |
| train_1/mu_grads          | 0.0037935562781058253 |
| train_1/mu_grads_std      | 0.11493037547916174   |
| train_1/mu_loss           | 4.03995422406892      |
| train_1/n_subgoals        | 1000.0                |
| train_1/next_q            | -3.0336023783111132   |
| train_1/q_grads           | 0.01871397364884615   |
| train_1/q_grads_std       | 0.1658079158514738    |
| train_1/q_loss            | 1.413051811691158     |
| train_1/reward            | -1.3087068541353801   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0002197265625       |
| train_1/reward_-10.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.076                 |
| train_1/target_q          | -3.5205810750453437   |
-----------------------------------------------------
Saving periodic policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_0.pkl ...
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Training epoch 1
Time for epoch 1: 353.53. Rollout time: 147.88, Training time: 205.61
Evaluating epoch 1
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
-----------------------------------------------------
| epoch                     | 1                     |
| policy/steps              | 60986.0               |
| test/episodes             | 50.0                  |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -25.0                 |
| test_1/avg_q              | -6.052527085183929    |
| test_1/n_subgoals         | 250.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 200.0                 |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -25.0                 |
| train_0/current_q         | -9.846988342553987    |
| train_0/fw_bonus          | -0.9996853351593018   |
| train_0/fw_loss           | 7.103216094037635e-05 |
| train_0/mu_grads          | 0.008902457566000522  |
| train_0/mu_grads_std      | 0.13165234848856927   |
| train_0/mu_loss           | 8.427261471157715     |
| train_0/next_q            | -9.846196150302724    |
| train_0/q_grads           | 0.04095825357362628   |
| train_0/q_grads_std       | 0.21968333534896373   |
| train_0/q_loss            | 0.7038416045519915    |
| train_0/reward            | -0.7421888653596398   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0025390625          |
| train_0/target_q          | -10.011795751020127   |
| train_1/avg_q             | -5.082949694935949    |
| train_1/current_q         | -4.8358918516223905   |
| train_1/fw_bonus          | -1.0118048161268234   |
| train_1/fw_loss           | 0.000774896341317799  |
| train_1/mu_grads          | 0.0006278965607634746 |
| train_1/mu_grads_std      | 0.11786453183740378   |
| train_1/mu_loss           | 5.817196182257029     |
| train_1/n_subgoals        | 1000.0                |
| train_1/next_q            | -4.793055878365783    |
| train_1/q_grads           | 0.006293479248415679  |
| train_1/q_grads_std       | 0.21810429878532886   |
| train_1/q_loss            | 1.3207248816201758    |
| train_1/reward            | -1.329193421304808    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 9.765625e-05          |
| train_1/reward_-10.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -4.786994899398465    |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Training epoch 2
Time for epoch 2: 345.13. Rollout time: 144.05, Training time: 201.04
Evaluating epoch 2
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
-----------------------------------------------------
| epoch                     | 2                     |
| policy/steps              | 92236.0               |
| test/episodes             | 75.0                  |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -25.0                 |
| test_1/avg_q              | -9.999995502717528    |
| test_1/n_subgoals         | 250.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 300.0                 |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -25.0                 |
| train_0/current_q         | -10.022888546595478   |
| train_0/fw_bonus          | -0.999708816409111    |
| train_0/fw_loss           | 6.591377714357805e-05 |
| train_0/mu_grads          | 0.011316174268722534  |
| train_0/mu_grads_std      | 0.14506289400160313   |
| train_0/mu_loss           | 8.716315229605476     |
| train_0/next_q            | -10.022555932935187   |
| train_0/q_grads           | 0.04018859742209315   |
| train_0/q_grads_std       | 0.22430177703499793   |
| train_0/q_loss            | 0.7557309232875745    |
| train_0/reward            | -0.7430795078413212   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0046875             |
| train_0/target_q          | -10.229629631776112   |
| train_1/avg_q             | -7.230244196249005    |
| train_1/current_q         | -8.301148734633285    |
| train_1/fw_bonus          | -1.0119656026363373   |
| train_1/fw_loss           | 0.0007372335327090696 |
| train_1/mu_grads          | 1.872660704975715e-05 |
| train_1/mu_grads_std      | 0.11777990516275168   |
| train_1/mu_loss           | 10.999997817098071    |
| train_1/n_subgoals        | 1000.0                |
| train_1/next_q            | -9.99999785140786     |
| train_1/q_grads           | 0.009168167505413294  |
| train_1/q_grads_std       | 0.27695552781224253   |
| train_1/q_loss            | 4.463826148190883     |
| train_1/reward            | -1.3138929450899013   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0001708984375       |
| train_1/reward_-10.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -8.406449998190705    |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Training epoch 3
Time for epoch 3: 354.54. Rollout time: 147.81, Training time: 206.68
Evaluating epoch 3
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 3                      |
| policy/steps              | 123486.0               |
| test/episodes             | 100.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -9.999999997279344     |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 400.0                  |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -9.00908370814316      |
| train_0/fw_bonus          | -0.9997093468904495    |
| train_0/fw_loss           | 6.579705950571224e-05  |
| train_0/mu_grads          | 0.011643484001979232   |
| train_0/mu_grads_std      | 0.14437390938401223    |
| train_0/mu_loss           | 8.32390531227218       |
| train_0/next_q            | -9.00704402122978      |
| train_0/q_grads           | 0.03566365083679557    |
| train_0/q_grads_std       | 0.22500110790133476    |
| train_0/q_loss            | 0.38450845464959726    |
| train_0/reward            | -0.7427257714683947    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.0026611328125        |
| train_0/target_q          | -9.320434232290074     |
| train_1/avg_q             | -9.99958303575676      |
| train_1/current_q         | -8.272256185752797     |
| train_1/fw_bonus          | -1.012064555287361     |
| train_1/fw_loss           | 0.0007140444562537595  |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 10.999999999930008     |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -9.999999999926327     |
| train_1/q_grads           | 0.019456325145438314   |
| train_1/q_grads_std       | 0.320906275510788      |
| train_1/q_loss            | 3.8831323090380905     |
| train_1/reward            | -1.316383843350195     |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 4.8828125e-05          |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.406549858916934     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 4
Time for epoch 4: 329.29. Rollout time: 137.89, Training time: 191.37
Evaluating epoch 4
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 4                      |
| policy/steps              | 154736.0               |
| test/episodes             | 125.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -9.999999999945272     |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 500.0                  |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.249562873716688    |
| train_0/fw_bonus          | -0.9997151896357537    |
| train_0/fw_loss           | 6.45229430119798e-05   |
| train_0/mu_grads          | 0.007736527291126549   |
| train_0/mu_grads_std      | 0.13885636813938618    |
| train_0/mu_loss           | 9.00155892676822       |
| train_0/next_q            | -10.243272470890275    |
| train_0/q_grads           | 0.03878998178988695    |
| train_0/q_grads_std       | 0.23049530424177647    |
| train_0/q_loss            | 0.6008577386494076     |
| train_0/reward            | -0.7445719546587497    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.0075927734375        |
| train_0/target_q          | -10.406356556648223    |
| train_1/avg_q             | -9.998092368564672     |
| train_1/current_q         | -8.26030341182533      |
| train_1/fw_bonus          | -1.011925819516182     |
| train_1/fw_loss           | 0.0007465467570000328  |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.02599869528785348    |
| train_1/q_grads_std       | 0.36205668821930886    |
| train_1/q_loss            | 3.3767719349265013     |
| train_1/reward            | -1.3188047085794097    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.000146484375         |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.412320333579412     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 5
Time for epoch 5: 301.72. Rollout time: 128.61, Training time: 173.08
Evaluating epoch 5
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 5                      |
| policy/steps              | 185872.0               |
| test/episodes             | 150.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -9.999999999735486     |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 600.0                  |
| train/success_rate        | 0.01                   |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.290399627722502    |
| train_0/fw_bonus          | -0.999761775135994     |
| train_0/fw_loss           | 5.436522169475211e-05  |
| train_0/mu_grads          | 0.005154387431684881   |
| train_0/mu_grads_std      | 0.13477949760854244    |
| train_0/mu_loss           | 9.151882508193678      |
| train_0/next_q            | -10.29178733254452     |
| train_0/q_grads           | 0.038914299197494985   |
| train_0/q_grads_std       | 0.23228590674698352    |
| train_0/q_loss            | 0.7789781070319861     |
| train_0/reward            | -0.7460967106788303    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.020458984375         |
| train_0/target_q          | -10.49574206836235     |
| train_1/avg_q             | -9.999869004693506     |
| train_1/current_q         | -8.270345986613652     |
| train_1/fw_bonus          | -1.011946052312851     |
| train_1/fw_loss           | 0.0007418074033921585  |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 996.0                  |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.027390526747331022   |
| train_1/q_grads_std       | 0.38456504642963407    |
| train_1/q_loss            | 2.8944176914905833     |
| train_1/reward            | -1.3331947065984422    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 7.32421875e-05         |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.445372440973445     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 6
Time for epoch 6: 297.52. Rollout time: 120.87, Training time: 176.62
Evaluating epoch 6
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 6                      |
| policy/steps              | 217122.0               |
| test/episodes             | 175.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -9.99999999999625      |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 700.0                  |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -9.568392855945714     |
| train_0/fw_bonus          | -0.999766343832016     |
| train_0/fw_loss           | 5.3370113437267716e-05 |
| train_0/mu_grads          | 0.002891679498134181   |
| train_0/mu_grads_std      | 0.1357129842042923     |
| train_0/mu_loss           | 8.60344526313827       |
| train_0/next_q            | -9.562124181910653     |
| train_0/q_grads           | 0.03715586392208934    |
| train_0/q_grads_std       | 0.23421216011047363    |
| train_0/q_loss            | 0.4972503618131968     |
| train_0/reward            | -0.7414603204349988    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.0191650390625        |
| train_0/target_q          | -9.875002433996336     |
| train_1/avg_q             | -9.999972840126796     |
| train_1/current_q         | -8.262316326973783     |
| train_1/fw_bonus          | -1.0119164794683457    |
| train_1/fw_loss           | 0.0007487369613954798  |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.02894249469973147    |
| train_1/q_grads_std       | 0.40324451476335527    |
| train_1/q_loss            | 2.7288643957681282     |
| train_1/reward            | -1.3193981782038464    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 9.765625e-05           |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.458133529766348     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 7
Time for epoch 7: 297.40. Rollout time: 119.78, Training time: 177.59
Evaluating epoch 7
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
-------------------------------------------------------
| epoch                     | 7                       |
| policy/steps              | 248372.0                |
| test/episodes             | 200.0                   |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -25.0                   |
| test_1/avg_q              | -9.99999999999999       |
| test_1/n_subgoals         | 250.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 800.0                   |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -25.0                   |
| train_0/current_q         | -10.241270847590531     |
| train_0/fw_bonus          | -0.9998119667172432     |
| train_0/fw_loss           | 4.342006791375752e-05   |
| train_0/mu_grads          | -0.00024690528007340615 |
| train_0/mu_grads_std      | 0.13982209786772729     |
| train_0/mu_loss           | 9.201897381311513       |
| train_0/next_q            | -10.23868963184228      |
| train_0/q_grads           | 0.03847666615620256     |
| train_0/q_grads_std       | 0.236988914757967       |
| train_0/q_loss            | 0.8235273663232883      |
| train_0/reward            | -0.7445471003993589     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.095849609375          |
| train_0/target_q          | -10.412435992806945     |
| train_1/avg_q             | -9.997807109160961      |
| train_1/current_q         | -8.304518651608834      |
| train_1/fw_bonus          | -1.012479129433632      |
| train_1/fw_loss           | 0.0006169206768390723   |
| train_1/mu_grads          | 1.8710847143665887e-05  |
| train_1/mu_grads_std      | 0.1177799254655838      |
| train_1/mu_loss           | 11.0                    |
| train_1/n_subgoals        | 1000.0                  |
| train_1/next_q            | -10.0                   |
| train_1/q_grads           | 0.032022590097039935    |
| train_1/q_grads_std       | 0.42254461869597437     |
| train_1/q_loss            | 3.054768458687685       |
| train_1/reward            | -1.3189104682001926     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.0001220703125         |
| train_1/reward_-10.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -8.492816718200196      |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 8
Time for epoch 8: 373.52. Rollout time: 150.69, Training time: 222.79
Evaluating epoch 8
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 8                      |
| policy/steps              | 279622.0               |
| test/episodes             | 225.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -9.999999999999996     |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 900.0                  |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.229278190536167    |
| train_0/fw_bonus          | -0.9998008862137795    |
| train_0/fw_loss           | 4.5836600702386934e-05 |
| train_0/mu_grads          | -0.0028565687709487973 |
| train_0/mu_grads_std      | 0.142673284932971      |
| train_0/mu_loss           | 9.237963046914242      |
| train_0/next_q            | -10.222705288468726    |
| train_0/q_grads           | 0.037465921975672244   |
| train_0/q_grads_std       | 0.23941751308739184    |
| train_0/q_loss            | 0.5644237013549681     |
| train_0/reward            | -0.7437114812739309    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.0767333984375        |
| train_0/target_q          | -10.399396765693261    |
| train_1/avg_q             | -9.995707975209891     |
| train_1/current_q         | -8.265112783177113     |
| train_1/fw_bonus          | -1.012400135397911     |
| train_1/fw_loss           | 0.0006354286306304857  |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.033395635150372985   |
| train_1/q_grads_std       | 0.43704139441251755    |
| train_1/q_loss            | 2.8269984044933354     |
| train_1/reward            | -1.3104481150112406    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0001220703125        |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.465213740011244     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 9
Time for epoch 9: 373.56. Rollout time: 155.37, Training time: 218.13
Evaluating epoch 9
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 9                      |
| policy/steps              | 310866.0               |
| test/episodes             | 250.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 1000.0                 |
| train/success_rate        | 0.01                   |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.361819631693905    |
| train_0/fw_bonus          | -0.999784903228283     |
| train_0/fw_loss           | 4.932503679810907e-05  |
| train_0/mu_grads          | -0.004111914290115237  |
| train_0/mu_grads_std      | 0.14530498832464217    |
| train_0/mu_loss           | 9.479607532055626      |
| train_0/next_q            | -10.361295564654245    |
| train_0/q_grads           | 0.0351933728903532     |
| train_0/q_grads_std       | 0.2416899487376213     |
| train_0/q_loss            | 0.6292998632109575     |
| train_0/reward            | -0.7441551425254147    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.0508056640625        |
| train_0/target_q          | -10.55511239524528     |
| train_1/avg_q             | -9.998589734317193     |
| train_1/current_q         | -8.272051412471217     |
| train_1/fw_bonus          | -1.012563082575798     |
| train_1/fw_loss           | 0.0005972529106657021  |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.03375078309327364    |
| train_1/q_grads_std       | 0.45555078983306885    |
| train_1/q_loss            | 2.730460620744507      |
| train_1/reward            | -1.3299143583033584    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0                    |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.44592021767836      |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 10
Time for epoch 10: 365.32. Rollout time: 153.51, Training time: 211.76
Evaluating epoch 10
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 10                     |
| policy/steps              | 342116.0               |
| test/episodes             | 275.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -9.999999999999861     |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 1100.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.268225444172243    |
| train_0/fw_bonus          | -0.9997414246201515    |
| train_0/fw_loss           | 5.8804410946322606e-05 |
| train_0/mu_grads          | -0.005776371143292635  |
| train_0/mu_grads_std      | 0.147381304949522      |
| train_0/mu_loss           | 9.377571207646316      |
| train_0/next_q            | -10.266591253474422    |
| train_0/q_grads           | 0.03468022653833032    |
| train_0/q_grads_std       | 0.24441176205873488    |
| train_0/q_loss            | 0.7538199153030074     |
| train_0/reward            | -0.7448780357277428    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.0659912109375        |
| train_0/target_q          | -10.460945390624833    |
| train_1/avg_q             | -9.999999999914987     |
| train_1/current_q         | -8.280239974951115     |
| train_1/fw_bonus          | -1.0126651734113694    |
| train_1/fw_loss           | 0.0005733360449085012  |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.03393195988610387    |
| train_1/q_grads_std       | 0.4663319803774357     |
| train_1/q_loss            | 2.7953762897473644     |
| train_1/reward            | -1.3117036852017918    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 4.8828125e-05          |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.474125560201795     |
------------------------------------------------------
Saving periodic policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_10.pkl ...
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 11
Time for epoch 11: 384.37. Rollout time: 164.85, Training time: 219.40
Evaluating epoch 11
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 11                     |
| policy/steps              | 373349.0               |
| test/episodes             | 300.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 1200.0                 |
| train/success_rate        | 0.01                   |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.278582271397866    |
| train_0/fw_bonus          | -0.9997915953397751    |
| train_0/fw_loss           | 4.786545409842802e-05  |
| train_0/mu_grads          | -0.006365200993604958  |
| train_0/mu_grads_std      | 0.15105402953922747    |
| train_0/mu_loss           | 9.535126751861814      |
| train_0/next_q            | -10.278789754364215    |
| train_0/q_grads           | 0.03396301884204149    |
| train_0/q_grads_std       | 0.2474606666713953     |
| train_0/q_loss            | 0.7019820351344704     |
| train_0/reward            | -0.7427790948735492    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.0932861328125        |
| train_0/target_q          | -10.461712412170055    |
| train_1/avg_q             | -9.999999348674683     |
| train_1/current_q         | -8.269067648803292     |
| train_1/fw_bonus          | -1.012999963760376     |
| train_1/fw_loss           | 0.0004949087960994802  |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.03225780175998807    |
| train_1/q_grads_std       | 0.47364596799016       |
| train_1/q_loss            | 2.619932535653628      |
| train_1/reward            | -1.3288998796371743    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 7.32421875e-05         |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.437488746824679     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 12
Time for epoch 12: 373.55. Rollout time: 157.08, Training time: 216.41
Evaluating epoch 12
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 12                     |
| policy/steps              | 404599.0               |
| test/episodes             | 325.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 1300.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.312979981633035    |
| train_0/fw_bonus          | -0.999820151925087     |
| train_0/fw_loss           | 4.163957214586844e-05  |
| train_0/mu_grads          | -0.006294529070146382  |
| train_0/mu_grads_std      | 0.1548709139227867     |
| train_0/mu_loss           | 9.617163671045681      |
| train_0/next_q            | -10.309906425567945    |
| train_0/q_grads           | 0.03353742742910981    |
| train_0/q_grads_std       | 0.2503731608390808     |
| train_0/q_loss            | 0.6594234859787227     |
| train_0/reward            | -0.7437821400177199    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.1732421875           |
| train_0/target_q          | -10.507977797125864    |
| train_1/avg_q             | -9.999919284971481     |
| train_1/current_q         | -8.24795581215029      |
| train_1/fw_bonus          | -1.0130463302135468    |
| train_1/fw_loss           | 0.0004840447225433309  |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.03310300363227725    |
| train_1/q_grads_std       | 0.47797081544995307    |
| train_1/q_loss            | 2.643545190054385      |
| train_1/reward            | -1.3253952345439757    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 4.8828125e-05          |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.412690156418979     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 13
Time for epoch 13: 383.09. Rollout time: 164.40, Training time: 218.64
Evaluating epoch 13
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 13                     |
| policy/steps              | 435753.0               |
| test/episodes             | 350.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 1400.0                 |
| train/success_rate        | 0.01                   |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.362874165238484    |
| train_0/fw_bonus          | -0.9997912898659707    |
| train_0/fw_loss           | 4.793205584974203e-05  |
| train_0/mu_grads          | -0.007292556366883219  |
| train_0/mu_grads_std      | 0.15970755629241468    |
| train_0/mu_loss           | 9.780424323118675      |
| train_0/next_q            | -10.358718043487524    |
| train_0/q_grads           | 0.03207339905202389    |
| train_0/q_grads_std       | 0.251713290810585      |
| train_0/q_loss            | 0.48053822833670035    |
| train_0/reward            | -0.7452813834432164    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.0740478515625        |
| train_0/target_q          | -10.551538248280744    |
| train_1/avg_q             | -9.993759535906463     |
| train_1/current_q         | -8.252828343567556     |
| train_1/fw_bonus          | -1.0130686730146408    |
| train_1/fw_loss           | 0.00047880729180178606 |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 997.0                  |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.03394306916743517    |
| train_1/q_grads_std       | 0.48598589599132536    |
| train_1/q_loss            | 2.490371787169871      |
| train_1/reward            | -1.2953893229248934    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 2.44140625e-05         |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.464271158862397     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 14
Time for epoch 14: 385.75. Rollout time: 163.52, Training time: 222.19
Evaluating epoch 14
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 14                     |
| policy/steps              | 467003.0               |
| test/episodes             | 375.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 1500.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.096497516232983    |
| train_0/fw_bonus          | -0.9997379183769226    |
| train_0/fw_loss           | 5.9571384872469936e-05 |
| train_0/mu_grads          | -0.008059458225034177  |
| train_0/mu_grads_std      | 0.1641540013253689     |
| train_0/mu_loss           | 9.49821121973786       |
| train_0/next_q            | -10.089508590066435    |
| train_0/q_grads           | 0.030078460229560733   |
| train_0/q_grads_std       | 0.25455085262656213    |
| train_0/q_loss            | 0.5526683571615114     |
| train_0/reward            | -0.744021784873621     |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.05087890625          |
| train_0/target_q          | -10.20929435615766     |
| train_1/avg_q             | -9.99062476068623      |
| train_1/current_q         | -8.299845162177728     |
| train_1/fw_bonus          | -1.0132028967142106    |
| train_1/fw_loss           | 0.00044736596319125963 |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.03508996181190014    |
| train_1/q_grads_std       | 0.48946180418133733    |
| train_1/q_loss            | 2.5864966201564643     |
| train_1/reward            | -1.3171849431280862    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 2.44140625e-05         |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.49085193531559      |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 15
Time for epoch 15: 381.03. Rollout time: 162.71, Training time: 218.27
Evaluating epoch 15
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 15                     |
| policy/steps              | 498253.0               |
| test/episodes             | 400.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 1600.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.098981814665676    |
| train_0/fw_bonus          | -0.9998017892241478    |
| train_0/fw_loss           | 4.5641353381142835e-05 |
| train_0/mu_grads          | -0.008010686794295906  |
| train_0/mu_grads_std      | 0.16976170614361763    |
| train_0/mu_loss           | 9.699886059981733      |
| train_0/next_q            | -10.097244227883307    |
| train_0/q_grads           | 0.029648726945742963   |
| train_0/q_grads_std       | 0.25653224214911463    |
| train_0/q_loss            | 0.5789268457921043     |
| train_0/reward            | -0.7474106593152101    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.1304443359375        |
| train_0/target_q          | -10.338323380940215    |
| train_1/avg_q             | -9.999999838559143     |
| train_1/current_q         | -8.27847999656782      |
| train_1/fw_bonus          | -1.0134094417095185    |
| train_1/fw_loss           | 0.0003989787554019131  |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.03629131084308028    |
| train_1/q_grads_std       | 0.49323693066835406    |
| train_1/q_loss            | 2.4475621056581383     |
| train_1/reward            | -1.3041168282579747    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0                    |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.485200812632979     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 16
Time for epoch 16: 388.89. Rollout time: 163.51, Training time: 225.32
Evaluating epoch 16
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 16                     |
| policy/steps              | 529491.0               |
| test/episodes             | 425.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 1700.0                 |
| train/success_rate        | 0.01                   |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -9.784808900034333     |
| train_0/fw_bonus          | -0.9998112410306931    |
| train_0/fw_loss           | 4.358086862339405e-05  |
| train_0/mu_grads          | -0.008014328102581203  |
| train_0/mu_grads_std      | 0.17530660107731819    |
| train_0/mu_loss           | 9.460659908497792      |
| train_0/next_q            | -9.77462939071881      |
| train_0/q_grads           | 0.029220737610012294   |
| train_0/q_grads_std       | 0.258848462253809      |
| train_0/q_loss            | 0.558257267098538      |
| train_0/reward            | -0.7425813995832868    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.1440673828125        |
| train_0/target_q          | -10.00629084826705     |
| train_1/avg_q             | -9.999999999997721     |
| train_1/current_q         | -8.282861818133032     |
| train_1/fw_bonus          | -1.0133341938257217    |
| train_1/fw_loss           | 0.0004166033693763893  |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.03469882691279054    |
| train_1/q_grads_std       | 0.4993018664419651     |
| train_1/q_loss            | 2.696253087493012      |
| train_1/reward            | -1.2960713994078106    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0001220703125        |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.487682727532814     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 17
Time for epoch 17: 385.13. Rollout time: 163.77, Training time: 221.30
Evaluating epoch 17
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 17                     |
| policy/steps              | 560741.0               |
| test/episodes             | 450.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 1800.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.201697024735342    |
| train_0/fw_bonus          | -0.9998324394226075    |
| train_0/fw_loss           | 3.8957957667662416e-05 |
| train_0/mu_grads          | -0.00781058482825756   |
| train_0/mu_grads_std      | 0.18090440891683102    |
| train_0/mu_loss           | 9.478061385841087      |
| train_0/next_q            | -10.199160824769354    |
| train_0/q_grads           | 0.03148804935626685    |
| train_0/q_grads_std       | 0.26325407698750497    |
| train_0/q_loss            | 0.4274238838752636     |
| train_0/reward            | -0.7424211013225431    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.153369140625         |
| train_0/target_q          | -10.391899391844762    |
| train_1/avg_q             | -9.998941443012471     |
| train_1/current_q         | -8.270310440338926     |
| train_1/fw_bonus          | -1.0134601593017578    |
| train_1/fw_loss           | 0.00038709837390342726 |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.03263913970440626    |
| train_1/q_grads_std       | 0.5067110314965249     |
| train_1/q_loss            | 2.5485503970927637     |
| train_1/reward            | -1.3188272425431933    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 2.44140625e-05         |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.452777437855696     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 18
Time for epoch 18: 393.53. Rollout time: 171.41, Training time: 222.06
Evaluating epoch 18
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 18                     |
| policy/steps              | 591991.0               |
| test/episodes             | 475.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 1900.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.30260718267349     |
| train_0/fw_bonus          | -0.9998283714056015    |
| train_0/fw_loss           | 3.984471068179119e-05  |
| train_0/mu_grads          | -0.008471395773813128  |
| train_0/mu_grads_std      | 0.18751380927860736    |
| train_0/mu_loss           | 9.432221040335524      |
| train_0/next_q            | -10.29882360985406     |
| train_0/q_grads           | 0.03214482516050339    |
| train_0/q_grads_std       | 0.269038599729538      |
| train_0/q_loss            | 0.679455933770695      |
| train_0/reward            | -0.7427852790468024    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.1182373046875        |
| train_0/target_q          | -10.488504624140159    |
| train_1/avg_q             | -9.999941621673486     |
| train_1/current_q         | -8.31433842482096      |
| train_1/fw_bonus          | -1.0135229259729386    |
| train_1/fw_loss           | 0.0003723927802639082  |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.032211856078356506   |
| train_1/q_grads_std       | 0.5127805903553962     |
| train_1/q_loss            | 2.4863961921877342     |
| train_1/reward            | -1.3139514021626382    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 7.32421875e-05         |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.50963011310014      |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 19
Time for epoch 19: 396.20. Rollout time: 165.25, Training time: 230.90
Evaluating epoch 19
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 19                     |
| policy/steps              | 623241.0               |
| test/episodes             | 500.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 2000.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.450079834822917    |
| train_0/fw_bonus          | -0.999840684235096     |
| train_0/fw_loss           | 3.716176984198682e-05  |
| train_0/mu_grads          | -0.009508190979249775  |
| train_0/mu_grads_std      | 0.194424844160676      |
| train_0/mu_loss           | 9.462446789968032      |
| train_0/next_q            | -10.443703236962728    |
| train_0/q_grads           | 0.031138621782884002   |
| train_0/q_grads_std       | 0.2717136278748512     |
| train_0/q_loss            | 0.7445919534514359     |
| train_0/reward            | -0.7448300051379192    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.193505859375         |
| train_0/target_q          | -10.645103636899204    |
| train_1/avg_q             | -9.999999999989464     |
| train_1/current_q         | -8.284929818765841     |
| train_1/fw_bonus          | -1.0135078698396682    |
| train_1/fw_loss           | 0.000375916637131013   |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.032376731652766465   |
| train_1/q_grads_std       | 0.5236051306128502     |
| train_1/q_loss            | 2.7376111384102506     |
| train_1/reward            | -1.323123568570736     |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 2.44140625e-05         |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.44750345138324      |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 20
Time for epoch 20: 402.17. Rollout time: 168.16, Training time: 233.96
Evaluating epoch 20
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 20                     |
| policy/steps              | 654491.0               |
| test/episodes             | 525.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 2100.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.533955766981283    |
| train_0/fw_bonus          | -0.9998365253210068    |
| train_0/fw_loss           | 3.8066988599894104e-05 |
| train_0/mu_grads          | -0.010195450065657497  |
| train_0/mu_grads_std      | 0.2025335218757391     |
| train_0/mu_loss           | 9.55456854915449       |
| train_0/next_q            | -10.537148954949256    |
| train_0/q_grads           | 0.031162609113380313   |
| train_0/q_grads_std       | 0.27482849135994913    |
| train_0/q_loss            | 0.6797654420806595     |
| train_0/reward            | -0.7467932613952144    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.1849609375           |
| train_0/target_q          | -10.746735559051803    |
| train_1/avg_q             | -9.990901922044355     |
| train_1/current_q         | -8.30346799952614      |
| train_1/fw_bonus          | -1.0135787099599838    |
| train_1/fw_loss           | 0.0003593240340705961  |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.03314281515777111    |
| train_1/q_grads_std       | 0.5312259122729301     |
| train_1/q_loss            | 2.5113845244065227     |
| train_1/reward            | -1.3123435515561142    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 9.765625e-05           |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.499887496868618     |
------------------------------------------------------
Saving periodic policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_20.pkl ...
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 21
Time for epoch 21: 418.77. Rollout time: 180.81, Training time: 237.90
Evaluating epoch 21
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 21                     |
| policy/steps              | 685741.0               |
| test/episodes             | 550.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 2200.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.281310830813567    |
| train_0/fw_bonus          | -0.9998672008514404    |
| train_0/fw_loss           | 3.1376615197586943e-05 |
| train_0/mu_grads          | -0.011555347079411148  |
| train_0/mu_grads_std      | 0.209952212870121      |
| train_0/mu_loss           | 9.640399699829686      |
| train_0/next_q            | -10.277472944692846    |
| train_0/q_grads           | 0.030699908593669535   |
| train_0/q_grads_std       | 0.276354606449604      |
| train_0/q_loss            | 0.5334265327188724     |
| train_0/reward            | -0.7434150473978661    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.2218505859375        |
| train_0/target_q          | -10.462246558576053    |
| train_1/avg_q             | -9.99061118908545      |
| train_1/current_q         | -8.31766094715344      |
| train_1/fw_bonus          | -1.0137353628873824    |
| train_1/fw_loss           | 0.0003226221480872482  |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.03360311593860388    |
| train_1/q_grads_std       | 0.5388909921050071     |
| train_1/q_loss            | 2.7903813483826214     |
| train_1/reward            | -1.3359326260659146    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0                    |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.491416024503417     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 22
Time for epoch 22: 430.02. Rollout time: 180.57, Training time: 249.36
Evaluating epoch 22
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 22                     |
| policy/steps              | 716954.0               |
| test/episodes             | 575.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 2300.0                 |
| train/success_rate        | 0.01                   |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.156721171782928    |
| train_0/fw_bonus          | -0.9998515173792839    |
| train_0/fw_loss           | 3.479694864836347e-05  |
| train_0/mu_grads          | -0.011808753432705998  |
| train_0/mu_grads_std      | 0.21515363603830337    |
| train_0/mu_loss           | 9.992487883545627      |
| train_0/next_q            | -10.151059802650414    |
| train_0/q_grads           | 0.029731486458331347   |
| train_0/q_grads_std       | 0.27762395814061164    |
| train_0/q_loss            | 0.5965948021565556     |
| train_0/reward            | -0.743517068138317     |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.1566650390625        |
| train_0/target_q          | -10.324372166295143    |
| train_1/avg_q             | -9.9813319285139       |
| train_1/current_q         | -8.282230863540477     |
| train_1/fw_bonus          | -1.0135716766119003    |
| train_1/fw_loss           | 0.00036097005868214184 |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 999.0                  |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.03431181162595749    |
| train_1/q_grads_std       | 0.5463363036513329     |
| train_1/q_loss            | 2.7116724417716718     |
| train_1/reward            | -1.3355333985135076    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0                    |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.432159375076012     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 23
Time for epoch 23: 425.88. Rollout time: 189.15, Training time: 236.66
Evaluating epoch 23
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 23                     |
| policy/steps              | 748119.0               |
| test/episodes             | 600.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 2400.0                 |
| train/success_rate        | 0.01                   |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.378659925121948    |
| train_0/fw_bonus          | -0.9998485162854195    |
| train_0/fw_loss           | 3.5452824113235694e-05 |
| train_0/mu_grads          | -0.012081600492820144  |
| train_0/mu_grads_std      | 0.21912398114800452    |
| train_0/mu_loss           | 10.1693018349845       |
| train_0/next_q            | -10.379060935736888    |
| train_0/q_grads           | 0.02913768175058067    |
| train_0/q_grads_std       | 0.28029866963624955    |
| train_0/q_loss            | 0.643313273067617      |
| train_0/reward            | -0.7452001346420729    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.1534423828125        |
| train_0/target_q          | -10.58413052821128     |
| train_1/avg_q             | -9.990535153290258     |
| train_1/current_q         | -8.2615103430885       |
| train_1/fw_bonus          | -1.0137226969003676    |
| train_1/fw_loss           | 0.00032558916354901156 |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 997.0                  |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.03463309928774834    |
| train_1/q_grads_std       | 0.5544021770358085     |
| train_1/q_loss            | 2.5956760660593665     |
| train_1/reward            | -1.3178476803630474    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0                    |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.437203149113051     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 24
Time for epoch 24: 356.56. Rollout time: 148.82, Training time: 207.68
Evaluating epoch 24
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 24                     |
| policy/steps              | 779369.0               |
| test/episodes             | 625.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 2500.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.439796649965862    |
| train_0/fw_bonus          | -0.9998690605163574    |
| train_0/fw_loss           | 3.0973189359428944e-05 |
| train_0/mu_grads          | -0.012776541500352323  |
| train_0/mu_grads_std      | 0.22188055627048014    |
| train_0/mu_loss           | 10.20425034728842      |
| train_0/next_q            | -10.438683334282917    |
| train_0/q_grads           | 0.029391463659703732   |
| train_0/q_grads_std       | 0.28248831406235697    |
| train_0/q_loss            | 0.7926465143170603     |
| train_0/reward            | -0.7437333553061762    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.175537109375         |
| train_0/target_q          | -10.644398333749757    |
| train_1/avg_q             | -9.994805899863534     |
| train_1/current_q         | -8.213667533777999     |
| train_1/fw_bonus          | -1.0139151692390442    |
| train_1/fw_loss           | 0.000280497745188768   |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.03526089293882251    |
| train_1/q_grads_std       | 0.5625802755355835     |
| train_1/q_loss            | 2.455318173099715      |
| train_1/reward            | -1.323205148735724     |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0                    |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.382985422173226     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 25
Time for epoch 25: 7601.36. Rollout time: 7086.28, Training time: 514.87
Evaluating epoch 25
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 25                     |
| policy/steps              | 810619.0               |
| test/episodes             | 650.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 2600.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.269625204111176    |
| train_0/fw_bonus          | -0.9998867884278297    |
| train_0/fw_loss           | 2.710757498789462e-05  |
| train_0/mu_grads          | -0.013438718463294207  |
| train_0/mu_grads_std      | 0.22424700558185579    |
| train_0/mu_loss           | 10.021129413145895     |
| train_0/next_q            | -10.267561767567754    |
| train_0/q_grads           | 0.02969652656465769    |
| train_0/q_grads_std       | 0.2855688326060772     |
| train_0/q_loss            | 0.7842768629649026     |
| train_0/reward            | -0.7437178049687645    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.2533447265625        |
| train_0/target_q          | -10.47111109346871     |
| train_1/avg_q             | -9.999999999990818     |
| train_1/current_q         | -8.229923655380393     |
| train_1/fw_bonus          | -1.013751021027565     |
| train_1/fw_loss           | 0.0003189558497979306  |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.03471623277291656    |
| train_1/q_grads_std       | 0.5704882547259331     |
| train_1/q_loss            | 2.591894576461841      |
| train_1/reward            | -1.3198149627547537    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0                    |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.405674337754757     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 26
Time for epoch 26: 14478.78. Rollout time: 1244.86, Training time: 13233.78
Evaluating epoch 26
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 26                     |
| policy/steps              | 841700.0               |
| test/episodes             | 675.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 2700.0                 |
| train/success_rate        | 0.01                   |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.008758981764498    |
| train_0/fw_bonus          | -0.9998730629682541    |
| train_0/fw_loss           | 3.009987649420509e-05  |
| train_0/mu_grads          | -0.014079646649770438  |
| train_0/mu_grads_std      | 0.22664619721472262    |
| train_0/mu_loss           | 9.99168335977849       |
| train_0/next_q            | -10.007353712051943    |
| train_0/q_grads           | 0.02976403278298676    |
| train_0/q_grads_std       | 0.28851408511400223    |
| train_0/q_loss            | 0.4206064239253526     |
| train_0/reward            | -0.7435400452603063    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.205859375            |
| train_0/target_q          | -10.195567555212767    |
| train_1/avg_q             | -9.99999999999993      |
| train_1/current_q         | -8.230845666107204     |
| train_1/fw_bonus          | -1.0138921558856964    |
| train_1/fw_loss           | 0.0002858892799849855  |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 994.0                  |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.03523711040616036    |
| train_1/q_grads_std       | 0.5795938089489937     |
| train_1/q_loss            | 2.543606571674689      |
| train_1/reward            | -1.3235354380762145    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 2.44140625e-05         |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.404370399013718     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 27
Time for epoch 27: 349.29. Rollout time: 144.85, Training time: 204.39
Evaluating epoch 27
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 27                     |
| policy/steps              | 872950.0               |
| test/episodes             | 700.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 2800.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.273028017270962    |
| train_0/fw_bonus          | -0.9998608022928238    |
| train_0/fw_loss           | 3.2773281236586624e-05 |
| train_0/mu_grads          | -0.014413852361030877  |
| train_0/mu_grads_std      | 0.2293980974704027     |
| train_0/mu_loss           | 10.25569563479456      |
| train_0/next_q            | -10.274711859444562    |
| train_0/q_grads           | 0.030643830308690667   |
| train_0/q_grads_std       | 0.2917982444167137     |
| train_0/q_loss            | 0.6516180728879997     |
| train_0/reward            | -0.7430944620617084    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.1642333984375        |
| train_0/target_q          | -10.438083431070329    |
| train_1/avg_q             | -9.992148536444514     |
| train_1/current_q         | -8.271566216062986     |
| train_1/fw_bonus          | -1.013963559269905     |
| train_1/fw_loss           | 0.0002691643090656726  |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.034269300382584335   |
| train_1/q_grads_std       | 0.5872478350996971     |
| train_1/q_loss            | 2.870500062192637      |
| train_1/reward            | -1.308947111015732     |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0                    |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.452467618828235     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 28
Time for epoch 28: 4847.09. Rollout time: 2001.89, Training time: 2845.13
Evaluating epoch 28
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 28                     |
| policy/steps              | 904200.0               |
| test/episodes             | 725.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 2900.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.31093653105614     |
| train_0/fw_bonus          | -0.9998436689376831    |
| train_0/fw_loss           | 3.650839016700047e-05  |
| train_0/mu_grads          | -0.013998941937461495  |
| train_0/mu_grads_std      | 0.230799912661314      |
| train_0/mu_loss           | 10.294418495937624     |
| train_0/next_q            | -10.30972960065109     |
| train_0/q_grads           | 0.030685562547296284   |
| train_0/q_grads_std       | 0.2942732870578766     |
| train_0/q_loss            | 0.7081263868507256     |
| train_0/reward            | -0.7431225189793622    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.1284912109375        |
| train_0/target_q          | -10.513591955085849    |
| train_1/avg_q             | -9.999999995809272     |
| train_1/current_q         | -8.278821629625105     |
| train_1/fw_bonus          | -1.013913205265999     |
| train_1/fw_loss           | 0.0002809602221532259  |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.03330396246165037    |
| train_1/q_grads_std       | 0.5947490230202674     |
| train_1/q_loss            | 2.448454635832548      |
| train_1/reward            | -1.3225327745960385    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0                    |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.460071837096041     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 29
Time for epoch 29: 1361.53. Rollout time: 638.46, Training time: 722.80
Evaluating epoch 29
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 29                     |
| policy/steps              | 935437.0               |
| test/episodes             | 750.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 3000.0                 |
| train/success_rate        | 0.01                   |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.500177188078364    |
| train_0/fw_bonus          | -0.9998897239565849    |
| train_0/fw_loss           | 2.6465725522939464e-05 |
| train_0/mu_grads          | -0.01393241211771965   |
| train_0/mu_grads_std      | 0.23312588669359685    |
| train_0/mu_loss           | 10.584379392936734     |
| train_0/next_q            | -10.497922878322449    |
| train_0/q_grads           | 0.03130400227382779    |
| train_0/q_grads_std       | 0.2977505996823311     |
| train_0/q_loss            | 0.4783223677396773     |
| train_0/reward            | -0.7446592180589505    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.2146484375           |
| train_0/target_q          | -10.683944096898056    |
| train_1/avg_q             | -9.999999999735577     |
| train_1/current_q         | -8.25675197991641      |
| train_1/fw_bonus          | -1.0139015197753907    |
| train_1/fw_loss           | 0.0002836919218680123  |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.032309741526842115   |
| train_1/q_grads_std       | 0.604163010418415      |
| train_1/q_loss            | 2.7555063815875416     |
| train_1/reward            | -1.312932218358037     |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0                    |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.441857999608041     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 30
Time for epoch 30: 1617.21. Rollout time: 782.58, Training time: 834.35
Evaluating epoch 30
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 30                     |
| policy/steps              | 966687.0               |
| test/episodes             | 775.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 3100.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.10921231716328     |
| train_0/fw_bonus          | -0.9998863026499748    |
| train_0/fw_loss           | 2.7214278998144436e-05 |
| train_0/mu_grads          | -0.01393097611144185   |
| train_0/mu_grads_std      | 0.2361164029687643     |
| train_0/mu_loss           | 10.23070751399833      |
| train_0/next_q            | -10.105443416919107    |
| train_0/q_grads           | 0.02988863466307521    |
| train_0/q_grads_std       | 0.3002363033592701     |
| train_0/q_loss            | 0.3634545049976108     |
| train_0/reward            | -0.7437908025858633    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.229638671875         |
| train_0/target_q          | -10.252491384550927    |
| train_1/avg_q             | -9.999999987216677     |
| train_1/current_q         | -8.266798057403703     |
| train_1/fw_bonus          | -1.014094814658165     |
| train_1/fw_loss           | 0.00023841352558520156 |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.031556913256645204   |
| train_1/q_grads_std       | 0.6139109253883361     |
| train_1/q_loss            | 2.4949185447664672     |
| train_1/reward            | -1.3165200661991547    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0                    |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.458844284949157     |
------------------------------------------------------
Saving periodic policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_30.pkl ...
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 31
Time for epoch 31: 6712.48. Rollout time: 6473.34, Training time: 239.05
Evaluating epoch 31
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 31                     |
| policy/steps              | 997937.0               |
| test/episodes             | 800.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 3200.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.297192400386454    |
| train_0/fw_bonus          | -0.9998907715082168    |
| train_0/fw_loss           | 2.6238724399263446e-05 |
| train_0/mu_grads          | -0.013834693538956343  |
| train_0/mu_grads_std      | 0.23960912562906742    |
| train_0/mu_loss           | 10.259552676932476     |
| train_0/next_q            | -10.290705161729292    |
| train_0/q_grads           | 0.030522336764261127   |
| train_0/q_grads_std       | 0.3040593914687634     |
| train_0/q_loss            | 0.6504062241825476     |
| train_0/reward            | -0.7411929418645741    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.2226806640625        |
| train_0/target_q          | -10.518396878458528    |
| train_1/avg_q             | -9.999979771666364     |
| train_1/current_q         | -8.22596344792772      |
| train_1/fw_bonus          | -1.0140715688467026    |
| train_1/fw_loss           | 0.00024385702417930588 |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.03153103180229664    |
| train_1/q_grads_std       | 0.6235122397542        |
| train_1/q_loss            | 2.4667925742869414     |
| train_1/reward            | -1.3183272957590817    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0                    |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.410168116071585     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 32
Time for epoch 32: 397.34. Rollout time: 165.74, Training time: 231.49
Evaluating epoch 32
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 32                     |
| policy/steps              | 1029187.0              |
| test/episodes             | 825.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 3300.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.44240998637918     |
| train_0/fw_bonus          | -0.999903267621994     |
| train_0/fw_loss           | 2.3513445057687932e-05 |
| train_0/mu_grads          | -0.01344137389678508   |
| train_0/mu_grads_std      | 0.24750535376369953    |
| train_0/mu_loss           | 10.337311967698025     |
| train_0/next_q            | -10.44531005371        |
| train_0/q_grads           | 0.031011086562648414   |
| train_0/q_grads_std       | 0.30756414234638213    |
| train_0/q_loss            | 0.7895811236850587     |
| train_0/reward            | -0.7447392297035549    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.257421875            |
| train_0/target_q          | -10.63486021526593     |
| train_1/avg_q             | -9.999999999985086     |
| train_1/current_q         | -8.275749262925094     |
| train_1/fw_bonus          | -1.0141966104507447    |
| train_1/fw_loss           | 0.00021456694812513888 |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.03045120444148779    |
| train_1/q_grads_std       | 0.6331355929374695     |
| train_1/q_loss            | 2.436965955067418      |
| train_1/reward            | -1.333099281239265     |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 4.8828125e-05          |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.452454749989268     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 33
Time for epoch 33: 396.79. Rollout time: 163.79, Training time: 232.93
Evaluating epoch 33
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 33                     |
| policy/steps              | 1060437.0              |
| test/episodes             | 850.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 3400.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.417876838827642    |
| train_0/fw_bonus          | -0.9999006524682045    |
| train_0/fw_loss           | 2.4083834000521163e-05 |
| train_0/mu_grads          | -0.014089083299040794  |
| train_0/mu_grads_std      | 0.255131646245718      |
| train_0/mu_loss           | 9.710820279854662      |
| train_0/next_q            | -10.417881073153984    |
| train_0/q_grads           | 0.03198759537190199    |
| train_0/q_grads_std       | 0.31122113168239596    |
| train_0/q_loss            | 0.5831489397306117     |
| train_0/reward            | -0.7439322578378779    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.2020751953125        |
| train_0/target_q          | -10.61742678474145     |
| train_1/avg_q             | -9.999999999999957     |
| train_1/current_q         | -8.262723742597537     |
| train_1/fw_bonus          | -1.0142025828361512    |
| train_1/fw_loss           | 0.0002131667384674074  |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.030113565176725386   |
| train_1/q_grads_std       | 0.640921114385128      |
| train_1/q_loss            | 2.6520970709139737     |
| train_1/reward            | -1.339791723453527     |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 7.32421875e-05         |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.41799484845353      |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 34
Time for epoch 34: 379.15. Rollout time: 161.77, Training time: 217.31
Evaluating epoch 34
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 34                     |
| policy/steps              | 1091687.0              |
| test/episodes             | 875.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 3500.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.154369301556537    |
| train_0/fw_bonus          | -0.9999115109443665    |
| train_0/fw_loss           | 2.1714747549594905e-05 |
| train_0/mu_grads          | -0.016487461095675827  |
| train_0/mu_grads_std      | 0.2623975194990635     |
| train_0/mu_loss           | 10.077587197367915     |
| train_0/next_q            | -10.157296980936561    |
| train_0/q_grads           | 0.03271875567734241    |
| train_0/q_grads_std       | 0.31475368440151213    |
| train_0/q_loss            | 0.6976223364284675     |
| train_0/reward            | -0.7416183540473866    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.278564453125         |
| train_0/target_q          | -10.345893085793303    |
| train_1/avg_q             | -10.0                  |
| train_1/current_q         | -8.274733886927649     |
| train_1/fw_bonus          | -1.0140281945466996    |
| train_1/fw_loss           | 0.00025402264691365417 |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.029542275704443455   |
| train_1/q_grads_std       | 0.6511345103383064     |
| train_1/q_loss            | 2.5128180396006323     |
| train_1/reward            | -1.3556224942265545    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 2.44140625e-05         |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.427604916101558     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 35
Time for epoch 35: 442.07. Rollout time: 195.91, Training time: 246.05
Evaluating epoch 35
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 35                     |
| policy/steps              | 1122937.0              |
| test/episodes             | 900.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 3600.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.389797407508294    |
| train_0/fw_bonus          | -0.9999012365937233    |
| train_0/fw_loss           | 2.39558384009797e-05   |
| train_0/mu_grads          | -0.017425579112023117  |
| train_0/mu_grads_std      | 0.2708112023770809     |
| train_0/mu_loss           | 10.362841766582278     |
| train_0/next_q            | -10.381536000083846    |
| train_0/q_grads           | 0.03296362366527319    |
| train_0/q_grads_std       | 0.3178934819996357     |
| train_0/q_loss            | 0.8287563031466822     |
| train_0/reward            | -0.7419428050015995    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.2130126953125        |
| train_0/target_q          | -10.560381696324756    |
| train_1/avg_q             | -9.981007514905444     |
| train_1/current_q         | -8.268635840052676     |
| train_1/fw_bonus          | -1.0141550540924071    |
| train_1/fw_loss           | 0.00022429960772569758 |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.027806539461016654   |
| train_1/q_grads_std       | 0.6613638907670975     |
| train_1/q_loss            | 2.526374428275136      |
| train_1/reward            | -1.3394017519756745    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0                    |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.431960345725678     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 36
Time for epoch 36: 485.20. Rollout time: 227.60, Training time: 257.45
Evaluating epoch 36
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 36                     |
| policy/steps              | 1154187.0              |
| test/episodes             | 925.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 3700.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.29488235478775     |
| train_0/fw_bonus          | -0.9999214425683022    |
| train_0/fw_loss           | 1.9547360761862364e-05 |
| train_0/mu_grads          | -0.01810912457294762   |
| train_0/mu_grads_std      | 0.27889650985598563    |
| train_0/mu_loss           | 9.847514909328728      |
| train_0/next_q            | -10.295216137086447    |
| train_0/q_grads           | 0.03297093091532588    |
| train_0/q_grads_std       | 0.3198124147951603     |
| train_0/q_loss            | 0.5430752183337871     |
| train_0/reward            | -0.7424167806806509    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.2937255859375        |
| train_0/target_q          | -10.463789286118573    |
| train_1/avg_q             | -9.999978957383872     |
| train_1/current_q         | -8.237071691033659     |
| train_1/fw_bonus          | -1.01421160697937      |
| train_1/fw_loss           | 0.0002110547233314719  |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.028401938639581202   |
| train_1/q_grads_std       | 0.6684946089982986     |
| train_1/q_loss            | 2.52436517129458       |
| train_1/reward            | -1.3268811915033438    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 2.44140625e-05         |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.417525722753348     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 37
Time for epoch 37: 406.83. Rollout time: 181.40, Training time: 225.34
Evaluating epoch 37
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 37                     |
| policy/steps              | 1185437.0              |
| test/episodes             | 950.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 3800.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.115115697983127    |
| train_0/fw_bonus          | -0.9999086201190949    |
| train_0/fw_loss           | 2.234497305835248e-05  |
| train_0/mu_grads          | -0.01676518488675356   |
| train_0/mu_grads_std      | 0.2872003696858883     |
| train_0/mu_loss           | 9.558035956210428      |
| train_0/next_q            | -10.116992275480985    |
| train_0/q_grads           | 0.032404749747365715   |
| train_0/q_grads_std       | 0.32177000045776366    |
| train_0/q_loss            | 0.9747099285619301     |
| train_0/reward            | -0.7428742329837406    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.27216796875          |
| train_0/target_q          | -10.37462229870614     |
| train_1/avg_q             | -9.990264240552532     |
| train_1/current_q         | -8.251664720089039     |
| train_1/fw_bonus          | -1.0142672121524812    |
| train_1/fw_loss           | 0.00019802168826572598 |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.02793033830821514    |
| train_1/q_grads_std       | 0.6751827716827392     |
| train_1/q_loss            | 2.398621808265729      |
| train_1/reward            | -1.3223801997868576    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 2.44140625e-05         |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.43671125447436      |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 38
Time for epoch 38: 482.65. Rollout time: 213.96, Training time: 268.59
Evaluating epoch 38
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 38                     |
| policy/steps              | 1216687.0              |
| test/episodes             | 975.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 3900.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.380436697020647    |
| train_0/fw_bonus          | -0.9998861744999885    |
| train_0/fw_loss           | 2.724074870457116e-05  |
| train_0/mu_grads          | -0.018153683841228486  |
| train_0/mu_grads_std      | 0.295989865064621      |
| train_0/mu_loss           | 9.788923941976952      |
| train_0/next_q            | -10.378440627243007    |
| train_0/q_grads           | 0.033010983001440765   |
| train_0/q_grads_std       | 0.3248648479580879     |
| train_0/q_loss            | 0.6947743885554287     |
| train_0/reward            | -0.741300507507549     |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.2128662109375        |
| train_0/target_q          | -10.566048764980458    |
| train_1/avg_q             | -9.990101993577252     |
| train_1/current_q         | -8.233121587273368     |
| train_1/fw_bonus          | -1.0142630875110625    |
| train_1/fw_loss           | 0.0001989940385101363  |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.027669529989361764   |
| train_1/q_grads_std       | 0.6820684880018234     |
| train_1/q_loss            | 2.578384651144555      |
| train_1/reward            | -1.3264674792771984    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 2.44140625e-05         |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.406106151152203     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 39
Time for epoch 39: 557.19. Rollout time: 261.77, Training time: 295.29
Evaluating epoch 39
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 39                     |
| policy/steps              | 1247927.0              |
| test/episodes             | 1000.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 4000.0                 |
| train/success_rate        | 0.01                   |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.370075453007697    |
| train_0/fw_bonus          | -0.9999122008681297    |
| train_0/fw_loss           | 2.1566986106336117e-05 |
| train_0/mu_grads          | -0.018471000762656332  |
| train_0/mu_grads_std      | 0.3046168014407158     |
| train_0/mu_loss           | 9.966929594440467      |
| train_0/next_q            | -10.372215531667768    |
| train_0/q_grads           | 0.03249771688133478    |
| train_0/q_grads_std       | 0.3279298827052116     |
| train_0/q_loss            | 1.0267051920172439     |
| train_0/reward            | -0.7427783607090532    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.2511474609375        |
| train_0/target_q          | -10.568435480988512    |
| train_1/avg_q             | -9.991057648439375     |
| train_1/current_q         | -8.258749298374997     |
| train_1/fw_bonus          | -1.0141957074403762    |
| train_1/fw_loss           | 0.00021477402187883854 |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.026448920788243412   |
| train_1/q_grads_std       | 0.6874805808067321     |
| train_1/q_loss            | 2.3793265428684074     |
| train_1/reward            | -1.3298029410230812    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 2.44140625e-05         |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.434802941023085     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 40
Time for epoch 40: 452.21. Rollout time: 211.46, Training time: 240.67
Evaluating epoch 40
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 40                     |
| policy/steps              | 1279177.0              |
| test/episodes             | 1025.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 4100.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.230701831189577    |
| train_0/fw_bonus          | -0.9999242007732392    |
| train_0/fw_loss           | 1.8946886257253936e-05 |
| train_0/mu_grads          | -0.01802460146136582   |
| train_0/mu_grads_std      | 0.31188385784626005    |
| train_0/mu_loss           | 9.320005145981565      |
| train_0/next_q            | -10.230012572289606    |
| train_0/q_grads           | 0.03257898194715381    |
| train_0/q_grads_std       | 0.3294725053012371     |
| train_0/q_loss            | 0.53049097376998       |
| train_0/reward            | -0.7422838802827755    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.3144287109375        |
| train_0/target_q          | -10.398434643398563    |
| train_1/avg_q             | -9.997879312649017     |
| train_1/current_q         | -8.233538877547181     |
| train_1/fw_bonus          | -1.0143291026353836    |
| train_1/fw_loss           | 0.00018352952793065924 |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.02631757645867765    |
| train_1/q_grads_std       | 0.6937320277094841     |
| train_1/q_loss            | 2.3534049240944896     |
| train_1/reward            | -1.325865719162539     |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0                    |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.412203609787543     |
------------------------------------------------------
Saving periodic policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_40.pkl ...
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 41
Time for epoch 41: 477.14. Rollout time: 217.72, Training time: 259.30
Evaluating epoch 41
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 41                     |
| policy/steps              | 1310427.0              |
| test/episodes             | 1050.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 4200.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.394598465230816    |
| train_0/fw_bonus          | -0.9999004319310189    |
| train_0/fw_loss           | 2.4131409986694052e-05 |
| train_0/mu_grads          | -0.01796345254406333   |
| train_0/mu_grads_std      | 0.31877151057124137    |
| train_0/mu_loss           | 9.521286343036065      |
| train_0/next_q            | -10.39316014492853     |
| train_0/q_grads           | 0.03193013146519661    |
| train_0/q_grads_std       | 0.3311151094734669     |
| train_0/q_loss            | 0.44756017565455863    |
| train_0/reward            | -0.7423818556395417    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.165966796875         |
| train_0/target_q          | -10.55657200983389     |
| train_1/avg_q             | -10.0                  |
| train_1/current_q         | -8.293397056992616     |
| train_1/fw_bonus          | -1.0143236130475999    |
| train_1/fw_loss           | 0.0001848088581027696  |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.02568839625455439    |
| train_1/q_grads_std       | 0.7002797320485115     |
| train_1/q_loss            | 2.513403424967587      |
| train_1/reward            | -1.3354455074426368    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 2.44140625e-05         |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.45408320275514      |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 42
Time for epoch 42: 611.49. Rollout time: 283.22, Training time: 328.18
Evaluating epoch 42
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 42                     |
| policy/steps              | 1341677.0              |
| test/episodes             | 1075.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 4300.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.26263689857918     |
| train_0/fw_bonus          | -0.9999200060963631    |
| train_0/fw_loss           | 1.9861984378621854e-05 |
| train_0/mu_grads          | -0.018057471280917524  |
| train_0/mu_grads_std      | 0.32537279576063155    |
| train_0/mu_loss           | 9.246569684175025      |
| train_0/next_q            | -10.257251966234923    |
| train_0/q_grads           | 0.03174481596797705    |
| train_0/q_grads_std       | 0.3322340726852417     |
| train_0/q_loss            | 1.083769579608076      |
| train_0/reward            | -0.7427578560869733    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.2819580078125        |
| train_0/target_q          | -10.463445390303036    |
| train_1/avg_q             | -9.990712344727354     |
| train_1/current_q         | -8.247931132561993     |
| train_1/fw_bonus          | -1.0143762916326522    |
| train_1/fw_loss           | 0.00017247153773496392 |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.0250428460072726     |
| train_1/q_grads_std       | 0.7063123837113381     |
| train_1/q_loss            | 2.485979069338284      |
| train_1/reward            | -1.3249615854889272    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0                    |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.42661197611393      |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 43
Time for epoch 43: 750.79. Rollout time: 360.68, Training time: 389.97
Evaluating epoch 43
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 43                     |
| policy/steps              | 1372927.0              |
| test/episodes             | 1100.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 4400.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.189312964911196    |
| train_0/fw_bonus          | -0.9999259248375892    |
| train_0/fw_loss           | 1.8573171496427675e-05 |
| train_0/mu_grads          | -0.017552967788651584  |
| train_0/mu_grads_std      | 0.3330055058002472     |
| train_0/mu_loss           | 9.376216746640955      |
| train_0/next_q            | -10.186462225594278    |
| train_0/q_grads           | 0.031914154160767795   |
| train_0/q_grads_std       | 0.3352773658931255     |
| train_0/q_loss            | 0.6347089000385389     |
| train_0/reward            | -0.7432612997945398    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.29794921875          |
| train_0/target_q          | -10.375137010004746    |
| train_1/avg_q             | -9.990112861691825     |
| train_1/current_q         | -8.260945461496528     |
| train_1/fw_bonus          | -1.0142455250024796    |
| train_1/fw_loss           | 0.00020310756299295462 |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.023913027439266443   |
| train_1/q_grads_std       | 0.7141945794224739     |
| train_1/q_loss            | 2.4791392989166297     |
| train_1/reward            | -1.350079305383406     |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0                    |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.41177364132091      |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 44
Time for epoch 44: 713.01. Rollout time: 340.95, Training time: 371.86
Evaluating epoch 44
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 44                     |
| policy/steps              | 1404177.0              |
| test/episodes             | 1125.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 4500.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.475844674880358    |
| train_0/fw_bonus          | -0.9999344199895859    |
| train_0/fw_loss           | 1.6719589211788843e-05 |
| train_0/mu_grads          | -0.01479535058606416   |
| train_0/mu_grads_std      | 0.3371154569089413     |
| train_0/mu_loss           | 9.285133558589186      |
| train_0/next_q            | -10.47394608689316     |
| train_0/q_grads           | 0.03255690122023225    |
| train_0/q_grads_std       | 0.338849763572216      |
| train_0/q_loss            | 0.5109822530634054     |
| train_0/reward            | -0.7453163311547542    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.32861328125          |
| train_0/target_q          | -10.63621436636589     |
| train_1/avg_q             | -9.99999999999999      |
| train_1/current_q         | -8.242102729770503     |
| train_1/fw_bonus          | -1.0141764163970948    |
| train_1/fw_loss           | 0.00021929889408056624 |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.02250769534148276    |
| train_1/q_grads_std       | 0.7220655307173729     |
| train_1/q_loss            | 2.5470755971329067     |
| train_1/reward            | -1.3334798073425191    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0                    |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.403069651092522     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 45
Time for epoch 45: 925.09. Rollout time: 440.72, Training time: 484.21
Evaluating epoch 45
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 45                     |
| policy/steps              | 1435427.0              |
| test/episodes             | 1150.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 4600.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.347550738049435    |
| train_0/fw_bonus          | -0.999914002418518     |
| train_0/fw_loss           | 2.11738880352641e-05   |
| train_0/mu_grads          | -0.013546757237054407  |
| train_0/mu_grads_std      | 0.3433461368083954     |
| train_0/mu_loss           | 9.436485397010422      |
| train_0/next_q            | -10.341831030584189    |
| train_0/q_grads           | 0.031962848734110594   |
| train_0/q_grads_std       | 0.339849853515625      |
| train_0/q_loss            | 0.4707783607859911     |
| train_0/reward            | -0.7450375628905022    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.2687744140625        |
| train_0/target_q          | -10.500679695284045    |
| train_1/avg_q             | -9.991197861829097     |
| train_1/current_q         | -8.249998934088314     |
| train_1/fw_bonus          | -1.014321517944336     |
| train_1/fw_loss           | 0.0001853020574344555  |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.021371628763154148   |
| train_1/q_grads_std       | 0.7294523298740387     |
| train_1/q_loss            | 2.4607137424540246     |
| train_1/reward            | -1.331916623502184     |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0                    |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.422082639127186     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 46
Time for epoch 46: 713.92. Rollout time: 359.66, Training time: 354.11
Evaluating epoch 46
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 46                     |
| policy/steps              | 1466677.0              |
| test/episodes             | 1175.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 4700.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.509037426694949    |
| train_0/fw_bonus          | -0.999922651052475     |
| train_0/fw_loss           | 1.9283246911072637e-05 |
| train_0/mu_grads          | -0.01499245841987431   |
| train_0/mu_grads_std      | 0.35008271113038064    |
| train_0/mu_loss           | 9.6212451158241        |
| train_0/next_q            | -10.50931819451982     |
| train_0/q_grads           | 0.031701257452368736   |
| train_0/q_grads_std       | 0.3416001424193382     |
| train_0/q_loss            | 0.680283696935492      |
| train_0/reward            | -0.7426126989412296    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.32802734375          |
| train_0/target_q          | -10.694568961732992    |
| train_1/avg_q             | -9.990497804290321     |
| train_1/current_q         | -8.228203596844551     |
| train_1/fw_bonus          | -1.0142237365245819    |
| train_1/fw_loss           | 0.00020821746184083168 |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.020672096917405725   |
| train_1/q_grads_std       | 0.7375966414809227     |
| train_1/q_loss            | 2.5048942307205158     |
| train_1/reward            | -1.3300488682805736    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0                    |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.398920938593076     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 47
Time for epoch 47: 681.49. Rollout time: 313.92, Training time: 367.45
Evaluating epoch 47
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 47                     |
| policy/steps              | 1497927.0              |
| test/episodes             | 1200.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 4800.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.255170001913061    |
| train_0/fw_bonus          | -0.9999143078923225    |
| train_0/fw_loss           | 2.11073971058795e-05   |
| train_0/mu_grads          | -0.017629245296120644  |
| train_0/mu_grads_std      | 0.3559422709047794     |
| train_0/mu_loss           | 9.672717054737058      |
| train_0/next_q            | -10.254277345966504    |
| train_0/q_grads           | 0.031972544733434916   |
| train_0/q_grads_std       | 0.34379917159676554    |
| train_0/q_loss            | 0.5225369455636969     |
| train_0/reward            | -0.741827266242035     |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.3072021484375        |
| train_0/target_q          | -10.412209156209238    |
| train_1/avg_q             | -9.999999984657348     |
| train_1/current_q         | -8.29388248090911      |
| train_1/fw_bonus          | -1.0143389999866486    |
| train_1/fw_loss           | 0.00018120861604984385 |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.019655936304479836   |
| train_1/q_grads_std       | 0.7473828926682472     |
| train_1/q_loss            | 2.4830127180248676     |
| train_1/reward            | -1.3374551451292063    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0                    |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.46901276231671      |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 48
Time for epoch 48: 2623.75. Rollout time: 386.98, Training time: 2236.64
Evaluating epoch 48
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 48                     |
| policy/steps              | 1529177.0              |
| test/episodes             | 1225.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 4900.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.344581497956531    |
| train_0/fw_bonus          | -0.9999286532402039    |
| train_0/fw_loss           | 1.7976215053749912e-05 |
| train_0/mu_grads          | -0.01878852555528283   |
| train_0/mu_grads_std      | 0.3614943377673626     |
| train_0/mu_loss           | 9.180393869544286      |
| train_0/next_q            | -10.342539440785966    |
| train_0/q_grads           | 0.03265503952279687    |
| train_0/q_grads_std       | 0.3459932379424572     |
| train_0/q_loss            | 0.6199986444664256     |
| train_0/reward            | -0.7424325903630233    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.2768798828125        |
| train_0/target_q          | -10.563504888098581    |
| train_1/avg_q             | -9.99999898273982      |
| train_1/current_q         | -8.310211524712923     |
| train_1/fw_bonus          | -1.0143004477024078    |
| train_1/fw_loss           | 0.00019023802524316126 |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.01904161893762648    |
| train_1/q_grads_std       | 0.7575654581189155     |
| train_1/q_loss            | 2.43421824243295       |
| train_1/reward            | -1.3447967136715306    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0                    |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.479464682421533     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 49
Time for epoch 49: 783.66. Rollout time: 377.84, Training time: 405.59
Evaluating epoch 49
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 49                     |
| policy/steps              | 1560427.0              |
| test/episodes             | 1250.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 5000.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.185270343757049    |
| train_0/fw_bonus          | -0.9999210014939308    |
| train_0/fw_loss           | 1.9645317911454187e-05 |
| train_0/mu_grads          | -0.02057264088653028   |
| train_0/mu_grads_std      | 0.36662106662988664    |
| train_0/mu_loss           | 9.174776602874823      |
| train_0/next_q            | -10.184543727194436    |
| train_0/q_grads           | 0.03291650805622339    |
| train_0/q_grads_std       | 0.3478683322668076     |
| train_0/q_loss            | 0.4995573378146204     |
| train_0/reward            | -0.7421235872898251    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.2988037109375        |
| train_0/target_q          | -10.385403917734221    |
| train_1/avg_q             | -9.999999869611894     |
| train_1/current_q         | -8.255336055933261     |
| train_1/fw_bonus          | -1.0143108785152435    |
| train_1/fw_loss           | 0.00018779704696498812 |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.019173416821286083   |
| train_1/q_grads_std       | 0.7651012375950813     |
| train_1/q_loss            | 2.5346827830562804     |
| train_1/reward            | -1.3334493843707604    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0                    |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.425050946870764     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 50
Time for epoch 50: 792.21. Rollout time: 389.97, Training time: 402.03
Evaluating epoch 50
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 50                     |
| policy/steps              | 1591677.0              |
| test/episodes             | 1275.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 5100.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -25.0                  |
| train_0/current_q         | -10.491181566342522    |
| train_0/fw_bonus          | -0.9999012261629104    |
| train_0/fw_loss           | 2.396065167431516e-05  |
| train_0/mu_grads          | -0.01977883051149547   |
| train_0/mu_grads_std      | 0.3732760936021805     |
| train_0/mu_loss           | 9.420476130812709      |
| train_0/next_q            | -10.49396974126535     |
| train_0/q_grads           | 0.032904007472097874   |
| train_0/q_grads_std       | 0.3492568895220757     |
| train_0/q_loss            | 0.5215658770058335     |
| train_0/reward            | -0.7442133019474568    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.21123046875          |
| train_0/target_q          | -10.677048024092265    |
| train_1/avg_q             | -9.99999587531402      |
| train_1/current_q         | -8.313440216467182     |
| train_1/fw_bonus          | -1.0143522500991822    |
| train_1/fw_loss           | 0.00017810045792430174 |
| train_1/mu_grads          | 1.8710847143665887e-05 |
| train_1/mu_grads_std      | 0.1177799254655838     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.018327192589640617   |
| train_1/q_grads_std       | 0.773539912700653      |
| train_1/q_loss            | 2.3900568391009296     |
| train_1/reward            | -1.326805040564068     |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 2.44140625e-05         |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.50071129056407      |
------------------------------------------------------
Saving periodic policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_50.pkl ...
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 51
Time for epoch 51: 644.06. Rollout time: 308.85, Training time: 335.00
Evaluating epoch 51
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
-------------------------------------------------------
| epoch                     | 51                      |
| policy/steps              | 1622904.0               |
| test/episodes             | 1300.0                  |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -25.0                   |
| test_1/avg_q              | -10.0                   |
| test_1/n_subgoals         | 250.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 5200.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -24.99894154587355      |
| train_0/current_q         | -10.33188591415632      |
| train_0/fw_bonus          | -0.9999333828687668     |
| train_0/fw_loss           | 1.6945835159276613e-05  |
| train_0/mu_grads          | -0.02071412596851587    |
| train_0/mu_grads_std      | 0.3802978344261646      |
| train_0/mu_loss           | 9.190914564361373       |
| train_0/next_q            | -10.326439433396349     |
| train_0/q_grads           | 0.03306079907342792     |
| train_0/q_grads_std       | 0.35093641877174375     |
| train_0/q_loss            | 0.6140407612897557      |
| train_0/reward            | -0.7414461834596295     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.3424072265625         |
| train_0/target_q          | -10.482465501113571     |
| train_1/avg_q             | -9.994795687495456      |
| train_1/current_q         | -8.311906290695443      |
| train_1/fw_bonus          | -1.014373505115509      |
| train_1/fw_loss           | 0.00017312462405243422  |
| train_1/mu_grads          | -0.00016732786025386304 |
| train_1/mu_grads_std      | 0.11784094572067261     |
| train_1/mu_loss           | 11.0                    |
| train_1/n_subgoals        | 1000.0                  |
| train_1/next_q            | -10.0                   |
| train_1/q_grads           | 0.018244517082348467    |
| train_1/q_grads_std       | 0.7811093538999557      |
| train_1/q_loss            | 2.366968921549141       |
| train_1/reward            | -1.3263884147490899     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 2.44140625e-05          |
| train_1/reward_-10.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.001                   |
| train_1/target_q          | -8.503883531936593      |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 52
Time for epoch 52: 686.22. Rollout time: 331.85, Training time: 354.20
Evaluating epoch 52
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
-------------------------------------------------------
| epoch                     | 52                      |
| policy/steps              | 1654154.0               |
| test/episodes             | 1325.0                  |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -25.0                   |
| test_1/avg_q              | -10.0                   |
| test_1/n_subgoals         | 250.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 5300.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -24.989707012272213     |
| train_0/current_q         | -10.331261373805727     |
| train_0/fw_bonus          | -0.9999309867620468     |
| train_0/fw_loss           | 1.7469278145654243e-05  |
| train_0/mu_grads          | -0.019854523008689284   |
| train_0/mu_grads_std      | 0.38819524720311166     |
| train_0/mu_loss           | 9.123706352236562       |
| train_0/next_q            | -10.329802346095756     |
| train_0/q_grads           | 0.03326644850894809     |
| train_0/q_grads_std       | 0.3522295609116554      |
| train_0/q_loss            | 0.46449693280067483     |
| train_0/reward            | -0.7421774050351815     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.32822265625           |
| train_0/target_q          | -10.52310794029702      |
| train_1/avg_q             | -9.999985506361012      |
| train_1/current_q         | -8.32343226836746       |
| train_1/fw_bonus          | -1.0144320636987687     |
| train_1/fw_loss           | 0.00015940550620143767  |
| train_1/mu_grads          | -0.00016732786025386304 |
| train_1/mu_grads_std      | 0.11784094572067261     |
| train_1/mu_loss           | 11.0                    |
| train_1/n_subgoals        | 1000.0                  |
| train_1/next_q            | -10.0                   |
| train_1/q_grads           | 0.018805312039330602    |
| train_1/q_grads_std       | 0.7886515513062478      |
| train_1/q_loss            | 2.478758337424113       |
| train_1/reward            | -1.325062714388332      |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.0                     |
| train_1/reward_-10.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -8.511888886263336      |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 53
Time for epoch 53: 719.76. Rollout time: 343.09, Training time: 376.44
Evaluating epoch 53
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
-------------------------------------------------------
| epoch                     | 53                      |
| policy/steps              | 1685404.0               |
| test/episodes             | 1350.0                  |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -25.0                   |
| test_1/avg_q              | -10.0                   |
| test_1/n_subgoals         | 250.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 5400.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -25.0                   |
| train_0/current_q         | -10.237878187069418     |
| train_0/fw_bonus          | -0.9999360248446465     |
| train_0/fw_loss           | 1.6369977925023704e-05  |
| train_0/mu_grads          | -0.022357838274911047   |
| train_0/mu_grads_std      | 0.39490080773830416     |
| train_0/mu_loss           | 9.486035916427705       |
| train_0/next_q            | -10.241391965103187     |
| train_0/q_grads           | 0.03357531195506454     |
| train_0/q_grads_std       | 0.3539786070585251      |
| train_0/q_loss            | 0.44560037840431177     |
| train_0/reward            | -0.744157464732416      |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.33349609375           |
| train_0/target_q          | -10.355205512604162     |
| train_1/avg_q             | -9.999999999999867      |
| train_1/current_q         | -8.312626565857432      |
| train_1/fw_bonus          | -1.0143262892961502     |
| train_1/fw_loss           | 0.00018418688232486603  |
| train_1/mu_grads          | -0.00016732793301343918 |
| train_1/mu_grads_std      | 0.11784094572067261     |
| train_1/mu_loss           | 11.0                    |
| train_1/n_subgoals        | 1000.0                  |
| train_1/next_q            | -10.0                   |
| train_1/q_grads           | 0.01844646641984582     |
| train_1/q_grads_std       | 0.7963393568992615      |
| train_1/q_loss            | 2.5593959820623007      |
| train_1/reward            | -1.3094702731643337     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.0                     |
| train_1/reward_-10.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -8.517111874726837      |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 54
Time for epoch 54: 845.28. Rollout time: 416.02, Training time: 429.11
Evaluating epoch 54
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
------------------------------------------------------
| epoch                     | 54                     |
| policy/steps              | 1716654.0              |
| test/episodes             | 1375.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -25.0                  |
| test_1/avg_q              | -10.0                  |
| test_1/n_subgoals         | 250.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 5500.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -24.999999795228042    |
| train_0/current_q         | -10.338806515080433    |
| train_0/fw_bonus          | -0.9999182254076004    |
| train_0/fw_loss           | 2.0251444607310986e-05 |
| train_0/mu_grads          | -0.024110058369114996  |
| train_0/mu_grads_std      | 0.40098299011588096    |
| train_0/mu_loss           | 9.45607575555482       |
| train_0/next_q            | -10.336963887232018    |
| train_0/q_grads           | 0.03329297248274088    |
| train_0/q_grads_std       | 0.35539553612470626    |
| train_0/q_loss            | 0.4937282716354202     |
| train_0/reward            | -0.7468829397395893    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.3296875              |
| train_0/target_q          | -10.51073034453933     |
| train_1/avg_q             | -10.0                  |
| train_1/current_q         | -8.273061942836947     |
| train_1/fw_bonus          | -1.0144034504890442    |
| train_1/fw_loss           | 0.00016611135397397446 |
| train_1/mu_grads          | -0.0001673269143793732 |
| train_1/mu_grads_std      | 0.1178409531712532     |
| train_1/mu_loss           | 11.0                   |
| train_1/n_subgoals        | 1000.0                 |
| train_1/next_q            | -10.0                  |
| train_1/q_grads           | 0.01790090133436024    |
| train_1/q_grads_std       | 0.8020266622304917     |
| train_1/q_loss            | 2.343865819193456      |
| train_1/reward            | -1.31293902740872      |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0                    |
| train_1/reward_-10.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -8.472968324283723     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 55
Time for epoch 55: 672.07. Rollout time: 334.93, Training time: 336.98
Evaluating epoch 55
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
-------------------------------------------------------
| epoch                     | 55                      |
| policy/steps              | 1747896.0               |
| test/episodes             | 1400.0                  |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -25.0                   |
| test_1/avg_q              | -10.0                   |
| test_1/n_subgoals         | 250.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 5600.0                  |
| train/success_rate        | 0.01                    |
| train_0/avg_q             | -24.997155561486583     |
| train_0/current_q         | -10.489235990932624     |
| train_0/fw_bonus          | -0.999931700527668      |
| train_0/fw_loss           | 1.7314983392680005e-05  |
| train_0/mu_grads          | -0.02492663487792015    |
| train_0/mu_grads_std      | 0.4060882404446602      |
| train_0/mu_loss           | 10.332833875273534      |
| train_0/next_q            | -10.488393777323674     |
| train_0/q_grads           | 0.03355541378259659     |
| train_0/q_grads_std       | 0.3571468755602837      |
| train_0/q_loss            | 0.4873829627175034      |
| train_0/reward            | -0.7447534425024059     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.3308837890625         |
| train_0/target_q          | -10.686245666330787     |
| train_1/avg_q             | -9.990442292632462      |
| train_1/current_q         | -8.319501894241302      |
| train_1/fw_bonus          | -1.0143838793039321     |
| train_1/fw_loss           | 0.0001706909344648011   |
| train_1/mu_grads          | -0.00016731767391320318 |
| train_1/mu_grads_std      | 0.1178409606218338      |
| train_1/mu_loss           | 10.999999999999995      |
| train_1/n_subgoals        | 1000.0                  |
| train_1/next_q            | -9.999999999999998      |
| train_1/q_grads           | 0.01767491828650236     |
| train_1/q_grads_std       | 0.8085528522729873      |
| train_1/q_loss            | 2.340072860287022       |
| train_1/reward            | -1.310365776348044      |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.0                     |
| train_1/reward_-10.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -8.527816948223045      |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 56
Time for epoch 56: 5862.97. Rollout time: 2158.70, Training time: 3704.12
Evaluating epoch 56
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
-------------------------------------------------------
| epoch                     | 56                      |
| policy/steps              | 1779146.0               |
| test/episodes             | 1425.0                  |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -25.0                   |
| test_1/avg_q              | -10.0                   |
| test_1/n_subgoals         | 250.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 5700.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -25.0                   |
| train_0/current_q         | -9.673324626505812      |
| train_0/fw_bonus          | -0.9999320402741432     |
| train_0/fw_loss           | 1.723855491491122e-05   |
| train_0/mu_grads          | -0.025734803220257164   |
| train_0/mu_grads_std      | 0.4077538773417473      |
| train_0/mu_loss           | 9.25371342533941        |
| train_0/next_q            | -9.677435588683693      |
| train_0/q_grads           | 0.03270529042929411     |
| train_0/q_grads_std       | 0.35811507999897        |
| train_0/q_loss            | 0.5427565889578133      |
| train_0/reward            | -0.741640824778733      |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.3562255859375         |
| train_0/target_q          | -10.014926533950836     |
| train_1/avg_q             | -9.999343800488795      |
| train_1/current_q         | -8.285407870321698      |
| train_1/fw_bonus          | -1.0144720047712326     |
| train_1/fw_loss           | 0.00015004916713223793  |
| train_1/mu_grads          | -0.00016731767391320318 |
| train_1/mu_grads_std      | 0.1178409606218338      |
| train_1/mu_loss           | 11.0                    |
| train_1/n_subgoals        | 1000.0                  |
| train_1/next_q            | -10.0                   |
| train_1/q_grads           | 0.017350517213344574    |
| train_1/q_grads_std       | 0.814573447406292       |
| train_1/q_loss            | 2.3770611348444595      |
| train_1/reward            | -1.3166110034457232     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.0                     |
| train_1/reward_-10.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -8.480946940945726      |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 57
Time for epoch 57: 810.36. Rollout time: 402.01, Training time: 408.23
Evaluating epoch 57
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
-------------------------------------------------------
| epoch                     | 57                      |
| policy/steps              | 1810291.0               |
| test/episodes             | 1450.0                  |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -25.0                   |
| test_1/avg_q              | -10.0                   |
| test_1/n_subgoals         | 250.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 5800.0                  |
| train/success_rate        | 0.01                    |
| train_0/avg_q             | -25.0                   |
| train_0/current_q         | -10.185926027673792     |
| train_0/fw_bonus          | -0.9999373555183411     |
| train_0/fw_loss           | 1.6078411977105134e-05  |
| train_0/mu_grads          | -0.026534525351598857   |
| train_0/mu_grads_std      | 0.41162349209189414     |
| train_0/mu_loss           | 9.640656613380937       |
| train_0/next_q            | -10.186668837339713     |
| train_0/q_grads           | 0.03352392436936498     |
| train_0/q_grads_std       | 0.3597031943500042      |
| train_0/q_loss            | 0.5974718305586002      |
| train_0/reward            | -0.7457694614175125     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.3550537109375         |
| train_0/target_q          | -10.426544368132705     |
| train_1/avg_q             | -9.999999999999986      |
| train_1/current_q         | -8.293366095648972      |
| train_1/fw_bonus          | -1.0144511312246323     |
| train_1/fw_loss           | 0.0001549407283164328   |
| train_1/mu_grads          | -0.00016731767391320318 |
| train_1/mu_grads_std      | 0.1178409606218338      |
| train_1/mu_loss           | 11.0                    |
| train_1/n_subgoals        | 996.0                   |
| train_1/next_q            | -10.0                   |
| train_1/q_grads           | 0.01682775462977588     |
| train_1/q_grads_std       | 0.8211945116519928      |
| train_1/q_loss            | 2.5935934568284744      |
| train_1/reward            | -1.3138417752561509     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 2.44140625e-05          |
| train_1/reward_-10.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -8.476742165881154      |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 58
Time for epoch 58: 802.21. Rollout time: 386.92, Training time: 415.05
Evaluating epoch 58
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
-------------------------------------------------------
| epoch                     | 58                      |
| policy/steps              | 1841541.0               |
| test/episodes             | 1475.0                  |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -25.0                   |
| test_1/avg_q              | -10.0                   |
| test_1/n_subgoals         | 250.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 5900.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -24.999080060027847     |
| train_0/current_q         | -10.500901081418593     |
| train_0/fw_bonus          | -0.9999352097511292     |
| train_0/fw_loss           | 1.6547587370041585e-05  |
| train_0/mu_grads          | -0.02822763007134199    |
| train_0/mu_grads_std      | 0.4159561194479465      |
| train_0/mu_loss           | 9.846129240830345       |
| train_0/next_q            | -10.502832739581478     |
| train_0/q_grads           | 0.03468337841331959     |
| train_0/q_grads_std       | 0.3611724816262722      |
| train_0/q_loss            | 0.5338462578921708      |
| train_0/reward            | -0.7461590105995128     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.3356689453125         |
| train_0/target_q          | -10.691804063084103     |
| train_1/avg_q             | -9.999999998204606      |
| train_1/current_q         | -8.284570330551853      |
| train_1/fw_bonus          | -1.014412698149681      |
| train_1/fw_loss           | 0.00016394567755924073  |
| train_1/mu_grads          | -0.00016731767391320318 |
| train_1/mu_grads_std      | 0.1178409606218338      |
| train_1/mu_loss           | 11.0                    |
| train_1/n_subgoals        | 1000.0                  |
| train_1/next_q            | -10.0                   |
| train_1/q_grads           | 0.015503530157729983    |
| train_1/q_grads_std       | 0.8284469425678254      |
| train_1/q_loss            | 2.873358088450268       |
| train_1/reward            | -1.3222454069873493     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 2.44140625e-05          |
| train_1/reward_-10.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -8.458827438237353      |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 59
Time for epoch 59: 739.85. Rollout time: 363.69, Training time: 375.97
Evaluating epoch 59
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
-------------------------------------------------------
| epoch                     | 59                      |
| policy/steps              | 1872791.0               |
| test/episodes             | 1500.0                  |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -25.0                   |
| test_1/avg_q              | -10.0                   |
| test_1/n_subgoals         | 250.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 6000.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -25.0                   |
| train_0/current_q         | -10.196104034070155     |
| train_0/fw_bonus          | -0.9999395579099655     |
| train_0/fw_loss           | 1.559865800118132e-05   |
| train_0/mu_grads          | -0.02855816106311977    |
| train_0/mu_grads_std      | 0.41963670402765274     |
| train_0/mu_loss           | 9.676215251643765       |
| train_0/next_q            | -10.194747870932273     |
| train_0/q_grads           | 0.03396211452782154     |
| train_0/q_grads_std       | 0.3625516042113304      |
| train_0/q_loss            | 0.5347636381987041      |
| train_0/reward            | -0.7434268113662256     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.3393310546875         |
| train_0/target_q          | -10.455634167433043     |
| train_1/avg_q             | -9.990973397749203      |
| train_1/current_q         | -8.266369746155084      |
| train_1/fw_bonus          | -1.0144671887159347     |
| train_1/fw_loss           | 0.00015118016344786155  |
| train_1/mu_grads          | -0.00016731767391320318 |
| train_1/mu_grads_std      | 0.1178409606218338      |
| train_1/mu_loss           | 11.0                    |
| train_1/n_subgoals        | 1000.0                  |
| train_1/next_q            | -10.0                   |
| train_1/q_grads           | 0.014352687983773649    |
| train_1/q_grads_std       | 0.8347456648945808      |
| train_1/q_loss            | 2.400821471281182       |
| train_1/reward            | -1.335480660583562      |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.0                     |
| train_1/reward_-10.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -8.439045113708564      |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 60
Time for epoch 60: 793.26. Rollout time: 384.15, Training time: 408.91
Evaluating epoch 60
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
-------------------------------------------------------
| epoch                     | 60                      |
| policy/steps              | 1904041.0               |
| test/episodes             | 1525.0                  |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -25.0                   |
| test_1/avg_q              | -10.0                   |
| test_1/n_subgoals         | 250.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 6100.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -24.999999999999943     |
| train_0/current_q         | -10.3907453377426       |
| train_0/fw_bonus          | -0.999925172328949      |
| train_0/fw_loss           | 1.8735575440587127e-05  |
| train_0/mu_grads          | -0.029429463529959322   |
| train_0/mu_grads_std      | 0.42293196320533755     |
| train_0/mu_loss           | 10.237480437573215      |
| train_0/next_q            | -10.390837434928594     |
| train_0/q_grads           | 0.034060201980173586    |
| train_0/q_grads_std       | 0.3642440505325794      |
| train_0/q_loss            | 0.4089673899847684      |
| train_0/reward            | -0.7430412006287952     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.3201904296875         |
| train_0/target_q          | -10.546018520188701     |
| train_1/avg_q             | -9.99010786150023       |
| train_1/current_q         | -8.298755083506535      |
| train_1/fw_bonus          | -1.0145195305347443     |
| train_1/fw_loss           | 0.00013891234339098447  |
| train_1/mu_grads          | -0.00016731767391320318 |
| train_1/mu_grads_std      | 0.1178409606218338      |
| train_1/mu_loss           | 11.0                    |
| train_1/n_subgoals        | 1000.0                  |
| train_1/next_q            | -10.0                   |
| train_1/q_grads           | 0.01292689663823694     |
| train_1/q_grads_std       | 0.8407172963023186      |
| train_1/q_loss            | 2.4833378115717357      |
| train_1/reward            | -1.3186733469512546     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.0                     |
| train_1/reward_-10.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -8.487076667263757      |
-------------------------------------------------------
Saving periodic policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_60.pkl ...
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 61
Time for epoch 61: 852.44. Rollout time: 415.61, Training time: 436.62
Evaluating epoch 61
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
-------------------------------------------------------
| epoch                     | 61                      |
| policy/steps              | 1935291.0               |
| test/episodes             | 1550.0                  |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -25.0                   |
| test_1/avg_q              | -10.0                   |
| test_1/n_subgoals         | 250.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 6200.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -25.0                   |
| train_0/current_q         | -10.581207977239188     |
| train_0/fw_bonus          | -0.9999403685331345     |
| train_0/fw_loss           | 1.5424307548528304e-05  |
| train_0/mu_grads          | -0.02924524350091815    |
| train_0/mu_grads_std      | 0.42601975724101065     |
| train_0/mu_loss           | 10.338588570141553      |
| train_0/next_q            | -10.584932019758662     |
| train_0/q_grads           | 0.034974119253456594    |
| train_0/q_grads_std       | 0.36541223526000977     |
| train_0/q_loss            | 0.5177040603330545      |
| train_0/reward            | -0.744818812792073      |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.3217529296875         |
| train_0/target_q          | -10.766554865411226     |
| train_1/avg_q             | -9.999996449918173      |
| train_1/current_q         | -8.301607794189211      |
| train_1/fw_bonus          | -1.0144691824913026     |
| train_1/fw_loss           | 0.00015070980480231811  |
| train_1/mu_grads          | -0.00016731767391320318 |
| train_1/mu_grads_std      | 0.1178409606218338      |
| train_1/mu_loss           | 11.0                    |
| train_1/n_subgoals        | 1000.0                  |
| train_1/next_q            | -10.0                   |
| train_1/q_grads           | 0.011946602747775615    |
| train_1/q_grads_std       | 0.8469323173165322      |
| train_1/q_loss            | 2.371731240408118       |
| train_1/reward            | -1.3205524648496065     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.0                     |
| train_1/reward_-10.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -8.499961644537109      |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 62
Time for epoch 62: 753.54. Rollout time: 376.27, Training time: 377.14
Evaluating epoch 62
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
-------------------------------------------------------
| epoch                     | 62                      |
| policy/steps              | 1966541.0               |
| test/episodes             | 1575.0                  |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -25.0                   |
| test_1/avg_q              | -10.0                   |
| test_1/n_subgoals         | 250.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 6300.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -25.0                   |
| train_0/current_q         | -10.118885415230324     |
| train_0/fw_bonus          | -0.9999248698353768     |
| train_0/fw_loss           | 1.8801890450959036e-05  |
| train_0/mu_grads          | -0.02906543416902423    |
| train_0/mu_grads_std      | 0.42922854647040365     |
| train_0/mu_loss           | 10.11415105019861       |
| train_0/next_q            | -10.11844757385423      |
| train_0/q_grads           | 0.033671384863555434    |
| train_0/q_grads_std       | 0.36661134734749795     |
| train_0/q_loss            | 0.8093708469455729      |
| train_0/reward            | -0.7455068689712789     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.323486328125          |
| train_0/target_q          | -10.419908273939466     |
| train_1/avg_q             | -10.0                   |
| train_1/current_q         | -8.350330820410957      |
| train_1/fw_bonus          | -1.014409738779068      |
| train_1/fw_loss           | 0.00016463732899865135  |
| train_1/mu_grads          | -0.00016731767391320318 |
| train_1/mu_grads_std      | 0.1178409606218338      |
| train_1/mu_loss           | 11.0                    |
| train_1/n_subgoals        | 1000.0                  |
| train_1/next_q            | -10.0                   |
| train_1/q_grads           | 0.011251441016793252    |
| train_1/q_grads_std       | 0.853631554543972       |
| train_1/q_loss            | 2.4425041782660744      |
| train_1/reward            | -1.3355495949392207     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 2.44140625e-05          |
| train_1/reward_-10.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -8.531946079314224      |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 63
Time for epoch 63: 819.02. Rollout time: 404.49, Training time: 414.35
Evaluating epoch 63
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
-------------------------------------------------------
| epoch                     | 63                      |
| policy/steps              | 1997791.0               |
| test/episodes             | 1600.0                  |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -25.0                   |
| test_1/avg_q              | -10.0                   |
| test_1/n_subgoals         | 250.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 6400.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -25.0                   |
| train_0/current_q         | -10.476504820365307     |
| train_0/fw_bonus          | -0.9999341905117035     |
| train_0/fw_loss           | 1.6768058003435726e-05  |
| train_0/mu_grads          | -0.030129280965775253   |
| train_0/mu_grads_std      | 0.43275782093405724     |
| train_0/mu_loss           | 10.310042854220459      |
| train_0/next_q            | -10.477332316541961     |
| train_0/q_grads           | 0.034542111400514844    |
| train_0/q_grads_std       | 0.368119265884161       |
| train_0/q_loss            | 0.5126079809054114      |
| train_0/reward            | -0.7418372159831051     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.3226318359375         |
| train_0/target_q          | -10.652240461213312     |
| train_1/avg_q             | -9.990157537980103      |
| train_1/current_q         | -8.264355682378172      |
| train_1/fw_bonus          | -1.014446485042572      |
| train_1/fw_loss           | 0.00015602649800712244  |
| train_1/mu_grads          | -0.00016731767391320318 |
| train_1/mu_grads_std      | 0.1178409606218338      |
| train_1/mu_loss           | 11.0                    |
| train_1/n_subgoals        | 1000.0                  |
| train_1/next_q            | -10.0                   |
| train_1/q_grads           | 0.01106674368493259     |
| train_1/q_grads_std       | 0.8591562882065773      |
| train_1/q_loss            | 3.0067448584307988      |
| train_1/reward            | -1.3171109420014546     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.0                     |
| train_1/reward_-10.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -8.438858988876458      |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 64
Time for epoch 64: 654.99. Rollout time: 319.21, Training time: 335.55
Evaluating epoch 64
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
-------------------------------------------------------
| epoch                     | 64                      |
| policy/steps              | 2029041.0               |
| test/episodes             | 1625.0                  |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -25.0                   |
| test_1/avg_q              | -10.0                   |
| test_1/n_subgoals         | 250.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 6500.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -25.0                   |
| train_0/current_q         | -10.467569243338545     |
| train_0/fw_bonus          | -0.9999410197138786     |
| train_0/fw_loss           | 1.528069210507965e-05   |
| train_0/mu_grads          | -0.0315820143558085     |
| train_0/mu_grads_std      | 0.43437184393405914     |
| train_0/mu_loss           | 10.630970544635053      |
| train_0/next_q            | -10.466052814301728     |
| train_0/q_grads           | 0.03397796768695116     |
| train_0/q_grads_std       | 0.3690892316401005      |
| train_0/q_loss            | 0.5919082859209412      |
| train_0/reward            | -0.7419993179319135     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.33046875              |
| train_0/target_q          | -10.619800575325298     |
| train_1/avg_q             | -9.991650518100244      |
| train_1/current_q         | -8.252973822592073      |
| train_1/fw_bonus          | -1.0144501745700836     |
| train_1/fw_loss           | 0.00015516269486397504  |
| train_1/mu_grads          | -0.00016731767391320318 |
| train_1/mu_grads_std      | 0.1178409606218338      |
| train_1/mu_loss           | 11.0                    |
| train_1/n_subgoals        | 1000.0                  |
| train_1/next_q            | -10.0                   |
| train_1/q_grads           | 0.010417326563037932    |
| train_1/q_grads_std       | 0.8655402779579162      |
| train_1/q_loss            | 2.351025392912268       |
| train_1/reward            | -1.3236191327952838     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.0                     |
| train_1/reward_-10.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -8.436514640607786      |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 65
Time for epoch 65: 1667.73. Rollout time: 1329.06, Training time: 338.55
Evaluating epoch 65
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
-------------------------------------------------------
| epoch                     | 65                      |
| policy/steps              | 2060291.0               |
| test/episodes             | 1650.0                  |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -25.0                   |
| test_1/avg_q              | -10.0                   |
| test_1/n_subgoals         | 250.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 6600.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -24.999993199150573     |
| train_0/current_q         | -10.519609319704855     |
| train_0/fw_bonus          | -0.999840846657753      |
| train_0/fw_loss           | 3.712332509167027e-05   |
| train_0/mu_grads          | -0.03220738917589187    |
| train_0/mu_grads_std      | 0.4375820368528366      |
| train_0/mu_loss           | 10.543547488144037      |
| train_0/next_q            | -10.521186341579028     |
| train_0/q_grads           | 0.03434381261467934     |
| train_0/q_grads_std       | 0.3694968782365322      |
| train_0/q_loss            | 0.5022986980347662      |
| train_0/reward            | -0.7436093900061678     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.1409423828125         |
| train_0/target_q          | -10.714600525570496     |
| train_1/avg_q             | -9.990904794889106      |
| train_1/current_q         | -8.209280126825153      |
| train_1/fw_bonus          | -1.0143481135368346     |
| train_1/fw_loss           | 0.00017907398141687737  |
| train_1/mu_grads          | -0.00016731767391320318 |
| train_1/mu_grads_std      | 0.1178409606218338      |
| train_1/mu_loss           | 11.0                    |
| train_1/n_subgoals        | 1000.0                  |
| train_1/next_q            | -10.0                   |
| train_1/q_grads           | 0.009357848903164268    |
| train_1/q_grads_std       | 0.8699615135788917      |
| train_1/q_loss            | 2.289944703500249       |
| train_1/reward            | -1.3057214197724534     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.0                     |
| train_1/reward_-10.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -8.408568099459956      |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 66
Time for epoch 66: 680.40. Rollout time: 330.06, Training time: 350.15
Evaluating epoch 66
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
-------------------------------------------------------
| epoch                     | 66                      |
| policy/steps              | 2091541.0               |
| test/episodes             | 1675.0                  |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -25.0                   |
| test_1/avg_q              | -10.0                   |
| test_1/n_subgoals         | 250.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 6700.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -25.0                   |
| train_0/current_q         | -10.334359626907325     |
| train_0/fw_bonus          | -0.999928955733776      |
| train_0/fw_loss           | 1.7910249357555586e-05  |
| train_0/mu_grads          | -0.03364698179066181    |
| train_0/mu_grads_std      | 0.43784986808896065     |
| train_0/mu_loss           | 10.33195450592496       |
| train_0/next_q            | -10.331351034697688     |
| train_0/q_grads           | 0.03386281607672572     |
| train_0/q_grads_std       | 0.37110433503985407     |
| train_0/q_loss            | 0.6622948309690468      |
| train_0/reward            | -0.7437511010859452     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.277685546875          |
| train_0/target_q          | -10.544697925964245     |
| train_1/avg_q             | -9.999999999656469      |
| train_1/current_q         | -8.19144238164844       |
| train_1/fw_bonus          | -1.0143543630838394     |
| train_1/fw_loss           | 0.00017761239578248933  |
| train_1/mu_grads          | -0.00016731767391320318 |
| train_1/mu_grads_std      | 0.1178409606218338      |
| train_1/mu_loss           | 11.0                    |
| train_1/n_subgoals        | 1000.0                  |
| train_1/next_q            | -10.0                   |
| train_1/q_grads           | 0.008827105700038373    |
| train_1/q_grads_std       | 0.873065185546875       |
| train_1/q_loss            | 2.3185121856230375      |
| train_1/reward            | -1.3132806916248227     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.0                     |
| train_1/reward_-10.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -8.377606863499826      |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 67
Time for epoch 67: 804.49. Rollout time: 383.11, Training time: 421.21
Evaluating epoch 67
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
-------------------------------------------------------
| epoch                     | 67                      |
| policy/steps              | 2122791.0               |
| test/episodes             | 1700.0                  |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -25.0                   |
| test_1/avg_q              | -10.0                   |
| test_1/n_subgoals         | 250.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 6800.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -24.999999999987214     |
| train_0/current_q         | -10.165682897102904     |
| train_0/fw_bonus          | -0.9999262720346451     |
| train_0/fw_loss           | 1.8494407004254754e-05  |
| train_0/mu_grads          | -0.038476118072867396   |
| train_0/mu_grads_std      | 0.4340848743915558      |
| train_0/mu_loss           | 10.24487185550847       |
| train_0/next_q            | -10.166249266255283     |
| train_0/q_grads           | 0.03402759609743953     |
| train_0/q_grads_std       | 0.37187072187662124     |
| train_0/q_loss            | 0.635052881794197       |
| train_0/reward            | -0.7436643123743124     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.337744140625          |
| train_0/target_q          | -10.395521892844494     |
| train_1/avg_q             | -9.990429995029618      |
| train_1/current_q         | -8.214537827353094      |
| train_1/fw_bonus          | -1.0143429070711136     |
| train_1/fw_loss           | 0.00018029189959634095  |
| train_1/mu_grads          | -0.00016731767391320318 |
| train_1/mu_grads_std      | 0.1178409606218338      |
| train_1/mu_loss           | 11.0                    |
| train_1/n_subgoals        | 1000.0                  |
| train_1/next_q            | -10.0                   |
| train_1/q_grads           | 0.008852891041897237    |
| train_1/q_grads_std       | 0.8782373398542405      |
| train_1/q_loss            | 2.662401707322968       |
| train_1/reward            | -1.3230283908662386     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.0                     |
| train_1/reward_-10.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -8.38472272680374       |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 68
Time for epoch 68: 864.43. Rollout time: 399.21, Training time: 465.05
Evaluating epoch 68
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
-------------------------------------------------------
| epoch                     | 68                      |
| policy/steps              | 2154041.0               |
| test/episodes             | 1725.0                  |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -25.0                   |
| test_1/avg_q              | -10.0                   |
| test_1/n_subgoals         | 250.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 6900.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -25.0                   |
| train_0/current_q         | -10.447254584662932     |
| train_0/fw_bonus          | -0.9999077782034874     |
| train_0/fw_loss           | 2.252780313938274e-05   |
| train_0/mu_grads          | -0.03990816492587328    |
| train_0/mu_grads_std      | 0.4322053030133247      |
| train_0/mu_loss           | 10.584766880711296      |
| train_0/next_q            | -10.450179510879911     |
| train_0/q_grads           | 0.034158424101769926    |
| train_0/q_grads_std       | 0.37226864099502566     |
| train_0/q_loss            | 0.7869130203909371      |
| train_0/reward            | -0.7449978372103943     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.2978515625            |
| train_0/target_q          | -10.641368185112123     |
| train_1/avg_q             | -10.0                   |
| train_1/current_q         | -8.232510622769002      |
| train_1/fw_bonus          | -1.0144512325525283     |
| train_1/fw_loss           | 0.00015491867779928725  |
| train_1/mu_grads          | -0.00016731767391320318 |
| train_1/mu_grads_std      | 0.1178409606218338      |
| train_1/mu_loss           | 11.0                    |
| train_1/n_subgoals        | 1000.0                  |
| train_1/next_q            | -10.0                   |
| train_1/q_grads           | 0.008714348054490983    |
| train_1/q_grads_std       | 0.8832059964537621      |
| train_1/q_loss            | 2.3717250188752623      |
| train_1/reward            | -1.3089101810153807     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.0                     |
| train_1/reward_-10.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -8.425394556015386      |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 69
Time for epoch 69: 942.67. Rollout time: 456.81, Training time: 485.57
Evaluating epoch 69
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
-------------------------------------------------------
| epoch                     | 69                      |
| policy/steps              | 2185291.0               |
| test/episodes             | 1750.0                  |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -25.0                   |
| test_1/avg_q              | -10.0                   |
| test_1/n_subgoals         | 250.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 7000.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -25.0                   |
| train_0/current_q         | -10.472558259727291     |
| train_0/fw_bonus          | -0.9999224424362183     |
| train_0/fw_loss           | 1.9330782060933414e-05  |
| train_0/mu_grads          | -0.04001559112221002    |
| train_0/mu_grads_std      | 0.4297761470079422      |
| train_0/mu_loss           | 10.527112251126773      |
| train_0/next_q            | -10.467737097288301     |
| train_0/q_grads           | 0.033845850452780726    |
| train_0/q_grads_std       | 0.37344675064086913     |
| train_0/q_loss            | 0.6246326689478627      |
| train_0/reward            | -0.7434670821901819     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.3185546875            |
| train_0/target_q          | -10.641912641988096     |
| train_1/avg_q             | -9.999993580243215      |
| train_1/current_q         | -8.222978039821994      |
| train_1/fw_bonus          | -1.0144234031438828     |
| train_1/fw_loss           | 0.0001614319106010953   |
| train_1/mu_grads          | -0.00016731767391320318 |
| train_1/mu_grads_std      | 0.1178409606218338      |
| train_1/mu_loss           | 11.0                    |
| train_1/n_subgoals        | 1000.0                  |
| train_1/next_q            | -10.0                   |
| train_1/q_grads           | 0.00922924350015819     |
| train_1/q_grads_std       | 0.8874301671981811      |
| train_1/q_loss            | 2.5264518267621967      |
| train_1/reward            | -1.3228489165507198     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.0                     |
| train_1/reward_-10.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -8.408947549363223      |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 70
Time for epoch 70: 859.46. Rollout time: 414.95, Training time: 444.34
Evaluating epoch 70
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
-------------------------------------------------------
| epoch                     | 70                      |
| policy/steps              | 2216524.0               |
| test/episodes             | 1775.0                  |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -25.0                   |
| test_1/avg_q              | -10.0                   |
| test_1/n_subgoals         | 250.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 7100.0                  |
| train/success_rate        | 0.01                    |
| train_0/avg_q             | -25.0                   |
| train_0/current_q         | -10.336097845691661     |
| train_0/fw_bonus          | -0.9999326825141907     |
| train_0/fw_loss           | 1.7099274987231184e-05  |
| train_0/mu_grads          | -0.04002019194886088    |
| train_0/mu_grads_std      | 0.4269587241113186      |
| train_0/mu_loss           | 10.365101442879237      |
| train_0/next_q            | -10.336136656458141     |
| train_0/q_grads           | 0.03355065044015646     |
| train_0/q_grads_std       | 0.3737978063523769      |
| train_0/q_loss            | 0.855539443079946       |
| train_0/reward            | -0.744424347266613      |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.319384765625          |
| train_0/target_q          | -10.546302982639919     |
| train_1/avg_q             | -9.999999999857149      |
| train_1/current_q         | -8.233573463760989      |
| train_1/fw_bonus          | -1.014417764544487      |
| train_1/fw_loss           | 0.00016275911475531756  |
| train_1/mu_grads          | -0.00016731767391320318 |
| train_1/mu_grads_std      | 0.1178409606218338      |
| train_1/mu_loss           | 11.0                    |
| train_1/n_subgoals        | 1000.0                  |
| train_1/next_q            | -10.0                   |
| train_1/q_grads           | 0.008177492138929664    |
| train_1/q_grads_std       | 0.8906477093696594      |
| train_1/q_loss            | 2.275932923583904       |
| train_1/reward            | -1.3034491112455726     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 4.8828125e-05           |
| train_1/reward_-10.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -8.437638564370577      |
-------------------------------------------------------
Saving periodic policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_70.pkl ...
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 71
Time for epoch 71: 1389.43. Rollout time: 594.85, Training time: 793.84
Evaluating epoch 71
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
-------------------------------------------------------
| epoch                     | 71                      |
| policy/steps              | 2247774.0               |
| test/episodes             | 1800.0                  |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -25.0                   |
| test_1/avg_q              | -10.0                   |
| test_1/n_subgoals         | 250.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 7200.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -24.99999998866845      |
| train_0/current_q         | -10.415554329317487     |
| train_0/fw_bonus          | -0.9990611508488655     |
| train_0/fw_loss           | 0.00020714287402370246  |
| train_0/mu_grads          | -0.03871499672532082    |
| train_0/mu_grads_std      | 0.42468110248446467     |
| train_0/mu_loss           | 10.437675067597134      |
| train_0/next_q            | -10.412556941430193     |
| train_0/q_grads           | 0.03407911602407694     |
| train_0/q_grads_std       | 0.3746253877878189      |
| train_0/q_loss            | 0.9850081180007152      |
| train_0/reward            | -0.743500304895133      |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 4.8828125e-05           |
| train_0/target_q          | -10.62230433152665      |
| train_1/avg_q             | -9.999999999077762      |
| train_1/current_q         | -8.231799746525141      |
| train_1/fw_bonus          | -1.014342901110649      |
| train_1/fw_loss           | 0.00018029245438810904  |
| train_1/mu_grads          | -0.00016731767391320318 |
| train_1/mu_grads_std      | 0.1178409606218338      |
| train_1/mu_loss           | 11.0                    |
| train_1/n_subgoals        | 1000.0                  |
| train_1/next_q            | -10.0                   |
| train_1/q_grads           | 0.00782257723622024     |
| train_1/q_grads_std       | 0.896076574921608       |
| train_1/q_loss            | 2.400451925408332       |
| train_1/reward            | -1.330096260659775      |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 0.0                     |
| train_1/reward_-10.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -8.403514229409778      |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 72
Time for epoch 72: 1185.51. Rollout time: 555.77, Training time: 629.31
Evaluating epoch 72
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109
-------------------------------------------------------
| epoch                     | 72                      |
| policy/steps              | 2279024.0               |
| test/episodes             | 1825.0                  |
| test/success_rate         | 0.0                     |
| test_0/avg_q              | -25.0                   |
| test_1/avg_q              | -10.0                   |
| test_1/n_subgoals         | 250.0                   |
| test_1/subgoal_succ_rate  | 0.0                     |
| train/episodes            | 7300.0                  |
| train/success_rate        | 0.0                     |
| train_0/avg_q             | -25.0                   |
| train_0/current_q         | -10.139778006639602     |
| train_0/fw_bonus          | -0.9999216049909592     |
| train_0/fw_loss           | 1.9514219775373932e-05  |
| train_0/mu_grads          | -0.037642941903322936   |
| train_0/mu_grads_std      | 0.4236073262989521      |
| train_0/mu_loss           | 10.17597319065796       |
| train_0/next_q            | -10.136871460494035     |
| train_0/q_grads           | 0.033297743368893865    |
| train_0/q_grads_std       | 0.3758825160562992      |
| train_0/q_loss            | 0.7051607532266803      |
| train_0/reward            | -0.7445164659591683     |
| train_0/reward_-0.0_frac  | 0.0                     |
| train_0/reward_-1.0_frac  | 0.281103515625          |
| train_0/target_q          | -10.407072439081722     |
| train_1/avg_q             | -9.995791618399341      |
| train_1/current_q         | -8.204461348984609      |
| train_1/fw_bonus          | -1.0144639700651168     |
| train_1/fw_loss           | 0.00015193242288660258  |
| train_1/mu_grads          | -0.00016731767391320318 |
| train_1/mu_grads_std      | 0.1178409606218338      |
| train_1/mu_loss           | 11.0                    |
| train_1/n_subgoals        | 1000.0                  |
| train_1/next_q            | -10.0                   |
| train_1/q_grads           | 0.007121803774498403    |
| train_1/q_grads_std       | 0.9012011244893074      |
| train_1/q_loss            | 2.3963152811779933      |
| train_1/reward            | -1.3323418474617938     |
| train_1/reward_-0.0_frac  | 0.0                     |
| train_1/reward_-1.0_frac  | 2.44140625e-05          |
| train_1/reward_-10.0_frac | 0.0                     |
| train_1/subgoal_succ_rate | 0.0                     |
| train_1/target_q          | -8.372024464649297      |
-------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:25,10|109/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 73
