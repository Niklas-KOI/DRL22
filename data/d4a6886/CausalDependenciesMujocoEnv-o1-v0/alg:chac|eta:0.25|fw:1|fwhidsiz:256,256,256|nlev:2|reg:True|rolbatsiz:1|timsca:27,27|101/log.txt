Starting process id: 24830
T: 50
alg: chac
algorithm: src.chac
atomic_noise: 0.2
base_logdir: data
batch_size: 1024
bind_core: 0
buffer_size: 500
chac_params: {}
early_stop_data_column: test/success_rate
early_stop_threshold: 100.0
env_name: CausalDependenciesMujocoEnv-o1-v0
eta: 0.25
fw: 1
fw_hidden_size: 256,256,256
fw_lr: 0.001
gamma: 0.98
graph: 1
info: 
make_env: <function prepare_params.<locals>.make_env at 0x7fc8b35a0ef0>
max_try_idx: 199
mu_hidden_size: 64
mu_lr: 0.001
n_episodes: 100
n_levels: 2
n_pre_episodes: 30
n_test_rollouts: 25
n_train_batches: 40
n_train_rollouts: 100
num_threads: 1
q_hidden_size: 64
q_lr: 0.001
random_action_perc: 0.3
regularization: True
render: 0
rollout_batch_size: 1
subgoal_noise: 0.2
subgoal_test_perc: 0.3
time_scales: 27,27
try_start_idx: 100
use_mpi: False
verbose: False

*** Warning ***
You are running src.chac with just a single MPI worker. This will work, but the HER experiments that we report in Plappert et al. (2018, https://arxiv.org/abs/1802.09464) were obtained with --num_cpu 19. This makes a significant difference and if you are looking to reproduce those results, be aware of this. Please also refer to https://github.com/openai/baselines/issues/314 for further details.
****************

dims: action = 2, subgoal = 2, end_goal = 2
subgoal_bounds: symmetric [0.2 0.2], offset [0.922 0.25 ]
Running on CPU ...
Creating a CHAC agent

Hierarchy Level 0 with time scale 27
Actor(
  (fc1): Linear(in_features=35, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
  (fc4): Linear(in_features=64, out_features=2, bias=True)
)
Critic(
  (fc1): Linear(in_features=37, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
  (fc4): Linear(in_features=64, out_features=1, bias=True)
  (mse_loss): MSELoss()
)
ForwardModel(
  (mlp): Sequential(
    (0): Linear(in_features=35, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=256, bias=True)
    (5): ReLU()
    (6): Linear(in_features=256, out_features=33, bias=True)
    (7): Identity()
  )
  (mse_loss): MSELoss()
)

Hierarchy Level 1 with time scale 27
Actor(
  (fc1): Linear(in_features=35, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
  (fc4): Linear(in_features=64, out_features=2, bias=True)
)
Critic(
  (fc1): Linear(in_features=37, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
  (fc4): Linear(in_features=64, out_features=1, bias=True)
  (mse_loss): MSELoss()
)
ForwardModel(
  (mlp): Sequential(
    (0): Linear(in_features=35, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=256, bias=True)
    (5): ReLU()
    (6): Linear(in_features=256, out_features=33, bias=True)
    (7): Identity()
  )
  (mse_loss): MSELoss()
)
Training epoch 0
Time for epoch 0: 2041.69. Rollout time: 1335.97, Training time: 705.37
Evaluating epoch 0
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
------------------------------------------------------
| epoch                     | 0                      |
| policy/steps              | 84253.0                |
| test/episodes             | 25.0                   |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -3.685642189476988     |
| test_1/avg_q              | -9.77529367788812      |
| test_1/n_subgoals         | 1581.0                 |
| test_1/subgoal_succ_rate  | 0.5939278937381404     |
| train/episodes            | 100.0                  |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -2.2490800431403426    |
| train_0/current_q         | -3.35083405688351      |
| train_0/fw_bonus          | -0.9991799905896187    |
| train_0/fw_loss           | 0.00022978579836490097 |
| train_0/mu_grads          | -0.006549196178093552  |
| train_0/mu_grads_std      | 0.1630187064409256     |
| train_0/mu_loss           | 3.2126503087534446     |
| train_0/next_q            | -3.21968420308029      |
| train_0/q_grads           | 0.015210024104453624   |
| train_0/q_grads_std       | 0.14790587238967418    |
| train_0/q_loss            | 0.8099359657055913     |
| train_0/reward            | -0.8675429530805558    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 4.8828125e-05          |
| train_0/target_q          | -3.4308635675938404    |
| train_1/avg_q             | -4.730852851826985     |
| train_1/current_q         | -7.57249816259899      |
| train_1/fw_bonus          | -0.9981741890311241    |
| train_1/fw_loss           | 0.0013763794850092381  |
| train_1/mu_grads          | -0.002384721057023853  |
| train_1/mu_grads_std      | 0.1323999110609293     |
| train_1/mu_loss           | 7.710649919734513      |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -7.619966248588389     |
| train_1/q_grads           | 0.013967933342792093   |
| train_1/q_grads_std       | 0.18540380634367465    |
| train_1/q_loss            | 3.715504244904082      |
| train_1/reward            | -1.47845242733747      |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.001904296875         |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.12                   |
| train_1/target_q          | -7.567790865085522     |
------------------------------------------------------
Saving periodic policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_0.pkl ...
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Training epoch 1
Time for epoch 1: 2407.61. Rollout time: 1680.18, Training time: 726.04
Evaluating epoch 1
Data_dir: data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101
------------------------------------------------------
| epoch                     | 1                      |
| policy/steps              | 168945.0               |
| test/episodes             | 50.0                   |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -2.355967034041814     |
| test_1/avg_q              | -7.678152262812859     |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 200.0                  |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -14.437122963576208    |
| train_0/current_q         | -4.765829650814661     |
| train_0/fw_bonus          | -0.9993336230516434    |
| train_0/fw_loss           | 0.0001875842262961669  |
| train_0/mu_grads          | -0.003920362616190687  |
| train_0/mu_grads_std      | 0.2086580254137516     |
| train_0/mu_loss           | 4.785361577107901      |
| train_0/next_q            | -4.829468839615229     |
| train_0/q_grads           | 0.022758836997672914   |
| train_0/q_grads_std       | 0.19035656452178956    |
| train_0/q_loss            | 1.0045178123294325     |
| train_0/reward            | -0.8650885874885716    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.0001220703125        |
| train_0/target_q          | -5.041001406167337     |
| train_1/avg_q             | -8.701941375623651     |
| train_1/current_q         | -6.232301857591476     |
| train_1/fw_bonus          | -0.9985935732722282    |
| train_1/fw_loss           | 0.0012659162079216913  |
| train_1/mu_grads          | -0.0013258333085104824 |
| train_1/mu_grads_std      | 0.1374832347035408     |
| train_1/mu_loss           | 6.383725347639611      |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -6.369125097698314     |
| train_1/q_grads           | 0.009504661639221012   |
| train_1/q_grads_std       | 0.20246944017708302    |
| train_1/q_loss            | 2.4537703867237775     |
| train_1/reward            | -1.4637706205467111    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0017822265625        |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.10333333333333333    |
| train_1/target_q          | -6.225099953393197     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/d4a6886/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.25|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|101/policy_best.pkl ...
Training epoch 2
