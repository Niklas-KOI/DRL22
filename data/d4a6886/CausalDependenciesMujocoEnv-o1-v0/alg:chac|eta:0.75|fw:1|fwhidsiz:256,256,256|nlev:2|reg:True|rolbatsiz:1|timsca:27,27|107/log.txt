Starting process id: 22879
T: 50
alg: chac
algorithm: src.chac
atomic_noise: 0.2
base_logdir: data
batch_size: 1024
bind_core: 0
buffer_size: 500
chac_params: {}
early_stop_data_column: test/success_rate
early_stop_threshold: 100.0
env_name: CausalDependenciesMujocoEnv-o1-v0
eta: 0.75
fw: 1
fw_hidden_size: 256,256,256
fw_lr: 0.001
gamma: 0.98
graph: 1
info: 
make_env: <function prepare_params.<locals>.make_env at 0x7fa3119735f0>
max_try_idx: 199
mu_hidden_size: 64
mu_lr: 0.001
n_episodes: 100
n_levels: 2
n_pre_episodes: 30
n_test_rollouts: 25
n_train_batches: 40
n_train_rollouts: 100
num_threads: 1
q_hidden_size: 64
q_lr: 0.001
random_action_perc: 0.3
regularization: True
render: 0
rollout_batch_size: 1
subgoal_noise: 0.2
subgoal_test_perc: 0.3
time_scales: 27,27
try_start_idx: 100
use_mpi: False
verbose: False

*** Warning ***
You are running src.chac with just a single MPI worker. This will work, but the HER experiments that we report in Plappert et al. (2018, https://arxiv.org/abs/1802.09464) were obtained with --num_cpu 19. This makes a significant difference and if you are looking to reproduce those results, be aware of this. Please also refer to https://github.com/openai/baselines/issues/314 for further details.
****************

dims: action = 2, subgoal = 2, end_goal = 2
subgoal_bounds: symmetric [0.2 0.2], offset [0.922 0.25 ]
Running on CPU ...
Creating a CHAC agent

Hierarchy Level 0 with time scale 27
Actor(
  (fc1): Linear(in_features=35, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
  (fc4): Linear(in_features=64, out_features=2, bias=True)
)
Critic(
  (fc1): Linear(in_features=37, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
  (fc4): Linear(in_features=64, out_features=1, bias=True)
  (mse_loss): MSELoss()
)
ForwardModel(
  (mlp): Sequential(
    (0): Linear(in_features=35, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=256, bias=True)
    (5): ReLU()
    (6): Linear(in_features=256, out_features=33, bias=True)
    (7): Identity()
  )
  (mse_loss): MSELoss()
)

Hierarchy Level 1 with time scale 27
Actor(
  (fc1): Linear(in_features=35, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
  (fc4): Linear(in_features=64, out_features=2, bias=True)
)
Critic(
  (fc1): Linear(in_features=37, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
  (fc4): Linear(in_features=64, out_features=1, bias=True)
  (mse_loss): MSELoss()
)
ForwardModel(
  (mlp): Sequential(
    (0): Linear(in_features=35, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=256, bias=True)
    (5): ReLU()
    (6): Linear(in_features=256, out_features=33, bias=True)
    (7): Identity()
  )
  (mse_loss): MSELoss()
)
Training epoch 0
Time for epoch 0: 880.80. Rollout time: 525.55, Training time: 355.16
Evaluating epoch 0
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 0                      |
| policy/steps              | 72362.0                |
| test/episodes             | 25.0                   |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -0.366290355312202     |
| test_1/avg_q              | -19.507714442855328    |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 100.0                  |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -3.6470469635039193    |
| train_0/current_q         | -0.5241821971144078    |
| train_0/fw_bonus          | -0.9994868099689483    |
| train_0/fw_loss           | 0.00015523875426879386 |
| train_0/mu_grads          | -0.010013231262564658  |
| train_0/mu_grads_std      | 0.17666352093219756    |
| train_0/mu_loss           | 0.507925861695915      |
| train_0/next_q            | -0.5051222650273873    |
| train_0/q_grads           | 0.035654074605554344   |
| train_0/q_grads_std       | 0.17637896873056888    |
| train_0/q_loss            | 0.4452351931453684     |
| train_0/reward            | -0.5703428948876536    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.006884765625         |
| train_0/target_q          | -0.7902676920573818    |
| train_1/avg_q             | -10.005727307610409    |
| train_1/current_q         | -10.406068856594421    |
| train_1/fw_bonus          | -0.9988203018903732    |
| train_1/fw_loss           | 0.0012298330111661926  |
| train_1/mu_grads          | 0.0017176071065478026  |
| train_1/mu_grads_std      | 0.15509661547839643    |
| train_1/mu_loss           | 12.289422576674722     |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -12.861364563521562    |
| train_1/q_grads           | 0.03162774592638016    |
| train_1/q_grads_std       | 0.20399244427680968    |
| train_1/q_loss            | 26.12904302383503      |
| train_1/reward            | -2.0940165999225426    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.004248046875         |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.27925925925925926    |
| train_1/target_q          | -10.294682824557475    |
------------------------------------------------------
Saving periodic policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100/policy_0.pkl ...
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Training epoch 1
Time for epoch 1: 857.76. Rollout time: 607.40, Training time: 250.24
Evaluating epoch 1
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 1                      |
| policy/steps              | 157062.0               |
| test/episodes             | 50.0                   |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -0.889267442589338     |
| test_1/avg_q              | -5.89139577235968      |
| test_1/n_subgoals         | 4386.0                 |
| test_1/subgoal_succ_rate  | 0.8789329685362517     |
| train/episodes            | 200.0                  |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -3.0653814944702553    |
| train_0/current_q         | -2.414968073189393     |
| train_0/fw_bonus          | -0.9995341360569       |
| train_0/fw_loss           | 0.00014122272114036605 |
| train_0/mu_grads          | 0.0010279310285113753  |
| train_0/mu_grads_std      | 0.2074142299592495     |
| train_0/mu_loss           | 2.155611463900317      |
| train_0/next_q            | -2.1543831423398956    |
| train_0/q_grads           | 0.039915687404572964   |
| train_0/q_grads_std       | 0.20152267031371593    |
| train_0/q_loss            | 0.23831557529865255    |
| train_0/reward            | -0.565465808637964     |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.006494140625         |
| train_0/target_q          | -2.369969556298156     |
| train_1/avg_q             | -16.611397580763942    |
| train_1/current_q         | -11.700330648012722    |
| train_1/fw_bonus          | -0.998859803378582     |
| train_1/fw_loss           | 0.001218748727114871   |
| train_1/mu_grads          | 0.000831818401638884   |
| train_1/mu_grads_std      | 0.21761084720492363    |
| train_1/mu_loss           | 14.733410579335645     |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -16.359808835194187    |
| train_1/q_grads           | 0.0424369621090591     |
| train_1/q_grads_std       | 0.2688106790184975     |
| train_1/q_loss            | 24.632493354645913     |
| train_1/reward            | -2.229780870734976     |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.003759765625         |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.09814814814814815    |
| train_1/target_q          | -11.603943555756784    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Training epoch 2
Time for epoch 2: 741.99. Rollout time: 497.33, Training time: 244.56
Evaluating epoch 2
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 2                      |
| policy/steps              | 231810.0               |
| test/episodes             | 75.0                   |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -26.995249439012426    |
| test_1/avg_q              | -18.274715276320432    |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 300.0                  |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -11.495527330212655    |
| train_0/current_q         | -6.890491362125519     |
| train_0/fw_bonus          | -0.9995934754610062    |
| train_0/fw_loss           | 0.00012364426875137723 |
| train_0/mu_grads          | 0.008312623016536236   |
| train_0/mu_grads_std      | 0.2240734100341797     |
| train_0/mu_loss           | 9.545576904643683      |
| train_0/next_q            | -9.488849768534207     |
| train_0/q_grads           | 0.041109271068125966   |
| train_0/q_grads_std       | 0.2276033226400614     |
| train_0/q_loss            | 9.216867913780039      |
| train_0/reward            | -0.5581564262618486    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.01181640625          |
| train_0/target_q          | -7.062092557023865     |
| train_1/avg_q             | -14.42129231796295     |
| train_1/current_q         | -10.856297231000024    |
| train_1/fw_bonus          | -0.9988847106695176    |
| train_1/fw_loss           | 0.0012117637030314653  |
| train_1/mu_grads          | -9.136334674622048e-05 |
| train_1/mu_grads_std      | 0.24631783738732338    |
| train_1/mu_loss           | 14.076268254282473     |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -14.972817123186683    |
| train_1/q_grads           | 0.04601281592622399    |
| train_1/q_grads_std       | 0.3014010667800903     |
| train_1/q_loss            | 19.67951672855194      |
| train_1/reward            | -2.215651352819259     |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.003662109375         |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.24481481481481482    |
| train_1/target_q          | -10.770221241051598    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Training epoch 3
Time for epoch 3: 865.52. Rollout time: 611.84, Training time: 253.54
Evaluating epoch 3
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 3                      |
| policy/steps              | 321908.0               |
| test/episodes             | 100.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -26.99192064079631     |
| test_1/avg_q              | -19.62550428550938     |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 400.0                  |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -26.96378164888084     |
| train_0/current_q         | -8.533511639817117     |
| train_0/fw_bonus          | -0.9996362358331681    |
| train_0/fw_loss           | 0.00011098259419668466 |
| train_0/mu_grads          | 0.008312623016536236   |
| train_0/mu_grads_std      | 0.2240734100341797     |
| train_0/mu_loss           | 10.13511526291509      |
| train_0/next_q            | -10.118016930122497    |
| train_0/q_grads           | 0.041522844042629      |
| train_0/q_grads_std       | 0.23722059279680252    |
| train_0/q_loss            | 4.328426758120225      |
| train_0/reward            | -0.5569952377351001    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.0105712890625        |
| train_0/target_q          | -8.651164912770719     |
| train_1/avg_q             | -20.22947306522539     |
| train_1/current_q         | -11.061372312037284    |
| train_1/fw_bonus          | -0.9992399707436561    |
| train_1/fw_loss           | 0.001112087616638746   |
| train_1/mu_grads          | -0.002441179769812152  |
| train_1/mu_grads_std      | 0.2627885676920414     |
| train_1/mu_loss           | 15.868446555057506     |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -16.51613236250521     |
| train_1/q_grads           | 0.04886105703189969    |
| train_1/q_grads_std       | 0.32819453403353693    |
| train_1/q_loss            | 18.740111987357885     |
| train_1/reward            | -2.289751398574663     |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0030517578125        |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.014814814814814815   |
| train_1/target_q          | -10.984401576363489    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 4
Time for epoch 4: 966.38. Rollout time: 680.39, Training time: 285.89
Evaluating epoch 4
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 4                      |
| policy/steps              | 412619.0               |
| test/episodes             | 125.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -26.999991799067367    |
| test_1/avg_q              | -22.40008552317993     |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 500.0                  |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -26.856126428199065    |
| train_0/current_q         | -8.958608391171994     |
| train_0/fw_bonus          | -0.9996976152062416    |
| train_0/fw_loss           | 9.279806272388669e-05  |
| train_0/mu_grads          | 0.008312620222568512   |
| train_0/mu_grads_std      | 0.2240734100341797     |
| train_0/mu_loss           | 10.152332810244866     |
| train_0/next_q            | -10.10906471794116     |
| train_0/q_grads           | 0.041636760160326955   |
| train_0/q_grads_std       | 0.24958060793578624    |
| train_0/q_loss            | 3.4214861397449448     |
| train_0/reward            | -0.5590796773722104    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.020361328125         |
| train_0/target_q          | -9.146864111033725     |
| train_1/avg_q             | -20.292631644272333    |
| train_1/current_q         | -10.101774551071061    |
| train_1/fw_bonus          | -0.9992702350020408    |
| train_1/fw_loss           | 0.0011035971940145828  |
| train_1/mu_grads          | -0.0056292697787284855 |
| train_1/mu_grads_std      | 0.278087941557169      |
| train_1/mu_loss           | 12.423059343962962     |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -12.91881643607834     |
| train_1/q_grads           | 0.05112901786342263    |
| train_1/q_grads_std       | 0.347099332511425      |
| train_1/q_loss            | 13.016501062178241     |
| train_1/reward            | -2.271228690124917     |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.00322265625          |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.006666666666666667   |
| train_1/target_q          | -10.049181288271768    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 5
Time for epoch 5: 934.58. Rollout time: 659.04, Training time: 275.44
Evaluating epoch 5
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100
-----------------------------------------------------
| epoch                     | 5                     |
| policy/steps              | 503023.0              |
| test/episodes             | 150.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -26.99790862673417    |
| test_1/avg_q              | -17.256976779112165   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 600.0                 |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.98727452657078    |
| train_0/current_q         | -9.386675829593457    |
| train_0/fw_bonus          | -0.9997359663248062   |
| train_0/fw_loss           | 8.143644936353667e-05 |
| train_0/mu_grads          | 0.008312620222568512  |
| train_0/mu_grads_std      | 0.2240734100341797    |
| train_0/mu_loss           | 10.297046782157848    |
| train_0/next_q            | -10.255051774454166   |
| train_0/q_grads           | 0.041354193724691866  |
| train_0/q_grads_std       | 0.25912808179855346   |
| train_0/q_loss            | 2.3476323790041436    |
| train_0/reward            | -0.5606090421199041   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.024951171875        |
| train_0/target_q          | -9.60307548280932     |
| train_1/avg_q             | -20.97163203804426    |
| train_1/current_q         | -9.65510451352408     |
| train_1/fw_bonus          | -0.9992728382349014   |
| train_1/fw_loss           | 0.0011028661101590842 |
| train_1/mu_grads          | -0.008891893969848751 |
| train_1/mu_grads_std      | 0.30358799546957016   |
| train_1/mu_loss           | 10.855535501109973    |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -11.137219418566048   |
| train_1/q_grads           | 0.05370461232960224   |
| train_1/q_grads_std       | 0.3632520601153374    |
| train_1/q_loss            | 7.5479293267061935    |
| train_1/reward            | -2.366425876298672    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0034912109375       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.01037037037037037   |
| train_1/target_q          | -9.593972651315259    |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 6
Time for epoch 6: 1006.50. Rollout time: 718.17, Training time: 288.20
Evaluating epoch 6
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100
-----------------------------------------------------
| epoch                     | 6                     |
| policy/steps              | 593673.0              |
| test/episodes             | 175.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -26.984228337483188   |
| test_1/avg_q              | -25.251094061083737   |
| test_1/n_subgoals         | 678.0                 |
| test_1/subgoal_succ_rate  | 0.004424778761061947  |
| train/episodes            | 700.0                 |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.981880046392053   |
| train_0/current_q         | -9.729607939994244    |
| train_0/fw_bonus          | -0.9997883766889573   |
| train_0/fw_loss           | 6.591208220925182e-05 |
| train_0/mu_grads          | 0.008312635123729706  |
| train_0/mu_grads_std      | 0.2240734100341797    |
| train_0/mu_loss           | 9.992851819253302     |
| train_0/next_q            | -9.962574238575431    |
| train_0/q_grads           | 0.04179519359022379   |
| train_0/q_grads_std       | 0.267492513358593     |
| train_0/q_loss            | 1.2693569802403506    |
| train_0/reward            | -0.5585896739899908   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0377685546875       |
| train_0/target_q          | -9.943991428695053    |
| train_1/avg_q             | -22.67420297474298    |
| train_1/current_q         | -14.640904332023188   |
| train_1/fw_bonus          | -0.9993421554565429   |
| train_1/fw_loss           | 0.0010834195927600376 |
| train_1/mu_grads          | -0.010365161346271634 |
| train_1/mu_grads_std      | 0.305708210170269     |
| train_1/mu_loss           | 23.25610356318205     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -23.66163878883783    |
| train_1/q_grads           | 0.0529061621055007    |
| train_1/q_grads_std       | 0.37294200286269186   |
| train_1/q_loss            | 23.10638334312199     |
| train_1/reward            | -2.3990909080428535   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0027587890625       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.007037037037037037  |
| train_1/target_q          | -14.233322980537007   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 7
Time for epoch 7: 989.28. Rollout time: 703.00, Training time: 286.15
Evaluating epoch 7
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100
-----------------------------------------------------
| epoch                     | 7                     |
| policy/steps              | 684440.0              |
| test/episodes             | 200.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -26.914170632938752   |
| test_1/avg_q              | -19.30965187910726    |
| test_1/n_subgoals         | 679.0                 |
| test_1/subgoal_succ_rate  | 0.005891016200294551  |
| train/episodes            | 800.0                 |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.962772262908853   |
| train_0/current_q         | -9.842099075993033    |
| train_0/fw_bonus          | -0.9998738616704941   |
| train_0/fw_loss           | 4.059248735757137e-05 |
| train_0/mu_grads          | 0.008312635123729706  |
| train_0/mu_grads_std      | 0.2240734100341797    |
| train_0/mu_loss           | 9.891558003309541     |
| train_0/next_q            | -9.866125441127371    |
| train_0/q_grads           | 0.041763694304972884  |
| train_0/q_grads_std       | 0.27025640606880186   |
| train_0/q_loss            | 0.562984640199722     |
| train_0/reward            | -0.56379292866186     |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.1187744140625       |
| train_0/target_q          | -9.969868246707332    |
| train_1/avg_q             | -25.085007428280896   |
| train_1/current_q         | -16.128966735355238   |
| train_1/fw_bonus          | -0.9995547711849213   |
| train_1/fw_loss           | 0.0010237626498565077 |
| train_1/mu_grads          | -0.010629527387209237 |
| train_1/mu_grads_std      | 0.30833085253834724   |
| train_1/mu_loss           | 23.78581364494044     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -25.878566661512316   |
| train_1/q_grads           | 0.05440338207408786   |
| train_1/q_grads_std       | 0.38792392164468764   |
| train_1/q_loss            | 27.012355053137536    |
| train_1/reward            | -2.506700217035541    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0033203125          |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.005555555555555556  |
| train_1/target_q          | -15.24427729789123    |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 8
Time for epoch 8: 1046.76. Rollout time: 744.23, Training time: 302.40
Evaluating epoch 8
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100
-----------------------------------------------------
| epoch                     | 8                     |
| policy/steps              | 775344.0              |
| test/episodes             | 225.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -18.99112264502297    |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 900.0                 |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.972039091311103   |
| train_0/current_q         | -9.762120921242271    |
| train_0/fw_bonus          | -0.9998819097876549   |
| train_0/fw_loss           | 3.82058477953251e-05  |
| train_0/mu_grads          | 0.008312635123729706  |
| train_0/mu_grads_std      | 0.2240734100341797    |
| train_0/mu_loss           | 9.808594891611438     |
| train_0/next_q            | -9.801049561956443    |
| train_0/q_grads           | 0.04158619716763497   |
| train_0/q_grads_std       | 0.27571103647351264   |
| train_0/q_loss            | 0.9859681678381804    |
| train_0/reward            | -0.5637217007973959   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.1555419921875       |
| train_0/target_q          | -9.994077506725883    |
| train_1/avg_q             | -21.460946474547885   |
| train_1/current_q         | -16.039472767916667   |
| train_1/fw_bonus          | -1.0001063644886017   |
| train_1/fw_loss           | 0.0008690038084750995 |
| train_1/mu_grads          | -0.012140181986615062 |
| train_1/mu_grads_std      | 0.3164219081401825    |
| train_1/mu_loss           | 23.893581871099542    |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -24.448448462690614   |
| train_1/q_grads           | 0.055352830607444045  |
| train_1/q_grads_std       | 0.40194698944687846   |
| train_1/q_loss            | 29.121901794071892    |
| train_1/reward            | -2.503908426783892    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.003173828125        |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.003703703703703704  |
| train_1/target_q          | -14.171047586868468   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 9
Time for epoch 9: 930.13. Rollout time: 654.22, Training time: 275.77
Evaluating epoch 9
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100
-----------------------------------------------------
| epoch                     | 9                     |
| policy/steps              | 866469.0              |
| test/episodes             | 250.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -19.73990736154897    |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 1000.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -27.0                 |
| train_0/current_q         | -10.02734093403701    |
| train_0/fw_bonus          | -0.9999320536851883   |
| train_0/fw_loss           | 2.335571541607351e-05 |
| train_0/mu_grads          | 0.008312635123729706  |
| train_0/mu_grads_std      | 0.2240734100341797    |
| train_0/mu_loss           | 10.132441486751683    |
| train_0/next_q            | -10.077093738367534   |
| train_0/q_grads           | 0.041470817755907774  |
| train_0/q_grads_std       | 0.2790204264223576    |
| train_0/q_loss            | 0.7914747999896885    |
| train_0/reward            | -0.5625623593456112   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.2638671875          |
| train_0/target_q          | -10.243594176896513   |
| train_1/avg_q             | -21.030207749365015   |
| train_1/current_q         | -16.006241595559466   |
| train_1/fw_bonus          | -1.0003315955400467   |
| train_1/fw_loss           | 0.0008058118241024203 |
| train_1/mu_grads          | -0.017128831380978225 |
| train_1/mu_grads_std      | 0.32593567818403246   |
| train_1/mu_loss           | 24.14988819339721     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -24.15387440642912    |
| train_1/q_grads           | 0.057307391241192815  |
| train_1/q_grads_std       | 0.42061205133795737   |
| train_1/q_loss            | 30.795708399266097    |
| train_1/reward            | -2.474612925526526    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0031982421875       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -14.010020799860342   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 10
Time for epoch 10: 880.93. Rollout time: 626.73, Training time: 254.09
Evaluating epoch 10
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 10                     |
| policy/steps              | 957594.0               |
| test/episodes             | 275.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -27.0                  |
| test_1/avg_q              | -20.840275157192142    |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 1100.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -27.0                  |
| train_0/current_q         | -9.740065692662677     |
| train_0/fw_bonus          | -0.9999321326613426    |
| train_0/fw_loss           | 2.3329716509579156e-05 |
| train_0/mu_grads          | 0.008312635123729706   |
| train_0/mu_grads_std      | 0.2240734100341797     |
| train_0/mu_loss           | 9.8138814542419        |
| train_0/next_q            | -9.783808102305816     |
| train_0/q_grads           | 0.04180958392098546    |
| train_0/q_grads_std       | 0.28267365172505377    |
| train_0/q_loss            | 0.8576961261674526     |
| train_0/reward            | -0.564631473056943     |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.2657958984375        |
| train_0/target_q          | -10.030396410208827    |
| train_1/avg_q             | -21.06135900739593     |
| train_1/current_q         | -16.152965129053605    |
| train_1/fw_bonus          | -1.0007651060819627    |
| train_1/fw_loss           | 0.0006841786249424331  |
| train_1/mu_grads          | -0.01647981470450759   |
| train_1/mu_grads_std      | 0.3336812451481819     |
| train_1/mu_loss           | 24.41241818027247      |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -24.35757487859409     |
| train_1/q_grads           | 0.060009945463389155   |
| train_1/q_grads_std       | 0.44555619433522226    |
| train_1/q_loss            | 26.301883009025875     |
| train_1/reward            | -2.4907796578769195    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0024169921875        |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -14.280451019931656    |
------------------------------------------------------
Saving periodic policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100/policy_10.pkl ...
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 11
Time for epoch 11: 906.69. Rollout time: 643.04, Training time: 263.54
Evaluating epoch 11
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 11                     |
| policy/steps              | 1048719.0              |
| test/episodes             | 300.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -27.0                  |
| test_1/avg_q              | -21.129566284538658    |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 1200.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -27.0                  |
| train_0/current_q         | -9.81192243250552      |
| train_0/fw_bonus          | -0.9999116718769073    |
| train_0/fw_loss           | 2.9391797670541565e-05 |
| train_0/mu_grads          | 0.008312635123729706   |
| train_0/mu_grads_std      | 0.2240734100341797     |
| train_0/mu_loss           | 9.835073456263672      |
| train_0/next_q            | -9.82987029235297      |
| train_0/q_grads           | 0.04193909326568246    |
| train_0/q_grads_std       | 0.28609717860817907    |
| train_0/q_loss            | 0.720586452579632      |
| train_0/reward            | -0.5639122451291769    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.196044921875         |
| train_0/target_q          | -10.09171893199316     |
| train_1/avg_q             | -21.108611834713134    |
| train_1/current_q         | -16.203377119108602    |
| train_1/fw_bonus          | -1.0007440984249114    |
| train_1/fw_loss           | 0.0006900735897943377  |
| train_1/mu_grads          | -0.01822987771593034   |
| train_1/mu_grads_std      | 0.3380628764629364     |
| train_1/mu_loss           | 24.064678166313236     |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -24.362019507632034    |
| train_1/q_grads           | 0.06237111808732152    |
| train_1/q_grads_std       | 0.47326501831412315    |
| train_1/q_loss            | 26.712604671633063     |
| train_1/reward            | -2.5092680551497324    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.00283203125          |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -14.442223643559922    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 12
Time for epoch 12: 902.71. Rollout time: 645.01, Training time: 257.57
Evaluating epoch 12
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 12                     |
| policy/steps              | 1139844.0              |
| test/episodes             | 325.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -26.951192700340258    |
| test_1/avg_q              | -20.12554825878594     |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 1300.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -26.97763398249969     |
| train_0/current_q         | -9.659228486474863     |
| train_0/fw_bonus          | -0.9999166905879975    |
| train_0/fw_loss           | 2.7906016771339637e-05 |
| train_0/mu_grads          | 0.008312635123729706   |
| train_0/mu_grads_std      | 0.2240734100341797     |
| train_0/mu_loss           | 9.70851443924752       |
| train_0/next_q            | -9.696580413658335     |
| train_0/q_grads           | 0.041682853177189824   |
| train_0/q_grads_std       | 0.2883540980517864     |
| train_0/q_loss            | 0.7624556208186768     |
| train_0/reward            | -0.5636205306844204    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.2477783203125        |
| train_0/target_q          | -9.952810396155513     |
| train_1/avg_q             | -22.70422446530999     |
| train_1/current_q         | -16.24582542641563     |
| train_1/fw_bonus          | -1.0010297447443008    |
| train_1/fw_loss           | 0.0006099301564972848  |
| train_1/mu_grads          | -0.018938867887482048  |
| train_1/mu_grads_std      | 0.34165695160627363    |
| train_1/mu_loss           | 24.205671907415        |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -24.232320265896956    |
| train_1/q_grads           | 0.06547341048717499    |
| train_1/q_grads_std       | 0.4981298170983791     |
| train_1/q_loss            | 25.96910711327875      |
| train_1/reward            | -2.4441172987415483    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0024658203125        |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -14.393829619319405    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 13
Time for epoch 13: 965.76. Rollout time: 684.63, Training time: 281.02
Evaluating epoch 13
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100
-----------------------------------------------------
| epoch                     | 13                    |
| policy/steps              | 1230556.0             |
| test/episodes             | 350.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -26.908173014889165   |
| test_1/avg_q              | -22.48756239971686    |
| test_1/n_subgoals         | 685.0                 |
| test_1/subgoal_succ_rate  | 0.014598540145985401  |
| train/episodes            | 1400.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.924452866698115   |
| train_0/current_q         | -9.593749417565084    |
| train_0/fw_bonus          | -0.9999504759907722   |
| train_0/fw_loss           | 1.789866812487162e-05 |
| train_0/mu_grads          | 0.008312635123729706  |
| train_0/mu_grads_std      | 0.2240734100341797    |
| train_0/mu_loss           | 9.61766607534712      |
| train_0/next_q            | -9.619979955719078    |
| train_0/q_grads           | 0.04196013044565916   |
| train_0/q_grads_std       | 0.2914804257452488    |
| train_0/q_loss            | 0.7519171066769026    |
| train_0/reward            | -0.5667783909993886   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.302978515625        |
| train_0/target_q          | -9.906820031308197    |
| train_1/avg_q             | -21.16502803155974    |
| train_1/current_q         | -15.876648578494928   |
| train_1/fw_bonus          | -1.0012607395648956   |
| train_1/fw_loss           | 0.000545118563604774  |
| train_1/mu_grads          | -0.02028222749941051  |
| train_1/mu_grads_std      | 0.3479878306388855    |
| train_1/mu_loss           | 24.84278209280866     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -24.81457831566044    |
| train_1/q_grads           | 0.06894807312637567   |
| train_1/q_grads_std       | 0.5216192543506623    |
| train_1/q_loss            | 21.665907262309588    |
| train_1/reward            | -2.511081786533032    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0022705078125       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.007037037037037037  |
| train_1/target_q          | -14.690817268496838   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 14
Time for epoch 14: 876.40. Rollout time: 623.61, Training time: 252.65
Evaluating epoch 14
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100
-----------------------------------------------------
| epoch                     | 14                    |
| policy/steps              | 1319873.0             |
| test/episodes             | 375.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -26.972900134720607   |
| test_1/avg_q              | -18.089571548844095   |
| test_1/n_subgoals         | 683.0                 |
| test_1/subgoal_succ_rate  | 0.01171303074670571   |
| train/episodes            | 1500.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.748024868491356   |
| train_0/current_q         | -9.938529228698329    |
| train_0/fw_bonus          | -0.9999559819698334   |
| train_0/fw_loss           | 1.626935677450092e-05 |
| train_0/mu_grads          | 0.008312635123729706  |
| train_0/mu_grads_std      | 0.2240734100341797    |
| train_0/mu_loss           | 10.00555505728924     |
| train_0/next_q            | -9.964553842592242    |
| train_0/q_grads           | 0.04157270723953843   |
| train_0/q_grads_std       | 0.29510039985179903   |
| train_0/q_loss            | 0.7023381792555929    |
| train_0/reward            | -0.564381084312845    |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.3501708984375       |
| train_0/target_q          | -10.159339181645418   |
| train_1/avg_q             | -22.642545934800935   |
| train_1/current_q         | -16.212451047184388   |
| train_1/fw_bonus          | -1.0012389481067658   |
| train_1/fw_loss           | 0.0005512330310011749 |
| train_1/mu_grads          | -0.022102658916264773 |
| train_1/mu_grads_std      | 0.351860498636961     |
| train_1/mu_loss           | 23.57447978162515     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -24.12598177363734    |
| train_1/q_grads           | 0.07545939758419991   |
| train_1/q_grads_std       | 0.5426632642745972    |
| train_1/q_loss            | 19.92717207124727     |
| train_1/reward            | -2.490422042467253    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.00244140625         |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.027407407407407408  |
| train_1/target_q          | -14.392785951358121   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 15
Time for epoch 15: 881.51. Rollout time: 623.38, Training time: 258.01
Evaluating epoch 15
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100
-----------------------------------------------------
| epoch                     | 15                    |
| policy/steps              | 1410052.0             |
| test/episodes             | 400.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -26.965587115631468   |
| test_1/avg_q              | -18.97810996599398    |
| test_1/n_subgoals         | 679.0                 |
| test_1/subgoal_succ_rate  | 0.005891016200294551  |
| train/episodes            | 1600.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.95822881231455    |
| train_0/current_q         | -9.966692984634392    |
| train_0/fw_bonus          | -0.9999504387378693   |
| train_0/fw_loss           | 1.790912288015534e-05 |
| train_0/mu_grads          | 0.008312635123729706  |
| train_0/mu_grads_std      | 0.2240734100341797    |
| train_0/mu_loss           | 10.029136273302907    |
| train_0/next_q            | -10.017104823447484   |
| train_0/q_grads           | 0.041623641084879634  |
| train_0/q_grads_std       | 0.2975428283214569    |
| train_0/q_loss            | 0.8027240379833358    |
| train_0/reward            | -0.5665769659139187   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.3529296875          |
| train_0/target_q          | -10.181366875969974   |
| train_1/avg_q             | -20.865483343634214   |
| train_1/current_q         | -15.929519153182843   |
| train_1/fw_bonus          | -1.001389703154564    |
| train_1/fw_loss           | 0.0005089351849164814 |
| train_1/mu_grads          | -0.023853917978703974 |
| train_1/mu_grads_std      | 0.3558399140834808    |
| train_1/mu_loss           | 24.148584349385978    |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -24.203786258245067   |
| train_1/q_grads           | 0.08147053327411413   |
| train_1/q_grads_std       | 0.566509734094143     |
| train_1/q_loss            | 19.16059790813564     |
| train_1/reward            | -2.4962824872076452   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.001611328125        |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.014444444444444444  |
| train_1/target_q          | -14.207145538613497   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 16
Time for epoch 16: 880.31. Rollout time: 620.80, Training time: 259.41
Evaluating epoch 16
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 16                     |
| policy/steps              | 1500587.0              |
| test/episodes             | 425.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -26.969097009599615    |
| test_1/avg_q              | -20.875863420035923    |
| test_1/n_subgoals         | 678.0                  |
| test_1/subgoal_succ_rate  | 0.004424778761061947   |
| train/episodes            | 1700.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -26.953807788120503    |
| train_0/current_q         | -9.878219558673656     |
| train_0/fw_bonus          | -0.9999504685401917    |
| train_0/fw_loss           | 1.7903418847708962e-05 |
| train_0/mu_grads          | 0.008312635123729706   |
| train_0/mu_grads_std      | 0.2240734100341797     |
| train_0/mu_loss           | 9.967783855654652      |
| train_0/next_q            | -9.916505163382116     |
| train_0/q_grads           | 0.041202061157673595   |
| train_0/q_grads_std       | 0.29813820123672485    |
| train_0/q_loss            | 0.7459299875885316     |
| train_0/reward            | -0.5622923029757658    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.3141845703125        |
| train_0/target_q          | -10.06544734604962     |
| train_1/avg_q             | -20.183076706237205    |
| train_1/current_q         | -15.818265611999669    |
| train_1/fw_bonus          | -1.001427337527275     |
| train_1/fw_loss           | 0.000498378377960762   |
| train_1/mu_grads          | -0.02316760318353772   |
| train_1/mu_grads_std      | 0.36042653024196625    |
| train_1/mu_loss           | 24.24035894019199      |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -24.286116651008363    |
| train_1/q_grads           | 0.0839672276750207     |
| train_1/q_grads_std       | 0.5880881786346436     |
| train_1/q_loss            | 21.794394094295114     |
| train_1/reward            | -2.4671781233770163    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0016845703125        |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.00962962962962963    |
| train_1/target_q          | -14.155917778177749    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 17
Time for epoch 17: 877.32. Rollout time: 620.85, Training time: 256.32
Evaluating epoch 17
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 17                     |
| policy/steps              | 1591451.0              |
| test/episodes             | 450.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -26.96766057208486     |
| test_1/avg_q              | -26.967275293238416    |
| test_1/n_subgoals         | 678.0                  |
| test_1/subgoal_succ_rate  | 0.004424778761061947   |
| train/episodes            | 1800.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -26.96173500585824     |
| train_0/current_q         | -9.842940193418624     |
| train_0/fw_bonus          | -0.9999504521489143    |
| train_0/fw_loss           | 1.7907353662849345e-05 |
| train_0/mu_grads          | 0.008312635123729706   |
| train_0/mu_grads_std      | 0.2240734100341797     |
| train_0/mu_loss           | 9.856077124772632      |
| train_0/next_q            | -9.854242910012426     |
| train_0/q_grads           | 0.04091560365632176    |
| train_0/q_grads_std       | 0.3003105320036411     |
| train_0/q_loss            | 0.8278632508800123     |
| train_0/reward            | -0.5668882468191441    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.297802734375         |
| train_0/target_q          | -10.082064751135078    |
| train_1/avg_q             | -20.959202619037256    |
| train_1/current_q         | -16.671299515992935    |
| train_1/fw_bonus          | -1.001409563422203     |
| train_1/fw_loss           | 0.0005033632842241786  |
| train_1/mu_grads          | -0.020026916544884442  |
| train_1/mu_grads_std      | 0.36582549214363097    |
| train_1/mu_loss           | 26.998329337516424     |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -26.996745494555434    |
| train_1/q_grads           | 0.08925167117267847    |
| train_1/q_grads_std       | 0.604644575715065      |
| train_1/q_loss            | 13.099779879537142     |
| train_1/reward            | -2.478617772634607     |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.002001953125         |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.004814814814814815   |
| train_1/target_q          | -16.223728711547643    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 18
Time for epoch 18: 875.74. Rollout time: 614.39, Training time: 261.24
Evaluating epoch 18
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 18                     |
| policy/steps              | 1681618.0              |
| test/episodes             | 475.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -26.89795936923446     |
| test_1/avg_q              | -24.43916209370553     |
| test_1/n_subgoals         | 683.0                  |
| test_1/subgoal_succ_rate  | 0.01171303074670571    |
| train/episodes            | 1900.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -26.899406575587683    |
| train_0/current_q         | -10.01144878443443     |
| train_0/fw_bonus          | -0.9999524250626564    |
| train_0/fw_loss           | 1.7319148628303083e-05 |
| train_0/mu_grads          | 0.008312635123729706   |
| train_0/mu_grads_std      | 0.2240734100341797     |
| train_0/mu_loss           | 10.088355990754625     |
| train_0/next_q            | -10.03422096807068     |
| train_0/q_grads           | 0.0405540531501174     |
| train_0/q_grads_std       | 0.3019677855074406     |
| train_0/q_loss            | 0.9002730645006111     |
| train_0/reward            | -0.5649661359526362    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.2909423828125        |
| train_0/target_q          | -10.181896858829209    |
| train_1/avg_q             | -22.900228862676034    |
| train_1/current_q         | -15.192615829980593    |
| train_1/fw_bonus          | -1.001689425110817     |
| train_1/fw_loss           | 0.00042484307050472125 |
| train_1/mu_grads          | -0.020947383437305688  |
| train_1/mu_grads_std      | 0.3712139390408993     |
| train_1/mu_loss           | 24.69150659618767      |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -24.554725376259306    |
| train_1/q_grads           | 0.09362968597561121    |
| train_1/q_grads_std       | 0.6224309340119362     |
| train_1/q_loss            | 18.99919186465012      |
| train_1/reward            | -2.4867443780713074    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0015869140625        |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.015925925925925927   |
| train_1/target_q          | -14.54814433259286     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 19
Time for epoch 19: 879.53. Rollout time: 612.39, Training time: 267.01
Evaluating epoch 19
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 19                     |
| policy/steps              | 1771713.0              |
| test/episodes             | 500.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -27.0                  |
| test_1/avg_q              | -21.772080508351426    |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 2000.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -26.90181044453481     |
| train_0/current_q         | -10.040243370012453    |
| train_0/fw_bonus          | -0.9999539792537689    |
| train_0/fw_loss           | 1.6857370610523503e-05 |
| train_0/mu_grads          | 0.008312635123729706   |
| train_0/mu_grads_std      | 0.2240734100341797     |
| train_0/mu_loss           | 10.110968608724278     |
| train_0/next_q            | -10.068530887249219    |
| train_0/q_grads           | 0.04037334779277444    |
| train_0/q_grads_std       | 0.3036968052387238     |
| train_0/q_loss            | 0.7267890422878592     |
| train_0/reward            | -0.5647101779108198    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.319091796875         |
| train_0/target_q          | -10.20457708769721     |
| train_1/avg_q             | -22.581209122579928    |
| train_1/current_q         | -14.5603976068353      |
| train_1/fw_bonus          | -1.0015301674604415    |
| train_1/fw_loss           | 0.0004695269657531753  |
| train_1/mu_grads          | -0.021651604445651174  |
| train_1/mu_grads_std      | 0.37315711230039594    |
| train_1/mu_loss           | 24.40503466473647      |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -24.45017263636309     |
| train_1/q_grads           | 0.09769946057349443    |
| train_1/q_grads_std       | 0.6414936289191246     |
| train_1/q_loss            | 10.57902309872364      |
| train_1/reward            | -2.4584147663252223    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.001904296875         |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.017037037037037038   |
| train_1/target_q          | -14.417475744558136    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 20
Time for epoch 20: 738.97. Rollout time: 504.70, Training time: 234.17
Evaluating epoch 20
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 20                     |
| policy/steps              | 1862838.0              |
| test/episodes             | 525.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -27.0                  |
| test_1/avg_q              | -21.480228746514783    |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 2100.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -27.0                  |
| train_0/current_q         | -10.030266238837802    |
| train_0/fw_bonus          | -0.999956738948822     |
| train_0/fw_loss           | 1.604623978437303e-05  |
| train_0/mu_grads          | 0.008312635123729706   |
| train_0/mu_grads_std      | 0.2240734100341797     |
| train_0/mu_loss           | 10.084846657277003     |
| train_0/next_q            | -10.053301709905025    |
| train_0/q_grads           | 0.040577421244233844   |
| train_0/q_grads_std       | 0.3049233675003052     |
| train_0/q_loss            | 0.6459180961625901     |
| train_0/reward            | -0.5655531476637407    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.3460205078125        |
| train_0/target_q          | -10.181160517557775    |
| train_1/avg_q             | -21.320935717625723    |
| train_1/current_q         | -14.246173191514714    |
| train_1/fw_bonus          | -1.001728704571724     |
| train_1/fw_loss           | 0.00041381923656444994 |
| train_1/mu_grads          | -0.022475123638287187  |
| train_1/mu_grads_std      | 0.3736809968948364     |
| train_1/mu_loss           | 24.51836623487123      |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -24.503417036561295    |
| train_1/q_grads           | 0.10238565485924482    |
| train_1/q_grads_std       | 0.660114187002182      |
| train_1/q_loss            | 10.85220173566548      |
| train_1/reward            | -2.5212512714737385    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.00185546875          |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -14.312418418258536    |
------------------------------------------------------
Saving periodic policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100/policy_20.pkl ...
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 21
Time for epoch 21: 752.44. Rollout time: 509.29, Training time: 243.08
Evaluating epoch 21
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100
-----------------------------------------------------
| epoch                     | 21                    |
| policy/steps              | 1953775.0             |
| test/episodes             | 550.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -26.985654813773476   |
| test_1/avg_q              | -21.01682231621538    |
| test_1/n_subgoals         | 677.0                 |
| test_1/subgoal_succ_rate  | 0.0029542097488921715 |
| train/episodes            | 2200.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.9897459733664     |
| train_0/current_q         | -10.032638038516044   |
| train_0/fw_bonus          | -0.9999625697731972   |
| train_0/fw_loss           | 1.431581354154332e-05 |
| train_0/mu_grads          | 0.008312635123729706  |
| train_0/mu_grads_std      | 0.2240734100341797    |
| train_0/mu_loss           | 10.074650117014226    |
| train_0/next_q            | -10.059862318501322   |
| train_0/q_grads           | 0.039989877678453925  |
| train_0/q_grads_std       | 0.30693734213709833   |
| train_0/q_loss            | 0.847900901755029     |
| train_0/reward            | -0.5631925477257027   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.337939453125        |
| train_0/target_q          | -10.22968409302217    |
| train_1/avg_q             | -21.382460611343713   |
| train_1/current_q         | -13.982359659795211   |
| train_1/fw_bonus          | -1.0017847537994384   |
| train_1/fw_loss           | 0.0003980996611062437 |
| train_1/mu_grads          | -0.023293750267475842 |
| train_1/mu_grads_std      | 0.3749395482242107    |
| train_1/mu_loss           | 24.6470017385188      |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -24.294080594786827   |
| train_1/q_grads           | 0.10733307506889105   |
| train_1/q_grads_std       | 0.6793434351682663    |
| train_1/q_loss            | 8.62363255147846      |
| train_1/reward            | -2.4937628353971375   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0019775390625       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.002962962962962963  |
| train_1/target_q          | -14.146601090131043   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 22
Time for epoch 22: 883.92. Rollout time: 632.22, Training time: 251.62
Evaluating epoch 22
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 22                     |
| policy/steps              | 2044492.0              |
| test/episodes             | 575.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -26.930692206305885    |
| test_1/avg_q              | -20.70539720050697     |
| test_1/n_subgoals         | 678.0                  |
| test_1/subgoal_succ_rate  | 0.004424778761061947   |
| train/episodes            | 2300.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -26.94335804716035     |
| train_0/current_q         | -10.021978158733663    |
| train_0/fw_bonus          | -0.9999568358063697    |
| train_0/fw_loss           | 1.6013500112421752e-05 |
| train_0/mu_grads          | 0.008312635123729706   |
| train_0/mu_grads_std      | 0.2240734100341797     |
| train_0/mu_loss           | 10.05781182944836      |
| train_0/next_q            | -10.04263734308492     |
| train_0/q_grads           | 0.0397768079303205     |
| train_0/q_grads_std       | 0.3087127566337585     |
| train_0/q_loss            | 0.7331612369084585     |
| train_0/reward            | -0.5647841323512693    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.316259765625         |
| train_0/target_q          | -10.193955863527547    |
| train_1/avg_q             | -20.767505735873353    |
| train_1/current_q         | -14.005362772730958    |
| train_1/fw_bonus          | -1.0017220377922058    |
| train_1/fw_loss           | 0.0004156903705734294  |
| train_1/mu_grads          | -0.02402808954939246   |
| train_1/mu_grads_std      | 0.37522119507193563    |
| train_1/mu_loss           | 24.416474197241183     |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -24.29369871400167     |
| train_1/q_grads           | 0.10963076613843441    |
| train_1/q_grads_std       | 0.6921402797102928     |
| train_1/q_loss            | 8.341941064730566      |
| train_1/reward            | -2.418582367163617     |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0018798828125        |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.006666666666666667   |
| train_1/target_q          | -14.163124914394064    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 23
Time for epoch 23: 742.62. Rollout time: 502.97, Training time: 239.58
Evaluating epoch 23
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 23                     |
| policy/steps              | 2135567.0              |
| test/episodes             | 600.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -26.999977764059388    |
| test_1/avg_q              | -19.901367787087587    |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 2400.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -26.972271778448803    |
| train_0/current_q         | -10.09522117966252     |
| train_0/fw_bonus          | -0.9999730363488197    |
| train_0/fw_loss           | 1.1220339297324245e-05 |
| train_0/mu_grads          | 0.008312635123729706   |
| train_0/mu_grads_std      | 0.2240734100341797     |
| train_0/mu_loss           | 10.127884286081828     |
| train_0/next_q            | -10.105100761470766    |
| train_0/q_grads           | 0.03949587559327483    |
| train_0/q_grads_std       | 0.30998462736606597    |
| train_0/q_loss            | 0.7421273573974843     |
| train_0/reward            | -0.5689629825144948    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.3810546875           |
| train_0/target_q          | -10.258503915051719    |
| train_1/avg_q             | -20.9139907573382      |
| train_1/current_q         | -13.24503940058338     |
| train_1/fw_bonus          | -1.001630488038063     |
| train_1/fw_loss           | 0.00044138487210148013 |
| train_1/mu_grads          | -0.024166527576744555  |
| train_1/mu_grads_std      | 0.37657655999064443    |
| train_1/mu_loss           | 24.051161764414964     |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -23.27567444564371     |
| train_1/q_grads           | 0.1106904311105609     |
| train_1/q_grads_std       | 0.705453471839428      |
| train_1/q_loss            | 5.545886960861404      |
| train_1/reward            | -2.5203661139330507    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0022216796875        |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0011111111111111111  |
| train_1/target_q          | -13.376966120931064    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 24
Time for epoch 24: 744.60. Rollout time: 501.69, Training time: 242.84
Evaluating epoch 24
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 24                     |
| policy/steps              | 2226692.0              |
| test/episodes             | 625.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -26.999685416021883    |
| test_1/avg_q              | -24.451591829151315    |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 2500.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -26.99975352610815     |
| train_0/current_q         | -9.98164132992451      |
| train_0/fw_bonus          | -0.9999554112553597    |
| train_0/fw_loss           | 1.6439094997622307e-05 |
| train_0/mu_grads          | 0.008312635123729706   |
| train_0/mu_grads_std      | 0.2240734100341797     |
| train_0/mu_loss           | 10.004490262319269     |
| train_0/next_q            | -9.990918701928186     |
| train_0/q_grads           | 0.038979547936469315   |
| train_0/q_grads_std       | 0.3113429419696331     |
| train_0/q_loss            | 0.7309429290233285     |
| train_0/reward            | -0.5651682947023801    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.3513671875           |
| train_0/target_q          | -10.184103794378615    |
| train_1/avg_q             | -22.694214903493407    |
| train_1/current_q         | -16.38271123910961     |
| train_1/fw_bonus          | -1.0018228769302369    |
| train_1/fw_loss           | 0.00038740254240110517 |
| train_1/mu_grads          | -0.02378671090118587   |
| train_1/mu_grads_std      | 0.37734796926379205    |
| train_1/mu_loss           | 25.921886366047126     |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -26.654504160993234    |
| train_1/q_grads           | 0.1124177223071456     |
| train_1/q_grads_std       | 0.7166292607784271     |
| train_1/q_loss            | 8.785110541915449      |
| train_1/reward            | -2.490018705865077     |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.001513671875         |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -15.935675782844914    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 25
Time for epoch 25: 742.57. Rollout time: 500.18, Training time: 242.30
Evaluating epoch 25
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 25                     |
| policy/steps              | 2317817.0              |
| test/episodes             | 650.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -26.998585704248562    |
| test_1/avg_q              | -20.17469860289928     |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 2600.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -26.99967594775098     |
| train_0/current_q         | -9.743929915464058     |
| train_0/fw_bonus          | -0.9999596044421196    |
| train_0/fw_loss           | 1.5193294427717773e-05 |
| train_0/mu_grads          | 0.008312635123729706   |
| train_0/mu_grads_std      | 0.2240734100341797     |
| train_0/mu_loss           | 9.790083684416976      |
| train_0/next_q            | -9.761321055368139     |
| train_0/q_grads           | 0.03849893184378743    |
| train_0/q_grads_std       | 0.312494707852602      |
| train_0/q_loss            | 0.6294262812558735     |
| train_0/reward            | -0.5621297953708563    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.3571533203125        |
| train_0/target_q          | -9.987396987919352     |
| train_1/avg_q             | -23.64598235778319     |
| train_1/current_q         | -13.490277695401304    |
| train_1/fw_bonus          | -1.0018704622983932    |
| train_1/fw_loss           | 0.0003740493935765699  |
| train_1/mu_grads          | -0.024284799955785273  |
| train_1/mu_grads_std      | 0.37752691060304644    |
| train_1/mu_loss           | 24.377535500091096     |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -23.67282765303638     |
| train_1/q_grads           | 0.10998600050806999    |
| train_1/q_grads_std       | 0.7254848703742027     |
| train_1/q_loss            | 6.974870469503413      |
| train_1/reward            | -2.532547694306049     |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.001416015625         |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -13.616908404812056    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 26
Time for epoch 26: 748.20. Rollout time: 503.59, Training time: 244.53
Evaluating epoch 26
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 26                     |
| policy/steps              | 2408900.0              |
| test/episodes             | 675.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -26.998330780009372    |
| test_1/avg_q              | -19.56732347179159     |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 2700.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -26.994199933304678    |
| train_0/current_q         | -9.860824310704885     |
| train_0/fw_bonus          | -0.9999679863452912    |
| train_0/fw_loss           | 1.2714410684111498e-05 |
| train_0/mu_grads          | 0.008312635123729706   |
| train_0/mu_grads_std      | 0.2240734100341797     |
| train_0/mu_loss           | 9.881758630549111      |
| train_0/next_q            | -9.870503362654746     |
| train_0/q_grads           | 0.037881067115813495   |
| train_0/q_grads_std       | 0.31359151005744934    |
| train_0/q_loss            | 0.7616838757140406     |
| train_0/reward            | -0.5654827174876118    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.3750244140625        |
| train_0/target_q          | -10.083710182484829    |
| train_1/avg_q             | -21.353144304733604    |
| train_1/current_q         | -13.992136898986507    |
| train_1/fw_bonus          | -1.0018958687782287    |
| train_1/fw_loss           | 0.0003669209239888005  |
| train_1/mu_grads          | -0.024363881256431342  |
| train_1/mu_grads_std      | 0.37753456830978394    |
| train_1/mu_loss           | 24.477900003837142     |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -24.356815656505802    |
| train_1/q_grads           | 0.11048518233001232    |
| train_1/q_grads_std       | 0.7322485372424126     |
| train_1/q_loss            | 6.408496717619667      |
| train_1/reward            | -2.440580168022643     |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0017333984375        |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0007407407407407407  |
| train_1/target_q          | -14.130813265000176    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 27
Time for epoch 27: 745.82. Rollout time: 504.64, Training time: 241.10
Evaluating epoch 27
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100
-----------------------------------------------------
| epoch                     | 27                    |
| policy/steps              | 2500025.0             |
| test/episodes             | 700.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -26.999977123316082   |
| test_1/avg_q              | -26.737005917883707   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 2800.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.99887943305772    |
| train_0/current_q         | -9.895288771541052    |
| train_0/fw_bonus          | -0.9999579086899757   |
| train_0/fw_loss           | 1.570081200270579e-05 |
| train_0/mu_grads          | 0.008312635123729706  |
| train_0/mu_grads_std      | 0.2240734100341797    |
| train_0/mu_loss           | 9.934223912020878     |
| train_0/next_q            | -9.905494910601382    |
| train_0/q_grads           | 0.03792643323540688   |
| train_0/q_grads_std       | 0.31462177708745004   |
| train_0/q_loss            | 0.5858549687490371    |
| train_0/reward            | -0.5648942597334099   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.3150146484375       |
| train_0/target_q          | -10.108490431539352   |
| train_1/avg_q             | -20.62185816132967    |
| train_1/current_q         | -16.15347363450024    |
| train_1/fw_bonus          | -1.0019121676683427   |
| train_1/fw_loss           | 0.0003623456330387853 |
| train_1/mu_grads          | -0.0243669040966779   |
| train_1/mu_grads_std      | 0.3775356709957123    |
| train_1/mu_loss           | 26.91862308865177     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -26.97024425014349    |
| train_1/q_grads           | 0.11097952127456664   |
| train_1/q_grads_std       | 0.7397395119071006    |
| train_1/q_loss            | 12.610480789855842    |
| train_1/reward            | -2.4685261398528384   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.001708984375        |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -15.902213424453214   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 28
Time for epoch 28: 747.81. Rollout time: 506.16, Training time: 241.59
Evaluating epoch 28
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 28                     |
| policy/steps              | 2590626.0              |
| test/episodes             | 725.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -26.999999683718453    |
| test_1/avg_q              | -20.50705557733887     |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 2900.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -26.88542987703316     |
| train_0/current_q         | -9.991000179732328     |
| train_0/fw_bonus          | -0.9999680206179619    |
| train_0/fw_loss           | 1.2702935180186614e-05 |
| train_0/mu_grads          | 0.008312635123729706   |
| train_0/mu_grads_std      | 0.2240734100341797     |
| train_0/mu_loss           | 10.020225032870071     |
| train_0/next_q            | -9.994064702157385     |
| train_0/q_grads           | 0.0371151277795434     |
| train_0/q_grads_std       | 0.3155537165701389     |
| train_0/q_loss            | 0.5823830662705369     |
| train_0/reward            | -0.5644756472767767    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.3341796875           |
| train_0/target_q          | -10.173710804879189    |
| train_1/avg_q             | -23.056517109142952    |
| train_1/current_q         | -13.334291892308425    |
| train_1/fw_bonus          | -1.0019696325063705    |
| train_1/fw_loss           | 0.000346224525856087   |
| train_1/mu_grads          | -0.024365688767284156  |
| train_1/mu_grads_std      | 0.3775357984006405     |
| train_1/mu_loss           | 24.424292606809793     |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -23.804117799179075    |
| train_1/q_grads           | 0.10776957254856825    |
| train_1/q_grads_std       | 0.7404627129435539     |
| train_1/q_loss            | 6.507229484829212      |
| train_1/reward            | -2.422800757591176     |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0013671875           |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.008148148148148147   |
| train_1/target_q          | -13.46523841217921     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 29
Time for epoch 29: 747.21. Rollout time: 500.46, Training time: 246.64
Evaluating epoch 29
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 29                     |
| policy/steps              | 2681747.0              |
| test/episodes             | 750.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -26.999967283601695    |
| test_1/avg_q              | -20.106817854358066    |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 3000.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -26.996935175874018    |
| train_0/current_q         | -10.121466166529814    |
| train_0/fw_bonus          | -0.99997019469738      |
| train_0/fw_loss           | 1.206004384357584e-05  |
| train_0/mu_grads          | 0.008312635123729706   |
| train_0/mu_grads_std      | 0.2240734100341797     |
| train_0/mu_loss           | 10.17796056664374      |
| train_0/next_q            | -10.137354396960665    |
| train_0/q_grads           | 0.036623725388199094   |
| train_0/q_grads_std       | 0.31681641936302185    |
| train_0/q_loss            | 0.6132323857247691     |
| train_0/reward            | -0.5674058772394346    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.385986328125         |
| train_0/target_q          | -10.273899432639558    |
| train_1/avg_q             | -20.42026715629097     |
| train_1/current_q         | -13.404843708641863    |
| train_1/fw_bonus          | -1.0018785834312438    |
| train_1/fw_loss           | 0.00037177174672251565 |
| train_1/mu_grads          | -0.024372116243466736  |
| train_1/mu_grads_std      | 0.37753922045230864    |
| train_1/mu_loss           | 24.377053365269557     |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -23.73223773170249     |
| train_1/q_grads           | 0.10758928880095482    |
| train_1/q_grads_std       | 0.7419969201087951     |
| train_1/q_loss            | 5.170710898340168      |
| train_1/reward            | -2.4187862959348423    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.00185546875          |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.00037037037037037035 |
| train_1/target_q          | -13.539719574368423    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 30
Time for epoch 30: 748.11. Rollout time: 506.82, Training time: 241.19
Evaluating epoch 30
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 30                     |
| policy/steps              | 2772872.0              |
| test/episodes             | 775.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -26.999976866483756    |
| test_1/avg_q              | -20.397772853667536    |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 3100.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -26.99897843918727     |
| train_0/current_q         | -9.84281773764026      |
| train_0/fw_bonus          | -0.9999563619494438    |
| train_0/fw_loss           | 1.6156963044977602e-05 |
| train_0/mu_grads          | 0.008312635123729706   |
| train_0/mu_grads_std      | 0.2240734100341797     |
| train_0/mu_loss           | 9.875850494839913      |
| train_0/next_q            | -9.862504255414901     |
| train_0/q_grads           | 0.0360637073405087     |
| train_0/q_grads_std       | 0.31717030629515647    |
| train_0/q_loss            | 0.6252124054744601     |
| train_0/reward            | -0.5676221459350927    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.30322265625          |
| train_0/target_q          | -10.078625275334566    |
| train_1/avg_q             | -20.42721378049682     |
| train_1/current_q         | -13.888068112313007    |
| train_1/fw_bonus          | -1.002046322822571     |
| train_1/fw_loss           | 0.0003247079439461231  |
| train_1/mu_grads          | -0.025142472237348557  |
| train_1/mu_grads_std      | 0.37759608030319214    |
| train_1/mu_loss           | 24.407903256162562     |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -24.300034545137553    |
| train_1/q_grads           | 0.10650692339986563    |
| train_1/q_grads_std       | 0.7447147458791733     |
| train_1/q_loss            | 5.4542816526287465     |
| train_1/reward            | -2.4625904387237822    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0017333984375        |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -14.050449264118914    |
------------------------------------------------------
Saving periodic policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100/policy_30.pkl ...
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 31
Time for epoch 31: 741.66. Rollout time: 503.16, Training time: 238.43
Evaluating epoch 31
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 31                     |
| policy/steps              | 2863852.0              |
| test/episodes             | 800.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -26.99990052026806     |
| test_1/avg_q              | -20.59727402404309     |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 3200.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -26.994974893592527    |
| train_0/current_q         | -9.88801454781527      |
| train_0/fw_bonus          | -0.9999695762991905    |
| train_0/fw_loss           | 1.2239968839367066e-05 |
| train_0/mu_grads          | 0.008312635123729706   |
| train_0/mu_grads_std      | 0.2240734100341797     |
| train_0/mu_loss           | 9.919037632807424      |
| train_0/next_q            | -9.897117422489305     |
| train_0/q_grads           | 0.03580364091321826    |
| train_0/q_grads_std       | 0.31806444451212884    |
| train_0/q_loss            | 0.5369111557514961     |
| train_0/reward            | -0.5640548953331745    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.3517333984375        |
| train_0/target_q          | -10.125059187608242    |
| train_1/avg_q             | -20.385570980279503    |
| train_1/current_q         | -13.777190395781883    |
| train_1/fw_bonus          | -1.0019924730062484    |
| train_1/fw_loss           | 0.0003398177512281109  |
| train_1/mu_grads          | -0.025142472237348557  |
| train_1/mu_grads_std      | 0.37759608030319214    |
| train_1/mu_loss           | 24.394122395319403     |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -24.353959859630926    |
| train_1/q_grads           | 0.10440312065184117    |
| train_1/q_grads_std       | 0.7466191783547401     |
| train_1/q_loss            | 6.556586549328918      |
| train_1/reward            | -2.4692603254512506    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0013671875           |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0025925925925925925  |
| train_1/target_q          | -13.933173619710852    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 32
Time for epoch 32: 749.47. Rollout time: 505.83, Training time: 243.55
Evaluating epoch 32
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 32                     |
| policy/steps              | 2954901.0              |
| test/episodes             | 825.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -26.99930762931634     |
| test_1/avg_q              | -20.547951544088942    |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 3300.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -26.996390117177132    |
| train_0/current_q         | -10.145923943405828    |
| train_0/fw_bonus          | -0.9999655246734619    |
| train_0/fw_loss           | 1.3439732958886452e-05 |
| train_0/mu_grads          | 0.008312635123729706   |
| train_0/mu_grads_std      | 0.2240734100341797     |
| train_0/mu_loss           | 10.200400685791745     |
| train_0/next_q            | -10.169704094355131    |
| train_0/q_grads           | 0.03570554964244366    |
| train_0/q_grads_std       | 0.3190696582198143     |
| train_0/q_loss            | 0.6813550100558862     |
| train_0/reward            | -0.5648229256647028    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.3439208984375        |
| train_0/target_q          | -10.29178637419233     |
| train_1/avg_q             | -20.315378664391357    |
| train_1/current_q         | -13.91132772138007     |
| train_1/fw_bonus          | -1.0020740181207657    |
| train_1/fw_loss           | 0.00031694307253928857 |
| train_1/mu_grads          | -0.025142474099993706  |
| train_1/mu_grads_std      | 0.37759608030319214    |
| train_1/mu_loss           | 24.37540904790388      |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -24.3525830436549      |
| train_1/q_grads           | 0.10461561977863312    |
| train_1/q_grads_std       | 0.7484343394637107     |
| train_1/q_loss            | 8.9569308446204        |
| train_1/reward            | -2.446685491576136     |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.001416015625         |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0011111111111111111  |
| train_1/target_q          | -14.100636350046154    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 33
Time for epoch 33: 744.83. Rollout time: 502.19, Training time: 242.55
Evaluating epoch 33
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100
-----------------------------------------------------
| epoch                     | 33                    |
| policy/steps              | 3045948.0             |
| test/episodes             | 850.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -26.99871908893726    |
| test_1/avg_q              | -20.45185379690326    |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 3400.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.996451395024668   |
| train_0/current_q         | -10.046935664399403   |
| train_0/fw_bonus          | -0.9999815613031388   |
| train_0/fw_loss           | 8.687186380029743e-06 |
| train_0/mu_grads          | 0.008312635123729706  |
| train_0/mu_grads_std      | 0.2240734100341797    |
| train_0/mu_loss           | 10.088687122102423    |
| train_0/next_q            | -10.067208405806356   |
| train_0/q_grads           | 0.03519537085667253   |
| train_0/q_grads_std       | 0.319581263512373     |
| train_0/q_loss            | 0.685764376897288     |
| train_0/reward            | -0.5620986652738793   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.3891357421875       |
| train_0/target_q          | -10.248876195572064   |
| train_1/avg_q             | -20.44459652605633    |
| train_1/current_q         | -13.978107873962614   |
| train_1/fw_bonus          | -1.0021790862083435   |
| train_1/fw_loss           | 0.0002874590158171486 |
| train_1/mu_grads          | -0.025142479687929153 |
| train_1/mu_grads_std      | 0.37759608030319214   |
| train_1/mu_loss           | 24.436697732839963    |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -24.399904707607607   |
| train_1/q_grads           | 0.10337241999804973   |
| train_1/q_grads_std       | 0.7490267232060432    |
| train_1/q_loss            | 5.256840108125101     |
| train_1/reward            | -2.4091934535616017   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0011962890625       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0011111111111111111 |
| train_1/target_q          | -14.163811767670007   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 34
Time for epoch 34: 746.62. Rollout time: 501.57, Training time: 244.99
Evaluating epoch 34
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100
-----------------------------------------------------
| epoch                     | 34                    |
| policy/steps              | 3136742.0             |
| test/episodes             | 875.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -26.99503562965408    |
| test_1/avg_q              | -20.490871857408465   |
| test_1/n_subgoals         | 677.0                 |
| test_1/subgoal_succ_rate  | 0.0029542097488921715 |
| train/episodes            | 3500.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.985092778638595   |
| train_0/current_q         | -9.802086924106666    |
| train_0/fw_bonus          | -0.9999666467308999   |
| train_0/fw_loss           | 1.311032492594677e-05 |
| train_0/mu_grads          | 0.008312635123729706  |
| train_0/mu_grads_std      | 0.2240734100341797    |
| train_0/mu_loss           | 9.832094367582679     |
| train_0/next_q            | -9.81321688928514     |
| train_0/q_grads           | 0.03474267451092601   |
| train_0/q_grads_std       | 0.32010782808065413   |
| train_0/q_loss            | 0.6376547230023212    |
| train_0/reward            | -0.5595497662507114   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.374658203125        |
| train_0/target_q          | -10.016888155076852   |
| train_1/avg_q             | -20.48830185373947    |
| train_1/current_q         | -13.587918055968236   |
| train_1/fw_bonus          | -1.0020761281251906   |
| train_1/fw_loss           | 0.0003163473629683722 |
| train_1/mu_grads          | -0.025142479687929153 |
| train_1/mu_grads_std      | 0.37759608030319214   |
| train_1/mu_loss           | 24.424484746100404    |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -23.865823661874554   |
| train_1/q_grads           | 0.10267625283449888   |
| train_1/q_grads_std       | 0.7520669505000115    |
| train_1/q_loss            | 6.598704015486672     |
| train_1/reward            | -2.5138124874141794   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0017333984375       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.005185185185185185  |
| train_1/target_q          | -13.75817787484542    |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 35
Time for epoch 35: 758.42. Rollout time: 516.41, Training time: 241.91
Evaluating epoch 35
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 35                     |
| policy/steps              | 3227795.0              |
| test/episodes             | 900.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -26.991368087732994    |
| test_1/avg_q              | -19.701582080663357    |
| test_1/n_subgoals         | 677.0                  |
| test_1/subgoal_succ_rate  | 0.0029542097488921715  |
| train/episodes            | 3600.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -26.99453908115594     |
| train_0/current_q         | -9.927493148129377     |
| train_0/fw_bonus          | -0.9999749258160591    |
| train_0/fw_loss           | 1.0656667143393861e-05 |
| train_0/mu_grads          | 0.008312635123729706   |
| train_0/mu_grads_std      | 0.2240734100341797     |
| train_0/mu_loss           | 9.963476141191702      |
| train_0/next_q            | -9.933040869671734     |
| train_0/q_grads           | 0.03491935720667243    |
| train_0/q_grads_std       | 0.3205203726887703     |
| train_0/q_loss            | 0.5560673951331644     |
| train_0/reward            | -0.5644407531723118    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.3855712890625        |
| train_0/target_q          | -10.120793635902464    |
| train_1/avg_q             | -20.475671862635046    |
| train_1/current_q         | -13.360298767047016    |
| train_1/fw_bonus          | -1.0021207511425019    |
| train_1/fw_loss           | 0.00030382881923287643 |
| train_1/mu_grads          | -0.02514183856546879   |
| train_1/mu_grads_std      | 0.37759634852409363    |
| train_1/mu_loss           | 24.207871627376086     |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -23.338611398424156    |
| train_1/q_grads           | 0.10245145205408335    |
| train_1/q_grads_std       | 0.75384271889925       |
| train_1/q_loss            | 8.735308266171817      |
| train_1/reward            | -2.4397391690123187    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0010009765625        |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0011111111111111111  |
| train_1/target_q          | -13.528222300796392    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 36
Time for epoch 36: 747.25. Rollout time: 507.24, Training time: 239.92
Evaluating epoch 36
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 36                     |
| policy/steps              | 3318127.0              |
| test/episodes             | 925.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -26.946858403457203    |
| test_1/avg_q              | -21.232486419138517    |
| test_1/n_subgoals         | 683.0                  |
| test_1/subgoal_succ_rate  | 0.01171303074670571    |
| train/episodes            | 3700.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -26.967576859111354    |
| train_0/current_q         | -10.051228722124492    |
| train_0/fw_bonus          | -0.99997069388628      |
| train_0/fw_loss           | 1.1913535536223207e-05 |
| train_0/mu_grads          | 0.008312635123729706   |
| train_0/mu_grads_std      | 0.2240734100341797     |
| train_0/mu_loss           | 10.08837731148769      |
| train_0/next_q            | -10.069259239067042    |
| train_0/q_grads           | 0.03384881438687444    |
| train_0/q_grads_std       | 0.3219880394637585     |
| train_0/q_loss            | 0.7357949420572136     |
| train_0/reward            | -0.5644946239444835    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.3739013671875        |
| train_0/target_q          | -10.277560532545646    |
| train_1/avg_q             | -20.328548403868016    |
| train_1/current_q         | -13.76454061033157     |
| train_1/fw_bonus          | -1.002137929201126     |
| train_1/fw_loss           | 0.0002990106935612857  |
| train_1/mu_grads          | -0.02513804375194013   |
| train_1/mu_grads_std      | 0.377597039192915      |
| train_1/mu_loss           | 24.390306640464495     |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -24.06759639380486     |
| train_1/q_grads           | 0.10215058252215385    |
| train_1/q_grads_std       | 0.7603591457009315     |
| train_1/q_loss            | 5.284792526206944      |
| train_1/reward            | -2.4270359437203295    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.001123046875         |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.012222222222222223   |
| train_1/target_q          | -13.949021946413911    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 37
Time for epoch 37: 661.39. Rollout time: 433.30, Training time: 228.03
Evaluating epoch 37
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 37                     |
| policy/steps              | 3408792.0              |
| test/episodes             | 950.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -26.999942607086208    |
| test_1/avg_q              | -20.792626579689067    |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 3800.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -26.976546469242876    |
| train_0/current_q         | -9.993603586760946     |
| train_0/fw_bonus          | -0.9999752953648567    |
| train_0/fw_loss           | 1.0549414173510741e-05 |
| train_0/mu_grads          | 0.008312635123729706   |
| train_0/mu_grads_std      | 0.2240734100341797     |
| train_0/mu_loss           | 10.012709370119065     |
| train_0/next_q            | -10.007397494347606    |
| train_0/q_grads           | 0.03380290996283293    |
| train_0/q_grads_std       | 0.32370374277234076    |
| train_0/q_loss            | 0.6479970594836619     |
| train_0/reward            | -0.5650450917961279    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.383837890625         |
| train_0/target_q          | -10.218394935367572    |
| train_1/avg_q             | -20.67583549952324     |
| train_1/current_q         | -13.536537338295332    |
| train_1/fw_bonus          | -1.0020057886838913    |
| train_1/fw_loss           | 0.0003360804315889254  |
| train_1/mu_grads          | -0.02805223558098078   |
| train_1/mu_grads_std      | 0.37754007503390313    |
| train_1/mu_loss           | 24.255713723076987     |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -23.921193106290552    |
| train_1/q_grads           | 0.10257926769554615    |
| train_1/q_grads_std       | 0.7636804729700089     |
| train_1/q_loss            | 5.408042895731353      |
| train_1/reward            | -2.4680917790243257    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.001416015625         |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.007407407407407408   |
| train_1/target_q          | -13.722567005066622    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 38
Time for epoch 38: 633.69. Rollout time: 407.55, Training time: 226.07
Evaluating epoch 38
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 38                     |
| policy/steps              | 3499670.0              |
| test/episodes             | 975.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -26.99999968165744     |
| test_1/avg_q              | -19.43254435456179     |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 3900.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -26.98899241742864     |
| train_0/current_q         | -10.03597648870221     |
| train_0/fw_bonus          | -0.9999699622392655    |
| train_0/fw_loss           | 1.2125655280215141e-05 |
| train_0/mu_grads          | 0.008312635123729706   |
| train_0/mu_grads_std      | 0.2240734100341797     |
| train_0/mu_loss           | 10.081766454013472     |
| train_0/next_q            | -10.0456840980825      |
| train_0/q_grads           | 0.03345300620421767    |
| train_0/q_grads_std       | 0.32453272938728334    |
| train_0/q_loss            | 0.7123562752919802     |
| train_0/reward            | -0.5649888297324651    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.37470703125          |
| train_0/target_q          | -10.23074314757725     |
| train_1/avg_q             | -20.46516134984998     |
| train_1/current_q         | -14.016451723096926    |
| train_1/fw_bonus          | -1.0020527601242066    |
| train_1/fw_loss           | 0.00032290097115037495 |
| train_1/mu_grads          | -0.028979768231511115  |
| train_1/mu_grads_std      | 0.37978578358888626    |
| train_1/mu_loss           | 24.24656483129791      |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -24.229892500121018    |
| train_1/q_grads           | 0.10417958293110133    |
| train_1/q_grads_std       | 0.7682942152023315     |
| train_1/q_loss            | 5.27284447406372       |
| train_1/reward            | -2.4465886298341504    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.001171875            |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.003703703703703704   |
| train_1/target_q          | -14.197858791272825    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 39
Time for epoch 39: 630.12. Rollout time: 406.80, Training time: 223.26
Evaluating epoch 39
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 39                     |
| policy/steps              | 3590698.0              |
| test/episodes             | 1000.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -26.99586278733117     |
| test_1/avg_q              | -20.1561362927036      |
| test_1/n_subgoals         | 676.0                  |
| test_1/subgoal_succ_rate  | 0.0014792899408284023  |
| train/episodes            | 4000.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -26.99574084620803     |
| train_0/current_q         | -10.061598815925526    |
| train_0/fw_bonus          | -0.9999708384275436    |
| train_0/fw_loss           | 1.1868476394738537e-05 |
| train_0/mu_grads          | 0.008312635123729706   |
| train_0/mu_grads_std      | 0.2240734100341797     |
| train_0/mu_loss           | 10.108682004512023     |
| train_0/next_q            | -10.076195540621066    |
| train_0/q_grads           | 0.03311255788430571    |
| train_0/q_grads_std       | 0.32654345855116845    |
| train_0/q_loss            | 0.6779212936907912     |
| train_0/reward            | -0.5644214198113332    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.3725830078125        |
| train_0/target_q          | -10.277149927988578    |
| train_1/avg_q             | -20.690100719787882    |
| train_1/current_q         | -14.16834547670474     |
| train_1/fw_bonus          | -1.0020542442798615    |
| train_1/fw_loss           | 0.0003224869695259258  |
| train_1/mu_grads          | -0.030369244795292617  |
| train_1/mu_grads_std      | 0.3826103091239929     |
| train_1/mu_loss           | 24.378750124342552     |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -24.353514402992516    |
| train_1/q_grads           | 0.10717135407030583    |
| train_1/q_grads_std       | 0.7729523435235024     |
| train_1/q_loss            | 6.250180628238329      |
| train_1/reward            | -2.4437142436105206    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0011474609375        |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0014814814814814814  |
| train_1/target_q          | -14.338524131876502    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 40
Time for epoch 40: 634.43. Rollout time: 402.41, Training time: 231.95
Evaluating epoch 40
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 40                     |
| policy/steps              | 3681727.0              |
| test/episodes             | 1025.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -26.992617540229144    |
| test_1/avg_q              | -20.510887057300067    |
| test_1/n_subgoals         | 677.0                  |
| test_1/subgoal_succ_rate  | 0.0029542097488921715  |
| train/episodes            | 4100.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -26.991964378935233    |
| train_0/current_q         | -10.15322849005523     |
| train_0/fw_bonus          | -0.9999737188220024    |
| train_0/fw_loss           | 1.1010912339770585e-05 |
| train_0/mu_grads          | 0.008312635123729706   |
| train_0/mu_grads_std      | 0.2240734100341797     |
| train_0/mu_loss           | 10.187481055543689     |
| train_0/next_q            | -10.161173103326984    |
| train_0/q_grads           | 0.032822356279939416   |
| train_0/q_grads_std       | 0.32787259444594385    |
| train_0/q_loss            | 0.7587748695513056     |
| train_0/reward            | -0.565868676858372     |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.3899169921875        |
| train_0/target_q          | -10.324309757818167    |
| train_1/avg_q             | -20.528328234341014    |
| train_1/current_q         | -13.996101082036102    |
| train_1/fw_bonus          | -1.0020670592784882    |
| train_1/fw_loss           | 0.00031889001038507556 |
| train_1/mu_grads          | -0.031725461967289446  |
| train_1/mu_grads_std      | 0.3837915524840355     |
| train_1/mu_loss           | 24.361677464993953     |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -24.35261897482282     |
| train_1/q_grads           | 0.10743311494588852    |
| train_1/q_grads_std       | 0.774983486533165      |
| train_1/q_loss            | 4.002877592681538      |
| train_1/reward            | -2.451133220342308     |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0009765625           |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0014814814814814814  |
| train_1/target_q          | -14.173963355318147    |
------------------------------------------------------
Saving periodic policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100/policy_40.pkl ...
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 41
Time for epoch 41: 639.59. Rollout time: 404.33, Training time: 235.21
Evaluating epoch 41
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 41                     |
| policy/steps              | 3772626.0              |
| test/episodes             | 1050.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -26.990976590798567    |
| test_1/avg_q              | -20.385973117331186    |
| test_1/n_subgoals         | 678.0                  |
| test_1/subgoal_succ_rate  | 0.004424778761061947   |
| train/episodes            | 4200.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -26.975943420632998    |
| train_0/current_q         | -10.170264977680437    |
| train_0/fw_bonus          | -0.9999727636575699    |
| train_0/fw_loss           | 1.1297696755718788e-05 |
| train_0/mu_grads          | 0.008312635123729706   |
| train_0/mu_grads_std      | 0.2240734100341797     |
| train_0/mu_loss           | 10.226507891350943     |
| train_0/next_q            | -10.191516201160741    |
| train_0/q_grads           | 0.03255676068365574    |
| train_0/q_grads_std       | 0.33013268783688543    |
| train_0/q_loss            | 0.6169429574856162     |
| train_0/reward            | -0.565795192627047     |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.386962890625         |
| train_0/target_q          | -10.351210834090878    |
| train_1/avg_q             | -20.4673324332034      |
| train_1/current_q         | -14.100871064096754    |
| train_1/fw_bonus          | -1.0019701302051545    |
| train_1/fw_loss           | 0.0003460822728811763  |
| train_1/mu_grads          | -0.03285086331889033   |
| train_1/mu_grads_std      | 0.3860030859708786     |
| train_1/mu_loss           | 24.3584032328098       |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -24.392389858722574    |
| train_1/q_grads           | 0.10787418056279421    |
| train_1/q_grads_std       | 0.7785667687654495     |
| train_1/q_loss            | 5.609407614805634      |
| train_1/reward            | -2.5018108944695996    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0010498046875        |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.003703703703703704   |
| train_1/target_q          | -14.26966323000681     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 42
Time for epoch 42: 628.43. Rollout time: 406.07, Training time: 222.29
Evaluating epoch 42
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100
-----------------------------------------------------
| epoch                     | 42                    |
| policy/steps              | 3862968.0             |
| test/episodes             | 1075.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -26.95720513548938    |
| test_1/avg_q              | -20.502949740923466   |
| test_1/n_subgoals         | 677.0                 |
| test_1/subgoal_succ_rate  | 0.0029542097488921715 |
| train/episodes            | 4300.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.915318252924106   |
| train_0/current_q         | -9.96005982528219     |
| train_0/fw_bonus          | -0.9999789074063301   |
| train_0/fw_loss           | 9.476948423525756e-06 |
| train_0/mu_grads          | 0.008312635123729706  |
| train_0/mu_grads_std      | 0.2240734100341797    |
| train_0/mu_loss           | 9.995538576904153     |
| train_0/next_q            | -9.95905135136247     |
| train_0/q_grads           | 0.03247082009911537   |
| train_0/q_grads_std       | 0.33177963420748713   |
| train_0/q_loss            | 0.5827939145542317    |
| train_0/reward            | -0.5648995124498469   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.38623046875         |
| train_0/target_q          | -10.142879540457653   |
| train_1/avg_q             | -20.46279191837735    |
| train_1/current_q         | -14.036284513781672   |
| train_1/fw_bonus          | -1.0019860506057738   |
| train_1/fw_loss           | 0.0003416208761336748 |
| train_1/mu_grads          | -0.03268133383244276  |
| train_1/mu_grads_std      | 0.38787246197462083   |
| train_1/mu_loss           | 24.358200207766433    |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -24.375424993651087   |
| train_1/q_grads           | 0.10707160215824843   |
| train_1/q_grads_std       | 0.7807085514068604    |
| train_1/q_loss            | 5.460111931810663     |
| train_1/reward            | -2.4628317163595055   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0011962890625       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.012592592592592593  |
| train_1/target_q          | -14.19919559869766    |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 43
Time for epoch 43: 604.28. Rollout time: 386.64, Training time: 217.58
Evaluating epoch 43
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 43                     |
| policy/steps              | 3952015.0              |
| test/episodes             | 1100.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -26.94728405139554     |
| test_1/avg_q              | -20.909857277509005    |
| test_1/n_subgoals         | 693.0                  |
| test_1/subgoal_succ_rate  | 0.025974025974025976   |
| train/episodes            | 4400.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -26.842487348810824    |
| train_0/current_q         | -9.91066890767937      |
| train_0/fw_bonus          | -0.9999654695391655    |
| train_0/fw_loss           | 1.3457390139137715e-05 |
| train_0/mu_grads          | 0.008312635123729706   |
| train_0/mu_grads_std      | 0.2240734100341797     |
| train_0/mu_loss           | 9.938968383464395      |
| train_0/next_q            | -9.917205366838568     |
| train_0/q_grads           | 0.03226074893027544    |
| train_0/q_grads_std       | 0.33369415402412417    |
| train_0/q_loss            | 0.5682959732181084     |
| train_0/reward            | -0.5630101667578856    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.3751220703125        |
| train_0/target_q          | -10.13463005346619     |
| train_1/avg_q             | -20.494815013474046    |
| train_1/current_q         | -13.727207251076726    |
| train_1/fw_bonus          | -1.001983305811882     |
| train_1/fw_loss           | 0.0003423906411626376  |
| train_1/mu_grads          | -0.03366908878087997   |
| train_1/mu_grads_std      | 0.3901792362332344     |
| train_1/mu_loss           | 24.265854267698813     |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -24.17758066920843     |
| train_1/q_grads           | 0.10420249588787556    |
| train_1/q_grads_std       | 0.7860380411148071     |
| train_1/q_loss            | 8.812658730329284      |
| train_1/reward            | -2.5249220958579826    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.001171875            |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.03185185185185185    |
| train_1/target_q          | -13.929954398366945    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 44
Time for epoch 44: 573.09. Rollout time: 361.54, Training time: 211.52
Evaluating epoch 44
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 44                     |
| policy/steps              | 4042686.0              |
| test/episodes             | 1125.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -26.99995968692101     |
| test_1/avg_q              | -19.936622103029002    |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 4500.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -26.95101484648965     |
| train_0/current_q         | -10.03221458468008     |
| train_0/fw_bonus          | -0.9999758690595627    |
| train_0/fw_loss           | 1.0379881211974862e-05 |
| train_0/mu_grads          | 0.008312635123729706   |
| train_0/mu_grads_std      | 0.2240734100341797     |
| train_0/mu_loss           | 10.075692718211934     |
| train_0/next_q            | -10.051326918344115    |
| train_0/q_grads           | 0.0318668719381094     |
| train_0/q_grads_std       | 0.33531571477651595    |
| train_0/q_loss            | 0.8369075569950126     |
| train_0/reward            | -0.5636353226691426    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.3852294921875        |
| train_0/target_q          | -10.23070493345919     |
| train_1/avg_q             | -20.60315764289543     |
| train_1/current_q         | -13.850635569287945    |
| train_1/fw_bonus          | -1.0019429117441176    |
| train_1/fw_loss           | 0.0003537231750669889  |
| train_1/mu_grads          | -0.032522601541131735  |
| train_1/mu_grads_std      | 0.3928396314382553     |
| train_1/mu_loss           | 23.864628000540044     |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -24.064521527321766    |
| train_1/q_grads           | 0.10512868091464042    |
| train_1/q_grads_std       | 0.7894044578075409     |
| train_1/q_loss            | 7.731501562647047      |
| train_1/reward            | -2.517075082152587     |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0011962890625        |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.008148148148148147   |
| train_1/target_q          | -13.99748905472315     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 45
Time for epoch 45: 563.02. Rollout time: 354.04, Training time: 208.95
Evaluating epoch 45
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100
-----------------------------------------------------
| epoch                     | 45                    |
| policy/steps              | 4132245.0             |
| test/episodes             | 1150.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -26.999999278045536   |
| test_1/avg_q              | -20.10978283343382    |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 4600.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.967967214794733   |
| train_0/current_q         | -9.952567641554639    |
| train_0/fw_bonus          | -0.999984210729599    |
| train_0/fw_loss           | 7.910122997145664e-06 |
| train_0/mu_grads          | 0.008312635123729706  |
| train_0/mu_grads_std      | 0.2240734100341797    |
| train_0/mu_loss           | 9.984775542198767     |
| train_0/next_q            | -9.961413124002148    |
| train_0/q_grads           | 0.031941380072385075  |
| train_0/q_grads_std       | 0.3365256771445274    |
| train_0/q_loss            | 0.46369176507060716   |
| train_0/reward            | -0.5613852098009374   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.389501953125        |
| train_0/target_q          | -10.137087759217097   |
| train_1/avg_q             | -20.484977541586876   |
| train_1/current_q         | -13.820956443340966   |
| train_1/fw_bonus          | -1.0020294725894927   |
| train_1/fw_loss           | 0.0003294323141744826 |
| train_1/mu_grads          | -0.032321611884981394 |
| train_1/mu_grads_std      | 0.39455191493034364   |
| train_1/mu_loss           | 24.087587283440648    |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -24.151421610187413   |
| train_1/q_grads           | 0.10475564077496528   |
| train_1/q_grads_std       | 0.7961635679006577    |
| train_1/q_loss            | 3.745158301282035     |
| train_1/reward            | -2.540735642294385    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0010009765625       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.02259259259259259   |
| train_1/target_q          | -13.988507899148356   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 46
Time for epoch 46: 575.81. Rollout time: 368.54, Training time: 207.24
Evaluating epoch 46
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 46                     |
| policy/steps              | 4223360.0              |
| test/episodes             | 1175.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -26.99999122540008     |
| test_1/avg_q              | -19.991141636378273    |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 4700.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -26.99682979132599     |
| train_0/current_q         | -9.956631350913597     |
| train_0/fw_bonus          | -0.9999725475907326    |
| train_0/fw_loss           | 1.1360685391537118e-05 |
| train_0/mu_grads          | 0.008312635123729706   |
| train_0/mu_grads_std      | 0.2240734100341797     |
| train_0/mu_loss           | 10.004349663325598     |
| train_0/next_q            | -9.967108775319426     |
| train_0/q_grads           | 0.03143105236813426    |
| train_0/q_grads_std       | 0.3379358947277069     |
| train_0/q_loss            | 0.6471687000955272     |
| train_0/reward            | -0.560704807302318     |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.3747802734375        |
| train_0/target_q          | -10.139708082697164    |
| train_1/avg_q             | -20.678957639075712    |
| train_1/current_q         | -13.802670613367846    |
| train_1/fw_bonus          | -1.0019768685102464    |
| train_1/fw_loss           | 0.000344192553893663   |
| train_1/mu_grads          | -0.03216641824692488   |
| train_1/mu_grads_std      | 0.39844836443662646    |
| train_1/mu_loss           | 24.177855717138353     |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -24.182129993851124    |
| train_1/q_grads           | 0.10458693746477365    |
| train_1/q_grads_std       | 0.7979765936732293     |
| train_1/q_loss            | 6.02872425489587       |
| train_1/reward            | -2.4812681452673133    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.001123046875         |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.00037037037037037035 |
| train_1/target_q          | -13.98830393969862     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 47
Time for epoch 47: 569.99. Rollout time: 361.45, Training time: 208.50
Evaluating epoch 47
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 47                     |
| policy/steps              | 4314180.0              |
| test/episodes             | 1200.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -26.99999455781712     |
| test_1/avg_q              | -20.35161065959414     |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 4800.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -26.99111404466459     |
| train_0/current_q         | -9.835073767914409     |
| train_0/fw_bonus          | -0.9999775603413582    |
| train_0/fw_loss           | 9.876263470687264e-06  |
| train_0/mu_grads          | 0.008312635123729706   |
| train_0/mu_grads_std      | 0.2240734100341797     |
| train_0/mu_loss           | 9.859783644166228      |
| train_0/next_q            | -9.841629804664175     |
| train_0/q_grads           | 0.03123086062259972    |
| train_0/q_grads_std       | 0.33943523094058037    |
| train_0/q_loss            | 0.7751422912141983     |
| train_0/reward            | -0.5623723199991219    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.3841064453125        |
| train_0/target_q          | -10.04101677966203     |
| train_1/avg_q             | -20.375935359215834    |
| train_1/current_q         | -14.133214340270456    |
| train_1/fw_bonus          | -1.0020716100931168    |
| train_1/fw_loss           | 0.00031761322534293865 |
| train_1/mu_grads          | -0.03181645208969712   |
| train_1/mu_grads_std      | 0.40244207382202146    |
| train_1/mu_loss           | 24.260737629861637     |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -24.27781508884102     |
| train_1/q_grads           | 0.10309855975210666    |
| train_1/q_grads_std       | 0.8027166292071343     |
| train_1/q_loss            | 4.112417983206295      |
| train_1/reward            | -2.4807608238232204    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0011962890625        |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.004814814814814815   |
| train_1/target_q          | -14.309567298192974    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 48
Time for epoch 48: 575.22. Rollout time: 367.05, Training time: 208.13
Evaluating epoch 48
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 48                     |
| policy/steps              | 4405151.0              |
| test/episodes             | 1225.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -26.99151540796424     |
| test_1/avg_q              | -20.413253894337018    |
| test_1/n_subgoals         | 677.0                  |
| test_1/subgoal_succ_rate  | 0.0029542097488921715  |
| train/episodes            | 4900.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -26.990892322897352    |
| train_0/current_q         | -9.845980987855423     |
| train_0/fw_bonus          | -0.9999592244625092    |
| train_0/fw_loss           | 1.5308580475448253e-05 |
| train_0/mu_grads          | 0.008312635123729706   |
| train_0/mu_grads_std      | 0.2240734100341797     |
| train_0/mu_loss           | 9.868982221038786      |
| train_0/next_q            | -9.861547644675204     |
| train_0/q_grads           | 0.03099448112770915    |
| train_0/q_grads_std       | 0.340498410910368      |
| train_0/q_loss            | 0.9078038971170599     |
| train_0/reward            | -0.5614888289212103    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.3219970703125        |
| train_0/target_q          | -10.056050246691004    |
| train_1/avg_q             | -20.41154617127354     |
| train_1/current_q         | -13.925815400229897    |
| train_1/fw_bonus          | -1.0019497275352478    |
| train_1/fw_loss           | 0.0003518088917189743  |
| train_1/mu_grads          | -0.03137819524854422   |
| train_1/mu_grads_std      | 0.4045510195195675     |
| train_1/mu_loss           | 24.367698601342443     |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -24.36955741003714     |
| train_1/q_grads           | 0.1023285573348403     |
| train_1/q_grads_std       | 0.8046224862337112     |
| train_1/q_loss            | 5.731694217433477      |
| train_1/reward            | -2.410857305471654     |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0013427734375        |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0025925925925925925  |
| train_1/target_q          | -14.096473707842296    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 49
Time for epoch 49: 579.62. Rollout time: 362.28, Training time: 217.30
Evaluating epoch 49
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 49                     |
| policy/steps              | 4496276.0              |
| test/episodes             | 1250.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -26.999986018474416    |
| test_1/avg_q              | -20.176042669509357    |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 5000.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -26.99942345430156     |
| train_0/current_q         | -9.838108111350486     |
| train_0/fw_bonus          | -0.9999673619866372    |
| train_0/fw_loss           | 1.2898009447326331e-05 |
| train_0/mu_grads          | 0.008312635123729706   |
| train_0/mu_grads_std      | 0.2240734100341797     |
| train_0/mu_loss           | 9.872971846214863      |
| train_0/next_q            | -9.835694349310938     |
| train_0/q_grads           | 0.03036186983808875    |
| train_0/q_grads_std       | 0.3417457818984985     |
| train_0/q_loss            | 0.5255559857704836     |
| train_0/reward            | -0.5621134218963562    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.343798828125         |
| train_0/target_q          | -10.024711345380307    |
| train_1/avg_q             | -20.45943482647569     |
| train_1/current_q         | -13.941735310497416    |
| train_1/fw_bonus          | -1.0020277947187424    |
| train_1/fw_loss           | 0.00032990922991302794 |
| train_1/mu_grads          | -0.03156819259747863   |
| train_1/mu_grads_std      | 0.4065993010997772     |
| train_1/mu_loss           | 24.141694518453686     |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -24.36694289116254     |
| train_1/q_grads           | 0.10189019311219454    |
| train_1/q_grads_std       | 0.8061748877167701     |
| train_1/q_loss            | 4.070120822002152      |
| train_1/reward            | -2.4700565062852546    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0015380859375        |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -14.111431305990175    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 50
Time for epoch 50: 571.55. Rollout time: 363.72, Training time: 207.79
Evaluating epoch 50
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100
-----------------------------------------------------
| epoch                     | 50                    |
| policy/steps              | 4587358.0             |
| test/episodes             | 1275.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -19.856821644629843   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 5100.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.99853943426285    |
| train_0/current_q         | -10.039103493872044   |
| train_0/fw_bonus          | -0.9999854624271393   |
| train_0/fw_loss           | 7.536083353443246e-06 |
| train_0/mu_grads          | 0.008312635123729706  |
| train_0/mu_grads_std      | 0.2240734100341797    |
| train_0/mu_loss           | 10.064833781236086    |
| train_0/next_q            | -10.051646998774967   |
| train_0/q_grads           | 0.030291709024459122  |
| train_0/q_grads_std       | 0.3437954470515251    |
| train_0/q_loss            | 0.6740383500385103    |
| train_0/reward            | -0.5666223418927985   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.3957275390625       |
| train_0/target_q          | -10.246022190966837   |
| train_1/avg_q             | -20.327048642078076   |
| train_1/current_q         | -13.92243085057333    |
| train_1/fw_bonus          | -1.0019980907440185   |
| train_1/fw_loss           | 0.0003382421607966535 |
| train_1/mu_grads          | -0.03230568515136838  |
| train_1/mu_grads_std      | 0.4095221385359764    |
| train_1/mu_loss           | 24.322652559045764    |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -24.423998687589847   |
| train_1/q_grads           | 0.102561616897583     |
| train_1/q_grads_std       | 0.8099466845393181    |
| train_1/q_loss            | 5.7474415095516305    |
| train_1/reward            | -2.488030038461511    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0013916015625       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0007407407407407407 |
| train_1/target_q          | -14.098448014339075   |
-----------------------------------------------------
Saving periodic policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100/policy_50.pkl ...
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 51
Time for epoch 51: 576.68. Rollout time: 365.46, Training time: 211.18
Evaluating epoch 51
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 51                     |
| policy/steps              | 4678465.0              |
| test/episodes             | 1300.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -27.0                  |
| test_1/avg_q              | -20.348859916643832    |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 5200.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -26.99914735167782     |
| train_0/current_q         | -10.266251373905934    |
| train_0/fw_bonus          | -0.9999803900718689    |
| train_0/fw_loss           | 9.035182745265048e-06  |
| train_0/mu_grads          | 0.008312635123729706   |
| train_0/mu_grads_std      | 0.2240734100341797     |
| train_0/mu_loss           | 10.34370144515844      |
| train_0/next_q            | -10.297524371590148    |
| train_0/q_grads           | 0.03011947642080486    |
| train_0/q_grads_std       | 0.3453972592949867     |
| train_0/q_loss            | 0.7055164893972444     |
| train_0/reward            | -0.5655956859998696    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.392919921875         |
| train_0/target_q          | -10.427692384078279    |
| train_1/avg_q             | -20.35576409441554     |
| train_1/current_q         | -14.043839153427035    |
| train_1/fw_bonus          | -1.0020336717367173    |
| train_1/fw_loss           | 0.0003282583391410299  |
| train_1/mu_grads          | -0.03334291512146592   |
| train_1/mu_grads_std      | 0.4136785216629505     |
| train_1/mu_loss           | 24.413877451658045     |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -24.421199336575313    |
| train_1/q_grads           | 0.10159145444631576    |
| train_1/q_grads_std       | 0.8111116766929627     |
| train_1/q_loss            | 4.3965153756139035     |
| train_1/reward            | -2.4170123230702303    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0011474609375        |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.00037037037037037035 |
| train_1/target_q          | -14.231638604034597    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 52
Time for epoch 52: 585.98. Rollout time: 369.69, Training time: 216.25
Evaluating epoch 52
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 52                     |
| policy/steps              | 4769590.0              |
| test/episodes             | 1325.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -27.0                  |
| test_1/avg_q              | -20.411462624602088    |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 5300.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -26.999999987149568    |
| train_0/current_q         | -10.095615752623322    |
| train_0/fw_bonus          | -0.9999826118350029    |
| train_0/fw_loss           | 8.382180226362834e-06  |
| train_0/mu_grads          | 0.008312635123729706   |
| train_0/mu_grads_std      | 0.2240734100341797     |
| train_0/mu_loss           | 10.159845920268669     |
| train_0/next_q            | -10.12059715902387     |
| train_0/q_grads           | 0.0297501552850008     |
| train_0/q_grads_std       | 0.3463184371590614     |
| train_0/q_loss            | 0.7507906785106649     |
| train_0/reward            | -0.5625933073635678    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.391650390625         |
| train_0/target_q          | -10.279169704372181    |
| train_1/avg_q             | -20.410202538287074    |
| train_1/current_q         | -13.996665574611239    |
| train_1/fw_bonus          | -1.0020318180322647    |
| train_1/fw_loss           | 0.00032877935082069597 |
| train_1/mu_grads          | -0.03431521942839026   |
| train_1/mu_grads_std      | 0.4163370683789253     |
| train_1/mu_loss           | 24.38831865367498      |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -24.404579401861163    |
| train_1/q_grads           | 0.10042423214763403    |
| train_1/q_grads_std       | 0.8125254079699517     |
| train_1/q_loss            | 5.845575643702745      |
| train_1/reward            | -2.4651869082190387    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0011474609375        |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -14.193095038548734    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 53
Time for epoch 53: 608.70. Rollout time: 379.23, Training time: 229.43
Evaluating epoch 53
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100
-----------------------------------------------------
| epoch                     | 53                    |
| policy/steps              | 4860715.0             |
| test/episodes             | 1350.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -20.46924055122328    |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 5400.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.999999999999957   |
| train_0/current_q         | -10.036047959705705   |
| train_0/fw_bonus          | -0.9999880939722061   |
| train_0/fw_loss           | 6.754596142855007e-06 |
| train_0/mu_grads          | 0.008312635123729706  |
| train_0/mu_grads_std      | 0.2240734100341797    |
| train_0/mu_loss           | 10.053039324321663    |
| train_0/next_q            | -10.046279127264768   |
| train_0/q_grads           | 0.02914216835051775   |
| train_0/q_grads_std       | 0.34755769148468973   |
| train_0/q_loss            | 0.5214131812107936    |
| train_0/reward            | -0.5648468784354919   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.3986572265625       |
| train_0/target_q          | -10.209823695495277   |
| train_1/avg_q             | -20.47822319478189    |
| train_1/current_q         | -14.129326676923233   |
| train_1/fw_bonus          | -1.0020794510841369   |
| train_1/fw_loss           | 0.0003154135494696675 |
| train_1/mu_grads          | -0.03465366661548615  |
| train_1/mu_grads_std      | 0.4183535560965538    |
| train_1/mu_loss           | 24.43811132760397     |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -24.44549841338524    |
| train_1/q_grads           | 0.0983221223577857    |
| train_1/q_grads_std       | 0.8119666576385498    |
| train_1/q_loss            | 3.9982167095444465    |
| train_1/reward            | -2.4754710338362202   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0009521484375       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -14.298096040915416   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 54
Time for epoch 54: 776.92. Rollout time: 503.80, Training time: 273.04
Evaluating epoch 54
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 54                     |
| policy/steps              | 4951693.0              |
| test/episodes             | 1375.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -27.0                  |
| test_1/avg_q              | -20.49936214943935     |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 5500.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -26.993972826483073    |
| train_0/current_q         | -10.074791543834404    |
| train_0/fw_bonus          | -0.9999765783548356    |
| train_0/fw_loss           | 1.0166533706978953e-05 |
| train_0/mu_grads          | 0.008312635123729706   |
| train_0/mu_grads_std      | 0.2240734100341797     |
| train_0/mu_loss           | 10.102086383898506     |
| train_0/next_q            | -10.092571915773291    |
| train_0/q_grads           | 0.028630613582208753   |
| train_0/q_grads_std       | 0.3483131527900696     |
| train_0/q_loss            | 0.7711274825745464     |
| train_0/reward            | -0.5673342331563618    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.390869140625         |
| train_0/target_q          | -10.260949366378345    |
| train_1/avg_q             | -20.481760246322125    |
| train_1/current_q         | -14.25558866031401     |
| train_1/fw_bonus          | -1.0018961668014525    |
| train_1/fw_loss           | 0.0003668416502478067  |
| train_1/mu_grads          | -0.03505752207711339   |
| train_1/mu_grads_std      | 0.41896571069955824    |
| train_1/mu_loss           | 24.45206193428958      |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -24.453966376837464    |
| train_1/q_grads           | 0.09866339601576328    |
| train_1/q_grads_std       | 0.815920141339302      |
| train_1/q_loss            | 3.109586728900454      |
| train_1/reward            | -2.455942789179244     |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0016357421875        |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0022222222222222222  |
| train_1/target_q          | -14.42767232352021     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 55
Time for epoch 55: 1105.32. Rollout time: 777.96, Training time: 327.24
Evaluating epoch 55
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 55                     |
| policy/steps              | 5042792.0              |
| test/episodes             | 1400.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -27.0                  |
| test_1/avg_q              | -20.559424319933488    |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 5600.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -26.995446960363637    |
| train_0/current_q         | -10.095057129203706    |
| train_0/fw_bonus          | -0.999981914460659     |
| train_0/fw_loss           | 8.587871013787662e-06  |
| train_0/mu_grads          | 0.008312635123729706   |
| train_0/mu_grads_std      | 0.2240734100341797     |
| train_0/mu_loss           | 10.134889047693466     |
| train_0/next_q            | -10.096131494065025    |
| train_0/q_grads           | 0.02855031844228506    |
| train_0/q_grads_std       | 0.34895257353782655    |
| train_0/q_loss            | 0.6579991615282506     |
| train_0/reward            | -0.5662735538429843    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.3970703125           |
| train_0/target_q          | -10.254213100043433    |
| train_1/avg_q             | -20.552538455543495    |
| train_1/current_q         | -14.153211919464113    |
| train_1/fw_bonus          | -1.0021037310361862    |
| train_1/fw_loss           | 0.0003086028904363047  |
| train_1/mu_grads          | -0.03526097051799297   |
| train_1/mu_grads_std      | 0.4200784705579281     |
| train_1/mu_loss           | 24.42491957877302      |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -24.429399590828723    |
| train_1/q_grads           | 0.0972188800573349     |
| train_1/q_grads_std       | 0.8180242851376534     |
| train_1/q_loss            | 3.6672162951483984     |
| train_1/reward            | -2.4577708059088765    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0013671875           |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.00037037037037037035 |
| train_1/target_q          | -14.335336256461968    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 56
Time for epoch 56: 1034.53. Rollout time: 734.01, Training time: 300.37
Evaluating epoch 56
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 56                     |
| policy/steps              | 5132975.0              |
| test/episodes             | 1425.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -26.980094603490702    |
| test_1/avg_q              | -20.133115571785556    |
| test_1/n_subgoals         | 683.0                  |
| test_1/subgoal_succ_rate  | 0.01171303074670571    |
| train/episodes            | 5700.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -26.96895448231108     |
| train_0/current_q         | -9.842498798169796     |
| train_0/fw_bonus          | -0.9999880820512772    |
| train_0/fw_loss           | 6.759593651395334e-06  |
| train_0/mu_grads          | 0.008312635123729706   |
| train_0/mu_grads_std      | 0.2240734100341797     |
| train_0/mu_loss           | 9.869069513424284      |
| train_0/next_q            | -9.85370098499109      |
| train_0/q_grads           | 0.027560351835563778   |
| train_0/q_grads_std       | 0.3495194599032402     |
| train_0/q_loss            | 0.6311206140737272     |
| train_0/reward            | -0.5600861320919648    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.3910888671875        |
| train_0/target_q          | -10.021300681171446    |
| train_1/avg_q             | -20.432539673388153    |
| train_1/current_q         | -14.054243883661565    |
| train_1/fw_bonus          | -1.0020813733339309    |
| train_1/fw_loss           | 0.00031487425512750634 |
| train_1/mu_grads          | -0.03629657365381718   |
| train_1/mu_grads_std      | 0.4201077498495579     |
| train_1/mu_loss           | 24.34953109093562      |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -24.382030469312816    |
| train_1/q_grads           | 0.09594522304832935    |
| train_1/q_grads_std       | 0.8196266770362854     |
| train_1/q_loss            | 4.767278972407432      |
| train_1/reward            | -2.5106643862862255    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.001318359375         |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.013703703703703704   |
| train_1/target_q          | -14.213912304509062    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 57
Time for epoch 57: 1161.83. Rollout time: 815.95, Training time: 345.73
Evaluating epoch 57
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 57                     |
| policy/steps              | 5222803.0              |
| test/episodes             | 1450.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -27.0                  |
| test_1/avg_q              | -20.42410801801581     |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 5800.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -26.942023474732647    |
| train_0/current_q         | -9.837212141898513     |
| train_0/fw_bonus          | -0.9999798655509948    |
| train_0/fw_loss           | 9.196813277867476e-06  |
| train_0/mu_grads          | 0.008312635123729706   |
| train_0/mu_grads_std      | 0.2240734100341797     |
| train_0/mu_loss           | 9.859943246444999      |
| train_0/next_q            | -9.835287133966792     |
| train_0/q_grads           | 0.02730097118765116    |
| train_0/q_grads_std       | 0.34994171410799024    |
| train_0/q_loss            | 0.5802836308521339     |
| train_0/reward            | -0.5624644462553988    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.384912109375         |
| train_0/target_q          | -10.009956247371084    |
| train_1/avg_q             | -20.134363401854383    |
| train_1/current_q         | -13.946496359305527    |
| train_1/fw_bonus          | -1.0020228892564773    |
| train_1/fw_loss           | 0.00033128491195384414 |
| train_1/mu_grads          | -0.036677506659179925  |
| train_1/mu_grads_std      | 0.42215741202235224    |
| train_1/mu_loss           | 24.36375748506899      |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -24.391483027184144    |
| train_1/q_grads           | 0.09775807224214077    |
| train_1/q_grads_std       | 0.8190315261483192     |
| train_1/q_loss            | 3.684405887962292      |
| train_1/reward            | -2.4735525949479777    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0010498046875        |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.01925925925925926    |
| train_1/target_q          | -14.11831983524417     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 58
Time for epoch 58: 1140.91. Rollout time: 792.65, Training time: 348.15
Evaluating epoch 58
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 58                     |
| policy/steps              | 5313580.0              |
| test/episodes             | 1475.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -26.999999995099856    |
| test_1/avg_q              | -20.37618439860715     |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 5900.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -26.893042071163354    |
| train_0/current_q         | -9.964500635841755     |
| train_0/fw_bonus          | -0.9999760612845421    |
| train_0/fw_loss           | 1.0320746525849245e-05 |
| train_0/mu_grads          | 0.008312635123729706   |
| train_0/mu_grads_std      | 0.2240734100341797     |
| train_0/mu_loss           | 9.974197164156749      |
| train_0/next_q            | -9.966542497261688     |
| train_0/q_grads           | 0.026985824946314095   |
| train_0/q_grads_std       | 0.3506645724177361     |
| train_0/q_loss            | 0.5127811294481264     |
| train_0/reward            | -0.5653931850913068    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.3842041015625        |
| train_0/target_q          | -10.12573022753159     |
| train_1/avg_q             | -20.324427645180307    |
| train_1/current_q         | -13.990394824201825    |
| train_1/fw_bonus          | -1.0019903004169464    |
| train_1/fw_loss           | 0.00034042484185192734 |
| train_1/mu_grads          | -0.03760630125179887   |
| train_1/mu_grads_std      | 0.42411899343132975    |
| train_1/mu_loss           | 24.323761286168754     |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -24.33039124922834     |
| train_1/q_grads           | 0.09698487762361765    |
| train_1/q_grads_std       | 0.8222733750939369     |
| train_1/q_loss            | 4.84695446700233       |
| train_1/reward            | -2.476126239362202     |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0009521484375        |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.005555555555555556   |
| train_1/target_q          | -14.163903112349749    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 59
Time for epoch 59: 1114.41. Rollout time: 779.38, Training time: 334.89
Evaluating epoch 59
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 59                     |
| policy/steps              | 5404607.0              |
| test/episodes             | 1500.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -26.997261189477033    |
| test_1/avg_q              | -20.58098342656693     |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 6000.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -26.976696257379324    |
| train_0/current_q         | -10.130344503472951    |
| train_0/fw_bonus          | -0.9999708354473114    |
| train_0/fw_loss           | 1.1865781141295884e-05 |
| train_0/mu_grads          | 0.008312635123729706   |
| train_0/mu_grads_std      | 0.2240734100341797     |
| train_0/mu_loss           | 10.156780013237977     |
| train_0/next_q            | -10.146420982035872    |
| train_0/q_grads           | 0.026528595387935637   |
| train_0/q_grads_std       | 0.3525755047798157     |
| train_0/q_loss            | 0.7264831792237295     |
| train_0/reward            | -0.5656299181464419    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.3677001953125        |
| train_0/target_q          | -10.311201824591425    |
| train_1/avg_q             | -20.45566907386569     |
| train_1/current_q         | -14.142807860081735    |
| train_1/fw_bonus          | -1.0020402997732163    |
| train_1/fw_loss           | 0.00032639909186400474 |
| train_1/mu_grads          | -0.03718984378501773   |
| train_1/mu_grads_std      | 0.4268465988337994     |
| train_1/mu_loss           | 24.315174439620804     |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -24.322219689485127    |
| train_1/q_grads           | 0.09504606984555722    |
| train_1/q_grads_std       | 0.823953264951706      |
| train_1/q_loss            | 4.76302728871078       |
| train_1/reward            | -2.492380191577831     |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0012939453125        |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.001851851851851852   |
| train_1/target_q          | -14.335048339364652    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 60
Time for epoch 60: 1192.13. Rollout time: 843.85, Training time: 348.14
Evaluating epoch 60
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100
-----------------------------------------------------
| epoch                     | 60                    |
| policy/steps              | 5495402.0             |
| test/episodes             | 1525.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -26.991322609428856   |
| test_1/avg_q              | -20.38379929097819    |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 6100.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.92944240974832    |
| train_0/current_q         | -10.165690895003642   |
| train_0/fw_bonus          | -0.9999794229865074   |
| train_0/fw_loss           | 9.32452551296592e-06  |
| train_0/mu_grads          | 0.008312635123729706  |
| train_0/mu_grads_std      | 0.2240734100341797    |
| train_0/mu_loss           | 10.211137675678062    |
| train_0/next_q            | -10.18406811884734    |
| train_0/q_grads           | 0.025703154550865293  |
| train_0/q_grads_std       | 0.3533946469426155    |
| train_0/q_loss            | 0.6912124654447354    |
| train_0/reward            | -0.5653390941999532   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.39208984375         |
| train_0/target_q          | -10.34401764392372    |
| train_1/avg_q             | -20.493301451598853   |
| train_1/current_q         | -14.073772749010876   |
| train_1/fw_bonus          | -1.0020963579416275   |
| train_1/fw_loss           | 0.0003106723823293578 |
| train_1/mu_grads          | -0.03744871076196432  |
| train_1/mu_grads_std      | 0.42798553332686423   |
| train_1/mu_loss           | 24.335233808209118    |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -24.34950664062243    |
| train_1/q_grads           | 0.09587264582514762   |
| train_1/q_grads_std       | 0.8282926067709923    |
| train_1/q_loss            | 4.592053165127995     |
| train_1/reward            | -2.5245475255498606   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.001123046875        |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.005925925925925926  |
| train_1/target_q          | -14.249529721043581   |
-----------------------------------------------------
Saving periodic policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100/policy_60.pkl ...
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 61
Time for epoch 61: 1455.92. Rollout time: 991.09, Training time: 464.65
Evaluating epoch 61
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 61                     |
| policy/steps              | 5586514.0              |
| test/episodes             | 1550.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -27.0                  |
| test_1/avg_q              | -20.460723934571007    |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 6200.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -26.99094041173667     |
| train_0/current_q         | -10.179321495939197    |
| train_0/fw_bonus          | -0.9999836325645447    |
| train_0/fw_loss           | 8.076062488271419e-06  |
| train_0/mu_grads          | 0.008312635123729706   |
| train_0/mu_grads_std      | 0.2240734100341797     |
| train_0/mu_loss           | 10.215410851229402     |
| train_0/next_q            | -10.187198577605589    |
| train_0/q_grads           | 0.025655894307419658   |
| train_0/q_grads_std       | 0.355048206448555      |
| train_0/q_loss            | 0.6517088766065469     |
| train_0/reward            | -0.5669331656579744    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.396240234375         |
| train_0/target_q          | -10.348502091187575    |
| train_1/avg_q             | -20.44702134043303     |
| train_1/current_q         | -14.039655232695633    |
| train_1/fw_bonus          | -1.0019934952259064    |
| train_1/fw_loss           | 0.00033953108722926115 |
| train_1/mu_grads          | -0.03714981535449624   |
| train_1/mu_grads_std      | 0.4293064005672932     |
| train_1/mu_loss           | 24.358048906762598     |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -24.365081594357104    |
| train_1/q_grads           | 0.09614882655441762    |
| train_1/q_grads_std       | 0.8365339562296867     |
| train_1/q_loss            | 4.028895036938306      |
| train_1/reward            | -2.4102090678064996    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.001318359375         |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.00037037037037037035 |
| train_1/target_q          | -14.211736313602284    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 62
Time for epoch 62: 1364.45. Rollout time: 960.78, Training time: 403.52
Evaluating epoch 62
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100
-----------------------------------------------------
| epoch                     | 62                    |
| policy/steps              | 5677639.0             |
| test/episodes             | 1575.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -27.0                 |
| test_1/avg_q              | -20.65406606320644    |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 6300.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -26.99999999999999    |
| train_0/current_q         | -10.147941670493266   |
| train_0/fw_bonus          | -0.9999748125672341   |
| train_0/fw_loss           | 1.068983494292297e-05 |
| train_0/mu_grads          | 0.008312635123729706  |
| train_0/mu_grads_std      | 0.2240734100341797    |
| train_0/mu_loss           | 10.198532536281718    |
| train_0/next_q            | -10.15638251514098    |
| train_0/q_grads           | 0.025404559867456555  |
| train_0/q_grads_std       | 0.3562166288495064    |
| train_0/q_loss            | 0.590931815218437     |
| train_0/reward            | -0.5664915063938679   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.383154296875        |
| train_0/target_q          | -10.323158801542919   |
| train_1/avg_q             | -20.531047730548458   |
| train_1/current_q         | -14.240656928268624   |
| train_1/fw_bonus          | -1.0020294964313508   |
| train_1/fw_loss           | 0.0003294340051070321 |
| train_1/mu_grads          | -0.037387095112353566 |
| train_1/mu_grads_std      | 0.4298517979681492    |
| train_1/mu_loss           | 24.428308363287947    |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -24.42540097918725    |
| train_1/q_grads           | 0.09715351928025484   |
| train_1/q_grads_std       | 0.8438613042235374    |
| train_1/q_loss            | 3.7246720874929737    |
| train_1/reward            | -2.4385078586998135   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.001171875           |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.0                   |
| train_1/target_q          | -14.414177290449846   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 63
Time for epoch 63: 1243.56. Rollout time: 914.94, Training time: 328.48
Evaluating epoch 63
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 63                     |
| policy/steps              | 5768764.0              |
| test/episodes             | 1600.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -26.999914923027895    |
| test_1/avg_q              | -20.597988497473725    |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 6400.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -26.999999784326157    |
| train_0/current_q         | -10.210189454194726    |
| train_0/fw_bonus          | -0.9999851509928703    |
| train_0/fw_loss           | 7.628063974607357e-06  |
| train_0/mu_grads          | 0.008312635123729706   |
| train_0/mu_grads_std      | 0.2240734100341797     |
| train_0/mu_loss           | 10.267150513547353     |
| train_0/next_q            | -10.249509979677791    |
| train_0/q_grads           | 0.025187249993905425   |
| train_0/q_grads_std       | 0.3577504031360149     |
| train_0/q_loss            | 0.7097377433114535     |
| train_0/reward            | -0.5646081041912112    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.3956298828125        |
| train_0/target_q          | -10.355245416684042    |
| train_1/avg_q             | -20.600764526013073    |
| train_1/current_q         | -14.286296308782903    |
| train_1/fw_bonus          | -1.0019973754882812    |
| train_1/fw_loss           | 0.00033844458957901227 |
| train_1/mu_grads          | -0.03769784485921264   |
| train_1/mu_grads_std      | 0.42977665066719056    |
| train_1/mu_loss           | 24.404394454272953     |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -24.414934231288946    |
| train_1/q_grads           | 0.09641604032367468    |
| train_1/q_grads_std       | 0.8496276140213013     |
| train_1/q_loss            | 5.583399785950439      |
| train_1/reward            | -2.5159351336817055    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0012939453125        |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -14.46996562827519     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 64
Time for epoch 64: 650.15. Rollout time: 465.55, Training time: 184.53
Evaluating epoch 64
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.75|fw:1|fwhidsiz:256,256,256|nlev:2|reg:True|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 64                     |
| policy/steps              | 5859889.0              |
| test/episodes             | 1625.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -27.0                  |
| test_1/avg_q              | -20.53514687008443     |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 6500.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -26.99761754628739     |
| train_0/current_q         | -10.15324789535212     |
| train_0/fw_bonus          | -0.9999792575836182    |
| train_0/fw_loss           | 9.376415687256667e-06  |
| train_0/mu_grads          | 0.008312635123729706   |
| train_0/mu_grads_std      | 0.2240734100341797     |
| train_0/mu_loss           | 10.169171231649461     |
| train_0/next_q            | -10.150474851424596    |
| train_0/q_grads           | 0.02513577528297901    |
| train_0/q_grads_std       | 0.35827826261520385    |
| train_0/q_loss            | 0.6221537570679561     |
| train_0/reward            | -0.5688546755322023    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.398876953125         |
| train_0/target_q          | -10.321014514039993    |
| train_1/avg_q             | -20.57241377099862     |
| train_1/current_q         | -14.26041438429532     |
| train_1/fw_bonus          | -1.0020444482564925    |
| train_1/fw_loss           | 0.00032523589979973624 |
| train_1/mu_grads          | -0.0382222137413919    |
| train_1/mu_grads_std      | 0.4298295624554157     |
| train_1/mu_loss           | 24.43280574645572      |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -24.440800789057207    |
| train_1/q_grads           | 0.09575658105313778    |
| train_1/q_grads_std       | 0.8558748468756676     |
| train_1/q_loss            | 3.229295310363777      |
| train_1/reward            | -2.4909713158132947    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0012451171875        |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.0                    |
| train_1/target_q          | -14.429553166961574    |
------------------------------------------------------
