Starting process id: 98152
T: 50
alg: chac
algorithm: src.chac
atomic_noise: 0.2
base_logdir: data
batch_size: 1024
bind_core: 0
buffer_size: 500
chac_params: {}
early_stop_data_column: test/success_rate
early_stop_threshold: 100.0
env_name: CausalDependenciesMujocoEnv-o1-v0
eta: 0.5
fw: 1
fw_hidden_size: 256,256,256
fw_lr: 0.001
gamma: 0.98
graph: 1
info: 
make_env: <function prepare_params.<locals>.make_env at 0x7f7c28e2d950>
max_try_idx: 199
mu_hidden_size: 64
mu_lr: 0.001
n_episodes: 100
n_levels: 2
n_pre_episodes: 30
n_test_rollouts: 25
n_train_batches: 40
n_train_rollouts: 100
num_threads: 1
q_hidden_size: 64
q_lr: 0.001
random_action_perc: 0.3
regularization: False
render: 0
rollout_batch_size: 1
subgoal_noise: 0.2
subgoal_test_perc: 0.3
time_scales: 27,27
try_start_idx: 100
use_mpi: False
verbose: False

*** Warning ***
You are running src.chac with just a single MPI worker. This will work, but the HER experiments that we report in Plappert et al. (2018, https://arxiv.org/abs/1802.09464) were obtained with --num_cpu 19. This makes a significant difference and if you are looking to reproduce those results, be aware of this. Please also refer to https://github.com/openai/baselines/issues/314 for further details.
****************

dims: action = 2, subgoal = 2, end_goal = 2
subgoal_bounds: symmetric [0.2 0.2], offset [0.922 0.25 ]
Running on CPU ...
Creating a CHAC agent

Hierarchy Level 0 with time scale 27
Actor(
  (fc1): Linear(in_features=35, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
  (fc4): Linear(in_features=64, out_features=2, bias=True)
)
Critic(
  (fc1): Linear(in_features=37, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
  (fc4): Linear(in_features=64, out_features=1, bias=True)
  (mse_loss): MSELoss()
)
ForwardModel(
  (mlp): Sequential(
    (0): Linear(in_features=35, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=256, bias=True)
    (5): ReLU()
    (6): Linear(in_features=256, out_features=33, bias=True)
    (7): Identity()
  )
  (mse_loss): MSELoss()
)

Hierarchy Level 1 with time scale 27
Actor(
  (fc1): Linear(in_features=35, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
  (fc4): Linear(in_features=64, out_features=2, bias=True)
)
Critic(
  (fc1): Linear(in_features=37, out_features=64, bias=True)
  (fc2): Linear(in_features=64, out_features=64, bias=True)
  (fc3): Linear(in_features=64, out_features=64, bias=True)
  (fc4): Linear(in_features=64, out_features=1, bias=True)
  (mse_loss): MSELoss()
)
ForwardModel(
  (mlp): Sequential(
    (0): Linear(in_features=35, out_features=256, bias=True)
    (1): ReLU()
    (2): Linear(in_features=256, out_features=256, bias=True)
    (3): ReLU()
    (4): Linear(in_features=256, out_features=256, bias=True)
    (5): ReLU()
    (6): Linear(in_features=256, out_features=33, bias=True)
    (7): Identity()
  )
  (mse_loss): MSELoss()
)
Training epoch 0
Time for epoch 0: 682.71. Rollout time: 444.81, Training time: 237.84
Evaluating epoch 0
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
-----------------------------------------------------
| epoch                     | 0                     |
| policy/steps              | 87013.0               |
| test/episodes             | 25.0                  |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -2.406622464899901    |
| test_1/avg_q              | -11.664467130711866   |
| test_1/n_subgoals         | 748.0                 |
| test_1/subgoal_succ_rate  | 0.10294117647058823   |
| train/episodes            | 100.0                 |
| train/success_rate        | 0.01                  |
| train_0/avg_q             | -2.0005803935752025   |
| train_0/current_q         | -4.990098725728542    |
| train_0/fw_bonus          | -0.998948159813881    |
| train_0/fw_loss           | 0.0002498152207408566 |
| train_0/mu_grads          | -0.004204758058767766 |
| train_0/mu_grads_std      | 0.17382191866636276   |
| train_0/mu_loss           | 4.814457126117726     |
| train_0/next_q            | -4.816086754540508    |
| train_0/q_grads           | 0.027836732985451817  |
| train_0/q_grads_std       | 0.12965468354523182   |
| train_0/q_loss            | 0.44674093203444254   |
| train_0/reward            | -0.7505360896117054   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0001708984375       |
| train_0/target_q          | -5.014758660154007    |
| train_1/avg_q             | -9.227092083273797    |
| train_1/current_q         | -12.941735834370514   |
| train_1/fw_bonus          | -0.995404988527298    |
| train_1/fw_loss           | 0.0015993293316569179 |
| train_1/mu_grads          | 0.01985492170788348   |
| train_1/mu_grads_std      | 0.08655303437262774   |
| train_1/mu_loss           | 15.363098882015425    |
| train_1/n_subgoals        | 2690.0                |
| train_1/next_q            | -15.311574467491004   |
| train_1/q_grads           | 0.043068331107497214  |
| train_1/q_grads_std       | 0.22672009877860547   |
| train_1/q_loss            | 29.534847012298655    |
| train_1/reward            | -2.006577007033775    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0048583984375       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.06654275092936802   |
| train_1/target_q          | -12.965888674101901   |
-----------------------------------------------------
Saving periodic policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_0.pkl ...
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Training epoch 1
Time for epoch 1: 695.07. Rollout time: 495.66, Training time: 199.36
Evaluating epoch 1
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
-----------------------------------------------------
| epoch                     | 1                     |
| policy/steps              | 170403.0              |
| test/episodes             | 50.0                  |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -0.2886812862940024   |
| test_1/avg_q              | -16.4742830156799     |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 200.0                 |
| train/success_rate        | 0.06                  |
| train_0/avg_q             | -5.540134497928973    |
| train_0/current_q         | -2.4744873980272493   |
| train_0/fw_bonus          | -0.9986319616436958   |
| train_0/fw_loss           | 0.0003234796407923568 |
| train_0/mu_grads          | -0.022032971726730466 |
| train_0/mu_grads_std      | 0.21707176081836224   |
| train_0/mu_loss           | 2.2622689533489626    |
| train_0/next_q            | -2.2868176264947797   |
| train_0/q_grads           | 0.027029498713091016  |
| train_0/q_grads_std       | 0.1486704982817173    |
| train_0/q_loss            | 0.5817664301831333    |
| train_0/reward            | -0.7716182294432656   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 9.765625e-05          |
| train_0/target_q          | -2.667967599146973    |
| train_1/avg_q             | -14.149395075655391   |
| train_1/current_q         | -16.05640319987878    |
| train_1/fw_bonus          | -0.995004978775978    |
| train_1/fw_loss           | 0.0016954687918769196 |
| train_1/mu_grads          | 0.019948460906744004  |
| train_1/mu_grads_std      | 0.08685755115002394   |
| train_1/mu_loss           | 19.17063146116909     |
| train_1/n_subgoals        | 2617.0                |
| train_1/next_q            | -19.132629311145145   |
| train_1/q_grads           | 0.042579478770494464  |
| train_1/q_grads_std       | 0.28612698167562484   |
| train_1/q_loss            | 32.78065688150523     |
| train_1/reward            | -2.0035571739426814   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.004931640625        |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.11119602598395109   |
| train_1/target_q          | -16.12690247260371    |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Training epoch 2
Time for epoch 2: 594.66. Rollout time: 401.98, Training time: 192.63
Evaluating epoch 2
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
-----------------------------------------------------
| epoch                     | 2                     |
| policy/steps              | 251368.0              |
| test/episodes             | 75.0                  |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -1.1037252268854378   |
| test_1/avg_q              | -11.776563205277409   |
| test_1/n_subgoals         | 677.0                 |
| test_1/subgoal_succ_rate  | 0.0029542097488921715 |
| train/episodes            | 300.0                 |
| train/success_rate        | 0.05                  |
| train_0/avg_q             | -4.806450091542587    |
| train_0/current_q         | -4.746069324527225    |
| train_0/fw_bonus          | -0.9985536381602287   |
| train_0/fw_loss           | 0.0003417282081500161 |
| train_0/mu_grads          | -0.03061166275292635  |
| train_0/mu_grads_std      | 0.2511577866971493    |
| train_0/mu_loss           | 4.53655058710581      |
| train_0/next_q            | -4.524993745252884    |
| train_0/q_grads           | 0.025365151884034275  |
| train_0/q_grads_std       | 0.1614970937371254    |
| train_0/q_loss            | 0.6456678860052214    |
| train_0/reward            | -0.7856781862479693   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0001220703125       |
| train_0/target_q          | -4.788764841882281    |
| train_1/avg_q             | -15.669130957829337   |
| train_1/current_q         | -15.870150894754797   |
| train_1/fw_bonus          | -0.993439470231533    |
| train_1/fw_loss           | 0.002071720617823303  |
| train_1/mu_grads          | 0.019958658469840886  |
| train_1/mu_grads_std      | 0.08689396232366561   |
| train_1/mu_loss           | 18.249271400511418    |
| train_1/n_subgoals        | 2631.0                |
| train_1/next_q            | -18.23711422432149    |
| train_1/q_grads           | 0.039100450742989776  |
| train_1/q_grads_std       | 0.31968130394816396   |
| train_1/q_loss            | 16.381719572351848    |
| train_1/reward            | -2.0091464915371033   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.004736328125        |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.1645762067654884    |
| train_1/target_q          | -15.920937802975246   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Training epoch 3
Time for epoch 3: 567.52. Rollout time: 361.67, Training time: 205.80
Evaluating epoch 3
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 3                      |
| policy/steps              | 322608.0               |
| test/episodes             | 100.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -2.3568333379203468    |
| test_1/avg_q              | -17.482490695446877    |
| test_1/n_subgoals         | 1865.0                 |
| test_1/subgoal_succ_rate  | 0.6659517426273458     |
| train/episodes            | 400.0                  |
| train/success_rate        | 0.09                   |
| train_0/avg_q             | -7.13041681200902      |
| train_0/current_q         | -5.283931429419974     |
| train_0/fw_bonus          | -0.998447185754776     |
| train_0/fw_loss           | 0.00036652942362707107 |
| train_0/mu_grads          | -0.038324616476893426  |
| train_0/mu_grads_std      | 0.28725055009126665    |
| train_0/mu_loss           | 5.062666261894348      |
| train_0/next_q            | -4.973779881268842     |
| train_0/q_grads           | 0.02418970256112516    |
| train_0/q_grads_std       | 0.1784967642277479     |
| train_0/q_loss            | 0.4488868070027717     |
| train_0/reward            | -0.7907641193254676    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 9.765625e-05           |
| train_0/target_q          | -5.204641639626336     |
| train_1/avg_q             | -14.461117942216761    |
| train_1/current_q         | -15.689157650595098    |
| train_1/fw_bonus          | -0.9939453169703484    |
| train_1/fw_loss           | 0.0019501451606629416  |
| train_1/mu_grads          | 0.019959809258580208   |
| train_1/mu_grads_std      | 0.08689817041158676    |
| train_1/mu_loss           | 17.963386609796185     |
| train_1/n_subgoals        | 2589.0                 |
| train_1/next_q            | -17.912259548803416    |
| train_1/q_grads           | 0.036974755115807056   |
| train_1/q_grads_std       | 0.34071881994605063    |
| train_1/q_loss            | 11.242076407375638     |
| train_1/reward            | -1.9002075226337183    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0050537109375        |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.32483584395519505    |
| train_1/target_q          | -15.792487198870646    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 4
Time for epoch 4: 598.28. Rollout time: 386.60, Training time: 211.61
Evaluating epoch 4
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
-----------------------------------------------------
| epoch                     | 4                     |
| policy/steps              | 391271.0              |
| test/episodes             | 125.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -0.6514922821497617   |
| test_1/avg_q              | -16.261413923472798   |
| test_1/n_subgoals         | 2061.0                |
| test_1/subgoal_succ_rate  | 0.7025715672003882    |
| train/episodes            | 500.0                 |
| train/success_rate        | 0.11                  |
| train_0/avg_q             | -8.808759350024982    |
| train_0/current_q         | -4.406895399174867    |
| train_0/fw_bonus          | -0.9984646752476692   |
| train_0/fw_loss           | 0.0003624561497417744 |
| train_0/mu_grads          | -0.049331944715231654 |
| train_0/mu_grads_std      | 0.3121997855603695    |
| train_0/mu_loss           | 4.1259503565653475    |
| train_0/next_q            | -4.1192564397057225   |
| train_0/q_grads           | 0.02222545528784394   |
| train_0/q_grads_std       | 0.20042233876883983   |
| train_0/q_loss            | 0.6367857999186318    |
| train_0/reward            | -0.7951063574517321   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 9.765625e-05          |
| train_0/target_q          | -4.390389415714209    |
| train_1/avg_q             | -16.449313991761684   |
| train_1/current_q         | -16.05009559082556    |
| train_1/fw_bonus          | -0.9932757213711738   |
| train_1/fw_loss           | 0.002111076400615275  |
| train_1/mu_grads          | 0.019959809258580208  |
| train_1/mu_grads_std      | 0.08689817041158676   |
| train_1/mu_loss           | 18.157328505278922    |
| train_1/n_subgoals        | 2605.0                |
| train_1/next_q            | -18.14877137894281    |
| train_1/q_grads           | 0.03537406288087368   |
| train_1/q_grads_std       | 0.35325550362467767   |
| train_1/q_loss            | 7.613185491739682     |
| train_1/reward            | -1.879242837991478    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0045166015625       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.381957773512476     |
| train_1/target_q          | -16.17384008793877    |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 5
Time for epoch 5: 653.86. Rollout time: 444.08, Training time: 209.70
Evaluating epoch 5
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
-----------------------------------------------------
| epoch                     | 5                     |
| policy/steps              | 468320.0              |
| test/episodes             | 150.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -1.2634154669052982   |
| test_1/avg_q              | -17.773108853036955   |
| test_1/n_subgoals         | 743.0                 |
| test_1/subgoal_succ_rate  | 0.09152086137281291   |
| train/episodes            | 600.0                 |
| train/success_rate        | 0.03                  |
| train_0/avg_q             | -7.287793742493939    |
| train_0/current_q         | -4.627985878258849    |
| train_0/fw_bonus          | -0.9984254717826844   |
| train_0/fw_loss           | 0.000371585941320518  |
| train_0/mu_grads          | -0.05738418325781822  |
| train_0/mu_grads_std      | 0.3337436579167843    |
| train_0/mu_loss           | 4.380513291366357     |
| train_0/next_q            | -4.311444964700479    |
| train_0/q_grads           | 0.024155656481161712  |
| train_0/q_grads_std       | 0.21756322383880616   |
| train_0/q_loss            | 0.49483788119467675   |
| train_0/reward            | -0.7978821736818645   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0001953125          |
| train_0/target_q          | -4.672730894132712    |
| train_1/avg_q             | -15.741826846512433   |
| train_1/current_q         | -15.928643698986054   |
| train_1/fw_bonus          | -0.992815163731575    |
| train_1/fw_loss           | 0.0022217665507923813 |
| train_1/mu_grads          | 0.019959809258580208  |
| train_1/mu_grads_std      | 0.08689817041158676   |
| train_1/mu_loss           | 17.784559540796675    |
| train_1/n_subgoals        | 2674.0                |
| train_1/next_q            | -17.764484301259255   |
| train_1/q_grads           | 0.03324008332565427   |
| train_1/q_grads_std       | 0.36252406463027      |
| train_1/q_loss            | 6.2339224184231385    |
| train_1/reward            | -1.8546595313258876   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0048583984375       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.26514584891548243   |
| train_1/target_q          | -16.085228044673254   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 6
Time for epoch 6: 647.46. Rollout time: 449.77, Training time: 197.63
Evaluating epoch 6
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 6                      |
| policy/steps              | 547155.0               |
| test/episodes             | 175.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -1.6352044173453664    |
| test_1/avg_q              | -16.941411799766595    |
| test_1/n_subgoals         | 17205.0                |
| test_1/subgoal_succ_rate  | 0.997965707643127      |
| train/episodes            | 700.0                  |
| train/success_rate        | 0.04                   |
| train_0/avg_q             | -7.388305746746897     |
| train_0/current_q         | -4.906026957976086     |
| train_0/fw_bonus          | -0.9983902767300605    |
| train_0/fw_loss           | 0.00037978846157784576 |
| train_0/mu_grads          | -0.0660526042804122    |
| train_0/mu_grads_std      | 0.35208177864551543    |
| train_0/mu_loss           | 4.686448543076113      |
| train_0/next_q            | -4.562151470696156     |
| train_0/q_grads           | 0.025636327778920533   |
| train_0/q_grads_std       | 0.2289560955017805     |
| train_0/q_loss            | 0.5107019733901845     |
| train_0/reward            | -0.8039924995071488    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 2.44140625e-05         |
| train_0/target_q          | -4.81515462881859      |
| train_1/avg_q             | -15.703057301609057    |
| train_1/current_q         | -15.82307647055227     |
| train_1/fw_bonus          | -0.9928318321704864    |
| train_1/fw_loss           | 0.0022177572129294275  |
| train_1/mu_grads          | 0.019959809258580208   |
| train_1/mu_grads_std      | 0.08689817041158676    |
| train_1/mu_loss           | 17.460954215998417     |
| train_1/n_subgoals        | 2658.0                 |
| train_1/next_q            | -17.42909438415078     |
| train_1/q_grads           | 0.030943932151421903   |
| train_1/q_grads_std       | 0.3746150828897953     |
| train_1/q_loss            | 5.374988275088851      |
| train_1/reward            | -1.7721483405963228    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.00419921875          |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.2200902934537246     |
| train_1/target_q          | -15.984556685726062    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 7
Time for epoch 7: 613.72. Rollout time: 406.99, Training time: 206.66
Evaluating epoch 7
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
-----------------------------------------------------
| epoch                     | 7                     |
| policy/steps              | 617749.0              |
| test/episodes             | 200.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -2.30622720985171     |
| test_1/avg_q              | -15.084167115269109   |
| test_1/n_subgoals         | 12223.0               |
| test_1/subgoal_succ_rate  | 0.9914096375685184    |
| train/episodes            | 800.0                 |
| train/success_rate        | 0.05                  |
| train_0/avg_q             | -8.932672979588371    |
| train_0/current_q         | -4.401311926168832    |
| train_0/fw_bonus          | -0.9984573557972908   |
| train_0/fw_loss           | 0.0003641579023678787 |
| train_0/mu_grads          | -0.07054994218051433  |
| train_0/mu_grads_std      | 0.37472695484757423   |
| train_0/mu_loss           | 4.4610253455745905    |
| train_0/next_q            | -4.3119280768221895   |
| train_0/q_grads           | 0.026588269462808966  |
| train_0/q_grads_std       | 0.2452831819653511    |
| train_0/q_loss            | 1.469320534683869     |
| train_0/reward            | -0.8111061228970357   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0001708984375       |
| train_0/target_q          | -4.2677191532798275   |
| train_1/avg_q             | -16.00203985732049    |
| train_1/current_q         | -15.441733422628584   |
| train_1/fw_bonus          | -0.9927997410297393   |
| train_1/fw_loss           | 0.002225468936376274  |
| train_1/mu_grads          | 0.019959809258580208  |
| train_1/mu_grads_std      | 0.08689817041158676   |
| train_1/mu_loss           | 16.967313022177738    |
| train_1/n_subgoals        | 2642.0                |
| train_1/next_q            | -16.97272213758819    |
| train_1/q_grads           | 0.02904615392908454   |
| train_1/q_grads_std       | 0.38443332985043527   |
| train_1/q_loss            | 5.826776290724793     |
| train_1/reward            | -1.720740696455323    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0044189453125       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.3735806207418622    |
| train_1/target_q          | -15.59599388298794    |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 8
Time for epoch 8: 615.61. Rollout time: 405.01, Training time: 210.51
Evaluating epoch 8
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 8                      |
| policy/steps              | 686896.0               |
| test/episodes             | 225.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -1.3925636810877504    |
| test_1/avg_q              | -17.16501233016914     |
| test_1/n_subgoals         | 3577.0                 |
| test_1/subgoal_succ_rate  | 0.8691641039977634     |
| train/episodes            | 900.0                  |
| train/success_rate        | 0.03                   |
| train_0/avg_q             | -9.990462305567943     |
| train_0/current_q         | -5.768885767145659     |
| train_0/fw_bonus          | -0.9985832735896111    |
| train_0/fw_loss           | 0.00033482533690403215 |
| train_0/mu_grads          | -0.08072166945785283   |
| train_0/mu_grads_std      | 0.39328746125102043    |
| train_0/mu_loss           | 5.520600514978001      |
| train_0/next_q            | -5.35755378003893      |
| train_0/q_grads           | 0.025432779127731918   |
| train_0/q_grads_std       | 0.2583432212471962     |
| train_0/q_loss            | 0.44132178040193093    |
| train_0/reward            | -0.8094234695505292    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.000927734375         |
| train_0/target_q          | -5.663189041359227     |
| train_1/avg_q             | -15.206876494612631    |
| train_1/current_q         | -15.307142891466409    |
| train_1/fw_bonus          | -0.9930936425924302    |
| train_1/fw_loss           | 0.0021548335236730052  |
| train_1/mu_grads          | 0.019959809258580208   |
| train_1/mu_grads_std      | 0.08689817041158676    |
| train_1/mu_loss           | 16.98606716507957      |
| train_1/n_subgoals        | 2652.0                 |
| train_1/next_q            | -16.974077181249346    |
| train_1/q_grads           | 0.02691536652855575    |
| train_1/q_grads_std       | 0.39368835389614104    |
| train_1/q_loss            | 5.29087610287433       |
| train_1/reward            | -1.7328993650226039    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0046630859375        |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.3902714932126697     |
| train_1/target_q          | -15.48553128206089     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 9
Time for epoch 9: 641.44. Rollout time: 427.36, Training time: 213.99
Evaluating epoch 9
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 9                      |
| policy/steps              | 759476.0               |
| test/episodes             | 250.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -1.8086659753492298    |
| test_1/avg_q              | -17.858100445834552    |
| test_1/n_subgoals         | 13655.0                |
| test_1/subgoal_succ_rate  | 0.9976565360673746     |
| train/episodes            | 1000.0                 |
| train/success_rate        | 0.04                   |
| train_0/avg_q             | -10.21523823268194     |
| train_0/current_q         | -3.7334661020397277    |
| train_0/fw_bonus          | -0.9986593574285507    |
| train_0/fw_loss           | 0.00031709439936093985 |
| train_0/mu_grads          | -0.083673407882452     |
| train_0/mu_grads_std      | 0.4072351060807705     |
| train_0/mu_loss           | 3.6453592361175153     |
| train_0/next_q            | -3.495333949799955     |
| train_0/q_grads           | 0.02646247330121696    |
| train_0/q_grads_std       | 0.2675652988255024     |
| train_0/q_loss            | 0.8546970635729906     |
| train_0/reward            | -0.8064585929198074    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.0016845703125        |
| train_0/target_q          | -3.7463665702177495    |
| train_1/avg_q             | -16.142398511855788    |
| train_1/current_q         | -15.398740834950644    |
| train_1/fw_bonus          | -0.9934231862425804    |
| train_1/fw_loss           | 0.002075631357729435   |
| train_1/mu_grads          | 0.019959809258580208   |
| train_1/mu_grads_std      | 0.08689817041158676    |
| train_1/mu_loss           | 17.155825823262386     |
| train_1/n_subgoals        | 2648.0                 |
| train_1/next_q            | -17.124762280278887    |
| train_1/q_grads           | 0.025812357384711503   |
| train_1/q_grads_std       | 0.40416874215006826    |
| train_1/q_loss            | 4.74841009588061       |
| train_1/reward            | -1.7399611092550913    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0041748046875        |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.3168429003021148     |
| train_1/target_q          | -15.563146113741038    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 10
Time for epoch 10: 649.10. Rollout time: 431.72, Training time: 217.30
Evaluating epoch 10
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 10                     |
| policy/steps              | 832767.0               |
| test/episodes             | 275.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -4.876251165305816     |
| test_1/avg_q              | -16.770478371506986    |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 1100.0                 |
| train/success_rate        | 0.08                   |
| train_0/avg_q             | -8.405108808304277     |
| train_0/current_q         | -5.978074592104974     |
| train_0/fw_bonus          | -0.9987105935811996    |
| train_0/fw_loss           | 0.00030516201222781093 |
| train_0/mu_grads          | -0.08589391820132733   |
| train_0/mu_grads_std      | 0.42236448675394056    |
| train_0/mu_loss           | 5.7785952954517334     |
| train_0/next_q            | -5.623664256748397     |
| train_0/q_grads           | 0.02681522537022829    |
| train_0/q_grads_std       | 0.2762520745396614     |
| train_0/q_loss            | 0.5486161218009047     |
| train_0/reward            | -0.8046165259809641    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.0010498046875        |
| train_0/target_q          | -5.906991284439149     |
| train_1/avg_q             | -16.054397154244366    |
| train_1/current_q         | -15.496603562482738    |
| train_1/fw_bonus          | -0.993807464838028     |
| train_1/fw_loss           | 0.001983275846578181   |
| train_1/mu_grads          | 0.019959809258580208   |
| train_1/mu_grads_std      | 0.08689817041158676    |
| train_1/mu_loss           | 17.377119138093583     |
| train_1/n_subgoals        | 2614.0                 |
| train_1/next_q            | -17.34943926074443     |
| train_1/q_grads           | 0.02347957165911794    |
| train_1/q_grads_std       | 0.4141508713364601     |
| train_1/q_loss            | 4.490609329158403      |
| train_1/reward            | -1.7399335185014935    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.005322265625         |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.306044376434583      |
| train_1/target_q          | -15.669119203448016    |
------------------------------------------------------
Saving periodic policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_10.pkl ...
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 11
Time for epoch 11: 614.23. Rollout time: 398.79, Training time: 215.36
Evaluating epoch 11
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 11                     |
| policy/steps              | 903954.0               |
| test/episodes             | 300.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -3.6405839303402763    |
| test_1/avg_q              | -18.19342569926815     |
| test_1/n_subgoals         | 715.0                  |
| test_1/subgoal_succ_rate  | 0.055944055944055944   |
| train/episodes            | 1200.0                 |
| train/success_rate        | 0.07                   |
| train_0/avg_q             | -11.747607583930709    |
| train_0/current_q         | -5.668921468006782     |
| train_0/fw_bonus          | -0.9987890094518661    |
| train_0/fw_loss           | 0.00028689089849649464 |
| train_0/mu_grads          | -0.09029258899390698   |
| train_0/mu_grads_std      | 0.4364799775183201     |
| train_0/mu_loss           | 5.425534384905566      |
| train_0/next_q            | -5.342483129856992     |
| train_0/q_grads           | 0.02746411468833685    |
| train_0/q_grads_std       | 0.28449506759643556    |
| train_0/q_loss            | 0.6280618763953715     |
| train_0/reward            | -0.8042651134914195    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.0013916015625        |
| train_0/target_q          | -5.6688814057059025    |
| train_1/avg_q             | -16.75303413676596     |
| train_1/current_q         | -15.169299235495547    |
| train_1/fw_bonus          | -0.9939208477735519    |
| train_1/fw_loss           | 0.0019560270040528847  |
| train_1/mu_grads          | 0.019959809258580208   |
| train_1/mu_grads_std      | 0.08689817041158676    |
| train_1/mu_loss           | 16.89737985485782      |
| train_1/n_subgoals        | 2608.0                 |
| train_1/next_q            | -16.872881231126115    |
| train_1/q_grads           | 0.021025878470391036   |
| train_1/q_grads_std       | 0.42602615654468534    |
| train_1/q_loss            | 4.636910419776183      |
| train_1/reward            | -1.7147460662134109    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0047607421875        |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.34547546012269936    |
| train_1/target_q          | -15.327278928932065    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 12
Time for epoch 12: 841.94. Rollout time: 563.49, Training time: 278.32
Evaluating epoch 12
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
-----------------------------------------------------
| epoch                     | 12                    |
| policy/steps              | 975611.0              |
| test/episodes             | 325.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -1.0557887952004008   |
| test_1/avg_q              | -16.310068696846322   |
| test_1/n_subgoals         | 739.0                 |
| test_1/subgoal_succ_rate  | 0.08660351826792964   |
| train/episodes            | 1300.0                |
| train/success_rate        | 0.06                  |
| train_0/avg_q             | -9.439731091677249    |
| train_0/current_q         | -3.013228585260399    |
| train_0/fw_bonus          | -0.9988375514745712   |
| train_0/fw_loss           | 0.0002755825644271681 |
| train_0/mu_grads          | -0.09649553038179874  |
| train_0/mu_grads_std      | 0.45036311373114585   |
| train_0/mu_loss           | 2.823044631729956     |
| train_0/next_q            | -2.699715816584802    |
| train_0/q_grads           | 0.027265856275334954  |
| train_0/q_grads_std       | 0.2895356111228466    |
| train_0/q_loss            | 0.3800048866087417    |
| train_0/reward            | -0.8080369414688903   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.003466796875        |
| train_0/target_q          | -3.269304961251509    |
| train_1/avg_q             | -16.947305026421628   |
| train_1/current_q         | -15.185148205558724   |
| train_1/fw_bonus          | -0.9942034587264061   |
| train_1/fw_loss           | 0.0018881046795286237 |
| train_1/mu_grads          | 0.019959809258580208  |
| train_1/mu_grads_std      | 0.08689817041158676   |
| train_1/mu_loss           | 16.834772007051374    |
| train_1/n_subgoals        | 2637.0                |
| train_1/next_q            | -16.80911385735209    |
| train_1/q_grads           | 0.020623209653422236  |
| train_1/q_grads_std       | 0.435876290500164     |
| train_1/q_loss            | 4.371115053873376     |
| train_1/reward            | -1.7358102224461618   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0048828125          |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.35267349260523323   |
| train_1/target_q          | -15.323789625038383   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 13
Time for epoch 13: 633.83. Rollout time: 427.81, Training time: 205.95
Evaluating epoch 13
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
-----------------------------------------------------
| epoch                     | 13                    |
| policy/steps              | 1053439.0             |
| test/episodes             | 350.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -1.1895453943788055   |
| test_1/avg_q              | -16.904125118670965   |
| test_1/n_subgoals         | 1749.0                |
| test_1/subgoal_succ_rate  | 0.7592910234419669    |
| train/episodes            | 1400.0                |
| train/success_rate        | 0.01                  |
| train_0/avg_q             | -9.794397820771744    |
| train_0/current_q         | -6.507461705434986    |
| train_0/fw_bonus          | -0.9989191919565201   |
| train_0/fw_loss           | 0.0002565626386058284 |
| train_0/mu_grads          | -0.10060317181050778  |
| train_0/mu_grads_std      | 0.46313019543886186   |
| train_0/mu_loss           | 6.315909346616585     |
| train_0/next_q            | -6.163512328668977    |
| train_0/q_grads           | 0.028732651378959417  |
| train_0/q_grads_std       | 0.29699688628315923   |
| train_0/q_loss            | 0.6024974916280239    |
| train_0/reward            | -0.8108365500971558   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0033203125          |
| train_0/target_q          | -6.479342090085533    |
| train_1/avg_q             | -16.318549524717064   |
| train_1/current_q         | -15.673547098762658   |
| train_1/fw_bonus          | -0.9946726188063622   |
| train_1/fw_loss           | 0.001775344819179736  |
| train_1/mu_grads          | 0.019959809258580208  |
| train_1/mu_grads_std      | 0.08689817041158676   |
| train_1/mu_loss           | 17.53074026935489     |
| train_1/n_subgoals        | 2682.0                |
| train_1/next_q            | -17.53134779782473    |
| train_1/q_grads           | 0.02003874578513205   |
| train_1/q_grads_std       | 0.44227798879146574   |
| train_1/q_loss            | 5.1026533815857285    |
| train_1/reward            | -1.7609593587440031   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0052490234375       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.26025354213273677   |
| train_1/target_q          | -15.827763604113523   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 14
Time for epoch 14: 835.29. Rollout time: 554.48, Training time: 280.72
Evaluating epoch 14
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 14                     |
| policy/steps              | 1123862.0              |
| test/episodes             | 375.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -6.164304432656662     |
| test_1/avg_q              | -16.531833763781226    |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 1500.0                 |
| train/success_rate        | 0.02                   |
| train_0/avg_q             | -11.934972182524978    |
| train_0/current_q         | -6.737897965104359     |
| train_0/fw_bonus          | -0.9990302413702011    |
| train_0/fw_loss           | 0.00023068870650604366 |
| train_0/mu_grads          | -0.10666770096868276   |
| train_0/mu_grads_std      | 0.4812185488641262     |
| train_0/mu_loss           | 6.622216505830406      |
| train_0/next_q            | -6.443766579732733     |
| train_0/q_grads           | 0.028962334012612702   |
| train_0/q_grads_std       | 0.3032141089439392     |
| train_0/q_loss            | 0.7247017893899267     |
| train_0/reward            | -0.8087192277409485    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.0053466796875        |
| train_0/target_q          | -6.702185002394134     |
| train_1/avg_q             | -16.44212253314941     |
| train_1/current_q         | -15.990808112478783    |
| train_1/fw_bonus          | -0.9953379064798356    |
| train_1/fw_loss           | 0.0016154504555743188  |
| train_1/mu_grads          | 0.019959809258580208   |
| train_1/mu_grads_std      | 0.08689817041158676    |
| train_1/mu_loss           | 17.9138940260128       |
| train_1/n_subgoals        | 2686.0                 |
| train_1/next_q            | -17.899108975786525    |
| train_1/q_grads           | 0.019455072982236744   |
| train_1/q_grads_std       | 0.4492140345275402     |
| train_1/q_loss            | 3.189496062578208      |
| train_1/reward            | -1.7310063582874136    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.00546875             |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.3812360387192852     |
| train_1/target_q          | -16.154130516657013    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 15
Time for epoch 15: 772.34. Rollout time: 528.72, Training time: 243.53
Evaluating epoch 15
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 15                     |
| policy/steps              | 1197186.0              |
| test/episodes             | 400.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -1.8837391674436752    |
| test_1/avg_q              | -18.330571045903444    |
| test_1/n_subgoals         | 15563.0                |
| test_1/subgoal_succ_rate  | 0.9981366060528176     |
| train/episodes            | 1600.0                 |
| train/success_rate        | 0.02                   |
| train_0/avg_q             | -12.873483897930592    |
| train_0/current_q         | -7.130831421327775     |
| train_0/fw_bonus          | -0.9990721672773362    |
| train_0/fw_loss           | 0.00022092317158239894 |
| train_0/mu_grads          | -0.10779910292476416   |
| train_0/mu_grads_std      | 0.4964009128510952     |
| train_0/mu_loss           | 6.938813096641887      |
| train_0/next_q            | -6.821559850705045     |
| train_0/q_grads           | 0.03001838568598032    |
| train_0/q_grads_std       | 0.3112847343087196     |
| train_0/q_loss            | 0.4968240048805786     |
| train_0/reward            | -0.8083495609731471    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.00927734375          |
| train_0/target_q          | -7.130689433763213     |
| train_1/avg_q             | -16.585758034134578    |
| train_1/current_q         | -15.98208221864021     |
| train_1/fw_bonus          | -0.995426693558693     |
| train_1/fw_loss           | 0.001594113642931916   |
| train_1/mu_grads          | 0.019959809258580208   |
| train_1/mu_grads_std      | 0.08689817041158676    |
| train_1/mu_loss           | 17.832657009557746     |
| train_1/n_subgoals        | 2677.0                 |
| train_1/next_q            | -17.804883413441747    |
| train_1/q_grads           | 0.018625243147835136   |
| train_1/q_grads_std       | 0.4557728961110115     |
| train_1/q_loss            | 2.8849935700675315     |
| train_1/reward            | -1.7068686189464644    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.00546875             |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.32461710870377286    |
| train_1/target_q          | -16.15968426898936     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 16
Time for epoch 16: 775.74. Rollout time: 539.68, Training time: 235.95
Evaluating epoch 16
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
-----------------------------------------------------
| epoch                     | 16                    |
| policy/steps              | 1264881.0             |
| test/episodes             | 425.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -2.2681488326812635   |
| test_1/avg_q              | -18.773043131300952   |
| test_1/n_subgoals         | 6819.0                |
| test_1/subgoal_succ_rate  | 0.957765068191817     |
| train/episodes            | 1700.0                |
| train/success_rate        | 0.07                  |
| train_0/avg_q             | -12.740707085940082   |
| train_0/current_q         | -6.343337787258525    |
| train_0/fw_bonus          | -0.9990816652774811   |
| train_0/fw_loss           | 0.0002187109370424878 |
| train_0/mu_grads          | -0.11212398093193769  |
| train_0/mu_grads_std      | 0.5064869076013565    |
| train_0/mu_loss           | 6.256841486589636     |
| train_0/next_q            | -6.063927924391433    |
| train_0/q_grads           | 0.031088824197649956  |
| train_0/q_grads_std       | 0.31818490400910376   |
| train_0/q_loss            | 0.9738017618994054    |
| train_0/reward            | -0.8113816525663424   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0154541015625       |
| train_0/target_q          | -6.29402490172567     |
| train_1/avg_q             | -17.283231915544334   |
| train_1/current_q         | -16.0919954660261     |
| train_1/fw_bonus          | -0.9953849017620087   |
| train_1/fw_loss           | 0.0016041550348745658 |
| train_1/mu_grads          | 0.019959809258580208  |
| train_1/mu_grads_std      | 0.08689817041158676   |
| train_1/mu_loss           | 17.900109854987896    |
| train_1/n_subgoals        | 2642.0                |
| train_1/next_q            | -17.871366857306423   |
| train_1/q_grads           | 0.018277026154100894  |
| train_1/q_grads_std       | 0.46253192946314814   |
| train_1/q_loss            | 2.7771820387119974    |
| train_1/reward            | -1.6917964579552063   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0052978515625       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.41521574564723696   |
| train_1/target_q          | -16.25617536452167    |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 17
Time for epoch 17: 761.03. Rollout time: 534.65, Training time: 226.26
Evaluating epoch 17
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
-----------------------------------------------------
| epoch                     | 17                    |
| policy/steps              | 1335474.0             |
| test/episodes             | 450.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -0.5250524496819171   |
| test_1/avg_q              | -18.380459516278115   |
| test_1/n_subgoals         | 3328.0                |
| test_1/subgoal_succ_rate  | 0.8894230769230769    |
| train/episodes            | 1800.0                |
| train/success_rate        | 0.05                  |
| train_0/avg_q             | -13.062359559628568   |
| train_0/current_q         | -3.154741863519301    |
| train_0/fw_bonus          | -0.9991257712244987   |
| train_0/fw_loss           | 0.0002084363590256544 |
| train_0/mu_grads          | -0.11217914689332247  |
| train_0/mu_grads_std      | 0.5169565841555596    |
| train_0/mu_loss           | 2.956370038992006     |
| train_0/next_q            | -2.9216324595145524   |
| train_0/q_grads           | 0.031413943693041804  |
| train_0/q_grads_std       | 0.3255398370325565    |
| train_0/q_loss            | 0.7233592891398944    |
| train_0/reward            | -0.8124779217403557   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0137939453125       |
| train_0/target_q          | -3.5120583789309734   |
| train_1/avg_q             | -17.42112961573167    |
| train_1/current_q         | -16.316825183359075   |
| train_1/fw_bonus          | -0.9957881346344948   |
| train_1/fw_loss           | 0.0015072466572746634 |
| train_1/mu_grads          | 0.019959809258580208  |
| train_1/mu_grads_std      | 0.08689817041158676   |
| train_1/mu_loss           | 18.1629140752338      |
| train_1/n_subgoals        | 2641.0                |
| train_1/next_q            | -18.139827301205763   |
| train_1/q_grads           | 0.017947700573131442  |
| train_1/q_grads_std       | 0.4714435741305351    |
| train_1/q_loss            | 2.341679661842122     |
| train_1/reward            | -1.6833799024519975   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0052734375          |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.3718288527073078    |
| train_1/target_q          | -16.4931816391446     |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 18
Time for epoch 18: 754.16. Rollout time: 528.68, Training time: 225.36
Evaluating epoch 18
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 18                     |
| policy/steps              | 1406511.0              |
| test/episodes             | 475.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -1.3675652588874434    |
| test_1/avg_q              | -17.28492150872468     |
| test_1/n_subgoals         | 17719.0                |
| test_1/subgoal_succ_rate  | 0.9997742536260511     |
| train/episodes            | 1900.0                 |
| train/success_rate        | 0.06                   |
| train_0/avg_q             | -9.961192960276147     |
| train_0/current_q         | -5.334222532332849     |
| train_0/fw_bonus          | -0.9991281554102898    |
| train_0/fw_loss           | 0.00020787655375897884 |
| train_0/mu_grads          | -0.11061414517462254   |
| train_0/mu_grads_std      | 0.5278348952531815     |
| train_0/mu_loss           | 5.139260206079895      |
| train_0/next_q            | -4.960131939373398     |
| train_0/q_grads           | 0.031555553711950776   |
| train_0/q_grads_std       | 0.33279079720377924    |
| train_0/q_loss            | 0.5136000258398661     |
| train_0/reward            | -0.8128087132572546    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.0114501953125        |
| train_0/target_q          | -5.239658292650074     |
| train_1/avg_q             | -17.537633554508687    |
| train_1/current_q         | -15.931589140781691    |
| train_1/fw_bonus          | -0.9957529678940773    |
| train_1/fw_loss           | 0.0015156974928686394  |
| train_1/mu_grads          | 0.019959809258580208   |
| train_1/mu_grads_std      | 0.08689817041158676    |
| train_1/mu_loss           | 17.56444267645595      |
| train_1/n_subgoals        | 2640.0                 |
| train_1/next_q            | -17.558772300673454    |
| train_1/q_grads           | 0.016368095483630895   |
| train_1/q_grads_std       | 0.4776571117341518     |
| train_1/q_loss            | 2.762317319574234      |
| train_1/reward            | -1.6951787277030235    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.005908203125         |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.3704545454545455     |
| train_1/target_q          | -16.099978389087738    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 19
Time for epoch 19: 761.24. Rollout time: 534.49, Training time: 226.66
Evaluating epoch 19
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 19                     |
| policy/steps              | 1476099.0              |
| test/episodes             | 500.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -0.9398622718851134    |
| test_1/avg_q              | -17.80598548674866     |
| test_1/n_subgoals         | 7727.0                 |
| test_1/subgoal_succ_rate  | 0.9843406237867219     |
| train/episodes            | 2000.0                 |
| train/success_rate        | 0.08                   |
| train_0/avg_q             | -12.478303045833766    |
| train_0/current_q         | -0.47848925145150983   |
| train_0/fw_bonus          | -0.9991310030221939    |
| train_0/fw_loss           | 0.00020721548462461216 |
| train_0/mu_grads          | -0.11426815409213305   |
| train_0/mu_grads_std      | 0.5408449754118919     |
| train_0/mu_loss           | 0.4648188239916521     |
| train_0/next_q            | -0.42195345187872296   |
| train_0/q_grads           | 0.03295529540628195    |
| train_0/q_grads_std       | 0.34155151098966596    |
| train_0/q_loss            | 0.631002185796467      |
| train_0/reward            | -0.811870190690388     |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.016162109375         |
| train_0/target_q          | -1.1384121399606446    |
| train_1/avg_q             | -16.811667614715688    |
| train_1/current_q         | -15.718414562652175    |
| train_1/fw_bonus          | -0.9961863964796066    |
| train_1/fw_loss           | 0.0014115244819549843  |
| train_1/mu_grads          | 0.019959809258580208   |
| train_1/mu_grads_std      | 0.08689817041158676    |
| train_1/mu_loss           | 17.453728636148593     |
| train_1/n_subgoals        | 2614.0                 |
| train_1/next_q            | -17.440467460903456    |
| train_1/q_grads           | 0.015065116458572447   |
| train_1/q_grads_std       | 0.48420563265681266    |
| train_1/q_loss            | 2.8798247108173824     |
| train_1/reward            | -1.698898866151285     |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.00625                |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.35807192042846214    |
| train_1/target_q          | -15.880692377346628    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 20
Time for epoch 20: 872.30. Rollout time: 611.63, Training time: 260.55
Evaluating epoch 20
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
-----------------------------------------------------
| epoch                     | 20                    |
| policy/steps              | 1549999.0             |
| test/episodes             | 525.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -0.6056278256555431   |
| test_1/avg_q              | -17.378552504605068   |
| test_1/n_subgoals         | 16760.0               |
| test_1/subgoal_succ_rate  | 0.9997016706443914    |
| train/episodes            | 2100.0                |
| train/success_rate        | 0.02                  |
| train_0/avg_q             | -10.063956949429768   |
| train_0/current_q         | -7.045438668998242    |
| train_0/fw_bonus          | -0.9991096049547196   |
| train_0/fw_loss           | 0.0002122021312970901 |
| train_0/mu_grads          | -0.11768757868558169  |
| train_0/mu_grads_std      | 0.5515677720308304    |
| train_0/mu_loss           | 6.8170665636737455    |
| train_0/next_q            | -6.730453817241299    |
| train_0/q_grads           | 0.03440859755501151   |
| train_0/q_grads_std       | 0.3487399183213711    |
| train_0/q_loss            | 0.7468816588001295    |
| train_0/reward            | -0.8108463048905833   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0213134765625       |
| train_0/target_q          | -7.061893598214001    |
| train_1/avg_q             | -17.006863883679006   |
| train_1/current_q         | -15.665209217201527   |
| train_1/fw_bonus          | -0.9961896941065789   |
| train_1/fw_loss           | 0.0014107337599853053 |
| train_1/mu_grads          | 0.019959809258580208  |
| train_1/mu_grads_std      | 0.08689817041158676   |
| train_1/mu_loss           | 17.56467092807507     |
| train_1/n_subgoals        | 2693.0                |
| train_1/next_q            | -17.56097375364221    |
| train_1/q_grads           | 0.013792919460684061  |
| train_1/q_grads_std       | 0.4908783033490181    |
| train_1/q_loss            | 3.4699711568479232    |
| train_1/reward            | -1.6533560775351361   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.006298828125        |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.3330857779428147    |
| train_1/target_q          | -15.837938446474539   |
-----------------------------------------------------
Saving periodic policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_20.pkl ...
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 21
Time for epoch 21: 813.33. Rollout time: 579.74, Training time: 233.50
Evaluating epoch 21
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 21                     |
| policy/steps              | 1621789.0              |
| test/episodes             | 550.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -2.07800713138472      |
| test_1/avg_q              | -17.729407777753654    |
| test_1/n_subgoals         | 14389.0                |
| test_1/subgoal_succ_rate  | 0.9995135172701369     |
| train/episodes            | 2200.0                 |
| train/success_rate        | 0.04                   |
| train_0/avg_q             | -12.006571053067079    |
| train_0/current_q         | -6.72472946528311      |
| train_0/fw_bonus          | -0.9991142600774765    |
| train_0/fw_loss           | 0.00021111782407388092 |
| train_0/mu_grads          | -0.1207748157903552    |
| train_0/mu_grads_std      | 0.5632295548915863     |
| train_0/mu_loss           | 6.534516481129678      |
| train_0/next_q            | -6.372266336025282     |
| train_0/q_grads           | 0.03504707682877779    |
| train_0/q_grads_std       | 0.35363869890570643    |
| train_0/q_loss            | 0.48387515717352236    |
| train_0/reward            | -0.8088710930620436    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.0142333984375        |
| train_0/target_q          | -6.6473706217800625    |
| train_1/avg_q             | -17.135143463356222    |
| train_1/current_q         | -15.533738787006635    |
| train_1/fw_bonus          | -0.9956860631704331    |
| train_1/fw_loss           | 0.0015317775018047542  |
| train_1/mu_grads          | 0.019959809258580208   |
| train_1/mu_grads_std      | 0.08689817041158676    |
| train_1/mu_loss           | 17.637449924752637     |
| train_1/n_subgoals        | 2639.0                 |
| train_1/next_q            | -17.59481566453716     |
| train_1/q_grads           | 0.012446940876543522   |
| train_1/q_grads_std       | 0.4976569302380085     |
| train_1/q_loss            | 3.1941684624984727     |
| train_1/reward            | -1.6449743765224412    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0064697265625        |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.3387646835922698     |
| train_1/target_q          | -15.702828982062991    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 22
Time for epoch 22: 876.16. Rollout time: 625.32, Training time: 250.72
Evaluating epoch 22
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
-----------------------------------------------------
| epoch                     | 22                    |
| policy/steps              | 1692434.0             |
| test/episodes             | 575.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -1.0728991309928981   |
| test_1/avg_q              | -20.674373471274674   |
| test_1/n_subgoals         | 675.0                 |
| test_1/subgoal_succ_rate  | 0.0                   |
| train/episodes            | 2300.0                |
| train/success_rate        | 0.04                  |
| train_0/avg_q             | -14.219433179798365   |
| train_0/current_q         | -6.427375577958225    |
| train_0/fw_bonus          | -0.9991063624620438   |
| train_0/fw_loss           | 0.0002129557957232464 |
| train_0/mu_grads          | -0.12299724500626326  |
| train_0/mu_grads_std      | 0.573784913122654     |
| train_0/mu_loss           | 6.195236452872338     |
| train_0/next_q            | -6.081531322604236    |
| train_0/q_grads           | 0.03503760201856494   |
| train_0/q_grads_std       | 0.3591779127717018    |
| train_0/q_loss            | 0.7117991596386762    |
| train_0/reward            | -0.8126391211153532   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0169677734375       |
| train_0/target_q          | -6.405174144403182    |
| train_1/avg_q             | -17.506399577957485   |
| train_1/current_q         | -15.724961308827739   |
| train_1/fw_bonus          | -0.996071594953537    |
| train_1/fw_loss           | 0.0014391208038432523 |
| train_1/mu_grads          | 0.019959809258580208  |
| train_1/mu_grads_std      | 0.08689817041158676   |
| train_1/mu_loss           | 18.126571212077824    |
| train_1/n_subgoals        | 2655.0                |
| train_1/next_q            | -18.068456361858253   |
| train_1/q_grads           | 0.01189144141972065   |
| train_1/q_grads_std       | 0.5073563635349274    |
| train_1/q_loss            | 4.107277219853822     |
| train_1/reward            | -1.6596446780000407   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0053955078125       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.3728813559322034    |
| train_1/target_q          | -15.896805467907981   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 23
Time for epoch 23: 876.22. Rollout time: 628.29, Training time: 247.83
Evaluating epoch 23
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 23                     |
| policy/steps              | 1768286.0              |
| test/episodes             | 600.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -1.5042824164884796    |
| test_1/avg_q              | -19.543550229137054    |
| test_1/n_subgoals         | 16076.0                |
| test_1/subgoal_succ_rate  | 0.9982582732022891     |
| train/episodes            | 2400.0                 |
| train/success_rate        | 0.04                   |
| train_0/avg_q             | -11.38926907354242     |
| train_0/current_q         | -6.9713822715170295    |
| train_0/fw_bonus          | -0.9991363510489464    |
| train_0/fw_loss           | 0.00020597090842784383 |
| train_0/mu_grads          | -0.12009338960051537   |
| train_0/mu_grads_std      | 0.5820254325866699     |
| train_0/mu_loss           | 6.7517910144376945     |
| train_0/next_q            | -6.595658093942456     |
| train_0/q_grads           | 0.035726049821823834   |
| train_0/q_grads_std       | 0.3634274087846279     |
| train_0/q_loss            | 0.5253425814890644     |
| train_0/reward            | -0.8143997169499926    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.020166015625         |
| train_0/target_q          | -6.899042625731336     |
| train_1/avg_q             | -17.66811383213351     |
| train_1/current_q         | -16.086370038074396    |
| train_1/fw_bonus          | -0.9965023711323738    |
| train_1/fw_loss           | 0.0013355854491237551  |
| train_1/mu_grads          | 0.019959809258580208   |
| train_1/mu_grads_std      | 0.08689817041158676    |
| train_1/mu_loss           | 18.425007329668528     |
| train_1/n_subgoals        | 2662.0                 |
| train_1/next_q            | -18.3996907028535      |
| train_1/q_grads           | 0.010012639150954784   |
| train_1/q_grads_std       | 0.5152821630239487     |
| train_1/q_loss            | 3.4275588651973594     |
| train_1/reward            | -1.6684172019886319    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0062744140625        |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.28399699474079637    |
| train_1/target_q          | -16.289882546855146    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 24
Time for epoch 24: 926.98. Rollout time: 657.74, Training time: 269.09
Evaluating epoch 24
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 24                     |
| policy/steps              | 1840899.0              |
| test/episodes             | 625.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -1.0949637785976647    |
| test_1/avg_q              | -21.50964303671917     |
| test_1/n_subgoals         | 16665.0                |
| test_1/subgoal_succ_rate  | 0.9967596759675967     |
| train/episodes            | 2500.0                 |
| train/success_rate        | 0.02                   |
| train_0/avg_q             | -14.342871694866083    |
| train_0/current_q         | -6.926165274980003     |
| train_0/fw_bonus          | -0.9991227984428406    |
| train_0/fw_loss           | 0.00020913088337692898 |
| train_0/mu_grads          | -0.11988620441406965   |
| train_0/mu_grads_std      | 0.5895810827612877     |
| train_0/mu_loss           | 6.667710738825276      |
| train_0/next_q            | -6.562980430885612     |
| train_0/q_grads           | 0.03606970589607954    |
| train_0/q_grads_std       | 0.3677633799612522     |
| train_0/q_loss            | 0.42033470477735124    |
| train_0/reward            | -0.8158577791800781    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.0216796875           |
| train_0/target_q          | -6.9440854310545745    |
| train_1/avg_q             | -17.80664793139737     |
| train_1/current_q         | -16.72955697156497     |
| train_1/fw_bonus          | -0.9962431788444519    |
| train_1/fw_loss           | 0.0013978814007714392  |
| train_1/mu_grads          | 0.019959809258580208   |
| train_1/mu_grads_std      | 0.08689817041158676    |
| train_1/mu_loss           | 19.063227668285435     |
| train_1/n_subgoals        | 2662.0                 |
| train_1/next_q            | -19.037387351181952    |
| train_1/q_grads           | 0.009093728754669428   |
| train_1/q_grads_std       | 0.5246802911162376     |
| train_1/q_loss            | 3.0166457968441884     |
| train_1/reward            | -1.6602675810485379    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0066162109375        |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.31930879038317056    |
| train_1/target_q          | -16.913473701578443    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 25
Time for epoch 25: 758.69. Rollout time: 526.51, Training time: 232.07
Evaluating epoch 25
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 25                     |
| policy/steps              | 1910238.0              |
| test/episodes             | 650.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -1.2548035858349535    |
| test_1/avg_q              | -19.03978694030475     |
| test_1/n_subgoals         | 7353.0                 |
| test_1/subgoal_succ_rate  | 0.9737522099823201     |
| train/episodes            | 2600.0                 |
| train/success_rate        | 0.03                   |
| train_0/avg_q             | -13.899382884874889    |
| train_0/current_q         | -8.28557289411524      |
| train_0/fw_bonus          | -0.9991688951849937    |
| train_0/fw_loss           | 0.00019839021770167163 |
| train_0/mu_grads          | -0.12023214977234602   |
| train_0/mu_grads_std      | 0.5974916860461235     |
| train_0/mu_loss           | 8.104557878560119      |
| train_0/next_q            | -7.943465613187934     |
| train_0/q_grads           | 0.037533174734562635   |
| train_0/q_grads_std       | 0.37318446561694146    |
| train_0/q_loss            | 0.44649279343973164    |
| train_0/reward            | -0.8205439161909454    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.0150146484375        |
| train_0/target_q          | -8.325937158728067     |
| train_1/avg_q             | -18.73745823913416     |
| train_1/current_q         | -16.950184495382423    |
| train_1/fw_bonus          | -0.9957215741276741    |
| train_1/fw_loss           | 0.0015232426667353138  |
| train_1/mu_grads          | 0.019959809258580208   |
| train_1/mu_grads_std      | 0.08689817041158676    |
| train_1/mu_loss           | 19.00945469032819      |
| train_1/n_subgoals        | 2661.0                 |
| train_1/next_q            | -18.98241938234184     |
| train_1/q_grads           | 0.008105930918827654   |
| train_1/q_grads_std       | 0.5314591243863106     |
| train_1/q_loss            | 3.2301212989383266     |
| train_1/reward            | -1.7110443783807567    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.005859375            |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.3769259676813228     |
| train_1/target_q          | -17.138135478617606    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 26
Time for epoch 26: 770.18. Rollout time: 538.15, Training time: 231.89
Evaluating epoch 26
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
-----------------------------------------------------
| epoch                     | 26                    |
| policy/steps              | 1982405.0             |
| test/episodes             | 675.0                 |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -1.4909889920442363   |
| test_1/avg_q              | -20.404040609656775   |
| test_1/n_subgoals         | 11452.0               |
| test_1/subgoal_succ_rate  | 0.9939748515543136    |
| train/episodes            | 2700.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -16.808660113789422   |
| train_0/current_q         | -6.169075174625476    |
| train_0/fw_bonus          | -0.9992203429341316   |
| train_0/fw_loss           | 0.0001864012549049221 |
| train_0/mu_grads          | -0.12351145576685667  |
| train_0/mu_grads_std      | 0.6042732477188111    |
| train_0/mu_loss           | 5.9679145420178       |
| train_0/next_q            | -5.787176167616576    |
| train_0/q_grads           | 0.03799068005755544   |
| train_0/q_grads_std       | 0.3774213492870331    |
| train_0/q_loss            | 0.5063696414382047    |
| train_0/reward            | -0.8252815601590555   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.018994140625        |
| train_0/target_q          | -6.147435851476107    |
| train_1/avg_q             | -17.56721314265376    |
| train_1/current_q         | -17.32412773579542    |
| train_1/fw_bonus          | -0.9959858387708664   |
| train_1/fw_loss           | 0.0014597309229429812 |
| train_1/mu_grads          | 0.019959809258580208  |
| train_1/mu_grads_std      | 0.08689817041158676   |
| train_1/mu_loss           | 19.121881549282783    |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -19.078881432246458   |
| train_1/q_grads           | 0.007042696024291218  |
| train_1/q_grads_std       | 0.5361861005425453    |
| train_1/q_loss            | 2.8633275532437987    |
| train_1/reward            | -1.7056833441827621   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0055908203125       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.3511111111111111    |
| train_1/target_q          | -17.505198623626768   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 27
Time for epoch 27: 749.46. Rollout time: 518.54, Training time: 230.82
Evaluating epoch 27
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 27                     |
| policy/steps              | 2052402.0              |
| test/episodes             | 700.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -1.5337950809655776    |
| test_1/avg_q              | -21.65153882580169     |
| test_1/n_subgoals         | 15326.0                |
| test_1/subgoal_succ_rate  | 0.99797729348819       |
| train/episodes            | 2800.0                 |
| train/success_rate        | 0.04                   |
| train_0/avg_q             | -13.185487642731436    |
| train_0/current_q         | -3.92451650508965      |
| train_0/fw_bonus          | -0.9992314293980599    |
| train_0/fw_loss           | 0.00018381962945568375 |
| train_0/mu_grads          | -0.12668081521987914   |
| train_0/mu_grads_std      | 0.6105088338255882     |
| train_0/mu_loss           | 3.8258911159861597     |
| train_0/next_q            | -3.7112955683434556    |
| train_0/q_grads           | 0.038106448110193016   |
| train_0/q_grads_std       | 0.3808517560362816     |
| train_0/q_loss            | 0.8477941225033281     |
| train_0/reward            | -0.825580069629359     |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.0336669921875        |
| train_0/target_q          | -3.9600052250069107    |
| train_1/avg_q             | -18.884110243818014    |
| train_1/current_q         | -17.159304505936312    |
| train_1/fw_bonus          | -0.99649318754673      |
| train_1/fw_loss           | 0.0013377944793319329  |
| train_1/mu_grads          | 0.019959809258580208   |
| train_1/mu_grads_std      | 0.08689817041158676    |
| train_1/mu_loss           | 18.95029917988857      |
| train_1/n_subgoals        | 2651.0                 |
| train_1/next_q            | -18.866678634498243    |
| train_1/q_grads           | 0.005727302515879273   |
| train_1/q_grads_std       | 0.5428337141871452     |
| train_1/q_loss            | 3.0678489480130446     |
| train_1/reward            | -1.6973432695223891    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.007177734375         |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.36099585062240663    |
| train_1/target_q          | -17.364828323862618    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 28
Time for epoch 28: 764.60. Rollout time: 521.72, Training time: 242.78
Evaluating epoch 28
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 28                     |
| policy/steps              | 2119176.0              |
| test/episodes             | 725.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -1.31564916285822      |
| test_1/avg_q              | -24.579803995429458    |
| test_1/n_subgoals         | 7930.0                 |
| test_1/subgoal_succ_rate  | 0.9727616645649433     |
| train/episodes            | 2900.0                 |
| train/success_rate        | 0.06                   |
| train_0/avg_q             | -10.063455630739263    |
| train_0/current_q         | -5.658455403847237     |
| train_0/fw_bonus          | -0.9992029711604118    |
| train_0/fw_loss           | 0.00019044788496103138 |
| train_0/mu_grads          | -0.13128449730575084   |
| train_0/mu_grads_std      | 0.6143713027238846     |
| train_0/mu_loss           | 5.4490130782161765     |
| train_0/next_q            | -5.286954126864151     |
| train_0/q_grads           | 0.040471992641687396   |
| train_0/q_grads_std       | 0.38675369545817373    |
| train_0/q_loss            | 0.7528826283016024     |
| train_0/reward            | -0.8274945552482678    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.03623046875          |
| train_0/target_q          | -5.526929155820494     |
| train_1/avg_q             | -20.18076884969051     |
| train_1/current_q         | -17.41799666850925     |
| train_1/fw_bonus          | -0.9962162658572197    |
| train_1/fw_loss           | 0.0014043500850675628  |
| train_1/mu_grads          | 0.019959809258580208   |
| train_1/mu_grads_std      | 0.08689817041158676    |
| train_1/mu_loss           | 19.15888416244868      |
| train_1/n_subgoals        | 2653.0                 |
| train_1/next_q            | -19.089591070994935    |
| train_1/q_grads           | 0.0038597136503085495  |
| train_1/q_grads_std       | 0.5502631559967994     |
| train_1/q_loss            | 3.457943830775124      |
| train_1/reward            | -1.6741091046496877    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.00634765625          |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.4334715416509612     |
| train_1/target_q          | -17.610711130813996    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 29
Time for epoch 29: 702.15. Rollout time: 466.60, Training time: 235.41
Evaluating epoch 29
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 29                     |
| policy/steps              | 2185043.0              |
| test/episodes             | 750.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -1.2955987550668837    |
| test_1/avg_q              | -22.041405172467112    |
| test_1/n_subgoals         | 675.0                  |
| test_1/subgoal_succ_rate  | 0.0                    |
| train/episodes            | 3000.0                 |
| train/success_rate        | 0.03                   |
| train_0/avg_q             | -9.645755102360011     |
| train_0/current_q         | -7.415480914175137     |
| train_0/fw_bonus          | -0.9992214515805244    |
| train_0/fw_loss           | 0.00018614489235915243 |
| train_0/mu_grads          | -0.13430981300771236   |
| train_0/mu_grads_std      | 0.6197606533765793     |
| train_0/mu_loss           | 7.205822372045624      |
| train_0/next_q            | -7.031535100405293     |
| train_0/q_grads           | 0.03968487540259957    |
| train_0/q_grads_std       | 0.3904063329100609     |
| train_0/q_loss            | 0.5112425320597991     |
| train_0/reward            | -0.8354383604244504    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.0218017578125        |
| train_0/target_q          | -7.401293894889653     |
| train_1/avg_q             | -22.308746558137283    |
| train_1/current_q         | -17.21949460668536     |
| train_1/fw_bonus          | -0.9960765674710274    |
| train_1/fw_loss           | 0.0014379256725078448  |
| train_1/mu_grads          | 0.019959809258580208   |
| train_1/mu_grads_std      | 0.08689817041158676    |
| train_1/mu_loss           | 18.704871160025043     |
| train_1/n_subgoals        | 2640.0                 |
| train_1/next_q            | -18.637911656475023    |
| train_1/q_grads           | 0.002917781553696841   |
| train_1/q_grads_std       | 0.5567320182919502     |
| train_1/q_loss            | 3.6405486524894144     |
| train_1/reward            | -1.6212682583740388    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0068603515625        |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.4715909090909091     |
| train_1/target_q          | -17.38694415149541     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 30
Time for epoch 30: 784.66. Rollout time: 523.96, Training time: 260.54
Evaluating epoch 30
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 30                     |
| policy/steps              | 2253994.0              |
| test/episodes             | 775.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -2.242499927908431     |
| test_1/avg_q              | -21.216811131536186    |
| test_1/n_subgoals         | 10656.0                |
| test_1/subgoal_succ_rate  | 0.9896771771771772     |
| train/episodes            | 3100.0                 |
| train/success_rate        | 0.05                   |
| train_0/avg_q             | -12.280506411601197    |
| train_0/current_q         | -7.222103357138425     |
| train_0/fw_bonus          | -0.9992849126458168    |
| train_0/fw_loss           | 0.00017136044516519178 |
| train_0/mu_grads          | -0.13413110449910165   |
| train_0/mu_grads_std      | 0.6223554328083992     |
| train_0/mu_loss           | 6.983034710801438      |
| train_0/next_q            | -6.829908587904884     |
| train_0/q_grads           | 0.0409623833373189     |
| train_0/q_grads_std       | 0.3975759260356426     |
| train_0/q_loss            | 0.4658709746277691     |
| train_0/reward            | -0.8373377657306265    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.026171875            |
| train_0/target_q          | -7.192147569313025     |
| train_1/avg_q             | -19.6875305840146      |
| train_1/current_q         | -16.433340552732336    |
| train_1/fw_bonus          | -0.9966815397143364    |
| train_1/fw_loss           | 0.001292524390737526   |
| train_1/mu_grads          | 0.019959809258580208   |
| train_1/mu_grads_std      | 0.08689817041158676    |
| train_1/mu_loss           | 17.68199419290185      |
| train_1/n_subgoals        | 2649.0                 |
| train_1/next_q            | -17.649752069937016    |
| train_1/q_grads           | 0.0015365036408184097  |
| train_1/q_grads_std       | 0.5615526273846626     |
| train_1/q_loss            | 3.191247520237625      |
| train_1/reward            | -1.6121103012366802    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.006298828125         |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.4163835409588524     |
| train_1/target_q          | -16.58626848889511     |
------------------------------------------------------
Saving periodic policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_30.pkl ...
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 31
Time for epoch 31: 692.35. Rollout time: 453.92, Training time: 238.32
Evaluating epoch 31
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 31                     |
| policy/steps              | 2319467.0              |
| test/episodes             | 800.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -1.8840208478847047    |
| test_1/avg_q              | -21.790838302174173    |
| test_1/n_subgoals         | 15953.0                |
| test_1/subgoal_succ_rate  | 0.9957374788441046     |
| train/episodes            | 3200.0                 |
| train/success_rate        | 0.0                    |
| train_0/avg_q             | -13.644535961483479    |
| train_0/current_q         | -7.258035053666146     |
| train_0/fw_bonus          | -0.999353238940239     |
| train_0/fw_loss           | 0.00015543868958047823 |
| train_0/mu_grads          | -0.1359250582754612    |
| train_0/mu_grads_std      | 0.6261536747217178     |
| train_0/mu_loss           | 7.027138265526735      |
| train_0/next_q            | -6.858183439800195     |
| train_0/q_grads           | 0.04014525441452861    |
| train_0/q_grads_std       | 0.40519582107663155    |
| train_0/q_loss            | 0.5027234466644734     |
| train_0/reward            | -0.8436097293269995    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.051123046875         |
| train_0/target_q          | -7.215562611333413     |
| train_1/avg_q             | -20.303725570291494    |
| train_1/current_q         | -16.251568781286842    |
| train_1/fw_bonus          | -0.9965517550706864    |
| train_1/fw_loss           | 0.001323717163177207   |
| train_1/mu_grads          | 0.019959809258580208   |
| train_1/mu_grads_std      | 0.08689817041158676    |
| train_1/mu_loss           | 17.427054797263867     |
| train_1/n_subgoals        | 2700.0                 |
| train_1/next_q            | -17.374426898557463    |
| train_1/q_grads           | 0.0012606803851667792  |
| train_1/q_grads_std       | 0.5684281498193741     |
| train_1/q_loss            | 3.0087124762561896     |
| train_1/reward            | -1.5566010722395731    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.00673828125          |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.4777777777777778     |
| train_1/target_q          | -16.419013590154528    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 32
Time for epoch 32: 681.43. Rollout time: 445.93, Training time: 235.41
Evaluating epoch 32
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 32                     |
| policy/steps              | 2383936.0              |
| test/episodes             | 825.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -1.601968090462523     |
| test_1/avg_q              | -19.73097050993248     |
| test_1/n_subgoals         | 16016.0                |
| test_1/subgoal_succ_rate  | 0.9964410589410589     |
| train/episodes            | 3300.0                 |
| train/success_rate        | 0.05                   |
| train_0/avg_q             | -13.561304943760852    |
| train_0/current_q         | -7.462162135092953     |
| train_0/fw_bonus          | -0.999374495446682     |
| train_0/fw_loss           | 0.00015048800869408297 |
| train_0/mu_grads          | -0.13913130536675453   |
| train_0/mu_grads_std      | 0.6321578681468963     |
| train_0/mu_loss           | 7.235288972154455      |
| train_0/next_q            | -7.063973287478578     |
| train_0/q_grads           | 0.03973320676013827    |
| train_0/q_grads_std       | 0.4099486730992794     |
| train_0/q_loss            | 0.45155985706018253    |
| train_0/reward            | -0.8462204517141799    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.0424072265625        |
| train_0/target_q          | -7.439274293324113     |
| train_1/avg_q             | -20.123408334500816    |
| train_1/current_q         | -16.238472389746033    |
| train_1/fw_bonus          | -0.996313638985157     |
| train_1/fw_loss           | 0.0013809466327074915  |
| train_1/mu_grads          | 0.019959809258580208   |
| train_1/mu_grads_std      | 0.08689817041158676    |
| train_1/mu_loss           | 17.28840715470193      |
| train_1/n_subgoals        | 2665.0                 |
| train_1/next_q            | -17.264167501774182    |
| train_1/q_grads           | 0.0004716450748674106  |
| train_1/q_grads_std       | 0.5747827097773552     |
| train_1/q_loss            | 3.3373633809580587     |
| train_1/reward            | -1.5116211726162874    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.006005859375         |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.4878048780487805     |
| train_1/target_q          | -16.407315911056315    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 33
Time for epoch 33: 676.04. Rollout time: 442.10, Training time: 233.82
Evaluating epoch 33
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 33                     |
| policy/steps              | 2447252.0              |
| test/episodes             | 850.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -3.4639215363377263    |
| test_1/avg_q              | -21.199505041624096    |
| test_1/n_subgoals         | 17193.0                |
| test_1/subgoal_succ_rate  | 0.9999418367940441     |
| train/episodes            | 3400.0                 |
| train/success_rate        | 0.07                   |
| train_0/avg_q             | -14.440851253128866    |
| train_0/current_q         | -7.289413277310004     |
| train_0/fw_bonus          | -0.9993954986333847    |
| train_0/fw_loss           | 0.00014559598075720715 |
| train_0/mu_grads          | -0.14007904157042503   |
| train_0/mu_grads_std      | 0.6368514493107795     |
| train_0/mu_loss           | 7.139568107380123      |
| train_0/next_q            | -6.911154690586743     |
| train_0/q_grads           | 0.0400672328658402     |
| train_0/q_grads_std       | 0.41455106139183046    |
| train_0/q_loss            | 0.5917678789260734     |
| train_0/reward            | -0.8499296810776287    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.0619384765625        |
| train_0/target_q          | -7.269479095752177     |
| train_1/avg_q             | -20.503976234466066    |
| train_1/current_q         | -16.596314080742268    |
| train_1/fw_bonus          | -0.9971004590392113    |
| train_1/fw_loss           | 0.0011918396514374763  |
| train_1/mu_grads          | 0.019959809258580208   |
| train_1/mu_grads_std      | 0.08689817041158676    |
| train_1/mu_loss           | 17.700523811417572     |
| train_1/n_subgoals        | 2606.0                 |
| train_1/next_q            | -17.688530734193634    |
| train_1/q_grads           | 9.310303912570816e-05  |
| train_1/q_grads_std       | 0.5814188480377197     |
| train_1/q_loss            | 2.9274454735011948     |
| train_1/reward            | -1.4970609759846412    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.006298828125         |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.49769762087490405    |
| train_1/target_q          | -16.768064623075674    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 34
Time for epoch 34: 688.60. Rollout time: 457.48, Training time: 231.00
Evaluating epoch 34
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 34                     |
| policy/steps              | 2512641.0              |
| test/episodes             | 875.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -0.9130751839812525    |
| test_1/avg_q              | -21.9771525609616      |
| test_1/n_subgoals         | 10812.0                |
| test_1/subgoal_succ_rate  | 0.9956529781724011     |
| train/episodes            | 3500.0                 |
| train/success_rate        | 0.07                   |
| train_0/avg_q             | -15.558017341000724    |
| train_0/current_q         | -7.8232924127403205    |
| train_0/fw_bonus          | -0.9994432657957077    |
| train_0/fw_loss           | 0.00013446636621665674 |
| train_0/mu_grads          | -0.13914060816168786   |
| train_0/mu_grads_std      | 0.6407442346215249     |
| train_0/mu_loss           | 7.566431073070952      |
| train_0/next_q            | -7.4283446037532554    |
| train_0/q_grads           | 0.03968363450840116    |
| train_0/q_grads_std       | 0.4197387047111988     |
| train_0/q_loss            | 0.4790679037113671     |
| train_0/reward            | -0.8530419459173573    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.081298828125         |
| train_0/target_q          | -7.817678333243633     |
| train_1/avg_q             | -20.248626354413137    |
| train_1/current_q         | -16.982666859838208    |
| train_1/fw_bonus          | -0.9972928002476692    |
| train_1/fw_loss           | 0.001145613464177586   |
| train_1/mu_grads          | 0.019959809258580208   |
| train_1/mu_grads_std      | 0.08689817041158676    |
| train_1/mu_loss           | 18.020890134536153     |
| train_1/n_subgoals        | 2606.0                 |
| train_1/next_q            | -18.00639323987279     |
| train_1/q_grads           | 1.634908809933222e-05  |
| train_1/q_grads_std       | 0.5893955945968627     |
| train_1/q_loss            | 2.414489304317029      |
| train_1/reward            | -1.4611586097416875    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.00576171875          |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.4535686876438987     |
| train_1/target_q          | -17.179963686393936    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 35
Time for epoch 35: 657.81. Rollout time: 426.65, Training time: 231.02
Evaluating epoch 35
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 35                     |
| policy/steps              | 2575887.0              |
| test/episodes             | 900.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -2.423849299836933     |
| test_1/avg_q              | -22.599682987032924    |
| test_1/n_subgoals         | 7696.0                 |
| test_1/subgoal_succ_rate  | 0.9533523908523909     |
| train/episodes            | 3600.0                 |
| train/success_rate        | 0.07                   |
| train_0/avg_q             | -12.50546904245504     |
| train_0/current_q         | -6.465900723155068     |
| train_0/fw_bonus          | -0.9994558706879616    |
| train_0/fw_loss           | 0.00013153242034604772 |
| train_0/mu_grads          | -0.13625328727066516   |
| train_0/mu_grads_std      | 0.6449385643005371     |
| train_0/mu_loss           | 6.211895233480797      |
| train_0/next_q            | -6.018862461218139     |
| train_0/q_grads           | 0.0380444067530334     |
| train_0/q_grads_std       | 0.42367520108819007    |
| train_0/q_loss            | 0.4779435201104921     |
| train_0/reward            | -0.8601283206575318    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.0431396484375        |
| train_0/target_q          | -6.422792139941521     |
| train_1/avg_q             | -21.13009981091148     |
| train_1/current_q         | -17.266078341596106    |
| train_1/fw_bonus          | -0.9971910506486893    |
| train_1/fw_loss           | 0.001170068192004692   |
| train_1/mu_grads          | 0.019959809258580208   |
| train_1/mu_grads_std      | 0.08689817041158676    |
| train_1/mu_loss           | 18.40857498166511      |
| train_1/n_subgoals        | 2622.0                 |
| train_1/next_q            | -18.380635370096186    |
| train_1/q_grads           | -0.0010648590978235006 |
| train_1/q_grads_std       | 0.5962705761194229     |
| train_1/q_loss            | 2.7264219749004646     |
| train_1/reward            | -1.4750359567609848    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.00703125             |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.4977116704805492     |
| train_1/target_q          | -17.46612430287727     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 36
Time for epoch 36: 685.71. Rollout time: 449.53, Training time: 236.06
Evaluating epoch 36
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 36                     |
| policy/steps              | 2639509.0              |
| test/episodes             | 925.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -0.12404321344428852   |
| test_1/avg_q              | -24.913488840826762    |
| test_1/n_subgoals         | 763.0                  |
| test_1/subgoal_succ_rate  | 0.11533420707732635    |
| train/episodes            | 3700.0                 |
| train/success_rate        | 0.1                    |
| train_0/avg_q             | -10.569018274723968    |
| train_0/current_q         | -3.5594493186345586    |
| train_0/fw_bonus          | -0.999484796822071     |
| train_0/fw_loss           | 0.00012479031356633642 |
| train_0/mu_grads          | -0.13763946667313576   |
| train_0/mu_grads_std      | 0.6504728838801384     |
| train_0/mu_loss           | 3.2356393738403426     |
| train_0/next_q            | -3.342924628429272     |
| train_0/q_grads           | 0.03735911473631859    |
| train_0/q_grads_std       | 0.429697797447443      |
| train_0/q_loss            | 1.5927151441415053     |
| train_0/reward            | -0.8612658370242571    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.1029296875           |
| train_0/target_q          | -3.943669913798318     |
| train_1/avg_q             | -22.01370859971053     |
| train_1/current_q         | -17.277548553340228    |
| train_1/fw_bonus          | -0.9971746549010276    |
| train_1/fw_loss           | 0.0011740094021661207  |
| train_1/mu_grads          | 0.019959809258580208   |
| train_1/mu_grads_std      | 0.08689817041158676    |
| train_1/mu_loss           | 18.44504846825435      |
| train_1/n_subgoals        | 2583.0                 |
| train_1/next_q            | -18.433500374267805    |
| train_1/q_grads           | -0.0025245113298296928 |
| train_1/q_grads_std       | 0.6010368674993515     |
| train_1/q_loss            | 3.2327997339445047     |
| train_1/reward            | -1.4665316640974198    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.007080078125         |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.4804490902051878     |
| train_1/target_q          | -17.470087323578404    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 37
Time for epoch 37: 798.33. Rollout time: 556.23, Training time: 241.99
Evaluating epoch 37
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 37                     |
| policy/steps              | 2713480.0              |
| test/episodes             | 950.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -2.9324180309053784    |
| test_1/avg_q              | -22.754542313987272    |
| test_1/n_subgoals         | 11251.0                |
| test_1/subgoal_succ_rate  | 0.9992889520931473     |
| train/episodes            | 3800.0                 |
| train/success_rate        | 0.02                   |
| train_0/avg_q             | -8.2713616494982       |
| train_0/current_q         | -4.490678233792452     |
| train_0/fw_bonus          | -0.999445590376854     |
| train_0/fw_loss           | 0.00013392350392678055 |
| train_0/mu_grads          | -0.1342979595065117    |
| train_0/mu_grads_std      | 0.6548342525959014     |
| train_0/mu_loss           | 4.279006413534406      |
| train_0/next_q            | -4.091572979828998     |
| train_0/q_grads           | 0.03766783019527793    |
| train_0/q_grads_std       | 0.43229086995124816    |
| train_0/q_loss            | 0.6816911488174947     |
| train_0/reward            | -0.8544102984422353    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.0918212890625        |
| train_0/target_q          | -4.4514445496794774    |
| train_1/avg_q             | -20.39726254187205     |
| train_1/current_q         | -17.3969129564883      |
| train_1/fw_bonus          | -0.9967299461364746    |
| train_1/fw_loss           | 0.0012808900064555928  |
| train_1/mu_grads          | 0.019959809258580208   |
| train_1/mu_grads_std      | 0.08689817041158676    |
| train_1/mu_loss           | 18.747855383182458     |
| train_1/n_subgoals        | 2687.0                 |
| train_1/next_q            | -18.727176495689314    |
| train_1/q_grads           | -0.0025796700094360856 |
| train_1/q_grads_std       | 0.6045642524957657     |
| train_1/q_loss            | 3.731421480555042      |
| train_1/reward            | -1.5316535484947962    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.006591796875         |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.3222925195385188     |
| train_1/target_q          | -17.554718685945012    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 38
Time for epoch 38: 587.83. Rollout time: 378.01, Training time: 209.75
Evaluating epoch 38
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 38                     |
| policy/steps              | 2780150.0              |
| test/episodes             | 975.0                  |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -1.4844727662024713    |
| test_1/avg_q              | -25.357687667428742    |
| test_1/n_subgoals         | 10654.0                |
| test_1/subgoal_succ_rate  | 0.984700581941055      |
| train/episodes            | 3900.0                 |
| train/success_rate        | 0.03                   |
| train_0/avg_q             | -10.249508788419012    |
| train_0/current_q         | -6.295177995030887     |
| train_0/fw_bonus          | -0.9994452863931655    |
| train_0/fw_loss           | 0.00013399366816884138 |
| train_0/mu_grads          | -0.13754606060683727   |
| train_0/mu_grads_std      | 0.6609158366918564     |
| train_0/mu_loss           | 6.016434322535353      |
| train_0/next_q            | -5.92971954396131      |
| train_0/q_grads           | 0.03870465364307165    |
| train_0/q_grads_std       | 0.4399056427180767     |
| train_0/q_loss            | 0.8284123440884571     |
| train_0/reward            | -0.8518954992701765    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.08349609375          |
| train_0/target_q          | -6.312651792282231     |
| train_1/avg_q             | -20.51691892076705     |
| train_1/current_q         | -17.016527081876426    |
| train_1/fw_bonus          | -0.9969544723629952    |
| train_1/fw_loss           | 0.0012269259343156591  |
| train_1/mu_grads          | 0.019959809258580208   |
| train_1/mu_grads_std      | 0.08689817041158676    |
| train_1/mu_loss           | 18.40900029625048      |
| train_1/n_subgoals        | 2685.0                 |
| train_1/next_q            | -18.37275129321882     |
| train_1/q_grads           | -0.0037087318953126667 |
| train_1/q_grads_std       | 0.6085900694131852     |
| train_1/q_loss            | 3.8551406902482968     |
| train_1/reward            | -1.5095612256845925    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0065185546875        |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.45884543761638735    |
| train_1/target_q          | -17.18482954079004     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 39
Time for epoch 39: 614.88. Rollout time: 404.88, Training time: 209.91
Evaluating epoch 39
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
-----------------------------------------------------
| epoch                     | 39                    |
| policy/steps              | 2848598.0             |
| test/episodes             | 1000.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -2.0931183844500856   |
| test_1/avg_q              | -19.741991233772406   |
| test_1/n_subgoals         | 8056.0                |
| test_1/subgoal_succ_rate  | 0.9548162859980139    |
| train/episodes            | 4000.0                |
| train/success_rate        | 0.01                  |
| train_0/avg_q             | -11.008682864257969   |
| train_0/current_q         | -5.858474473493674    |
| train_0/fw_bonus          | -0.9993930324912071   |
| train_0/fw_loss           | 0.0001461707613998442 |
| train_0/mu_grads          | -0.14119380638003348  |
| train_0/mu_grads_std      | 0.6681399777531624    |
| train_0/mu_loss           | 5.714813300129695     |
| train_0/next_q            | -5.533491946250358    |
| train_0/q_grads           | 0.03967918874695897   |
| train_0/q_grads_std       | 0.4465367831289768    |
| train_0/q_loss            | 0.783649610219223     |
| train_0/reward            | -0.8466691702298703   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0583740234375       |
| train_0/target_q          | -5.894999020929826    |
| train_1/avg_q             | -20.372996284906964   |
| train_1/current_q         | -16.85548345036574    |
| train_1/fw_bonus          | -0.9969872504472732   |
| train_1/fw_loss           | 0.0012190483190352096 |
| train_1/mu_grads          | 0.019959809258580208  |
| train_1/mu_grads_std      | 0.08689817041158676   |
| train_1/mu_loss           | 18.493384920425363    |
| train_1/n_subgoals        | 2688.0                |
| train_1/next_q            | -18.446466883061632   |
| train_1/q_grads           | -0.003924780205124989 |
| train_1/q_grads_std       | 0.615323144197464     |
| train_1/q_loss            | 3.5361824788832785    |
| train_1/reward            | -1.5245800550801505   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.00634765625         |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.4330357142857143    |
| train_1/target_q          | -17.02247925110823    |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 40
Time for epoch 40: 721.97. Rollout time: 470.28, Training time: 251.57
Evaluating epoch 40
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 40                     |
| policy/steps              | 2912185.0              |
| test/episodes             | 1025.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -2.573771099224263     |
| test_1/avg_q              | -22.47730482760565     |
| test_1/n_subgoals         | 8683.0                 |
| test_1/subgoal_succ_rate  | 0.962800875273523      |
| train/episodes            | 4100.0                 |
| train/success_rate        | 0.05                   |
| train_0/avg_q             | -12.623077422712164    |
| train_0/current_q         | -7.205846126145888     |
| train_0/fw_bonus          | -0.9993478119373321    |
| train_0/fw_loss           | 0.00015670410539314616 |
| train_0/mu_grads          | -0.14226308539509774   |
| train_0/mu_grads_std      | 0.6726306542754174     |
| train_0/mu_loss           | 7.015743958474633      |
| train_0/next_q            | -6.8252860461020335    |
| train_0/q_grads           | 0.04035253524780273    |
| train_0/q_grads_std       | 0.452209310233593      |
| train_0/q_loss            | 0.5377543641366349     |
| train_0/reward            | -0.8415562322625192    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.0685546875           |
| train_0/target_q          | -7.141286637675137     |
| train_1/avg_q             | -20.509730040722033    |
| train_1/current_q         | -16.618932584622957    |
| train_1/fw_bonus          | -0.9971624538302422    |
| train_1/fw_loss           | 0.0011769410804845393  |
| train_1/mu_grads          | 0.019959809258580208   |
| train_1/mu_grads_std      | 0.08689817041158676    |
| train_1/mu_loss           | 18.411645607828923     |
| train_1/n_subgoals        | 2638.0                 |
| train_1/next_q            | -18.437614364767075    |
| train_1/q_grads           | -0.004985382710583508  |
| train_1/q_grads_std       | 0.6235564723610878     |
| train_1/q_loss            | 3.9241627218777424     |
| train_1/reward            | -1.5674532990153238    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0064453125           |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.4806671721000758     |
| train_1/target_q          | -16.75413634736647     |
------------------------------------------------------
Saving periodic policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_40.pkl ...
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 41
Time for epoch 41: 581.47. Rollout time: 377.82, Training time: 203.56
Evaluating epoch 41
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 41                     |
| policy/steps              | 2979346.0              |
| test/episodes             | 1050.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -1.9628089767567412    |
| test_1/avg_q              | -23.045690810297327    |
| test_1/n_subgoals         | 13065.0                |
| test_1/subgoal_succ_rate  | 0.9999234596249522     |
| train/episodes            | 4200.0                 |
| train/success_rate        | 0.03                   |
| train_0/avg_q             | -13.320187543542678    |
| train_0/current_q         | -6.043158067092021     |
| train_0/fw_bonus          | -0.9993704095482826    |
| train_0/fw_loss           | 0.00015144062454055529 |
| train_0/mu_grads          | -0.14429128877818584   |
| train_0/mu_grads_std      | 0.6789853945374489     |
| train_0/mu_loss           | 5.847314958264372      |
| train_0/next_q            | -5.624156142032055     |
| train_0/q_grads           | 0.03990514725446701    |
| train_0/q_grads_std       | 0.4544041663408279     |
| train_0/q_loss            | 0.6266112339279608     |
| train_0/reward            | -0.8414332509666564    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.053564453125         |
| train_0/target_q          | -5.984047105571057     |
| train_1/avg_q             | -19.545163038179883    |
| train_1/current_q         | -16.512571844191893    |
| train_1/fw_bonus          | -0.9971454948186874    |
| train_1/fw_loss           | 0.0011810185824288055  |
| train_1/mu_grads          | 0.019959809258580208   |
| train_1/mu_grads_std      | 0.08689817041158676    |
| train_1/mu_loss           | 18.375100748897502     |
| train_1/n_subgoals        | 2682.0                 |
| train_1/next_q            | -18.40029270929296     |
| train_1/q_grads           | -0.005766846647020429  |
| train_1/q_grads_std       | 0.6280079945921898     |
| train_1/q_loss            | 3.843516466301385      |
| train_1/reward            | -1.5811899337684736    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0068115234375        |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.4519015659955257     |
| train_1/target_q          | -16.663971697598434    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 42
Time for epoch 42: 585.64. Rollout time: 380.39, Training time: 205.16
Evaluating epoch 42
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 42                     |
| policy/steps              | 3046723.0              |
| test/episodes             | 1075.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -2.5508027379341645    |
| test_1/avg_q              | -23.212764396533455    |
| test_1/n_subgoals         | 7248.0                 |
| test_1/subgoal_succ_rate  | 0.9746136865342163     |
| train/episodes            | 4300.0                 |
| train/success_rate        | 0.03                   |
| train_0/avg_q             | -9.836155843474279     |
| train_0/current_q         | -6.495763730053595     |
| train_0/fw_bonus          | -0.9993569791316986    |
| train_0/fw_loss           | 0.00015457051958946976 |
| train_0/mu_grads          | -0.1460112038999796    |
| train_0/mu_grads_std      | 0.6839253157377243     |
| train_0/mu_loss           | 6.321117179874117      |
| train_0/next_q            | -6.105565074130189     |
| train_0/q_grads           | 0.03912814604118466    |
| train_0/q_grads_std       | 0.45547681823372843    |
| train_0/q_loss            | 0.6164287113058143     |
| train_0/reward            | -0.8413044635526603    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.0804443359375        |
| train_0/target_q          | -6.462128065625947     |
| train_1/avg_q             | -20.034028112598634    |
| train_1/current_q         | -16.44541276184347     |
| train_1/fw_bonus          | -0.9971750169992447    |
| train_1/fw_loss           | 0.0011739217152353376  |
| train_1/mu_grads          | 0.019959809258580208   |
| train_1/mu_grads_std      | 0.08689817041158676    |
| train_1/mu_loss           | 18.22570402046235      |
| train_1/n_subgoals        | 2680.0                 |
| train_1/next_q            | -18.25377474079474     |
| train_1/q_grads           | -0.007089924497995526  |
| train_1/q_grads_std       | 0.6315000250935554     |
| train_1/q_loss            | 3.0748220405083497     |
| train_1/reward            | -1.525064561713225     |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0072509765625        |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.45186567164179103    |
| train_1/target_q          | -16.63524104035047     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 43
Time for epoch 43: 590.12. Rollout time: 381.00, Training time: 209.04
Evaluating epoch 43
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 43                     |
| policy/steps              | 3113669.0              |
| test/episodes             | 1100.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -2.5060809065073237    |
| test_1/avg_q              | -22.3560483437528      |
| test_1/n_subgoals         | 6658.0                 |
| test_1/subgoal_succ_rate  | 0.9606488434965456     |
| train/episodes            | 4400.0                 |
| train/success_rate        | 0.09                   |
| train_0/avg_q             | -10.926828775493155    |
| train_0/current_q         | -4.337748958924712     |
| train_0/fw_bonus          | -0.999342530965805     |
| train_0/fw_loss           | 0.00015793526836205274 |
| train_0/mu_grads          | -0.15032409876585007   |
| train_0/mu_grads_std      | 0.6891057401895523     |
| train_0/mu_loss           | 4.295517155963601      |
| train_0/next_q            | -4.124176487420556     |
| train_0/q_grads           | 0.03901958661153913    |
| train_0/q_grads_std       | 0.4570003017783165     |
| train_0/q_loss            | 1.0213446666395798     |
| train_0/reward            | -0.8400032970639586    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.0609130859375        |
| train_0/target_q          | -4.575680055513128     |
| train_1/avg_q             | -19.936659910913317    |
| train_1/current_q         | -16.52774540464937     |
| train_1/fw_bonus          | -0.9969834640622139    |
| train_1/fw_loss           | 0.0012199620192404836  |
| train_1/mu_grads          | 0.019959809258580208   |
| train_1/mu_grads_std      | 0.08689817041158676    |
| train_1/mu_loss           | 18.27789860655989      |
| train_1/n_subgoals        | 2602.0                 |
| train_1/next_q            | -18.255419170661337    |
| train_1/q_grads           | -0.007786465145181864  |
| train_1/q_grads_std       | 0.6345695301890373     |
| train_1/q_loss            | 2.9604601751122694     |
| train_1/reward            | -1.541655791226367     |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0058837890625        |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.4404304381245196     |
| train_1/target_q          | -16.698372152655413    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 44
Time for epoch 44: 568.55. Rollout time: 362.21, Training time: 206.23
Evaluating epoch 44
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 44                     |
| policy/steps              | 3177679.0              |
| test/episodes             | 1125.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -1.3219629053738662    |
| test_1/avg_q              | -23.7584571412839      |
| test_1/n_subgoals         | 17516.0                |
| test_1/subgoal_succ_rate  | 0.9999429093400319     |
| train/episodes            | 4500.0                 |
| train/success_rate        | 0.08                   |
| train_0/avg_q             | -9.727289594143114     |
| train_0/current_q         | -6.679082160552538     |
| train_0/fw_bonus          | -0.9993753179907798    |
| train_0/fw_loss           | 0.00015029850728751626 |
| train_0/mu_grads          | -0.148750963807106     |
| train_0/mu_grads_std      | 0.6975688725709915     |
| train_0/mu_loss           | 6.443528686702012      |
| train_0/next_q            | -6.270877076094439     |
| train_0/q_grads           | 0.03804908832535148    |
| train_0/q_grads_std       | 0.4552341282367706     |
| train_0/q_loss            | 0.7017134622713638     |
| train_0/reward            | -0.8480641201567778    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.067724609375         |
| train_0/target_q          | -6.587874861981898     |
| train_1/avg_q             | -20.741533639999677    |
| train_1/current_q         | -16.478466137871305    |
| train_1/fw_bonus          | -0.9973044529557228    |
| train_1/fw_loss           | 0.001142813713522628   |
| train_1/mu_grads          | 0.019959809258580208   |
| train_1/mu_grads_std      | 0.08689817041158676    |
| train_1/mu_loss           | 18.098711634305488     |
| train_1/n_subgoals        | 2599.0                 |
| train_1/next_q            | -18.068830970971266    |
| train_1/q_grads           | -0.009108395618386566  |
| train_1/q_grads_std       | 0.6387237489223481     |
| train_1/q_loss            | 3.4616568790075917     |
| train_1/reward            | -1.5288280082349957    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.006201171875         |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.46748749519045785    |
| train_1/target_q          | -16.631707234405095    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 45
Time for epoch 45: 578.04. Rollout time: 366.56, Training time: 211.42
Evaluating epoch 45
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 45                     |
| policy/steps              | 3241666.0              |
| test/episodes             | 1150.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -1.638634404675337     |
| test_1/avg_q              | -23.650854606524977    |
| test_1/n_subgoals         | 1721.0                 |
| test_1/subgoal_succ_rate  | 0.6914584543869843     |
| train/episodes            | 4600.0                 |
| train/success_rate        | 0.08                   |
| train_0/avg_q             | -10.972572155362608    |
| train_0/current_q         | -5.561366765121879     |
| train_0/fw_bonus          | -0.9993759393692017    |
| train_0/fw_loss           | 0.00015015441913419637 |
| train_0/mu_grads          | -0.1503915324807167    |
| train_0/mu_grads_std      | 0.7032714888453484     |
| train_0/mu_loss           | 5.3080513259518955     |
| train_0/next_q            | -5.121996951170274     |
| train_0/q_grads           | 0.03784553837031126    |
| train_0/q_grads_std       | 0.45723687037825583    |
| train_0/q_loss            | 0.5180370034170194     |
| train_0/reward            | -0.8458793707119184    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.0763427734375        |
| train_0/target_q          | -5.546537164172127     |
| train_1/avg_q             | -20.535183997311517    |
| train_1/current_q         | -16.779185643207626    |
| train_1/fw_bonus          | -0.9976763740181923    |
| train_1/fw_loss           | 0.0010534292217926123  |
| train_1/mu_grads          | 0.019959809258580208   |
| train_1/mu_grads_std      | 0.08689817041158676    |
| train_1/mu_loss           | 18.452276361476365     |
| train_1/n_subgoals        | 2586.0                 |
| train_1/next_q            | -18.437595599761107    |
| train_1/q_grads           | -0.008945436985231935  |
| train_1/q_grads_std       | 0.6435350775718689     |
| train_1/q_loss            | 3.3806187867677098     |
| train_1/reward            | -1.5451516593398993    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0065673828125        |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.4717710750193349     |
| train_1/target_q          | -16.947970579276802    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 46
Time for epoch 46: 607.36. Rollout time: 395.16, Training time: 212.11
Evaluating epoch 46
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 46                     |
| policy/steps              | 3309602.0              |
| test/episodes             | 1175.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -2.1208237050009484    |
| test_1/avg_q              | -22.0592599306641      |
| test_1/n_subgoals         | 14688.0                |
| test_1/subgoal_succ_rate  | 0.9944852941176471     |
| train/episodes            | 4700.0                 |
| train/success_rate        | 0.06                   |
| train_0/avg_q             | -10.208979298517544    |
| train_0/current_q         | -5.438473694323109     |
| train_0/fw_bonus          | -0.999386091530323     |
| train_0/fw_loss           | 0.00014778554686927236 |
| train_0/mu_grads          | -0.15085184648633004   |
| train_0/mu_grads_std      | 0.7075812116265296     |
| train_0/mu_loss           | 5.286104826872909      |
| train_0/next_q            | -5.0426491498374       |
| train_0/q_grads           | 0.03748914385214448    |
| train_0/q_grads_std       | 0.45909728184342385    |
| train_0/q_loss            | 0.5043345435513483     |
| train_0/reward            | -0.8464459781134792    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.08701171875          |
| train_0/target_q          | -5.466585745728096     |
| train_1/avg_q             | -20.52184828978095     |
| train_1/current_q         | -16.59480641334843     |
| train_1/fw_bonus          | -0.9975947007536888    |
| train_1/fw_loss           | 0.0010730565743870101  |
| train_1/mu_grads          | 0.019959809258580208   |
| train_1/mu_grads_std      | 0.08689817041158676    |
| train_1/mu_loss           | 18.105173052124968     |
| train_1/n_subgoals        | 2621.0                 |
| train_1/next_q            | -18.077934413987226    |
| train_1/q_grads           | -0.008134260773658752  |
| train_1/q_grads_std       | 0.6482536271214485     |
| train_1/q_loss            | 3.185450916777305      |
| train_1/reward            | -1.5543271350572467    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0066650390625        |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.4311331552842427     |
| train_1/target_q          | -16.725492097794408    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 47
Time for epoch 47: 572.21. Rollout time: 357.07, Training time: 215.08
Evaluating epoch 47
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
-----------------------------------------------------
| epoch                     | 47                    |
| policy/steps              | 3373739.0             |
| test/episodes             | 1200.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -1.462016026037037    |
| test_1/avg_q              | -22.989463062705916   |
| test_1/n_subgoals         | 8159.0                |
| test_1/subgoal_succ_rate  | 0.9803897536462802    |
| train/episodes            | 4800.0                |
| train/success_rate        | 0.03                  |
| train_0/avg_q             | -11.466908096667249   |
| train_0/current_q         | -6.864070749504833    |
| train_0/fw_bonus          | -0.9994269803166389   |
| train_0/fw_loss           | 0.0001382606933475472 |
| train_0/mu_grads          | -0.15294517166912555  |
| train_0/mu_grads_std      | 0.7097879812121392    |
| train_0/mu_loss           | 6.590363699614119     |
| train_0/next_q            | -6.387484202271257    |
| train_0/q_grads           | 0.0374789415858686    |
| train_0/q_grads_std       | 0.46289123594760895   |
| train_0/q_loss            | 0.3132039341463811    |
| train_0/reward            | -0.8505679821922968   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0829345703125       |
| train_0/target_q          | -6.864580818041465    |
| train_1/avg_q             | -20.784139527415597   |
| train_1/current_q         | -16.693399424714265   |
| train_1/fw_bonus          | -0.9979211464524269   |
| train_1/fw_loss           | 0.0009945988465915434 |
| train_1/mu_grads          | 0.019959809258580208  |
| train_1/mu_grads_std      | 0.08689817041158676   |
| train_1/mu_loss           | 18.178576359626803    |
| train_1/n_subgoals        | 2650.0                |
| train_1/next_q            | -18.152528661716836   |
| train_1/q_grads           | -0.008561733458191157 |
| train_1/q_grads_std       | 0.6558119118213653    |
| train_1/q_loss            | 3.415362607179239     |
| train_1/reward            | -1.5078717053795117   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.006689453125        |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.4977358490566038    |
| train_1/target_q          | -16.826648200450304   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 48
Time for epoch 48: 576.92. Rollout time: 361.93, Training time: 214.92
Evaluating epoch 48
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
-----------------------------------------------------
| epoch                     | 48                    |
| policy/steps              | 3439897.0             |
| test/episodes             | 1225.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -2.3360379895214014   |
| test_1/avg_q              | -23.343898797543364   |
| test_1/n_subgoals         | 16006.0               |
| test_1/subgoal_succ_rate  | 0.9949393977258528    |
| train/episodes            | 4900.0                |
| train/success_rate        | 0.03                  |
| train_0/avg_q             | -11.190945038494734   |
| train_0/current_q         | -6.654852840566963    |
| train_0/fw_bonus          | -0.9994498640298843   |
| train_0/fw_loss           | 0.0001329311862718896 |
| train_0/mu_grads          | -0.1530553698539734   |
| train_0/mu_grads_std      | 0.7139539092779159    |
| train_0/mu_loss           | 6.433875084950041     |
| train_0/next_q            | -6.22798898010097     |
| train_0/q_grads           | 0.03817536113783717   |
| train_0/q_grads_std       | 0.46507512778043747   |
| train_0/q_loss            | 0.5270344482986067    |
| train_0/reward            | -0.852703443732753    |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0849365234375       |
| train_0/target_q          | -6.592267574010076    |
| train_1/avg_q             | -20.576926041523887   |
| train_1/current_q         | -16.895211512355843   |
| train_1/fw_bonus          | -0.9966522753238678   |
| train_1/fw_loss           | 0.001299556906451471  |
| train_1/mu_grads          | 0.019959809258580208  |
| train_1/mu_grads_std      | 0.08689817041158676   |
| train_1/mu_loss           | 18.340524861773908    |
| train_1/n_subgoals        | 2652.0                |
| train_1/next_q            | -18.302060947963415   |
| train_1/q_grads           | -0.009187498153187335 |
| train_1/q_grads_std       | 0.6636181578040123    |
| train_1/q_loss            | 3.5785473756037844    |
| train_1/reward            | -1.5118759440789291   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.005908203125        |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.46153846153846156   |
| train_1/target_q          | -17.039455694828117   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 49
Time for epoch 49: 576.00. Rollout time: 358.93, Training time: 217.00
Evaluating epoch 49
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 49                     |
| policy/steps              | 3502866.0              |
| test/episodes             | 1250.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -2.7724065254580514    |
| test_1/avg_q              | -24.12105530389957     |
| test_1/n_subgoals         | 15110.0                |
| test_1/subgoal_succ_rate  | 0.9923891462607545     |
| train/episodes            | 5000.0                 |
| train/success_rate        | 0.06                   |
| train_0/avg_q             | -12.02908376510654     |
| train_0/current_q         | -6.660088849639331     |
| train_0/fw_bonus          | -0.9993829414248466    |
| train_0/fw_loss           | 0.00014852135736873607 |
| train_0/mu_grads          | -0.157984609156847     |
| train_0/mu_grads_std      | 0.72069071829319       |
| train_0/mu_loss           | 6.437240069260369      |
| train_0/next_q            | -6.239576131193025     |
| train_0/q_grads           | 0.03746258746832609    |
| train_0/q_grads_std       | 0.4656550377607346     |
| train_0/q_loss            | 0.5415763777350524     |
| train_0/reward            | -0.85371978974581      |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.0939208984375        |
| train_0/target_q          | -6.618660709713373     |
| train_1/avg_q             | -21.252503765500478    |
| train_1/current_q         | -16.690724049071584    |
| train_1/fw_bonus          | -0.9978456199169159    |
| train_1/fw_loss           | 0.0010127508416189813  |
| train_1/mu_grads          | 0.019959809258580208   |
| train_1/mu_grads_std      | 0.08689817041158676    |
| train_1/mu_loss           | 18.11522889540278      |
| train_1/n_subgoals        | 2624.0                 |
| train_1/next_q            | -18.07251433377176     |
| train_1/q_grads           | -0.009996813582256436  |
| train_1/q_grads_std       | 0.6705475822091103     |
| train_1/q_loss            | 3.3130873811114343     |
| train_1/reward            | -1.4955292545142584    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0069580078125        |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.49809451219512196    |
| train_1/target_q          | -16.87801995113054     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 50
Time for epoch 50: 525.23. Rollout time: 312.27, Training time: 212.89
Evaluating epoch 50
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 50                     |
| policy/steps              | 3559643.0              |
| test/episodes             | 1275.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -1.8516688274135427    |
| test_1/avg_q              | -23.837537000723184    |
| test_1/n_subgoals         | 7493.0                 |
| test_1/subgoal_succ_rate  | 0.981182436941145      |
| train/episodes            | 5100.0                 |
| train/success_rate        | 0.09                   |
| train_0/avg_q             | -11.411575585728347    |
| train_0/current_q         | -6.657816336982788     |
| train_0/fw_bonus          | -0.9994112595915794    |
| train_0/fw_loss           | 0.00014192155649652704 |
| train_0/mu_grads          | -0.15993363372981548   |
| train_0/mu_grads_std      | 0.7249069407582283     |
| train_0/mu_loss           | 6.420995497115934      |
| train_0/next_q            | -6.229801396063967     |
| train_0/q_grads           | 0.03775876834988594    |
| train_0/q_grads_std       | 0.4692462645471096     |
| train_0/q_loss            | 0.5222019004132928     |
| train_0/reward            | -0.8538804382114904    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.1034912109375        |
| train_0/target_q          | -6.610142891274743     |
| train_1/avg_q             | -22.584924811755364    |
| train_1/current_q         | -16.581605276809036    |
| train_1/fw_bonus          | -0.9979707553982735    |
| train_1/fw_loss           | 0.0009826756227994337  |
| train_1/mu_grads          | 0.019959809258580208   |
| train_1/mu_grads_std      | 0.08689817041158676    |
| train_1/mu_loss           | 17.915464630728458     |
| train_1/n_subgoals        | 2590.0                 |
| train_1/next_q            | -17.897808223753543    |
| train_1/q_grads           | -0.010264839208684862  |
| train_1/q_grads_std       | 0.6775492846965789     |
| train_1/q_loss            | 2.9326954948793946     |
| train_1/reward            | -1.4699448511382798    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.007763671875         |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.5942084942084942     |
| train_1/target_q          | -16.75088139109179     |
------------------------------------------------------
Saving periodic policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_50.pkl ...
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 51
Time for epoch 51: 518.48. Rollout time: 313.70, Training time: 204.70
Evaluating epoch 51
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 51                     |
| policy/steps              | 3617488.0              |
| test/episodes             | 1300.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -2.9884910428985925    |
| test_1/avg_q              | -25.708707405060586    |
| test_1/n_subgoals         | 15401.0                |
| test_1/subgoal_succ_rate  | 0.993247191740796      |
| train/episodes            | 5200.0                 |
| train/success_rate        | 0.06                   |
| train_0/avg_q             | -11.496803545768694    |
| train_0/current_q         | -6.610347259472422     |
| train_0/fw_bonus          | -0.9994007289409638    |
| train_0/fw_loss           | 0.00014437872705457265 |
| train_0/mu_grads          | -0.16297529488801957   |
| train_0/mu_grads_std      | 0.7284524798393249     |
| train_0/mu_loss           | 6.386051468976525      |
| train_0/next_q            | -6.186650920736755     |
| train_0/q_grads           | 0.037294605281203985   |
| train_0/q_grads_std       | 0.47307822331786153    |
| train_0/q_loss            | 0.5395441896068446     |
| train_0/reward            | -0.8573663755087182    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.061279296875         |
| train_0/target_q          | -6.601778191842594     |
| train_1/avg_q             | -22.59246230457204     |
| train_1/current_q         | -16.79416097264683     |
| train_1/fw_bonus          | -0.9982243835926056    |
| train_1/fw_loss           | 0.0009217201441060752  |
| train_1/mu_grads          | 0.019959809258580208   |
| train_1/mu_grads_std      | 0.08689817041158676    |
| train_1/mu_loss           | 18.06990557889886      |
| train_1/n_subgoals        | 2640.0                 |
| train_1/next_q            | -18.019991932792063    |
| train_1/q_grads           | -0.010602151486091315  |
| train_1/q_grads_std       | 0.6851569309830665     |
| train_1/q_loss            | 2.748893563145818      |
| train_1/reward            | -1.4355990065771038    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0076171875           |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.5969696969696969     |
| train_1/target_q          | -16.995734670911023    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 52
Time for epoch 52: 521.53. Rollout time: 315.52, Training time: 205.89
Evaluating epoch 52
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
-----------------------------------------------------
| epoch                     | 52                    |
| policy/steps              | 3674974.0             |
| test/episodes             | 1325.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -0.887198764506213    |
| test_1/avg_q              | -26.11132473213805    |
| test_1/n_subgoals         | 3878.0                |
| test_1/subgoal_succ_rate  | 0.870293965961836     |
| train/episodes            | 5300.0                |
| train/success_rate        | 0.06                  |
| train_0/avg_q             | -10.405553994432728   |
| train_0/current_q         | -6.697323473681564    |
| train_0/fw_bonus          | -0.9993746668100357   |
| train_0/fw_loss           | 0.00015045109539642   |
| train_0/mu_grads          | -0.16600765101611614  |
| train_0/mu_grads_std      | 0.7323652446269989    |
| train_0/mu_loss           | 6.467096245849864     |
| train_0/next_q            | -6.283529857699039    |
| train_0/q_grads           | 0.036680277157574895  |
| train_0/q_grads_std       | 0.47626820877194403   |
| train_0/q_loss            | 0.5300134184939497    |
| train_0/reward            | -0.8590989488271589   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.0950439453125       |
| train_0/target_q          | -6.700573262922267    |
| train_1/avg_q             | -23.196895286842487   |
| train_1/current_q         | -17.011562061705547   |
| train_1/fw_bonus          | -0.9977949306368827   |
| train_1/fw_loss           | 0.0010249301834846848 |
| train_1/mu_grads          | 0.019959809258580208  |
| train_1/mu_grads_std      | 0.08689817041158676   |
| train_1/mu_loss           | 18.123547955192578    |
| train_1/n_subgoals        | 2601.0                |
| train_1/next_q            | -18.09364402764377    |
| train_1/q_grads           | -0.012054117093794049 |
| train_1/q_grads_std       | 0.6906521528959274    |
| train_1/q_loss            | 3.4065776594286303    |
| train_1/reward            | -1.3929038193731684   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.007421875           |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.5909265667051135    |
| train_1/target_q          | -17.208914199400038   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 53
Time for epoch 53: 548.55. Rollout time: 342.99, Training time: 205.50
Evaluating epoch 53
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
-----------------------------------------------------
| epoch                     | 53                    |
| policy/steps              | 3735023.0             |
| test/episodes             | 1350.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -1.4099184586669176   |
| test_1/avg_q              | -26.343006099678963   |
| test_1/n_subgoals         | 16685.0               |
| test_1/subgoal_succ_rate  | 0.9967635600839077    |
| train/episodes            | 5400.0                |
| train/success_rate        | 0.07                  |
| train_0/avg_q             | -11.493536435176921   |
| train_0/current_q         | -6.560625867659249    |
| train_0/fw_bonus          | -0.9993952482938766   |
| train_0/fw_loss           | 0.0001456529691495234 |
| train_0/mu_grads          | -0.16838304698467255  |
| train_0/mu_grads_std      | 0.7362541317939758    |
| train_0/mu_loss           | 6.320220970909265     |
| train_0/next_q            | -6.11273021399666     |
| train_0/q_grads           | 0.03557525267824531   |
| train_0/q_grads_std       | 0.48054973855614663   |
| train_0/q_loss            | 0.40070824249924397   |
| train_0/reward            | -0.8587430355066317   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.1216064453125       |
| train_0/target_q          | -6.5543223949342275   |
| train_1/avg_q             | -22.640953159501734   |
| train_1/current_q         | -16.918131700345622   |
| train_1/fw_bonus          | -0.9981708735227585   |
| train_1/fw_loss           | 0.0009345798272988759 |
| train_1/mu_grads          | 0.019959809258580208  |
| train_1/mu_grads_std      | 0.08689817041158676   |
| train_1/mu_loss           | 17.9428939530247      |
| train_1/n_subgoals        | 2624.0                |
| train_1/next_q            | -17.914800565534843   |
| train_1/q_grads           | -0.013776299869641661 |
| train_1/q_grads_std       | 0.6962324053049087    |
| train_1/q_loss            | 2.6535031079953697    |
| train_1/reward            | -1.3867807487695245   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.007275390625        |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.5560213414634146    |
| train_1/target_q          | -17.131833500329655   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 54
Time for epoch 54: 562.71. Rollout time: 352.10, Training time: 210.54
Evaluating epoch 54
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
-----------------------------------------------------
| epoch                     | 54                    |
| policy/steps              | 3796336.0             |
| test/episodes             | 1375.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -2.282847645225671    |
| test_1/avg_q              | -24.57625803997644    |
| test_1/n_subgoals         | 15285.0               |
| test_1/subgoal_succ_rate  | 0.9929342492639843    |
| train/episodes            | 5500.0                |
| train/success_rate        | 0.05                  |
| train_0/avg_q             | -14.479761164187495   |
| train_0/current_q         | -7.120139722234073    |
| train_0/fw_bonus          | -0.9994280159473419   |
| train_0/fw_loss           | 0.0001380178293402423 |
| train_0/mu_grads          | -0.17121484987437724  |
| train_0/mu_grads_std      | 0.7395407915115356    |
| train_0/mu_loss           | 6.912773173130608     |
| train_0/next_q            | -6.698586798761508    |
| train_0/q_grads           | 0.034690112620592115  |
| train_0/q_grads_std       | 0.48688693195581434   |
| train_0/q_loss            | 0.45161252712496525   |
| train_0/reward            | -0.8610177223621577   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.1218505859375       |
| train_0/target_q          | -7.116947316898063    |
| train_1/avg_q             | -22.146602116270866   |
| train_1/current_q         | -16.932791790705295   |
| train_1/fw_bonus          | -0.9982997611165046   |
| train_1/fw_loss           | 0.0009036013783770613 |
| train_1/mu_grads          | 0.019959809258580208  |
| train_1/mu_grads_std      | 0.08689817041158676   |
| train_1/mu_loss           | 17.879252174158374    |
| train_1/n_subgoals        | 2637.0                |
| train_1/next_q            | -17.85884098477938    |
| train_1/q_grads           | -0.01444475471507758  |
| train_1/q_grads_std       | 0.7001709580421448    |
| train_1/q_loss            | 4.2234447032810785    |
| train_1/reward            | -1.3449262510497646   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0079345703125       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.5328024270003792    |
| train_1/target_q          | -17.138355362269046   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 55
Time for epoch 55: 552.27. Rollout time: 338.64, Training time: 213.50
Evaluating epoch 55
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 55                     |
| policy/steps              | 3857770.0              |
| test/episodes             | 1400.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -3.312417646286168     |
| test_1/avg_q              | -24.3624900340574      |
| test_1/n_subgoals         | 9086.0                 |
| test_1/subgoal_succ_rate  | 0.9629099713845477     |
| train/episodes            | 5600.0                 |
| train/success_rate        | 0.05                   |
| train_0/avg_q             | -14.424219507297936    |
| train_0/current_q         | -7.672461616127788     |
| train_0/fw_bonus          | -0.9994161456823349    |
| train_0/fw_loss           | 0.00014078510866966099 |
| train_0/mu_grads          | -0.17159083895385266   |
| train_0/mu_grads_std      | 0.7454077392816544     |
| train_0/mu_loss           | 7.436426991677618      |
| train_0/next_q            | -7.255290991137154     |
| train_0/q_grads           | 0.03506816467270255    |
| train_0/q_grads_std       | 0.4945658206939697     |
| train_0/q_loss            | 0.4443116639938772     |
| train_0/reward            | -0.864649478760839     |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.0916015625           |
| train_0/target_q          | -7.685428814076505     |
| train_1/avg_q             | -22.12602799421449     |
| train_1/current_q         | -17.262304752343493    |
| train_1/fw_bonus          | -0.9983958467841149    |
| train_1/fw_loss           | 0.0008805093544651754  |
| train_1/mu_grads          | 0.019959809258580208   |
| train_1/mu_grads_std      | 0.08689817041158676    |
| train_1/mu_loss           | 18.241660680036752     |
| train_1/n_subgoals        | 2633.0                 |
| train_1/next_q            | -18.231053456441423    |
| train_1/q_grads           | -0.016671792743727563  |
| train_1/q_grads_std       | 0.7032354012131691     |
| train_1/q_loss            | 2.902073410919984      |
| train_1/reward            | -1.3418737057254475    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0075439453125        |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.5393087732624383     |
| train_1/target_q          | -17.47923227910926     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 56
Time for epoch 56: 568.51. Rollout time: 353.64, Training time: 214.79
Evaluating epoch 56
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 56                     |
| policy/steps              | 3919507.0              |
| test/episodes             | 1425.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -1.8793450066582158    |
| test_1/avg_q              | -25.31496616617672     |
| test_1/n_subgoals         | 12780.0                |
| test_1/subgoal_succ_rate  | 0.997339593114241      |
| train/episodes            | 5700.0                 |
| train/success_rate        | 0.07                   |
| train_0/avg_q             | -16.70905632643808     |
| train_0/current_q         | -7.35501268843228      |
| train_0/fw_bonus          | -0.9993722602725029    |
| train_0/fw_loss           | 0.00015100676955626113 |
| train_0/mu_grads          | -0.17385462783277034   |
| train_0/mu_grads_std      | 0.7506930381059647     |
| train_0/mu_loss           | 7.1198939397584144     |
| train_0/next_q            | -6.934395850162605     |
| train_0/q_grads           | 0.03473708424717188    |
| train_0/q_grads_std       | 0.5004839524626732     |
| train_0/q_loss            | 0.5042276352990387     |
| train_0/reward            | -0.86167343994166      |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.0986083984375        |
| train_0/target_q          | -7.337089688084051     |
| train_1/avg_q             | -21.711727106962996    |
| train_1/current_q         | -17.176335944798097    |
| train_1/fw_bonus          | -0.9988783553242684    |
| train_1/fw_loss           | 0.0007645448320545256  |
| train_1/mu_grads          | 0.019959809258580208   |
| train_1/mu_grads_std      | 0.08689817041158676    |
| train_1/mu_loss           | 18.319445882298265     |
| train_1/n_subgoals        | 2582.0                 |
| train_1/next_q            | -18.294671104698654    |
| train_1/q_grads           | -0.01747477729804814   |
| train_1/q_grads_std       | 0.7078921258449554     |
| train_1/q_loss            | 3.264957188439676      |
| train_1/reward            | -1.380896696101263     |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.00732421875          |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.512006196746708      |
| train_1/target_q          | -17.39632260816046     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 57
Time for epoch 57: 535.21. Rollout time: 323.37, Training time: 211.71
Evaluating epoch 57
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 57                     |
| policy/steps              | 3977978.0              |
| test/episodes             | 1450.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -1.7039796104746188    |
| test_1/avg_q              | -25.711825158353854    |
| test_1/n_subgoals         | 15282.0                |
| test_1/subgoal_succ_rate  | 0.9929328621908127     |
| train/episodes            | 5800.0                 |
| train/success_rate        | 0.07                   |
| train_0/avg_q             | -15.005106680904749    |
| train_0/current_q         | -7.812804497140201     |
| train_0/fw_bonus          | -0.9994132444262505    |
| train_0/fw_loss           | 0.00014146008597890614 |
| train_0/mu_grads          | -0.17579594664275647   |
| train_0/mu_grads_std      | 0.7565807655453682     |
| train_0/mu_loss           | 7.585997638533387      |
| train_0/next_q            | -7.393095691858821     |
| train_0/q_grads           | 0.03498858297243714    |
| train_0/q_grads_std       | 0.5060491383075714     |
| train_0/q_loss            | 0.4168272355063527     |
| train_0/reward            | -0.8629512527943006    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.108154296875         |
| train_0/target_q          | -7.817720484972691     |
| train_1/avg_q             | -22.295660690573328    |
| train_1/current_q         | -17.044220672462142    |
| train_1/fw_bonus          | -0.9988636434078216    |
| train_1/fw_loss           | 0.0007680792798055336  |
| train_1/mu_grads          | 0.019959809258580208   |
| train_1/mu_grads_std      | 0.08689817041158676    |
| train_1/mu_loss           | 18.208621202557243     |
| train_1/n_subgoals        | 2586.0                 |
| train_1/next_q            | -18.194275774336326    |
| train_1/q_grads           | -0.01825266843661666   |
| train_1/q_grads_std       | 0.7133907735347748     |
| train_1/q_loss            | 2.6975475579650943     |
| train_1/reward            | -1.369463562160672     |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.007763671875         |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.5576179427687549     |
| train_1/target_q          | -17.272604206653714    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 58
Time for epoch 58: 550.52. Rollout time: 340.59, Training time: 209.85
Evaluating epoch 58
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 58                     |
| policy/steps              | 4038703.0              |
| test/episodes             | 1475.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -1.733155042989908     |
| test_1/avg_q              | -26.158116412852298    |
| test_1/n_subgoals         | 16594.0                |
| test_1/subgoal_succ_rate  | 0.9967458117391829     |
| train/episodes            | 5900.0                 |
| train/success_rate        | 0.05                   |
| train_0/avg_q             | -15.262939589227884    |
| train_0/current_q         | -7.791581277486662     |
| train_0/fw_bonus          | -0.9993753075599671    |
| train_0/fw_loss           | 0.00015029804708319717 |
| train_0/mu_grads          | -0.17856702990829945   |
| train_0/mu_grads_std      | 0.7624307855963707     |
| train_0/mu_loss           | 7.565943333549271      |
| train_0/next_q            | -7.37006954146738      |
| train_0/q_grads           | 0.03474932638928294    |
| train_0/q_grads_std       | 0.5100909441709518     |
| train_0/q_loss            | 0.4065090568591138     |
| train_0/reward            | -0.8619313085859176    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.0580810546875        |
| train_0/target_q          | -7.797376643001184     |
| train_1/avg_q             | -22.522652892374637    |
| train_1/current_q         | -17.16551858367888     |
| train_1/fw_bonus          | -0.9948001503944397    |
| train_1/fw_loss           | 0.0017446941987145693  |
| train_1/mu_grads          | 0.019959809258580208   |
| train_1/mu_grads_std      | 0.08689817041158676    |
| train_1/mu_loss           | 18.378165342179805     |
| train_1/n_subgoals        | 2662.0                 |
| train_1/next_q            | -18.3717047731702      |
| train_1/q_grads           | -0.019090277282521127  |
| train_1/q_grads_std       | 0.7184248894453049     |
| train_1/q_loss            | 2.702592794766611      |
| train_1/reward            | -1.3578737110379735    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0056396484375        |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.5477084898572502     |
| train_1/target_q          | -17.398678143746775    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 59
Time for epoch 59: 496.22. Rollout time: 296.74, Training time: 199.43
Evaluating epoch 59
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 59                     |
| policy/steps              | 4100094.0              |
| test/episodes             | 1500.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -2.0198857781839887    |
| test_1/avg_q              | -25.63397202899891     |
| test_1/n_subgoals         | 10105.0                |
| test_1/subgoal_succ_rate  | 0.9910935180603662     |
| train/episodes            | 6000.0                 |
| train/success_rate        | 0.05                   |
| train_0/avg_q             | -15.555900327875657    |
| train_0/current_q         | -8.016879364816287     |
| train_0/fw_bonus          | -0.9994384258985519    |
| train_0/fw_loss           | 0.00013559648650698363 |
| train_0/mu_grads          | -0.18123848512768745   |
| train_0/mu_grads_std      | 0.7686040624976158     |
| train_0/mu_loss           | 7.820403221875004      |
| train_0/next_q            | -7.602182842415982     |
| train_0/q_grads           | 0.03467454863712192    |
| train_0/q_grads_std       | 0.5128330901265145     |
| train_0/q_loss            | 0.4403006205858798     |
| train_0/reward            | -0.8633178407966625    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.0859619140625        |
| train_0/target_q          | -8.03085689922911      |
| train_1/avg_q             | -21.726896301781235    |
| train_1/current_q         | -17.098224186206682    |
| train_1/fw_bonus          | -0.9986097142100334    |
| train_1/fw_loss           | 0.0008291095786262303  |
| train_1/mu_grads          | 0.019959809258580208   |
| train_1/mu_grads_std      | 0.08689817041158676    |
| train_1/mu_loss           | 18.39796042416391      |
| train_1/n_subgoals        | 2635.0                 |
| train_1/next_q            | -18.370029600431643    |
| train_1/q_grads           | -0.0204595307353884    |
| train_1/q_grads_std       | 0.7228551581501961     |
| train_1/q_loss            | 2.319668299176171      |
| train_1/reward            | -1.3311896211002021    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.007470703125         |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.5351043643263758     |
| train_1/target_q          | -17.32431346111514     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 60
Time for epoch 60: 461.53. Rollout time: 264.68, Training time: 196.80
Evaluating epoch 60
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
-----------------------------------------------------
| epoch                     | 60                    |
| policy/steps              | 4159548.0             |
| test/episodes             | 1525.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -3.3303489388750065   |
| test_1/avg_q              | -26.57349627248986    |
| test_1/n_subgoals         | 15297.0               |
| test_1/subgoal_succ_rate  | 0.9929397921161012    |
| train/episodes            | 6100.0                |
| train/success_rate        | 0.07                  |
| train_0/avg_q             | -14.167756325468092   |
| train_0/current_q         | -7.978225693801607    |
| train_0/fw_bonus          | -0.9994988337159156   |
| train_0/fw_loss           | 0.0001215241394675104 |
| train_0/mu_grads          | -0.18380138874053956  |
| train_0/mu_grads_std      | 0.7747234314680099    |
| train_0/mu_loss           | 7.8234554132769745    |
| train_0/next_q            | -7.561728620488924    |
| train_0/q_grads           | 0.03483525440096855   |
| train_0/q_grads_std       | 0.5158775612711907    |
| train_0/q_loss            | 0.47816064498290645   |
| train_0/reward            | -0.8626278271432966   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.1836669921875       |
| train_0/target_q          | -7.9471925966769685   |
| train_1/avg_q             | -23.315030610301225   |
| train_1/current_q         | -16.692018694529928   |
| train_1/fw_bonus          | -0.998695382475853    |
| train_1/fw_loss           | 0.0008085220018983818 |
| train_1/mu_grads          | 0.019959809258580208  |
| train_1/mu_grads_std      | 0.08689817041158676   |
| train_1/mu_loss           | 17.91551635878065     |
| train_1/n_subgoals        | 2606.0                |
| train_1/next_q            | -17.87122656806513    |
| train_1/q_grads           | -0.021459700632840396 |
| train_1/q_grads_std       | 0.7265960738062859    |
| train_1/q_loss            | 2.4046561611358594    |
| train_1/reward            | -1.3602973935405316   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0069091796875       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.5587106676899463    |
| train_1/target_q          | -16.909109961099652   |
-----------------------------------------------------
Saving periodic policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_60.pkl ...
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 61
Time for epoch 61: 464.38. Rollout time: 272.30, Training time: 192.03
Evaluating epoch 61
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 61                     |
| policy/steps              | 4221253.0              |
| test/episodes             | 1550.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -1.3957146337819273    |
| test_1/avg_q              | -26.774200975918085    |
| test_1/n_subgoals         | 17393.0                |
| test_1/subgoal_succ_rate  | 0.9984476513539929     |
| train/episodes            | 6200.0                 |
| train/success_rate        | 0.07                   |
| train_0/avg_q             | -13.482582807028786    |
| train_0/current_q         | -7.453574441318675     |
| train_0/fw_bonus          | -0.9994442984461784    |
| train_0/fw_loss           | 0.00013422812589851673 |
| train_0/mu_grads          | -0.18645981773734094   |
| train_0/mu_grads_std      | 0.779484574496746      |
| train_0/mu_loss           | 7.220576893164713      |
| train_0/next_q            | -7.0110705249438       |
| train_0/q_grads           | 0.034283877443522216   |
| train_0/q_grads_std       | 0.5177168816328048     |
| train_0/q_loss            | 0.4163455504383046     |
| train_0/reward            | -0.8596464611699048    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.14140625             |
| train_0/target_q          | -7.435501983489262     |
| train_1/avg_q             | -22.214572944688335    |
| train_1/current_q         | -16.91154359286845     |
| train_1/fw_bonus          | -0.9974619403481484    |
| train_1/fw_loss           | 0.0011049641514546237  |
| train_1/mu_grads          | 0.019959809258580208   |
| train_1/mu_grads_std      | 0.08689817041158676    |
| train_1/mu_loss           | 18.081340631331223     |
| train_1/n_subgoals        | 2611.0                 |
| train_1/next_q            | -18.072056898298456    |
| train_1/q_grads           | -0.022406319761648773  |
| train_1/q_grads_std       | 0.7317131027579308     |
| train_1/q_loss            | 2.2791914582645765     |
| train_1/reward            | -1.3975346808954783    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.008154296875         |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.5112983531214095     |
| train_1/target_q          | -17.130146704768958    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 62
Time for epoch 62: 467.89. Rollout time: 281.17, Training time: 186.66
Evaluating epoch 62
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 62                     |
| policy/steps              | 4285469.0              |
| test/episodes             | 1575.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -1.5857042482033785    |
| test_1/avg_q              | -26.062124358281356    |
| test_1/n_subgoals         | 7329.0                 |
| test_1/subgoal_succ_rate  | 0.9843089098103425     |
| train/episodes            | 6300.0                 |
| train/success_rate        | 0.03                   |
| train_0/avg_q             | -13.592762718124726    |
| train_0/current_q         | -7.3585143629952565    |
| train_0/fw_bonus          | -0.9994961977005005    |
| train_0/fw_loss           | 0.00012213453592266888 |
| train_0/mu_grads          | -0.1893409460783005    |
| train_0/mu_grads_std      | 0.7849438533186912     |
| train_0/mu_loss           | 7.13087017722965       |
| train_0/next_q            | -6.92367608980787      |
| train_0/q_grads           | 0.034549844823777674   |
| train_0/q_grads_std       | 0.5181098401546478     |
| train_0/q_loss            | 0.41936822043535515    |
| train_0/reward            | -0.8596480016407441    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.1552734375           |
| train_0/target_q          | -7.349626893896243     |
| train_1/avg_q             | -21.750904985712193    |
| train_1/current_q         | -16.817336256061033    |
| train_1/fw_bonus          | -0.9988124430179596    |
| train_1/fw_loss           | 0.0007803863685694523  |
| train_1/mu_grads          | 0.019959809258580208   |
| train_1/mu_grads_std      | 0.08689817041158676    |
| train_1/mu_loss           | 17.896912861297476     |
| train_1/n_subgoals        | 2657.0                 |
| train_1/next_q            | -17.892489398713682    |
| train_1/q_grads           | -0.022965563414618372  |
| train_1/q_grads_std       | 0.7368969649076462     |
| train_1/q_loss            | 2.5055109962632613     |
| train_1/reward            | -1.3978803413359855    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.00693359375          |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.49115543846443355    |
| train_1/target_q          | -17.05229130208187     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 63
Time for epoch 63: 479.50. Rollout time: 287.24, Training time: 192.20
Evaluating epoch 63
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 63                     |
| policy/steps              | 4349958.0              |
| test/episodes             | 1600.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -1.788103615805234     |
| test_1/avg_q              | -25.46635006927913     |
| test_1/n_subgoals         | 9638.0                 |
| test_1/subgoal_succ_rate  | 0.9881718198796431     |
| train/episodes            | 6400.0                 |
| train/success_rate        | 0.1                    |
| train_0/avg_q             | -13.51543099356784     |
| train_0/current_q         | -8.268004148045609     |
| train_0/fw_bonus          | -0.9995340451598167    |
| train_0/fw_loss           | 0.00011331901150697376 |
| train_0/mu_grads          | -0.19000601321458815   |
| train_0/mu_grads_std      | 0.7901895597577095     |
| train_0/mu_loss           | 8.082632591407005      |
| train_0/next_q            | -7.885472056576558     |
| train_0/q_grads           | 0.03451471002772451    |
| train_0/q_grads_std       | 0.5209446653723717     |
| train_0/q_loss            | 0.4760420774445925     |
| train_0/reward            | -0.8598500265114126    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.1665283203125        |
| train_0/target_q          | -8.27626516853592      |
| train_1/avg_q             | -21.14594123291042     |
| train_1/current_q         | -16.789553955019567    |
| train_1/fw_bonus          | -0.9986777558922768    |
| train_1/fw_loss           | 0.0008127570996293798  |
| train_1/mu_grads          | 0.019959809258580208   |
| train_1/mu_grads_std      | 0.08689817041158676    |
| train_1/mu_loss           | 17.966408066670702     |
| train_1/n_subgoals        | 2619.0                 |
| train_1/next_q            | -17.934856254577973    |
| train_1/q_grads           | -0.02338212006725371   |
| train_1/q_grads_std       | 0.7412179857492447     |
| train_1/q_loss            | 3.248629256908866      |
| train_1/reward            | -1.398691902848077     |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.008447265625         |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.48033600610920196    |
| train_1/target_q          | -17.01245723066926     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 64
Time for epoch 64: 488.35. Rollout time: 291.58, Training time: 196.71
Evaluating epoch 64
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 64                     |
| policy/steps              | 4412960.0              |
| test/episodes             | 1625.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -2.2348678272103117    |
| test_1/avg_q              | -26.637638274329483    |
| test_1/n_subgoals         | 7371.0                 |
| test_1/subgoal_succ_rate  | 0.9867046533713201     |
| train/episodes            | 6500.0                 |
| train/success_rate        | 0.07                   |
| train_0/avg_q             | -16.16581596582059     |
| train_0/current_q         | -7.374249642572619     |
| train_0/fw_bonus          | -0.9995114356279373    |
| train_0/fw_loss           | 0.00011858429825224448 |
| train_0/mu_grads          | -0.19086724668741226   |
| train_0/mu_grads_std      | 0.7954957142472268     |
| train_0/mu_loss           | 7.213449445621348      |
| train_0/next_q            | -7.001624381144443     |
| train_0/q_grads           | 0.03476607203483582    |
| train_0/q_grads_std       | 0.5246305003762245     |
| train_0/q_loss            | 0.6053301012568999     |
| train_0/reward            | -0.8584227109517087    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.161083984375         |
| train_0/target_q          | -7.357323316835062     |
| train_1/avg_q             | -21.696241264743367    |
| train_1/current_q         | -17.046702367192403    |
| train_1/fw_bonus          | -0.9987630546092987    |
| train_1/fw_loss           | 0.0007922536256955937  |
| train_1/mu_grads          | 0.019959809258580208   |
| train_1/mu_grads_std      | 0.08689817041158676    |
| train_1/mu_loss           | 18.2097238798096       |
| train_1/n_subgoals        | 2641.0                 |
| train_1/next_q            | -18.180128116972107    |
| train_1/q_grads           | -0.02445344510488212   |
| train_1/q_grads_std       | 0.7469616666436195     |
| train_1/q_loss            | 2.432351460680522      |
| train_1/reward            | -1.4216959842509822    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.00830078125          |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.508898144642181      |
| train_1/target_q          | -17.266677467549602    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 65
Time for epoch 65: 483.87. Rollout time: 291.11, Training time: 192.72
Evaluating epoch 65
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 65                     |
| policy/steps              | 4475583.0              |
| test/episodes             | 1650.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -1.5175067733289096    |
| test_1/avg_q              | -26.081531696775972    |
| test_1/n_subgoals         | 15014.0                |
| test_1/subgoal_succ_rate  | 0.9959371253496736     |
| train/episodes            | 6600.0                 |
| train/success_rate        | 0.12                   |
| train_0/avg_q             | -14.794020929154028    |
| train_0/current_q         | -7.988312048350423     |
| train_0/fw_bonus          | -0.9995334714651107    |
| train_0/fw_loss           | 0.00011344871581968619 |
| train_0/mu_grads          | -0.19325264878571033   |
| train_0/mu_grads_std      | 0.8004811182618141     |
| train_0/mu_loss           | 7.820304080625325      |
| train_0/next_q            | -7.6355684860990625    |
| train_0/q_grads           | 0.035339214373379946   |
| train_0/q_grads_std       | 0.5288532555103302     |
| train_0/q_loss            | 0.6114999082020851     |
| train_0/reward            | -0.8611070722843579    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.174658203125         |
| train_0/target_q          | -8.04427270624583      |
| train_1/avg_q             | -22.085872584017512    |
| train_1/current_q         | -17.022446702747015    |
| train_1/fw_bonus          | -0.9985512539744377    |
| train_1/fw_loss           | 0.0008431633526924997  |
| train_1/mu_grads          | 0.019959809258580208   |
| train_1/mu_grads_std      | 0.08689817041158676    |
| train_1/mu_loss           | 18.196269009136238     |
| train_1/n_subgoals        | 2548.0                 |
| train_1/next_q            | -18.152486038225724    |
| train_1/q_grads           | -0.025789788411930203  |
| train_1/q_grads_std       | 0.7517630696296692     |
| train_1/q_loss            | 3.0910203247935693     |
| train_1/reward            | -1.455720931274118     |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0082275390625        |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.5058869701726845     |
| train_1/target_q          | -17.22595636627603     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 66
Time for epoch 66: 466.94. Rollout time: 273.50, Training time: 193.39
Evaluating epoch 66
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 66                     |
| policy/steps              | 4535615.0              |
| test/episodes             | 1675.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -1.5120796241832175    |
| test_1/avg_q              | -26.400456149287656    |
| test_1/n_subgoals         | 18091.0                |
| test_1/subgoal_succ_rate  | 1.0                    |
| train/episodes            | 6700.0                 |
| train/success_rate        | 0.07                   |
| train_0/avg_q             | -16.18353223062698     |
| train_0/current_q         | -7.767295941514628     |
| train_0/fw_bonus          | -0.9995036542415618    |
| train_0/fw_loss           | 0.00012039898392686154 |
| train_0/mu_grads          | -0.19385790936648845   |
| train_0/mu_grads_std      | 0.8043102532625198     |
| train_0/mu_loss           | 7.5488987693521095     |
| train_0/next_q            | -7.374788129467804     |
| train_0/q_grads           | 0.03571813814342022    |
| train_0/q_grads_std       | 0.5352204605937004     |
| train_0/q_loss            | 0.542100399290003      |
| train_0/reward            | -0.8596395193184435    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.14375                |
| train_0/target_q          | -7.742627860088024     |
| train_1/avg_q             | -22.336727659910522    |
| train_1/current_q         | -17.012070269364244    |
| train_1/fw_bonus          | -0.9985735729336739    |
| train_1/fw_loss           | 0.0008377935431781225  |
| train_1/mu_grads          | 0.019959809258580208   |
| train_1/mu_grads_std      | 0.08689817041158676    |
| train_1/mu_loss           | 18.207620874717243     |
| train_1/n_subgoals        | 2579.0                 |
| train_1/next_q            | -18.166390763651112    |
| train_1/q_grads           | -0.026200541062280536  |
| train_1/q_grads_std       | 0.7569159150123597     |
| train_1/q_loss            | 3.0999329920115817     |
| train_1/reward            | -1.4497750717855524    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0073974609375        |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.5420705699883676     |
| train_1/target_q          | -17.23006416920201     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 67
Time for epoch 67: 402.74. Rollout time: 225.04, Training time: 177.67
Evaluating epoch 67
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 67                     |
| policy/steps              | 4591819.0              |
| test/episodes             | 1700.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -2.2006785797308908    |
| test_1/avg_q              | -26.085248367940956    |
| test_1/n_subgoals         | 17369.0                |
| test_1/subgoal_succ_rate  | 0.9984455063619091     |
| train/episodes            | 6800.0                 |
| train/success_rate        | 0.13                   |
| train_0/avg_q             | -13.291010960330873    |
| train_0/current_q         | -7.23482454899889      |
| train_0/fw_bonus          | -0.9994414925575257    |
| train_0/fw_loss           | 0.00013488048625731608 |
| train_0/mu_grads          | -0.19398928359150885   |
| train_0/mu_grads_std      | 0.8076394885778427     |
| train_0/mu_loss           | 7.0086512455271714     |
| train_0/next_q            | -6.81609829992004      |
| train_0/q_grads           | 0.035036251693964      |
| train_0/q_grads_std       | 0.5408478140830993     |
| train_0/q_loss            | 0.4838008024767163     |
| train_0/reward            | -0.8626235765354068    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.1181396484375        |
| train_0/target_q          | -7.211016500605227     |
| train_1/avg_q             | -22.60226653535004     |
| train_1/current_q         | -17.058361140568625    |
| train_1/fw_bonus          | -0.9983977496623992    |
| train_1/fw_loss           | 0.000880051615240518   |
| train_1/mu_grads          | 0.019959809258580208   |
| train_1/mu_grads_std      | 0.08689817041158676    |
| train_1/mu_loss           | 18.125411627734096     |
| train_1/n_subgoals        | 2530.0                 |
| train_1/next_q            | -18.093666857577645    |
| train_1/q_grads           | -0.026652897195890545  |
| train_1/q_grads_std       | 0.7613962516188622     |
| train_1/q_loss            | 3.0017622334371534     |
| train_1/reward            | -1.4169234860244615    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0080810546875        |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.5885375494071147     |
| train_1/target_q          | -17.24073416982447     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 68
Time for epoch 68: 406.06. Rollout time: 226.67, Training time: 179.36
Evaluating epoch 68
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 68                     |
| policy/steps              | 4648149.0              |
| test/episodes             | 1725.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -1.7054383722691675    |
| test_1/avg_q              | -24.95287131284434     |
| test_1/n_subgoals         | 15808.0                |
| test_1/subgoal_succ_rate  | 0.9944331983805668     |
| train/episodes            | 6900.0                 |
| train/success_rate        | 0.11                   |
| train_0/avg_q             | -13.834529705135374    |
| train_0/current_q         | -7.36504337431403      |
| train_0/fw_bonus          | -0.9994143471121788    |
| train_0/fw_loss           | 0.00014120356408966472 |
| train_0/mu_grads          | -0.19455055445432662   |
| train_0/mu_grads_std      | 0.813480032980442      |
| train_0/mu_loss           | 7.169163796182351      |
| train_0/next_q            | -7.003740570218975     |
| train_0/q_grads           | 0.034871130529791114   |
| train_0/q_grads_std       | 0.5443075463175774     |
| train_0/q_loss            | 0.5350628931499628     |
| train_0/reward            | -0.8597657681755664    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.1378173828125        |
| train_0/target_q          | -7.364460497962861     |
| train_1/avg_q             | -23.070417283368943    |
| train_1/current_q         | -16.94756979559667     |
| train_1/fw_bonus          | -0.9983475416898727    |
| train_1/fw_loss           | 0.0008921184853534214  |
| train_1/mu_grads          | 0.019959809258580208   |
| train_1/mu_grads_std      | 0.08689817041158676    |
| train_1/mu_loss           | 17.96553769403976      |
| train_1/n_subgoals        | 2578.0                 |
| train_1/next_q            | -17.945368995880095    |
| train_1/q_grads           | -0.02862214855849743   |
| train_1/q_grads_std       | 0.7651950344443321     |
| train_1/q_loss            | 3.1627718402120957     |
| train_1/reward            | -1.3898353945485724    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.007958984375         |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.5806826997672614     |
| train_1/target_q          | -17.117135848914295    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 69
Time for epoch 69: 416.78. Rollout time: 236.34, Training time: 180.40
Evaluating epoch 69
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 69                     |
| policy/steps              | 4706266.0              |
| test/episodes             | 1750.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -1.6935487893482093    |
| test_1/avg_q              | -23.276088491283556    |
| test_1/n_subgoals         | 14595.0                |
| test_1/subgoal_succ_rate  | 0.9907502569373073     |
| train/episodes            | 7000.0                 |
| train/success_rate        | 0.09                   |
| train_0/avg_q             | -14.936948908348551    |
| train_0/current_q         | -7.118551366953424     |
| train_0/fw_bonus          | -0.9994436517357826    |
| train_0/fw_loss           | 0.00013437673787848327 |
| train_0/mu_grads          | -0.19556759521365166   |
| train_0/mu_grads_std      | 0.8181321635842324     |
| train_0/mu_loss           | 6.904154761595082      |
| train_0/next_q            | -6.702223627967326     |
| train_0/q_grads           | 0.03454072820022702    |
| train_0/q_grads_std       | 0.5465032279491424     |
| train_0/q_loss            | 0.4561421793547652     |
| train_0/reward            | -0.8589381344849244    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.1664794921875        |
| train_0/target_q          | -7.102582951547009     |
| train_1/avg_q             | -22.274090001653324    |
| train_1/current_q         | -16.736826257305314    |
| train_1/fw_bonus          | -0.9983288303017617    |
| train_1/fw_loss           | 0.0008966155990492552  |
| train_1/mu_grads          | 0.019959809258580208   |
| train_1/mu_grads_std      | 0.08689817041158676    |
| train_1/mu_loss           | 17.722183822714214     |
| train_1/n_subgoals        | 2582.0                 |
| train_1/next_q            | -17.707354995651144    |
| train_1/q_grads           | -0.03010465423576534   |
| train_1/q_grads_std       | 0.7693894997239112     |
| train_1/q_loss            | 2.930568753784545      |
| train_1/reward            | -1.3948919559719797    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0072509765625        |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.5499612703330752     |
| train_1/target_q          | -16.900132835736297    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 70
Time for epoch 70: 424.44. Rollout time: 243.70, Training time: 180.70
Evaluating epoch 70
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 70                     |
| policy/steps              | 4766305.0              |
| test/episodes             | 1775.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -1.4184005672781546    |
| test_1/avg_q              | -21.940438774650584    |
| test_1/n_subgoals         | 15298.0                |
| test_1/subgoal_succ_rate  | 0.9929402536279253     |
| train/episodes            | 7100.0                 |
| train/success_rate        | 0.03                   |
| train_0/avg_q             | -15.34572703956213     |
| train_0/current_q         | -7.9582411826952155    |
| train_0/fw_bonus          | -0.9994074329733849    |
| train_0/fw_loss           | 0.00014281510302680545 |
| train_0/mu_grads          | -0.19798414707183837   |
| train_0/mu_grads_std      | 0.8218521803617478     |
| train_0/mu_loss           | 7.737984221401939      |
| train_0/next_q            | -7.5619076182422305    |
| train_0/q_grads           | 0.03484205035492778    |
| train_0/q_grads_std       | 0.54948311150074       |
| train_0/q_loss            | 0.4468130507647733     |
| train_0/reward            | -0.8587979538846412    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.1173095703125        |
| train_0/target_q          | -7.961807345835567     |
| train_1/avg_q             | -22.086811125430337    |
| train_1/current_q         | -16.537915929213767    |
| train_1/fw_bonus          | -0.998624150454998     |
| train_1/fw_loss           | 0.0008256413508206606  |
| train_1/mu_grads          | 0.019959809258580208   |
| train_1/mu_grads_std      | 0.08689817041158676    |
| train_1/mu_loss           | 17.60303272245476      |
| train_1/n_subgoals        | 2679.0                 |
| train_1/next_q            | -17.567537220909276    |
| train_1/q_grads           | -0.03169160159304738   |
| train_1/q_grads_std       | 0.775864465534687      |
| train_1/q_loss            | 2.728740445509107      |
| train_1/reward            | -1.372932677026256     |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.00888671875          |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.5625233296005973     |
| train_1/target_q          | -16.72800243082316     |
------------------------------------------------------
Saving periodic policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_70.pkl ...
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 71
Time for epoch 71: 423.99. Rollout time: 247.69, Training time: 176.26
Evaluating epoch 71
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 71                     |
| policy/steps              | 4827339.0              |
| test/episodes             | 1800.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -1.247854870790466     |
| test_1/avg_q              | -26.280055193945355    |
| test_1/n_subgoals         | 14262.0                |
| test_1/subgoal_succ_rate  | 0.9980367409900435     |
| train/episodes            | 7200.0                 |
| train/success_rate        | 0.04                   |
| train_0/avg_q             | -15.040852512594974    |
| train_0/current_q         | -7.403992506555045     |
| train_0/fw_bonus          | -0.9993922218680382    |
| train_0/fw_loss           | 0.00014635653587902198 |
| train_0/mu_grads          | -0.19949441440403462   |
| train_0/mu_grads_std      | 0.8248252466320991     |
| train_0/mu_loss           | 7.15779129119876       |
| train_0/next_q            | -7.061867389042391     |
| train_0/q_grads           | 0.03497522259131074    |
| train_0/q_grads_std       | 0.5520714491605758     |
| train_0/q_loss            | 0.7969193716986777     |
| train_0/reward            | -0.8586926973046503    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.13125                |
| train_0/target_q          | -7.4264243648240775    |
| train_1/avg_q             | -21.768816397185013    |
| train_1/current_q         | -16.381440740928504    |
| train_1/fw_bonus          | -0.9986144244670868    |
| train_1/fw_loss           | 0.0008279780318844132  |
| train_1/mu_grads          | 0.019959809258580208   |
| train_1/mu_grads_std      | 0.08689817041158676    |
| train_1/mu_loss           | 17.326893312591704     |
| train_1/n_subgoals        | 2668.0                 |
| train_1/next_q            | -17.287005193427767    |
| train_1/q_grads           | -0.03200410008430481   |
| train_1/q_grads_std       | 0.7816376179456711     |
| train_1/q_loss            | 2.998337514207904      |
| train_1/reward            | -1.3784698080962698    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.00673828125          |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.5427286356821589     |
| train_1/target_q          | -16.557333288845097    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 72
Time for epoch 72: 407.28. Rollout time: 232.71, Training time: 174.53
Evaluating epoch 72
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
-----------------------------------------------------
| epoch                     | 72                    |
| policy/steps              | 4885897.0             |
| test/episodes             | 1825.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -4.9355558176272245   |
| test_1/avg_q              | -26.11772375592382    |
| test_1/n_subgoals         | 14604.0               |
| test_1/subgoal_succ_rate  | 0.9907559572719803    |
| train/episodes            | 7300.0                |
| train/success_rate        | 0.12                  |
| train_0/avg_q             | -12.174319949948545   |
| train_0/current_q         | -7.141055487833131    |
| train_0/fw_bonus          | -0.9993874356150627   |
| train_0/fw_loss           | 0.0001474733424402075 |
| train_0/mu_grads          | -0.19926269091665744  |
| train_0/mu_grads_std      | 0.8272711008787155    |
| train_0/mu_loss           | 6.977717835830953     |
| train_0/next_q            | -6.7394611963762285   |
| train_0/q_grads           | 0.035318360570818184  |
| train_0/q_grads_std       | 0.5549577847123146    |
| train_0/q_loss            | 0.6171820647341939    |
| train_0/reward            | -0.859813347319141    |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.102783203125        |
| train_0/target_q          | -7.086478212302214    |
| train_1/avg_q             | -22.689732576826827   |
| train_1/current_q         | -16.45049914417268    |
| train_1/fw_bonus          | -0.9987365990877152   |
| train_1/fw_loss           | 0.0007986124503077008 |
| train_1/mu_grads          | 0.019959809258580208  |
| train_1/mu_grads_std      | 0.08689817041158676   |
| train_1/mu_loss           | 17.358149216133278    |
| train_1/n_subgoals        | 2561.0                |
| train_1/next_q            | -17.32477429504728    |
| train_1/q_grads           | -0.03347932379692793  |
| train_1/q_grads_std       | 0.7868865609169007    |
| train_1/q_loss            | 2.918719916689194     |
| train_1/reward            | -1.3603180098492884   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.00791015625         |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.5654041390082       |
| train_1/target_q          | -16.655002017764      |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 73
Time for epoch 73: 419.49. Rollout time: 240.93, Training time: 178.53
Evaluating epoch 73
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
-----------------------------------------------------
| epoch                     | 73                    |
| policy/steps              | 4947144.0             |
| test/episodes             | 1850.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -1.4949791275780295   |
| test_1/avg_q              | -25.853279931065373   |
| test_1/n_subgoals         | 1332.0                |
| test_1/subgoal_succ_rate  | 0.6426426426426426    |
| train/episodes            | 7400.0                |
| train/success_rate        | 0.04                  |
| train_0/avg_q             | -13.67196590981278    |
| train_0/current_q         | -7.0931040218257975   |
| train_0/fw_bonus          | -0.9994626402854919   |
| train_0/fw_loss           | 0.0001299546102018212 |
| train_0/mu_grads          | -0.20090026445686818  |
| train_0/mu_grads_std      | 0.8304276272654534    |
| train_0/mu_loss           | 6.866548078034482     |
| train_0/next_q            | -6.668293691603532    |
| train_0/q_grads           | 0.03553563579916954   |
| train_0/q_grads_std       | 0.5577641680836678    |
| train_0/q_loss            | 0.5153822307801577    |
| train_0/reward            | -0.8619871754744963   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.152490234375        |
| train_0/target_q          | -7.084003974367841    |
| train_1/avg_q             | -22.834484798679917   |
| train_1/current_q         | -16.4799839622951     |
| train_1/fw_bonus          | -0.9988773807883262   |
| train_1/fw_loss           | 0.0007647774953511543 |
| train_1/mu_grads          | 0.019959809258580208  |
| train_1/mu_grads_std      | 0.08689817041158676   |
| train_1/mu_loss           | 17.530843495968934    |
| train_1/n_subgoals        | 2661.0                |
| train_1/next_q            | -17.504443884829026   |
| train_1/q_grads           | -0.03388850623741746  |
| train_1/q_grads_std       | 0.7894561439752579    |
| train_1/q_loss            | 2.550616833036826     |
| train_1/reward            | -1.3619753137012593   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0075439453125       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.5422773393461104    |
| train_1/target_q          | -16.68561079690883    |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 74
Time for epoch 74: 426.31. Rollout time: 245.41, Training time: 180.86
Evaluating epoch 74
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
-----------------------------------------------------
| epoch                     | 74                    |
| policy/steps              | 5008641.0             |
| test/episodes             | 1875.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -1.41414494876758     |
| test_1/avg_q              | -26.789130699179815   |
| test_1/n_subgoals         | 13917.0               |
| test_1/subgoal_succ_rate  | 0.9883595602500539    |
| train/episodes            | 7500.0                |
| train/success_rate        | 0.06                  |
| train_0/avg_q             | -12.701921884579876   |
| train_0/current_q         | -6.685839739218515    |
| train_0/fw_bonus          | -0.9994631975889205   |
| train_0/fw_loss           | 0.0001298261640840792 |
| train_0/mu_grads          | -0.2018856957554817   |
| train_0/mu_grads_std      | 0.8321061283349991    |
| train_0/mu_loss           | 6.414901279457837     |
| train_0/next_q            | -6.213689592272377    |
| train_0/q_grads           | 0.03570114728063345   |
| train_0/q_grads_std       | 0.5581569954752922    |
| train_0/q_loss            | 0.40076230860604645   |
| train_0/reward            | -0.8651490723852475   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.156787109375        |
| train_0/target_q          | -6.677085217978361    |
| train_1/avg_q             | -22.283378822731184   |
| train_1/current_q         | -16.57948863305107    |
| train_1/fw_bonus          | -0.9988715842366218   |
| train_1/fw_loss           | 0.0007661722338525578 |
| train_1/mu_grads          | 0.019959809258580208  |
| train_1/mu_grads_std      | 0.08689817041158676   |
| train_1/mu_loss           | 17.58121541576829     |
| train_1/n_subgoals        | 2635.0                |
| train_1/next_q            | -17.552943755780127   |
| train_1/q_grads           | -0.03476565666496754  |
| train_1/q_grads_std       | 0.7915142744779586    |
| train_1/q_loss            | 2.0485159050992685    |
| train_1/reward            | -1.365207275042485    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.007568359375        |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.5332068311195446    |
| train_1/target_q          | -16.804038306300022   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 75
Time for epoch 75: 424.10. Rollout time: 247.50, Training time: 176.56
Evaluating epoch 75
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
-----------------------------------------------------
| epoch                     | 75                    |
| policy/steps              | 5072340.0             |
| test/episodes             | 1900.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -1.951592328237078    |
| test_1/avg_q              | -24.29665747891065    |
| test_1/n_subgoals         | 16579.0               |
| test_1/subgoal_succ_rate  | 0.9965619156764581    |
| train/episodes            | 7600.0                |
| train/success_rate        | 0.03                  |
| train_0/avg_q             | -11.988639616208951   |
| train_0/current_q         | -7.637384554306484    |
| train_0/fw_bonus          | -0.9994787156581879   |
| train_0/fw_loss           | 0.0001262088135263184 |
| train_0/mu_grads          | -0.2040989514440298   |
| train_0/mu_grads_std      | 0.834069450199604     |
| train_0/mu_loss           | 7.408003421531769     |
| train_0/next_q            | -7.197405056709475    |
| train_0/q_grads           | 0.03542721541598439   |
| train_0/q_grads_std       | 0.5586335733532906    |
| train_0/q_loss            | 0.4459944442972562    |
| train_0/reward            | -0.863972753687267    |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.1698974609375       |
| train_0/target_q          | -7.6327678408411      |
| train_1/avg_q             | -22.083328583092083   |
| train_1/current_q         | -16.983183700376365   |
| train_1/fw_bonus          | -0.9988798677921296   |
| train_1/fw_loss           | 0.0007641785137820989 |
| train_1/mu_grads          | 0.019959809258580208  |
| train_1/mu_grads_std      | 0.08689817041158676   |
| train_1/mu_loss           | 18.00146294681242     |
| train_1/n_subgoals        | 2672.0                |
| train_1/next_q            | -17.997639592030254   |
| train_1/q_grads           | -0.03620571531355381  |
| train_1/q_grads_std       | 0.7941230878233909    |
| train_1/q_loss            | 2.571615197095583     |
| train_1/reward            | -1.395851071663492    |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0082275390625       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.5014970059880239    |
| train_1/target_q          | -17.195871564383573   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 76
Time for epoch 76: 419.29. Rollout time: 243.76, Training time: 175.50
Evaluating epoch 76
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 76                     |
| policy/steps              | 5135372.0              |
| test/episodes             | 1925.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -1.431829022269751     |
| test_1/avg_q              | -26.67110141943569     |
| test_1/n_subgoals         | 9757.0                 |
| test_1/subgoal_succ_rate  | 0.9967203033719381     |
| train/episodes            | 7700.0                 |
| train/success_rate        | 0.02                   |
| train_0/avg_q             | -13.777431271734947    |
| train_0/current_q         | -6.757264656337057     |
| train_0/fw_bonus          | -0.9995247945189476    |
| train_0/fw_loss           | 0.00011547354788490338 |
| train_0/mu_grads          | -0.20620994456112385   |
| train_0/mu_grads_std      | 0.8358538001775742     |
| train_0/mu_loss           | 6.49551755366528       |
| train_0/next_q            | -6.283336220544061     |
| train_0/q_grads           | 0.03617768110707402    |
| train_0/q_grads_std       | 0.5597323104739189     |
| train_0/q_loss            | 0.36548063802281766    |
| train_0/reward            | -0.8635563399730017    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.179931640625         |
| train_0/target_q          | -6.741024807317993     |
| train_1/avg_q             | -22.211557136095553    |
| train_1/current_q         | -16.71198924649266     |
| train_1/fw_bonus          | -0.9987820118665696    |
| train_1/fw_loss           | 0.0007877007607021369  |
| train_1/mu_grads          | 0.019959809258580208   |
| train_1/mu_grads_std      | 0.08689817041158676    |
| train_1/mu_loss           | 17.70826827373564      |
| train_1/n_subgoals        | 2674.0                 |
| train_1/next_q            | -17.668928077959787    |
| train_1/q_grads           | -0.03707066429778934   |
| train_1/q_grads_std       | 0.7969329789280891     |
| train_1/q_loss            | 2.492616713687383      |
| train_1/reward            | -1.425758190761553     |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.00693359375          |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.5190725504861631     |
| train_1/target_q          | -16.93976358523513     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 77
Time for epoch 77: 426.90. Rollout time: 246.43, Training time: 180.43
Evaluating epoch 77
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 77                     |
| policy/steps              | 5198554.0              |
| test/episodes             | 1950.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -1.0325212639721195    |
| test_1/avg_q              | -25.877395078046366    |
| test_1/n_subgoals         | 14420.0                |
| test_1/subgoal_succ_rate  | 0.9922330097087378     |
| train/episodes            | 7800.0                 |
| train/success_rate        | 0.06                   |
| train_0/avg_q             | -11.444822858053334    |
| train_0/current_q         | -8.236636516484637     |
| train_0/fw_bonus          | -0.999589116871357     |
| train_0/fw_loss           | 0.00010048815674963407 |
| train_0/mu_grads          | -0.2074997279793024    |
| train_0/mu_grads_std      | 0.838749623298645      |
| train_0/mu_loss           | 8.009556628928848      |
| train_0/next_q            | -7.809119383413344     |
| train_0/q_grads           | 0.03691104026511312    |
| train_0/q_grads_std       | 0.5622216612100601     |
| train_0/q_loss            | 0.3985321635010559     |
| train_0/reward            | -0.8659932860689878    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.2149169921875        |
| train_0/target_q          | -8.237611987769487     |
| train_1/avg_q             | -22.231406828867314    |
| train_1/current_q         | -16.844890114107326    |
| train_1/fw_bonus          | -0.9987563803792       |
| train_1/fw_loss           | 0.000793856632662937   |
| train_1/mu_grads          | 0.019959809258580208   |
| train_1/mu_grads_std      | 0.08689817041158676    |
| train_1/mu_loss           | 17.953792141070988     |
| train_1/n_subgoals        | 2641.0                 |
| train_1/next_q            | -17.90211185596086     |
| train_1/q_grads           | -0.037496038153767584  |
| train_1/q_grads_std       | 0.8010899409651756     |
| train_1/q_loss            | 2.1557836670185386     |
| train_1/reward            | -1.431397239207581     |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.00712890625          |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.4929950776221128     |
| train_1/target_q          | -17.074012688942453    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 78
Time for epoch 78: 449.72. Rollout time: 261.66, Training time: 188.03
Evaluating epoch 78
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 78                     |
| policy/steps              | 5263377.0              |
| test/episodes             | 1975.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -1.2781100307826936    |
| test_1/avg_q              | -25.43534606353731     |
| test_1/n_subgoals         | 16700.0                |
| test_1/subgoal_succ_rate  | 0.9967664670658682     |
| train/episodes            | 7900.0                 |
| train/success_rate        | 0.01                   |
| train_0/avg_q             | -16.475370891303182    |
| train_0/current_q         | -8.225468793740612     |
| train_0/fw_bonus          | -0.9995520606637001    |
| train_0/fw_loss           | 0.00010911952613241737 |
| train_0/mu_grads          | -0.20763151571154595   |
| train_0/mu_grads_std      | 0.8406803280115127     |
| train_0/mu_loss           | 8.019771683154772      |
| train_0/next_q            | -7.837179163508767     |
| train_0/q_grads           | 0.03766924608498812    |
| train_0/q_grads_std       | 0.5656960785388947     |
| train_0/q_loss            | 0.5354718513530298     |
| train_0/reward            | -0.8633868713965057    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.1866943359375        |
| train_0/target_q          | -8.284360320073281     |
| train_1/avg_q             | -21.42485068235406     |
| train_1/current_q         | -17.28181955104011     |
| train_1/fw_bonus          | -0.9987141460180282    |
| train_1/fw_loss           | 0.00080401166487718    |
| train_1/mu_grads          | 0.019959809258580208   |
| train_1/mu_grads_std      | 0.08689817041158676    |
| train_1/mu_loss           | 18.428471552023712     |
| train_1/n_subgoals        | 2684.0                 |
| train_1/next_q            | -18.39682682669774     |
| train_1/q_grads           | -0.0392934950068593    |
| train_1/q_grads_std       | 0.8052627176046372     |
| train_1/q_loss            | 2.42098267993626       |
| train_1/reward            | -1.4423380260093837    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.006689453125         |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.4690760059612519     |
| train_1/target_q          | -17.517914221094795    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 79
Time for epoch 79: 443.50. Rollout time: 258.66, Training time: 184.81
Evaluating epoch 79
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 79                     |
| policy/steps              | 5325938.0              |
| test/episodes             | 2000.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -1.9072177394598488    |
| test_1/avg_q              | -26.554024836365624    |
| test_1/n_subgoals         | 14838.0                |
| test_1/subgoal_succ_rate  | 0.9927213910230489     |
| train/episodes            | 8000.0                 |
| train/success_rate        | 0.06                   |
| train_0/avg_q             | -16.01953346638273     |
| train_0/current_q         | -7.1974810190812       |
| train_0/fw_bonus          | -0.9995730638504028    |
| train_0/fw_loss           | 0.00010422880041005556 |
| train_0/mu_grads          | -0.20846725143492223   |
| train_0/mu_grads_std      | 0.8438453942537307     |
| train_0/mu_loss           | 6.973473441719508      |
| train_0/next_q            | -6.757163192252946     |
| train_0/q_grads           | 0.03760245218873024    |
| train_0/q_grads_std       | 0.5675908535718918     |
| train_0/q_loss            | 0.44617743166626056    |
| train_0/reward            | -0.8617494181868096    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.165576171875         |
| train_0/target_q          | -7.207247877749372     |
| train_1/avg_q             | -21.48351864946244     |
| train_1/current_q         | -17.164398201662028    |
| train_1/fw_bonus          | -0.9986823916435241    |
| train_1/fw_loss           | 0.0008116399330901913  |
| train_1/mu_grads          | 0.019959809258580208   |
| train_1/mu_grads_std      | 0.08689817041158676    |
| train_1/mu_loss           | 18.176367685401335     |
| train_1/n_subgoals        | 2629.0                 |
| train_1/next_q            | -18.13002002541454     |
| train_1/q_grads           | -0.040601386595517396  |
| train_1/q_grads_std       | 0.8071012809872627     |
| train_1/q_loss            | 4.344492916538341      |
| train_1/reward            | -1.4826908094742977    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0074951171875        |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.49638645872955495    |
| train_1/target_q          | -17.36212621417796     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 80
Time for epoch 80: 499.26. Rollout time: 294.48, Training time: 204.73
Evaluating epoch 80
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
-----------------------------------------------------
| epoch                     | 80                    |
| policy/steps              | 5389391.0             |
| test/episodes             | 2025.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -1.3312938099432479   |
| test_1/avg_q              | -25.90559847838599    |
| test_1/n_subgoals         | 15260.0               |
| test_1/subgoal_succ_rate  | 0.9929226736566186    |
| train/episodes            | 8100.0                |
| train/success_rate        | 0.03                  |
| train_0/avg_q             | -12.529092316297737   |
| train_0/current_q         | -6.446248801020393    |
| train_0/fw_bonus          | -0.9996198356151581   |
| train_0/fw_loss           | 9.332860208814964e-05 |
| train_0/mu_grads          | -0.20926543772220613  |
| train_0/mu_grads_std      | 0.8475386694073677    |
| train_0/mu_loss           | 6.138292306527225     |
| train_0/next_q            | -5.92769966985424     |
| train_0/q_grads           | 0.03808507490903139   |
| train_0/q_grads_std       | 0.5668213829398155    |
| train_0/q_loss            | 0.3090067384617366    |
| train_0/reward            | -0.8611990810670249   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.1858154296875       |
| train_0/target_q          | -6.42266678845493     |
| train_1/avg_q             | -22.169431892402127   |
| train_1/current_q         | -17.11004778278846    |
| train_1/fw_bonus          | -0.9986839011311531   |
| train_1/fw_loss           | 0.0008112785624689422 |
| train_1/mu_grads          | 0.019959809258580208  |
| train_1/mu_grads_std      | 0.08689817041158676   |
| train_1/mu_loss           | 18.17934675354879     |
| train_1/n_subgoals        | 2651.0                |
| train_1/next_q            | -18.15423330340183    |
| train_1/q_grads           | -0.04146681949496269  |
| train_1/q_grads_std       | 0.809660616517067     |
| train_1/q_loss            | 2.1520333677809442    |
| train_1/reward            | -1.4584972249882413   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.006982421875        |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.49566201433421353   |
| train_1/target_q          | -17.334846731499795   |
-----------------------------------------------------
Saving periodic policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_80.pkl ...
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 81
Time for epoch 81: 806.78. Rollout time: 525.85, Training time: 280.79
Evaluating epoch 81
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 81                     |
| policy/steps              | 5451460.0              |
| test/episodes             | 2050.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -1.0017241923133755    |
| test_1/avg_q              | -26.316303967614676    |
| test_1/n_subgoals         | 13776.0                |
| test_1/subgoal_succ_rate  | 0.9878774680603949     |
| train/episodes            | 8200.0                 |
| train/success_rate        | 0.05                   |
| train_0/avg_q             | -12.957590243780098    |
| train_0/current_q         | -7.853137277868596     |
| train_0/fw_bonus          | -0.9995679140090943    |
| train_0/fw_loss           | 0.00010542576255829772 |
| train_0/mu_grads          | -0.2106317762285471    |
| train_0/mu_grads_std      | 0.8510242730379105     |
| train_0/mu_loss           | 7.615434528048011      |
| train_0/next_q            | -7.451425662807137     |
| train_0/q_grads           | 0.03872367264702916    |
| train_0/q_grads_std       | 0.567575852572918      |
| train_0/q_loss            | 0.4371869285619213     |
| train_0/reward            | -0.8611609117899206    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.20146484375          |
| train_0/target_q          | -7.917556867959169     |
| train_1/avg_q             | -21.762910716411355    |
| train_1/current_q         | -17.09286114629025     |
| train_1/fw_bonus          | -0.9986601829528808    |
| train_1/fw_loss           | 0.0008169810680556111  |
| train_1/mu_grads          | 0.019959809258580208   |
| train_1/mu_grads_std      | 0.08689817041158676    |
| train_1/mu_loss           | 18.192981685851123     |
| train_1/n_subgoals        | 2629.0                 |
| train_1/next_q            | -18.156149941756244    |
| train_1/q_grads           | -0.04114779867231846   |
| train_1/q_grads_std       | 0.8127416372299194     |
| train_1/q_loss            | 2.4773139186294637     |
| train_1/reward            | -1.4538833111771965    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.007275390625         |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.5142639786991251     |
| train_1/target_q          | -17.310397765717333    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 82
Time for epoch 82: 841.85. Rollout time: 571.07, Training time: 270.64
Evaluating epoch 82
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
-----------------------------------------------------
| epoch                     | 82                    |
| policy/steps              | 5513937.0             |
| test/episodes             | 2075.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -2.2034875839187325   |
| test_1/avg_q              | -26.506154310778133   |
| test_1/n_subgoals         | 1602.0                |
| test_1/subgoal_succ_rate  | 0.6254681647940075    |
| train/episodes            | 8300.0                |
| train/success_rate        | 0.06                  |
| train_0/avg_q             | -12.849338354588129   |
| train_0/current_q         | -6.7940454918594835   |
| train_0/fw_bonus          | -0.9995713993906975   |
| train_0/fw_loss           | 0.0001046141107508447 |
| train_0/mu_grads          | -0.21194057092070578  |
| train_0/mu_grads_std      | 0.8544220551848412    |
| train_0/mu_loss           | 6.65545849044535      |
| train_0/next_q            | -6.396703789195763    |
| train_0/q_grads           | 0.0392807288095355    |
| train_0/q_grads_std       | 0.5720908775925636    |
| train_0/q_loss            | 0.68787043948787      |
| train_0/reward            | -0.8587822827001219   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.188818359375        |
| train_0/target_q          | -6.780262514506053    |
| train_1/avg_q             | -22.20169559676107    |
| train_1/current_q         | -17.02255298072991    |
| train_1/fw_bonus          | -0.9988334819674491   |
| train_1/fw_loss           | 0.0007753333367872984 |
| train_1/mu_grads          | 0.019959809258580208  |
| train_1/mu_grads_std      | 0.08689817041158676   |
| train_1/mu_loss           | 18.13780568050076     |
| train_1/n_subgoals        | 2636.0                |
| train_1/next_q            | -18.101893810005528   |
| train_1/q_grads           | -0.042232010141015054 |
| train_1/q_grads_std       | 0.8166582003235817    |
| train_1/q_loss            | 2.552308412313722     |
| train_1/reward            | -1.4714748381942626   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0071044921875       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.4996206373292868    |
| train_1/target_q          | -17.214536209223372   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 83
Time for epoch 83: 812.22. Rollout time: 542.44, Training time: 269.65
Evaluating epoch 83
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 83                     |
| policy/steps              | 5577365.0              |
| test/episodes             | 2100.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -0.9327808531444955    |
| test_1/avg_q              | -26.64559420249966     |
| test_1/n_subgoals         | 14890.0                |
| test_1/subgoal_succ_rate  | 0.9924110141034251     |
| train/episodes            | 8400.0                 |
| train/success_rate        | 0.06                   |
| train_0/avg_q             | -12.068136212126095    |
| train_0/current_q         | -6.808683311565858     |
| train_0/fw_bonus          | -0.9995164558291435    |
| train_0/fw_loss           | 0.00011741105736291501 |
| train_0/mu_grads          | -0.21090719290077686   |
| train_0/mu_grads_std      | 0.8578518405556679     |
| train_0/mu_loss           | 6.570479128275565      |
| train_0/next_q            | -6.375973295603224     |
| train_0/q_grads           | 0.03942885957658291    |
| train_0/q_grads_std       | 0.5743425533175468     |
| train_0/q_loss            | 0.5204777446697381     |
| train_0/reward            | -0.8571312834457785    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.1765625              |
| train_0/target_q          | -6.80607288844072      |
| train_1/avg_q             | -22.21751891131639     |
| train_1/current_q         | -16.906292567857328    |
| train_1/fw_bonus          | -0.998760299384594     |
| train_1/fw_loss           | 0.0007929186715045944  |
| train_1/mu_grads          | 0.019959809258580208   |
| train_1/mu_grads_std      | 0.08689817041158676    |
| train_1/mu_loss           | 17.9778986755856       |
| train_1/n_subgoals        | 2626.0                 |
| train_1/next_q            | -17.944747486807614    |
| train_1/q_grads           | -0.042480615060776473  |
| train_1/q_grads_std       | 0.8189461678266525     |
| train_1/q_loss            | 2.661091929991877      |
| train_1/reward            | -1.443862568996701     |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.007666015625         |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.5                    |
| train_1/target_q          | -17.107024101624255    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 84
Time for epoch 84: 855.83. Rollout time: 562.96, Training time: 292.75
Evaluating epoch 84
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 84                     |
| policy/steps              | 5640900.0              |
| test/episodes             | 2125.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -1.5958703640172127    |
| test_1/avg_q              | -26.35252840794196     |
| test_1/n_subgoals         | 16685.0                |
| test_1/subgoal_succ_rate  | 0.9967635600839077     |
| train/episodes            | 8500.0                 |
| train/success_rate        | 0.03                   |
| train_0/avg_q             | -12.878464019080008    |
| train_0/current_q         | -7.779881935205532     |
| train_0/fw_bonus          | -0.9995422974228859    |
| train_0/fw_loss           | 0.00011139371745230164 |
| train_0/mu_grads          | -0.21231011413037776   |
| train_0/mu_grads_std      | 0.8602378860116004     |
| train_0/mu_loss           | 7.498377095739679      |
| train_0/next_q            | -7.332875975598322     |
| train_0/q_grads           | 0.03999421810731292    |
| train_0/q_grads_std       | 0.5759262546896935     |
| train_0/q_loss            | 0.3670084861897309     |
| train_0/reward            | -0.8597809163053171    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.167626953125         |
| train_0/target_q          | -7.769141143542242     |
| train_1/avg_q             | -21.68037195468391     |
| train_1/current_q         | -16.845644314065293    |
| train_1/fw_bonus          | -0.99881102591753      |
| train_1/fw_loss           | 0.0007807251357007771  |
| train_1/mu_grads          | 0.019959809258580208   |
| train_1/mu_grads_std      | 0.08689817041158676    |
| train_1/mu_loss           | 17.943398616114553     |
| train_1/n_subgoals        | 2674.0                 |
| train_1/next_q            | -17.911729868591483    |
| train_1/q_grads           | -0.041586614493280646  |
| train_1/q_grads_std       | 0.8221075370907783     |
| train_1/q_loss            | 2.807776566375468      |
| train_1/reward            | -1.4381750464577636    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.007763671875         |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.4992520568436799     |
| train_1/target_q          | -17.037882527312657    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 85
Time for epoch 85: 812.08. Rollout time: 514.62, Training time: 297.30
Evaluating epoch 85
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 85                     |
| policy/steps              | 5701678.0              |
| test/episodes             | 2150.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -1.376414030784398     |
| test_1/avg_q              | -25.306271495089717    |
| test_1/n_subgoals         | 14692.0                |
| test_1/subgoal_succ_rate  | 0.9910835829022597     |
| train/episodes            | 8600.0                 |
| train/success_rate        | 0.02                   |
| train_0/avg_q             | -15.272805800571707    |
| train_0/current_q         | -7.919062641336905     |
| train_0/fw_bonus          | -0.9995508581399918    |
| train_0/fw_loss           | 0.00010940115080302349 |
| train_0/mu_grads          | -0.21366685815155506   |
| train_0/mu_grads_std      | 0.8624468132853508     |
| train_0/mu_loss           | 7.68138888804757       |
| train_0/next_q            | -7.499805080086827     |
| train_0/q_grads           | 0.04042008090764284    |
| train_0/q_grads_std       | 0.5808362871408462     |
| train_0/q_loss            | 0.3591507239614923     |
| train_0/reward            | -0.8590039426439035    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.175048828125         |
| train_0/target_q          | -7.919128846502092     |
| train_1/avg_q             | -22.433525492820113    |
| train_1/current_q         | -16.71400648166703     |
| train_1/fw_bonus          | -0.9988976031541824    |
| train_1/fw_loss           | 0.0007599176809890196  |
| train_1/mu_grads          | 0.019959809258580208   |
| train_1/mu_grads_std      | 0.08689817041158676    |
| train_1/mu_loss           | 17.82227632017466      |
| train_1/n_subgoals        | 2682.0                 |
| train_1/next_q            | -17.78665472069887     |
| train_1/q_grads           | -0.042525797802954915  |
| train_1/q_grads_std       | 0.8227414071559906     |
| train_1/q_loss            | 2.9594139687667664     |
| train_1/reward            | -1.4244365963699237    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0078125              |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.5495898583146905     |
| train_1/target_q          | -16.903407563392296    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 86
Time for epoch 86: 841.57. Rollout time: 551.27, Training time: 290.14
Evaluating epoch 86
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 86                     |
| policy/steps              | 5765603.0              |
| test/episodes             | 2175.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -1.7337430360869501    |
| test_1/avg_q              | -26.75466978634933     |
| test_1/n_subgoals         | 15353.0                |
| test_1/subgoal_succ_rate  | 0.993095811893441      |
| train/episodes            | 8700.0                 |
| train/success_rate        | 0.02                   |
| train_0/avg_q             | -14.8328267231657      |
| train_0/current_q         | -7.823826360271175     |
| train_0/fw_bonus          | -0.9995694130659103    |
| train_0/fw_loss           | 0.00010507812185096554 |
| train_0/mu_grads          | -0.21496185660362244   |
| train_0/mu_grads_std      | 0.866467273235321      |
| train_0/mu_loss           | 7.604337972956918      |
| train_0/next_q            | -7.4275120898529705    |
| train_0/q_grads           | 0.0400720676407218     |
| train_0/q_grads_std       | 0.5818181335926056     |
| train_0/q_loss            | 0.46467472393799564    |
| train_0/reward            | -0.8590864626843541    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.2037841796875        |
| train_0/target_q          | -7.8351958385465945    |
| train_1/avg_q             | -21.25266190984052     |
| train_1/current_q         | -16.986341438455106    |
| train_1/fw_bonus          | -0.9987504452466964    |
| train_1/fw_loss           | 0.0007952865591505543  |
| train_1/mu_grads          | 0.019959809258580208   |
| train_1/mu_grads_std      | 0.08689817041158676    |
| train_1/mu_loss           | 18.081900343964993     |
| train_1/n_subgoals        | 2677.0                 |
| train_1/next_q            | -18.033530731303983    |
| train_1/q_grads           | -0.04391051037237048   |
| train_1/q_grads_std       | 0.8266223385930062     |
| train_1/q_loss            | 2.645054415821246      |
| train_1/reward            | -1.434580299618392     |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0069091796875        |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.49346283152782966    |
| train_1/target_q          | -17.18493696746882     |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 87
Time for epoch 87: 764.47. Rollout time: 497.96, Training time: 266.30
Evaluating epoch 87
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
-----------------------------------------------------
| epoch                     | 87                    |
| policy/steps              | 5826149.0             |
| test/episodes             | 2200.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -1.6252608057953803   |
| test_1/avg_q              | -26.54080550581828    |
| test_1/n_subgoals         | 15164.0               |
| test_1/subgoal_succ_rate  | 0.9926140859931416    |
| train/episodes            | 8800.0                |
| train/success_rate        | 0.0                   |
| train_0/avg_q             | -15.10254678527926    |
| train_0/current_q         | -7.017410519335949    |
| train_0/fw_bonus          | -0.9995442301034927   |
| train_0/fw_loss           | 0.0001109448065108154 |
| train_0/mu_grads          | -0.21612298414111136  |
| train_0/mu_grads_std      | 0.8700529620051384    |
| train_0/mu_loss           | 6.766654918315224     |
| train_0/next_q            | -6.58805659815597     |
| train_0/q_grads           | 0.03997133010998368   |
| train_0/q_grads_std       | 0.5853596091270447    |
| train_0/q_loss            | 0.546314269602744     |
| train_0/reward            | -0.8623453563392104   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.144775390625        |
| train_0/target_q          | -7.013489885877041    |
| train_1/avg_q             | -22.540433232228025   |
| train_1/current_q         | -16.986828417871603   |
| train_1/fw_bonus          | -0.9991173893213272   |
| train_1/fw_loss           | 0.0007070985637255944 |
| train_1/mu_grads          | 0.01360953040421009   |
| train_1/mu_grads_std      | 0.09266714006662369   |
| train_1/mu_loss           | 18.029439302650616    |
| train_1/n_subgoals        | 2700.0                |
| train_1/next_q            | -17.967223933479282   |
| train_1/q_grads           | -0.04633527109399438  |
| train_1/q_grads_std       | 0.8270347982645034    |
| train_1/q_loss            | 2.650465115885719     |
| train_1/reward            | -1.3927593715998228   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.007861328125        |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.5688888888888889    |
| train_1/target_q          | -17.187133458362066   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 88
Time for epoch 88: 947.02. Rollout time: 622.49, Training time: 324.32
Evaluating epoch 88
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 88                     |
| policy/steps              | 5888278.0              |
| test/episodes             | 2225.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -2.047522054116578     |
| test_1/avg_q              | -26.914303649231464    |
| test_1/n_subgoals         | 18094.0                |
| test_1/subgoal_succ_rate  | 1.0                    |
| train/episodes            | 8900.0                 |
| train/success_rate        | 0.04                   |
| train_0/avg_q             | -14.214223853860673    |
| train_0/current_q         | -7.686623475605923     |
| train_0/fw_bonus          | -0.9995552510023117    |
| train_0/fw_loss           | 0.00010837632908078376 |
| train_0/mu_grads          | -0.21792076602578164   |
| train_0/mu_grads_std      | 0.8739256754517555     |
| train_0/mu_loss           | 7.455254481523452      |
| train_0/next_q            | -7.256144299236955     |
| train_0/q_grads           | 0.039587214775383475   |
| train_0/q_grads_std       | 0.5900158330798149     |
| train_0/q_loss            | 0.39029923834758257    |
| train_0/reward            | -0.8634983295378333    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.2086669921875        |
| train_0/target_q          | -7.687821632016903     |
| train_1/avg_q             | -21.393882645196413    |
| train_1/current_q         | -17.280791369326415    |
| train_1/fw_bonus          | -0.9994207456707954    |
| train_1/fw_loss           | 0.000634187956165988   |
| train_1/mu_grads          | 0.01360952015966177    |
| train_1/mu_grads_std      | 0.09266714006662369    |
| train_1/mu_loss           | 18.338792141355412     |
| train_1/n_subgoals        | 2630.0                 |
| train_1/next_q            | -18.31693478111971     |
| train_1/q_grads           | -0.04791438616812229   |
| train_1/q_grads_std       | 0.8311699002981185     |
| train_1/q_loss            | 3.3122808361009533     |
| train_1/reward            | -1.3951693548820914    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.0066162109375        |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.5201520912547528     |
| train_1/target_q          | -17.510795471479767    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 89
Time for epoch 89: 1142.05. Rollout time: 756.74, Training time: 385.17
Evaluating epoch 89
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
------------------------------------------------------
| epoch                     | 89                     |
| policy/steps              | 5949998.0              |
| test/episodes             | 2250.0                 |
| test/success_rate         | 0.0                    |
| test_0/avg_q              | -1.788497282251486     |
| test_1/avg_q              | -26.96790289919924     |
| test_1/n_subgoals         | 16691.0                |
| test_1/subgoal_succ_rate  | 0.9967647235036846     |
| train/episodes            | 9000.0                 |
| train/success_rate        | 0.01                   |
| train_0/avg_q             | -16.236739314621854    |
| train_0/current_q         | -7.7267734001025925    |
| train_0/fw_bonus          | -0.9995911598205567    |
| train_0/fw_loss           | 0.00010000978600146481 |
| train_0/mu_grads          | -0.22002167254686356   |
| train_0/mu_grads_std      | 0.878253872692585      |
| train_0/mu_loss           | 7.4839948994887155     |
| train_0/next_q            | -7.289633800539228     |
| train_0/q_grads           | 0.039595719520002605   |
| train_0/q_grads_std       | 0.593857952952385      |
| train_0/q_loss            | 0.35695762674215475    |
| train_0/reward            | -0.8661652111091825    |
| train_0/reward_-0.0_frac  | 0.0                    |
| train_0/reward_-1.0_frac  | 0.2042724609375        |
| train_0/target_q          | -7.739172345553277     |
| train_1/avg_q             | -21.995793496640985    |
| train_1/current_q         | -17.056238970732437    |
| train_1/fw_bonus          | -0.9991734489798546    |
| train_1/fw_loss           | 0.0006936219040653668  |
| train_1/mu_grads          | 0.013609290914610028   |
| train_1/mu_grads_std      | 0.09266717862337828    |
| train_1/mu_loss           | 18.082502419100326     |
| train_1/n_subgoals        | 2684.0                 |
| train_1/next_q            | -18.03623574496954     |
| train_1/q_grads           | -0.04874068489298224   |
| train_1/q_grads_std       | 0.8343712195754052     |
| train_1/q_loss            | 1.944869012578188      |
| train_1/reward            | -1.3842669004981871    |
| train_1/reward_-0.0_frac  | 0.0                    |
| train_1/reward_-1.0_frac  | 0.006591796875         |
| train_1/reward_-27.0_frac | 0.0                    |
| train_1/subgoal_succ_rate | 0.5409836065573771     |
| train_1/target_q          | -17.277302073014972    |
------------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 90
Time for epoch 90: 982.80. Rollout time: 625.22, Training time: 357.43
Evaluating epoch 90
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
-----------------------------------------------------
| epoch                     | 90                    |
| policy/steps              | 6010389.0             |
| test/episodes             | 2275.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -1.872840823693468    |
| test_1/avg_q              | -26.96121123978245    |
| test_1/n_subgoals         | 16440.0               |
| test_1/subgoal_succ_rate  | 0.9961070559610705    |
| train/episodes            | 9100.0                |
| train/success_rate        | 0.07                  |
| train_0/avg_q             | -14.365126830957932   |
| train_0/current_q         | -8.143957944989868    |
| train_0/fw_bonus          | -0.999612084031105    |
| train_0/fw_loss           | 9.513737986708293e-05 |
| train_0/mu_grads          | -0.22406722903251647  |
| train_0/mu_grads_std      | 0.8809037491679191    |
| train_0/mu_loss           | 7.940147403347083     |
| train_0/next_q            | -7.749190321867566    |
| train_0/q_grads           | 0.03986355690285563   |
| train_0/q_grads_std       | 0.5982508033514022    |
| train_0/q_loss            | 0.40103946920174716   |
| train_0/reward            | -0.8652479338539706   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.2302978515625       |
| train_0/target_q          | -8.1833328687779      |
| train_1/avg_q             | -22.058862269708236   |
| train_1/current_q         | -17.03385147369668    |
| train_1/fw_bonus          | -0.9993660390377045   |
| train_1/fw_loss           | 0.0006473317262134515 |
| train_1/mu_grads          | 0.01360000935383141   |
| train_1/mu_grads_std      | 0.0926648473367095    |
| train_1/mu_loss           | 18.04388696736799     |
| train_1/n_subgoals        | 2634.0                |
| train_1/next_q            | -18.00613134447885    |
| train_1/q_grads           | -0.048654308449476956 |
| train_1/q_grads_std       | 0.839627879858017     |
| train_1/q_loss            | 2.2071180458032136    |
| train_1/reward            | -1.3885380066582003   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.007177734375        |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.5539104024297646    |
| train_1/target_q          | -17.268905076878518   |
-----------------------------------------------------
Saving periodic policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_90.pkl ...
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 91
Time for epoch 91: 1086.83. Rollout time: 701.83, Training time: 384.83
Evaluating epoch 91
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
-----------------------------------------------------
| epoch                     | 91                    |
| policy/steps              | 6076344.0             |
| test/episodes             | 2300.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -2.147868073198399    |
| test_1/avg_q              | -26.894475865244235   |
| test_1/n_subgoals         | 15305.0               |
| test_1/subgoal_succ_rate  | 0.9928781443972557    |
| train/episodes            | 9200.0                |
| train/success_rate        | 0.04                  |
| train_0/avg_q             | -17.07971014480254    |
| train_0/current_q         | -7.746776121730518    |
| train_0/fw_bonus          | -0.9996518492698669   |
| train_0/fw_loss           | 8.587191750848433e-05 |
| train_0/mu_grads          | -0.2254844516515732   |
| train_0/mu_grads_std      | 0.8848764657974243    |
| train_0/mu_loss           | 7.520473550788178     |
| train_0/next_q            | -7.3084621446091775   |
| train_0/q_grads           | 0.03961294256150723   |
| train_0/q_grads_std       | 0.6010107859969139    |
| train_0/q_loss            | 0.42758896657045115   |
| train_0/reward            | -0.8684904750065471   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.2617431640625       |
| train_0/target_q          | -7.7160683446777      |
| train_1/avg_q             | -21.089714463871587   |
| train_1/current_q         | -16.957625035688327   |
| train_1/fw_bonus          | -0.999326515197754    |
| train_1/fw_loss           | 0.0006568335375050083 |
| train_1/mu_grads          | 0.013562084315344692  |
| train_1/mu_grads_std      | 0.09265376441180706   |
| train_1/mu_loss           | 18.093420072934492    |
| train_1/n_subgoals        | 2658.0                |
| train_1/next_q            | -18.058719509553576   |
| train_1/q_grads           | -0.05014747977256775  |
| train_1/q_grads_std       | 0.8461448341608048    |
| train_1/q_loss            | 2.176188901725059     |
| train_1/reward            | -1.3928883560940448   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0066162109375       |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.45861550037622273   |
| train_1/target_q          | -17.195065456630367   |
-----------------------------------------------------
New best value for test/success_rate: 0.0. Saving policy to data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100/policy_best.pkl ...
Mean of test/success_rate of last 4 epochs: 0.0
Training epoch 92
Time for epoch 92: 844.20. Rollout time: 566.82, Training time: 277.27
Evaluating epoch 92
Data_dir: data/eef7a77/CausalDependenciesMujocoEnv-o1-v0/alg:chac|eta:0.5|fw:1|fwhidsiz:256,256,256|nlev:2|reg:False|rolbatsiz:1|timsca:27,27|100
-----------------------------------------------------
| epoch                     | 92                    |
| policy/steps              | 6144643.0             |
| test/episodes             | 2325.0                |
| test/success_rate         | 0.0                   |
| test_0/avg_q              | -1.5020078377332717   |
| test_1/avg_q              | -26.90534581148338    |
| test_1/n_subgoals         | 16003.0               |
| test_1/subgoal_succ_rate  | 0.9949384490408049    |
| train/episodes            | 9300.0                |
| train/success_rate        | 0.02                  |
| train_0/avg_q             | -17.22710908095087    |
| train_0/current_q         | -8.293303395330135    |
| train_0/fw_bonus          | -0.9996603026986122   |
| train_0/fw_loss           | 8.390204584429739e-05 |
| train_0/mu_grads          | -0.22838362045586108  |
| train_0/mu_grads_std      | 0.888067401945591     |
| train_0/mu_loss           | 8.072458273930192     |
| train_0/next_q            | -7.8893868051113545   |
| train_0/q_grads           | 0.039619664661586285  |
| train_0/q_grads_std       | 0.6043998062610626    |
| train_0/q_loss            | 0.37326429382490356   |
| train_0/reward            | -0.8664439210828278   |
| train_0/reward_-0.0_frac  | 0.0                   |
| train_0/reward_-1.0_frac  | 0.20771484375         |
| train_0/target_q          | -8.313718131357453    |
| train_1/avg_q             | -20.539248371133585   |
| train_1/current_q         | -16.946932121815586   |
| train_1/fw_bonus          | -0.9992416456341744   |
| train_1/fw_loss           | 0.000677230310975574  |
| train_1/mu_grads          | 0.008546424261294306  |
| train_1/mu_grads_std      | 0.09740588590502738   |
| train_1/mu_loss           | 18.151154702282405    |
| train_1/n_subgoals        | 2661.0                |
| train_1/next_q            | -18.114674377241627   |
| train_1/q_grads           | -0.05065418863669038  |
| train_1/q_grads_std       | 0.8509424567222595    |
| train_1/q_loss            | 2.6029740867364106    |
| train_1/reward            | -1.4407341066769732   |
| train_1/reward_-0.0_frac  | 0.0                   |
| train_1/reward_-1.0_frac  | 0.0072265625          |
| train_1/reward_-27.0_frac | 0.0                   |
| train_1/subgoal_succ_rate | 0.4208944006012777    |
| train_1/target_q          | -17.158197732570294   |
-----------------------------------------------------
